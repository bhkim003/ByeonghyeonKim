{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17348/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    \n",
    "                    random_select_ratio = 4,\n",
    "                    leaky_temporal_filter= 1.0,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "\n",
    "                now_T = inputs.shape[1]\n",
    "                window_T = temporal_filter * TIME  # Ìïú Î∞ïÏä§(time window) Í∏∏Ïù¥\n",
    "\n",
    "                # 1) Î∞ïÏä§ Í∞úÏàò Í≥ÑÏÇ∞\n",
    "                num_windows = now_T // window_T   # Îî± Îñ®Ïñ¥ÏßÄÎäî Î∞ïÏä§ Í∞úÏàò\n",
    "\n",
    "                assert num_windows >= 1\n",
    "\n",
    "                # 2) ÎÇ®Îäî ÎÇòÎ®∏ÏßÄ timestepÏùÄ ÏûòÎùºÎ≤ÑÎ¶¨Í≥†, Îî± Î∞ïÏä§ Îã®ÏúÑÍπåÏßÄÎßå ÏÇ¨Ïö©\n",
    "                valid_T = num_windows * window_T\n",
    "                inputs = inputs[:, :valid_T]   # shape: [1, valid_T, C, H, W]\n",
    "\n",
    "                # 3) (B=1, num_windows, window_T, ...) ÌòïÌÉúÎ°ú reshape\n",
    "                B, T, *rest = inputs.shape  # B=1\n",
    "                inputs_reshaped = inputs.view(B, num_windows, window_T, *rest)\n",
    "                # inputs_reshaped: [1, num_windows, window_T, C, H, W] Í∞ÄÏ†ï\n",
    "\n",
    "                # 4) Í∞Å Î∞ïÏä§Î≥Ñ Ìï© (Ïä§ÌååÏù¥ÌÅ¨ Í∞úÏàò) Í≥ÑÏÇ∞\n",
    "                #    dim=(2,3,4,5): window_T, C, H, W Ï†ÑÏ≤¥ Ìï©\n",
    "                window_sums = inputs_reshaped.sum(dim=tuple(range(2, inputs_reshaped.dim())))  # shape: [1, num_windows]\n",
    "                window_sums = window_sums[0]  # [num_windows]\n",
    "\n",
    "                # 5) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ ÏÑ†ÌÉù\n",
    "                #    random_select_ratio ÎπÑÏú®ÎßåÌÅº ÏÉÅÏúÑ Î∞ïÏä§Îßå ÌõÑÎ≥¥Î°ú ÏîÄ\n",
    "                # print(f'num_windows: {num_windows}, random_select_ratio: {random_select_ratio}')\n",
    "                N = min(num_windows, round(random_select_ratio))\n",
    "                # N = max(1, min(num_windows, round(num_windows * random_select_ratio)))\n",
    "                topk_vals, topk_idx = torch.topk(window_sums, k=N)  # top-N Î∞ïÏä§ Ïù∏Îç±Ïä§\n",
    "                # print(f'N: {N}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 6) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ Ï§ëÏóêÏÑú ÎûúÎç§ÌïòÍ≤å ÌïòÎÇò ÏÑ†ÌÉù\n",
    "                #    (python random ÎòêÎäî torch.randperm Îëò Îã§ Í∞ÄÎä•)\n",
    "                rand_pos = random.randint(0, N - 1)\n",
    "                chosen_win_idx = topk_idx[rand_pos].item()  # 0 ~ num_windows-1 Ï§ë ÌïòÎÇò\n",
    "                # print(f'chosen_win_idx: {chosen_win_idx}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 7) ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í∑∏ Î∞ïÏä§ Íµ¨Í∞ÑÎßå ÏÇ¨Ïö©\n",
    "                start_idx = chosen_win_idx * window_T\n",
    "                end_idx = start_idx + window_T\n",
    "                inputs = inputs[:, start_idx:end_idx]\n",
    "                # shape: [1, window_T, C, H, W]\n",
    "\n",
    "                # print(f'inputs.shape after random select: {inputs.shape}')\n",
    "\n",
    "\n",
    "                if temporal_filter_accumulation == False:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # print(f'inputs.shape: {inputs.shape}')\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if labels.item() == 6:\n",
    "            #     # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            #     ##############################################################################################\n",
    "            #     dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            #     #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if temporal_filter_accumulation == False:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 random_select_ratio = 4,\n",
    "#                 leaky_temporal_filter= 0.0,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y5eru5fk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_144420-y5eru5fk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5eru5fk' target=\"_blank\">wise-sweep-41</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5eru5fk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5eru5fk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251114_144427_004', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "lif layer 1 self.abs_max_v: 1338.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "lif layer 2 self.abs_max_v: 1921.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "fc layer 1 self.abs_max_out: 1426.0\n",
      "lif layer 1 self.abs_max_v: 1854.0\n",
      "lif layer 2 self.abs_max_v: 2098.0\n",
      "lif layer 1 self.abs_max_v: 1877.0\n",
      "lif layer 2 self.abs_max_v: 2537.0\n",
      "fc layer 1 self.abs_max_out: 1448.0\n",
      "lif layer 1 self.abs_max_v: 2178.5\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "lif layer 2 self.abs_max_v: 3256.0\n",
      "fc layer 1 self.abs_max_out: 1956.0\n",
      "lif layer 1 self.abs_max_v: 2190.5\n",
      "lif layer 2 self.abs_max_v: 3543.0\n",
      "fc layer 1 self.abs_max_out: 2462.0\n",
      "lif layer 1 self.abs_max_v: 2499.0\n",
      "fc layer 1 self.abs_max_out: 3110.0\n",
      "lif layer 1 self.abs_max_v: 3115.5\n",
      "lif layer 2 self.abs_max_v: 3572.0\n",
      "fc layer 3 self.abs_max_out: 807.0\n",
      "lif layer 1 self.abs_max_v: 3369.0\n",
      "fc layer 2 self.abs_max_out: 2388.0\n",
      "fc layer 3 self.abs_max_out: 960.0\n",
      "fc layer 1 self.abs_max_out: 3333.0\n",
      "lif layer 1 self.abs_max_v: 3733.0\n",
      "fc layer 3 self.abs_max_out: 1271.0\n",
      "lif layer 1 self.abs_max_v: 4213.5\n",
      "fc layer 1 self.abs_max_out: 3576.0\n",
      "lif layer 1 self.abs_max_v: 4930.5\n",
      "fc layer 1 self.abs_max_out: 3981.0\n",
      "lif layer 1 self.abs_max_v: 6255.5\n",
      "lif layer 1 self.abs_max_v: 6378.0\n",
      "fc layer 1 self.abs_max_out: 4177.0\n",
      "lif layer 1 self.abs_max_v: 7131.5\n",
      "lif layer 2 self.abs_max_v: 3619.5\n",
      "lif layer 2 self.abs_max_v: 4003.0\n",
      "fc layer 2 self.abs_max_out: 2566.0\n",
      "fc layer 1 self.abs_max_out: 4304.0\n",
      "fc layer 1 self.abs_max_out: 4450.0\n",
      "fc layer 1 self.abs_max_out: 4725.0\n",
      "fc layer 1 self.abs_max_out: 4843.0\n",
      "fc layer 1 self.abs_max_out: 5043.0\n",
      "fc layer 1 self.abs_max_out: 5289.0\n",
      "lif layer 1 self.abs_max_v: 7132.0\n",
      "lif layer 2 self.abs_max_v: 4046.0\n",
      "lif layer 2 self.abs_max_v: 4563.0\n",
      "fc layer 2 self.abs_max_out: 2580.0\n",
      "fc layer 2 self.abs_max_out: 2884.0\n",
      "fc layer 2 self.abs_max_out: 2901.0\n",
      "fc layer 2 self.abs_max_out: 3062.0\n",
      "fc layer 2 self.abs_max_out: 3269.0\n",
      "lif layer 1 self.abs_max_v: 7201.0\n",
      "lif layer 1 self.abs_max_v: 7399.5\n",
      "lif layer 2 self.abs_max_v: 4564.5\n",
      "lif layer 1 self.abs_max_v: 7500.5\n",
      "lif layer 1 self.abs_max_v: 7593.0\n",
      "lif layer 1 self.abs_max_v: 7711.0\n",
      "fc layer 2 self.abs_max_out: 3382.0\n",
      "lif layer 1 self.abs_max_v: 8139.5\n",
      "lif layer 2 self.abs_max_v: 4586.0\n",
      "lif layer 2 self.abs_max_v: 4614.0\n",
      "lif layer 1 self.abs_max_v: 8477.0\n",
      "lif layer 1 self.abs_max_v: 8587.5\n",
      "lif layer 2 self.abs_max_v: 4752.0\n",
      "lif layer 2 self.abs_max_v: 4788.0\n",
      "lif layer 2 self.abs_max_v: 4907.0\n",
      "lif layer 2 self.abs_max_v: 4924.0\n",
      "lif layer 2 self.abs_max_v: 4965.5\n",
      "lif layer 2 self.abs_max_v: 5138.5\n",
      "lif layer 2 self.abs_max_v: 5360.5\n",
      "fc layer 1 self.abs_max_out: 5449.0\n",
      "lif layer 1 self.abs_max_v: 8742.5\n",
      "lif layer 1 self.abs_max_v: 8782.5\n",
      "lif layer 1 self.abs_max_v: 9147.5\n",
      "fc layer 1 self.abs_max_out: 5674.0\n",
      "lif layer 1 self.abs_max_v: 9320.0\n",
      "fc layer 3 self.abs_max_out: 1274.0\n",
      "lif layer 1 self.abs_max_v: 9793.5\n",
      "fc layer 1 self.abs_max_out: 5775.0\n",
      "fc layer 2 self.abs_max_out: 3428.0\n",
      "fc layer 2 self.abs_max_out: 3488.0\n",
      "lif layer 1 self.abs_max_v: 10533.0\n",
      "fc layer 1 self.abs_max_out: 6120.0\n",
      "lif layer 1 self.abs_max_v: 10916.0\n",
      "fc layer 3 self.abs_max_out: 1277.0\n",
      "lif layer 1 self.abs_max_v: 11199.0\n",
      "fc layer 3 self.abs_max_out: 1287.0\n",
      "fc layer 1 self.abs_max_out: 6715.0\n",
      "lif layer 1 self.abs_max_v: 11224.0\n",
      "lif layer 1 self.abs_max_v: 11704.0\n",
      "lif layer 1 self.abs_max_v: 11806.5\n",
      "fc layer 3 self.abs_max_out: 1305.0\n",
      "fc layer 3 self.abs_max_out: 1340.0\n",
      "fc layer 3 self.abs_max_out: 1536.0\n",
      "lif layer 1 self.abs_max_v: 12145.5\n",
      "fc layer 1 self.abs_max_out: 6819.0\n",
      "fc layer 1 self.abs_max_out: 6997.0\n",
      "lif layer 1 self.abs_max_v: 12241.0\n",
      "lif layer 1 self.abs_max_v: 12322.5\n",
      "lif layer 2 self.abs_max_v: 5367.0\n",
      "lif layer 2 self.abs_max_v: 5615.5\n",
      "lif layer 2 self.abs_max_v: 5692.5\n",
      "lif layer 1 self.abs_max_v: 12367.5\n",
      "fc layer 1 self.abs_max_out: 7489.0\n",
      "lif layer 1 self.abs_max_v: 13539.0\n",
      "lif layer 1 self.abs_max_v: 14244.5\n",
      "fc layer 1 self.abs_max_out: 7716.0\n",
      "lif layer 1 self.abs_max_v: 14353.0\n",
      "fc layer 1 self.abs_max_out: 8027.0\n",
      "lif layer 1 self.abs_max_v: 14420.5\n",
      "fc layer 3 self.abs_max_out: 1571.0\n",
      "fc layer 3 self.abs_max_out: 1586.0\n",
      "fc layer 1 self.abs_max_out: 8032.0\n",
      "fc layer 3 self.abs_max_out: 1782.0\n",
      "lif layer 1 self.abs_max_v: 14442.5\n",
      "fc layer 1 self.abs_max_out: 8084.0\n",
      "fc layer 1 self.abs_max_out: 8132.0\n",
      "lif layer 1 self.abs_max_v: 15251.5\n",
      "fc layer 3 self.abs_max_out: 1818.0\n",
      "fc layer 1 self.abs_max_out: 8473.0\n",
      "lif layer 1 self.abs_max_v: 15629.5\n",
      "fc layer 1 self.abs_max_out: 8633.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.759056/  2.007051, val:  32.08%, val_best:  32.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 83.31 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 81.7255%\n",
      "layer   2  Sparsity: 71.3183%\n",
      "layer   3  Sparsity: 63.7672%\n",
      "total_backward_count 9790 real_backward_count 1560  15.935%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 9195.0\n",
      "lif layer 1 self.abs_max_v: 16343.5\n",
      "fc layer 3 self.abs_max_out: 1863.0\n",
      "fc layer 3 self.abs_max_out: 1908.0\n",
      "lif layer 1 self.abs_max_v: 16755.5\n",
      "fc layer 3 self.abs_max_out: 1965.0\n",
      "fc layer 3 self.abs_max_out: 2087.0\n",
      "fc layer 3 self.abs_max_out: 2094.0\n",
      "lif layer 1 self.abs_max_v: 16986.5\n",
      "lif layer 1 self.abs_max_v: 17296.0\n",
      "fc layer 2 self.abs_max_out: 3648.0\n",
      "fc layer 1 self.abs_max_out: 9469.0\n",
      "fc layer 1 self.abs_max_out: 10007.0\n",
      "fc layer 1 self.abs_max_out: 10167.0\n",
      "lif layer 1 self.abs_max_v: 17739.5\n",
      "lif layer 1 self.abs_max_v: 18122.5\n",
      "lif layer 1 self.abs_max_v: 18146.5\n",
      "lif layer 1 self.abs_max_v: 18708.5\n",
      "lif layer 1 self.abs_max_v: 18958.5\n",
      "lif layer 1 self.abs_max_v: 19077.5\n",
      "lif layer 1 self.abs_max_v: 19411.0\n",
      "lif layer 2 self.abs_max_v: 5744.5\n",
      "lif layer 2 self.abs_max_v: 5906.5\n",
      "fc layer 1 self.abs_max_out: 10489.0\n",
      "fc layer 1 self.abs_max_out: 11233.0\n",
      "lif layer 1 self.abs_max_v: 20026.5\n",
      "lif layer 1 self.abs_max_v: 20031.5\n",
      "lif layer 1 self.abs_max_v: 20189.0\n",
      "fc layer 1 self.abs_max_out: 11513.0\n",
      "lif layer 1 self.abs_max_v: 20676.0\n",
      "lif layer 1 self.abs_max_v: 20858.0\n",
      "fc layer 1 self.abs_max_out: 11963.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.646943/  1.921805, val:  42.08%, val_best:  42.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 81.7652%\n",
      "layer   2  Sparsity: 76.2646%\n",
      "layer   3  Sparsity: 66.4658%\n",
      "total_backward_count 19580 real_backward_count 2988  15.260%\n",
      "lif layer 1 self.abs_max_v: 21245.0\n",
      "lif layer 1 self.abs_max_v: 21319.5\n",
      "lif layer 2 self.abs_max_v: 6082.5\n",
      "lif layer 1 self.abs_max_v: 21563.0\n",
      "lif layer 2 self.abs_max_v: 6286.5\n",
      "lif layer 2 self.abs_max_v: 6396.5\n",
      "lif layer 1 self.abs_max_v: 21717.5\n",
      "lif layer 1 self.abs_max_v: 22132.0\n",
      "fc layer 2 self.abs_max_out: 3649.0\n",
      "fc layer 2 self.abs_max_out: 3718.0\n",
      "fc layer 1 self.abs_max_out: 11986.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.601642/  1.849153, val:  46.25%, val_best:  46.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 88.28 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 81.7715%\n",
      "layer   2  Sparsity: 76.2107%\n",
      "layer   3  Sparsity: 65.0334%\n",
      "total_backward_count 29370 real_backward_count 4327  14.733%\n",
      "fc layer 3 self.abs_max_out: 2198.0\n",
      "fc layer 1 self.abs_max_out: 12164.0\n",
      "fc layer 1 self.abs_max_out: 12211.0\n",
      "lif layer 2 self.abs_max_v: 6506.0\n",
      "lif layer 2 self.abs_max_v: 6651.5\n",
      "lif layer 1 self.abs_max_v: 22138.0\n",
      "lif layer 1 self.abs_max_v: 22496.5\n",
      "fc layer 2 self.abs_max_out: 3795.0\n",
      "fc layer 2 self.abs_max_out: 3917.0\n",
      "lif layer 1 self.abs_max_v: 22790.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.564427/  1.885453, val:  33.33%, val_best:  46.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 81.6832%\n",
      "layer   2  Sparsity: 75.8315%\n",
      "layer   3  Sparsity: 65.0546%\n",
      "total_backward_count 39160 real_backward_count 5656  14.443%\n",
      "lif layer 2 self.abs_max_v: 7055.5\n",
      "fc layer 1 self.abs_max_out: 12869.0\n",
      "lif layer 1 self.abs_max_v: 23257.0\n",
      "lif layer 1 self.abs_max_v: 23271.5\n",
      "lif layer 1 self.abs_max_v: 23520.0\n",
      "fc layer 1 self.abs_max_out: 12874.0\n",
      "fc layer 1 self.abs_max_out: 13212.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.565935/  1.909295, val:  38.33%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 93.91 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 81.7117%\n",
      "layer   2  Sparsity: 77.8258%\n",
      "layer   3  Sparsity: 66.3471%\n",
      "total_backward_count 48950 real_backward_count 6913  14.123%\n",
      "lif layer 1 self.abs_max_v: 24570.5\n",
      "fc layer 1 self.abs_max_out: 13631.0\n",
      "lif layer 1 self.abs_max_v: 25201.0\n",
      "fc layer 1 self.abs_max_out: 13654.0\n",
      "lif layer 1 self.abs_max_v: 25619.0\n",
      "lif layer 1 self.abs_max_v: 25694.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.579012/  1.894465, val:  35.83%, val_best:  46.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 95.76 seconds, 1.60 minutes\n",
      "layer   1  Sparsity: 81.7110%\n",
      "layer   2  Sparsity: 78.3318%\n",
      "layer   3  Sparsity: 67.2484%\n",
      "total_backward_count 58740 real_backward_count 8178  13.922%\n",
      "fc layer 1 self.abs_max_out: 15129.0\n",
      "lif layer 1 self.abs_max_v: 27425.0\n",
      "lif layer 2 self.abs_max_v: 7057.0\n",
      "lif layer 2 self.abs_max_v: 7097.5\n",
      "fc layer 3 self.abs_max_out: 2330.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.563310/  1.863819, val:  47.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 96.94 seconds, 1.62 minutes\n",
      "layer   1  Sparsity: 81.7004%\n",
      "layer   2  Sparsity: 79.4044%\n",
      "layer   3  Sparsity: 66.5551%\n",
      "total_backward_count 68530 real_backward_count 9424  13.752%\n",
      "fc layer 2 self.abs_max_out: 4050.0\n",
      "fc layer 3 self.abs_max_out: 2489.0\n",
      "lif layer 1 self.abs_max_v: 28135.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.550724/  1.801158, val:  43.33%, val_best:  47.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 96.73 seconds, 1.61 minutes\n",
      "layer   1  Sparsity: 81.7211%\n",
      "layer   2  Sparsity: 77.4406%\n",
      "layer   3  Sparsity: 68.2128%\n",
      "total_backward_count 78320 real_backward_count 10670  13.624%\n",
      "lif layer 1 self.abs_max_v: 28340.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.549971/  1.827099, val:  42.08%, val_best:  47.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 99.28 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.7034%\n",
      "layer   2  Sparsity: 77.3810%\n",
      "layer   3  Sparsity: 68.9249%\n",
      "total_backward_count 88110 real_backward_count 12002  13.622%\n",
      "lif layer 2 self.abs_max_v: 7121.0\n",
      "lif layer 2 self.abs_max_v: 7318.5\n",
      "lif layer 1 self.abs_max_v: 28388.0\n",
      "lif layer 1 self.abs_max_v: 28406.5\n",
      "lif layer 1 self.abs_max_v: 29264.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.549022/  1.848267, val:  48.33%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 99.93 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 81.7226%\n",
      "layer   2  Sparsity: 77.2370%\n",
      "layer   3  Sparsity: 69.3740%\n",
      "total_backward_count 97900 real_backward_count 13267  13.552%\n",
      "lif layer 2 self.abs_max_v: 7448.5\n",
      "lif layer 2 self.abs_max_v: 7491.5\n",
      "fc layer 2 self.abs_max_out: 4125.0\n",
      "lif layer 1 self.abs_max_v: 29347.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.556580/  1.879459, val:  31.25%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 98.63 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.6927%\n",
      "layer   2  Sparsity: 77.1229%\n",
      "layer   3  Sparsity: 68.6203%\n",
      "total_backward_count 107690 real_backward_count 14495  13.460%\n",
      "lif layer 2 self.abs_max_v: 7711.0\n",
      "lif layer 2 self.abs_max_v: 7793.0\n",
      "fc layer 2 self.abs_max_out: 4325.0\n",
      "lif layer 2 self.abs_max_v: 7976.5\n",
      "lif layer 2 self.abs_max_v: 8173.5\n",
      "lif layer 2 self.abs_max_v: 8213.0\n",
      "fc layer 2 self.abs_max_out: 4457.0\n",
      "lif layer 2 self.abs_max_v: 8330.0\n",
      "lif layer 2 self.abs_max_v: 8419.0\n",
      "lif layer 1 self.abs_max_v: 29372.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.565978/  1.915883, val:  43.75%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 98.75 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.6955%\n",
      "layer   2  Sparsity: 77.1187%\n",
      "layer   3  Sparsity: 69.8799%\n",
      "total_backward_count 117480 real_backward_count 15703  13.367%\n",
      "fc layer 1 self.abs_max_out: 15157.0\n",
      "lif layer 1 self.abs_max_v: 29466.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.567983/  1.849086, val:  39.17%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 97.97 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 81.6744%\n",
      "layer   2  Sparsity: 77.7995%\n",
      "layer   3  Sparsity: 68.7062%\n",
      "total_backward_count 127270 real_backward_count 16897  13.276%\n",
      "fc layer 1 self.abs_max_out: 15183.0\n",
      "fc layer 1 self.abs_max_out: 15205.0\n",
      "lif layer 1 self.abs_max_v: 29563.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.547061/  1.839709, val:  37.50%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 98.90 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.7461%\n",
      "layer   2  Sparsity: 77.2741%\n",
      "layer   3  Sparsity: 67.8137%\n",
      "total_backward_count 137060 real_backward_count 18120  13.220%\n",
      "fc layer 1 self.abs_max_out: 15316.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.531908/  1.812868, val:  49.17%, val_best:  49.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 96.43 seconds, 1.61 minutes\n",
      "layer   1  Sparsity: 81.6604%\n",
      "layer   2  Sparsity: 77.1194%\n",
      "layer   3  Sparsity: 67.9480%\n",
      "total_backward_count 146850 real_backward_count 19276  13.126%\n",
      "fc layer 1 self.abs_max_out: 15606.0\n",
      "fc layer 1 self.abs_max_out: 16076.0\n",
      "lif layer 1 self.abs_max_v: 29712.0\n",
      "fc layer 1 self.abs_max_out: 16268.0\n",
      "lif layer 1 self.abs_max_v: 29874.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.543148/  1.798427, val:  44.17%, val_best:  49.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 96.41 seconds, 1.61 minutes\n",
      "layer   1  Sparsity: 81.6937%\n",
      "layer   2  Sparsity: 76.7780%\n",
      "layer   3  Sparsity: 68.6215%\n",
      "total_backward_count 156640 real_backward_count 20504  13.090%\n",
      "fc layer 1 self.abs_max_out: 16555.0\n",
      "fc layer 1 self.abs_max_out: 16677.0\n",
      "lif layer 1 self.abs_max_v: 31147.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.559888/  1.831298, val:  52.50%, val_best:  52.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 98.51 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.7330%\n",
      "layer   2  Sparsity: 76.1606%\n",
      "layer   3  Sparsity: 70.6243%\n",
      "total_backward_count 166430 real_backward_count 21714  13.047%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.570100/  1.793959, val:  48.75%, val_best:  52.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 99.05 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.6675%\n",
      "layer   2  Sparsity: 75.6715%\n",
      "layer   3  Sparsity: 70.6701%\n",
      "total_backward_count 176220 real_backward_count 22929  13.012%\n",
      "fc layer 2 self.abs_max_out: 4517.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.523012/  1.815411, val:  52.08%, val_best:  52.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 97.57 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 81.6838%\n",
      "layer   2  Sparsity: 76.2320%\n",
      "layer   3  Sparsity: 68.4661%\n",
      "total_backward_count 186010 real_backward_count 24196  13.008%\n",
      "fc layer 1 self.abs_max_out: 16683.0\n",
      "fc layer 2 self.abs_max_out: 4599.0\n",
      "lif layer 2 self.abs_max_v: 8582.0\n",
      "fc layer 2 self.abs_max_out: 4618.0\n",
      "fc layer 2 self.abs_max_out: 5181.0\n",
      "lif layer 2 self.abs_max_v: 8990.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.573794/  1.851662, val:  37.08%, val_best:  52.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 97.46 seconds, 1.62 minutes\n",
      "layer   1  Sparsity: 81.7064%\n",
      "layer   2  Sparsity: 75.5841%\n",
      "layer   3  Sparsity: 69.6935%\n",
      "total_backward_count 195800 real_backward_count 25345  12.944%\n",
      "fc layer 1 self.abs_max_out: 16839.0\n",
      "fc layer 2 self.abs_max_out: 5435.0\n",
      "lif layer 2 self.abs_max_v: 9224.5\n",
      "lif layer 2 self.abs_max_v: 9250.5\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.568576/  1.838086, val:  39.58%, val_best:  52.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 99.08 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.6885%\n",
      "layer   2  Sparsity: 75.8956%\n",
      "layer   3  Sparsity: 70.4066%\n",
      "total_backward_count 205590 real_backward_count 26565  12.921%\n",
      "fc layer 1 self.abs_max_out: 17415.0\n",
      "lif layer 1 self.abs_max_v: 31194.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.592589/  1.898456, val:  35.42%, val_best:  52.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 98.59 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.7118%\n",
      "layer   2  Sparsity: 76.5252%\n",
      "layer   3  Sparsity: 70.8931%\n",
      "total_backward_count 215380 real_backward_count 27812  12.913%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.594663/  1.839542, val:  45.42%, val_best:  52.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 98.35 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.7545%\n",
      "layer   2  Sparsity: 77.5284%\n",
      "layer   3  Sparsity: 71.2087%\n",
      "total_backward_count 225170 real_backward_count 29047  12.900%\n",
      "lif layer 1 self.abs_max_v: 31286.5\n",
      "lif layer 1 self.abs_max_v: 31808.5\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.594180/  1.867319, val:  44.58%, val_best:  52.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 98.32 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.6608%\n",
      "layer   2  Sparsity: 78.4715%\n",
      "layer   3  Sparsity: 71.1225%\n",
      "total_backward_count 234960 real_backward_count 30335  12.911%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.555079/  1.834577, val:  52.92%, val_best:  52.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 99.67 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.7372%\n",
      "layer   2  Sparsity: 77.7502%\n",
      "layer   3  Sparsity: 69.6062%\n",
      "total_backward_count 244750 real_backward_count 31565  12.897%\n",
      "lif layer 1 self.abs_max_v: 32186.0\n",
      "lif layer 2 self.abs_max_v: 9409.0\n",
      "fc layer 2 self.abs_max_out: 5574.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.569070/  1.800619, val:  57.08%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 100.09 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 81.6852%\n",
      "layer   2  Sparsity: 77.7556%\n",
      "layer   3  Sparsity: 69.4187%\n",
      "total_backward_count 254540 real_backward_count 32859  12.909%\n",
      "lif layer 2 self.abs_max_v: 9471.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.570750/  1.801916, val:  53.33%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 101.97 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 81.7158%\n",
      "layer   2  Sparsity: 77.8881%\n",
      "layer   3  Sparsity: 69.6449%\n",
      "total_backward_count 264330 real_backward_count 34074  12.891%\n",
      "lif layer 2 self.abs_max_v: 9514.5\n",
      "lif layer 2 self.abs_max_v: 9519.0\n",
      "lif layer 2 self.abs_max_v: 9622.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.555735/  1.818623, val:  57.50%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 100.10 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 81.6462%\n",
      "layer   2  Sparsity: 78.6976%\n",
      "layer   3  Sparsity: 70.2127%\n",
      "total_backward_count 274120 real_backward_count 35299  12.877%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.555919/  1.882493, val:  43.75%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 97.71 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 81.7360%\n",
      "layer   2  Sparsity: 79.0439%\n",
      "layer   3  Sparsity: 71.5592%\n",
      "total_backward_count 283910 real_backward_count 36529  12.866%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.572186/  1.808761, val:  49.58%, val_best:  57.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 98.75 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.6962%\n",
      "layer   2  Sparsity: 78.5942%\n",
      "layer   3  Sparsity: 70.5610%\n",
      "total_backward_count 293700 real_backward_count 37750  12.853%\n",
      "fc layer 1 self.abs_max_out: 17579.0\n",
      "lif layer 1 self.abs_max_v: 32546.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.551020/  1.827751, val:  51.25%, val_best:  57.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 97.45 seconds, 1.62 minutes\n",
      "layer   1  Sparsity: 81.7555%\n",
      "layer   2  Sparsity: 78.1646%\n",
      "layer   3  Sparsity: 69.6252%\n",
      "total_backward_count 303490 real_backward_count 38993  12.848%\n",
      "fc layer 1 self.abs_max_out: 17620.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.553593/  1.838045, val:  49.17%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 97.78 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 81.7029%\n",
      "layer   2  Sparsity: 78.5965%\n",
      "layer   3  Sparsity: 69.8523%\n",
      "total_backward_count 313280 real_backward_count 40202  12.833%\n",
      "fc layer 1 self.abs_max_out: 18772.0\n",
      "lif layer 1 self.abs_max_v: 32772.0\n",
      "fc layer 1 self.abs_max_out: 19030.0\n",
      "lif layer 1 self.abs_max_v: 35374.5\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.562774/  1.855547, val:  48.75%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 101.25 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.6892%\n",
      "layer   2  Sparsity: 79.6195%\n",
      "layer   3  Sparsity: 70.4326%\n",
      "total_backward_count 323070 real_backward_count 41388  12.811%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.561554/  1.837319, val:  42.50%, val_best:  57.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 100.84 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7316%\n",
      "layer   2  Sparsity: 79.5545%\n",
      "layer   3  Sparsity: 69.7364%\n",
      "total_backward_count 332860 real_backward_count 42564  12.787%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.547607/  1.772213, val:  55.42%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 99.72 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.6914%\n",
      "layer   2  Sparsity: 79.3143%\n",
      "layer   3  Sparsity: 70.6914%\n",
      "total_backward_count 342650 real_backward_count 43779  12.777%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.531812/  1.862020, val:  40.83%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 99.86 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.6651%\n",
      "layer   2  Sparsity: 79.3311%\n",
      "layer   3  Sparsity: 68.9785%\n",
      "total_backward_count 352440 real_backward_count 44973  12.760%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.515247/  1.737527, val:  52.92%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 99.33 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.7728%\n",
      "layer   2  Sparsity: 79.3507%\n",
      "layer   3  Sparsity: 70.0430%\n",
      "total_backward_count 362230 real_backward_count 46171  12.746%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.515563/  1.820763, val:  42.92%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 99.13 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.7382%\n",
      "layer   2  Sparsity: 79.3019%\n",
      "layer   3  Sparsity: 69.8654%\n",
      "total_backward_count 372020 real_backward_count 47336  12.724%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.539646/  1.788295, val:  52.08%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 98.83 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.6502%\n",
      "layer   2  Sparsity: 79.7487%\n",
      "layer   3  Sparsity: 70.6733%\n",
      "total_backward_count 381810 real_backward_count 48574  12.722%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.526423/  1.797653, val:  50.83%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 99.14 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.7404%\n",
      "layer   2  Sparsity: 79.8600%\n",
      "layer   3  Sparsity: 70.8890%\n",
      "total_backward_count 391600 real_backward_count 49755  12.706%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.536331/  1.797226, val:  53.33%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 99.59 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.6608%\n",
      "layer   2  Sparsity: 79.6374%\n",
      "layer   3  Sparsity: 70.2469%\n",
      "total_backward_count 401390 real_backward_count 50967  12.698%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.558559/  1.823017, val:  48.75%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 98.42 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.6992%\n",
      "layer   2  Sparsity: 79.7152%\n",
      "layer   3  Sparsity: 70.2453%\n",
      "total_backward_count 411180 real_backward_count 52180  12.690%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.546470/  1.833058, val:  47.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 98.39 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.7808%\n",
      "layer   2  Sparsity: 78.6138%\n",
      "layer   3  Sparsity: 70.0683%\n",
      "total_backward_count 420970 real_backward_count 53328  12.668%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.545180/  1.817974, val:  52.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 98.22 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.7966%\n",
      "layer   2  Sparsity: 78.2216%\n",
      "layer   3  Sparsity: 69.3785%\n",
      "total_backward_count 430760 real_backward_count 54465  12.644%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.511440/  1.826552, val:  49.17%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 100.34 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 81.7552%\n",
      "layer   2  Sparsity: 77.8673%\n",
      "layer   3  Sparsity: 68.2802%\n",
      "total_backward_count 440550 real_backward_count 55645  12.631%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.496767/  1.805565, val:  57.92%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 99.10 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.6524%\n",
      "layer   2  Sparsity: 77.8269%\n",
      "layer   3  Sparsity: 67.7554%\n",
      "total_backward_count 450340 real_backward_count 56791  12.611%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.511833/  1.778420, val:  52.08%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 99.34 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.6444%\n",
      "layer   2  Sparsity: 77.5713%\n",
      "layer   3  Sparsity: 68.3413%\n",
      "total_backward_count 460130 real_backward_count 57960  12.596%\n",
      "lif layer 2 self.abs_max_v: 9697.5\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.505360/  1.803190, val:  41.67%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 99.02 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.7780%\n",
      "layer   2  Sparsity: 76.8735%\n",
      "layer   3  Sparsity: 67.7589%\n",
      "total_backward_count 469920 real_backward_count 59076  12.572%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.518104/  1.828941, val:  52.50%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 100.85 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.6413%\n",
      "layer   2  Sparsity: 76.0676%\n",
      "layer   3  Sparsity: 68.5510%\n",
      "total_backward_count 479710 real_backward_count 60267  12.563%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.516773/  1.751022, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 100.81 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7026%\n",
      "layer   2  Sparsity: 76.4433%\n",
      "layer   3  Sparsity: 68.5217%\n",
      "total_backward_count 489500 real_backward_count 61497  12.563%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.494952/  1.749788, val:  47.92%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 98.25 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.7390%\n",
      "layer   2  Sparsity: 76.8303%\n",
      "layer   3  Sparsity: 67.8773%\n",
      "total_backward_count 499290 real_backward_count 62666  12.551%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.450959/  1.754327, val:  50.42%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 98.53 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 81.6815%\n",
      "layer   2  Sparsity: 77.4959%\n",
      "layer   3  Sparsity: 66.5782%\n",
      "total_backward_count 509080 real_backward_count 63795  12.531%\n",
      "lif layer 2 self.abs_max_v: 9759.5\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.495804/  1.790934, val:  58.33%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 99.37 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 81.7343%\n",
      "layer   2  Sparsity: 76.2744%\n",
      "layer   3  Sparsity: 68.9628%\n",
      "total_backward_count 518870 real_backward_count 64974  12.522%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.507801/  1.781867, val:  56.67%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 98.06 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 81.7402%\n",
      "layer   2  Sparsity: 76.8593%\n",
      "layer   3  Sparsity: 68.2119%\n",
      "total_backward_count 528660 real_backward_count 66123  12.508%\n",
      "lif layer 2 self.abs_max_v: 10076.5\n",
      "lif layer 2 self.abs_max_v: 10091.5\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.477052/  1.741077, val:  53.33%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 100.79 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7550%\n",
      "layer   2  Sparsity: 77.0829%\n",
      "layer   3  Sparsity: 66.4332%\n",
      "total_backward_count 538450 real_backward_count 67260  12.491%\n",
      "fc layer 2 self.abs_max_out: 5609.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.463210/  1.777552, val:  49.17%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 100.94 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7192%\n",
      "layer   2  Sparsity: 77.3108%\n",
      "layer   3  Sparsity: 67.3361%\n",
      "total_backward_count 548240 real_backward_count 68458  12.487%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.482247/  1.798505, val:  46.25%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.49 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.6872%\n",
      "layer   2  Sparsity: 77.0920%\n",
      "layer   3  Sparsity: 67.4211%\n",
      "total_backward_count 558030 real_backward_count 69610  12.474%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.474363/  1.782600, val:  50.42%, val_best:  58.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 98.84 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 81.7541%\n",
      "layer   2  Sparsity: 77.8843%\n",
      "layer   3  Sparsity: 68.6312%\n",
      "total_backward_count 567820 real_backward_count 70788  12.467%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.479715/  1.788899, val:  50.83%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 100.43 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 81.7329%\n",
      "layer   2  Sparsity: 77.7164%\n",
      "layer   3  Sparsity: 68.8202%\n",
      "total_backward_count 577610 real_backward_count 71975  12.461%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.437129/  1.759444, val:  53.33%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 102.03 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 81.7209%\n",
      "layer   2  Sparsity: 77.7968%\n",
      "layer   3  Sparsity: 66.0364%\n",
      "total_backward_count 587400 real_backward_count 73130  12.450%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.420741/  1.710125, val:  52.08%, val_best:  58.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.75 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7440%\n",
      "layer   2  Sparsity: 77.4678%\n",
      "layer   3  Sparsity: 64.2016%\n",
      "total_backward_count 597190 real_backward_count 74341  12.448%\n",
      "fc layer 3 self.abs_max_out: 2525.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.440898/  1.763147, val:  51.67%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 107.16 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 81.6776%\n",
      "layer   2  Sparsity: 77.2646%\n",
      "layer   3  Sparsity: 66.5048%\n",
      "total_backward_count 606980 real_backward_count 75493  12.437%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.463242/  1.749171, val:  52.08%, val_best:  58.33%, tr:  99.28%, tr_best: 100.00%, epoch time: 107.17 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 81.7024%\n",
      "layer   2  Sparsity: 76.7264%\n",
      "layer   3  Sparsity: 67.8281%\n",
      "total_backward_count 616770 real_backward_count 76636  12.425%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.408258/  1.681545, val:  52.92%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.66 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7405%\n",
      "layer   2  Sparsity: 76.4264%\n",
      "layer   3  Sparsity: 64.2121%\n",
      "total_backward_count 626560 real_backward_count 77764  12.411%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.466511/  1.809785, val:  47.50%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.81 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6936%\n",
      "layer   2  Sparsity: 77.2301%\n",
      "layer   3  Sparsity: 68.1870%\n",
      "total_backward_count 636350 real_backward_count 78954  12.407%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.480881/  1.766820, val:  48.33%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.15 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7009%\n",
      "layer   2  Sparsity: 76.4215%\n",
      "layer   3  Sparsity: 67.6251%\n",
      "total_backward_count 646140 real_backward_count 80152  12.405%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.425604/  1.758425, val:  48.75%, val_best:  58.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.66 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6930%\n",
      "layer   2  Sparsity: 76.1055%\n",
      "layer   3  Sparsity: 64.8469%\n",
      "total_backward_count 655930 real_backward_count 81313  12.397%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.433011/  1.677672, val:  56.67%, val_best:  58.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.22 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6871%\n",
      "layer   2  Sparsity: 76.5284%\n",
      "layer   3  Sparsity: 64.1793%\n",
      "total_backward_count 665720 real_backward_count 82403  12.378%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.387641/  1.675093, val:  62.92%, val_best:  62.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.16 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6986%\n",
      "layer   2  Sparsity: 76.3593%\n",
      "layer   3  Sparsity: 63.9470%\n",
      "total_backward_count 675510 real_backward_count 83590  12.374%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.376355/  1.770185, val:  41.67%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 103.77 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6839%\n",
      "layer   2  Sparsity: 76.4246%\n",
      "layer   3  Sparsity: 61.5789%\n",
      "total_backward_count 685300 real_backward_count 84718  12.362%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.382388/  1.707070, val:  53.75%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 102.95 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.7180%\n",
      "layer   2  Sparsity: 76.4552%\n",
      "layer   3  Sparsity: 64.6142%\n",
      "total_backward_count 695090 real_backward_count 85878  12.355%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.418589/  1.761735, val:  45.42%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.21 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7073%\n",
      "layer   2  Sparsity: 76.1178%\n",
      "layer   3  Sparsity: 65.2821%\n",
      "total_backward_count 704880 real_backward_count 87059  12.351%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.432346/  1.718705, val:  56.67%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 100.95 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7270%\n",
      "layer   2  Sparsity: 74.9963%\n",
      "layer   3  Sparsity: 65.1260%\n",
      "total_backward_count 714670 real_backward_count 88175  12.338%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.397286/  1.740702, val:  44.17%, val_best:  62.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 102.68 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.6967%\n",
      "layer   2  Sparsity: 74.8118%\n",
      "layer   3  Sparsity: 64.0012%\n",
      "total_backward_count 724460 real_backward_count 89329  12.330%\n",
      "fc layer 2 self.abs_max_out: 5665.0\n",
      "lif layer 2 self.abs_max_v: 10115.5\n",
      "lif layer 2 self.abs_max_v: 10162.5\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.394439/  1.753786, val:  46.67%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 103.20 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.6937%\n",
      "layer   2  Sparsity: 74.8473%\n",
      "layer   3  Sparsity: 64.7366%\n",
      "total_backward_count 734250 real_backward_count 90498  12.325%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.421392/  1.740070, val:  50.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.25 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7144%\n",
      "layer   2  Sparsity: 74.6539%\n",
      "layer   3  Sparsity: 65.6082%\n",
      "total_backward_count 744040 real_backward_count 91629  12.315%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.443722/  1.780137, val:  50.00%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.83 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7352%\n",
      "layer   2  Sparsity: 75.3676%\n",
      "layer   3  Sparsity: 66.3791%\n",
      "total_backward_count 753830 real_backward_count 92836  12.315%\n",
      "fc layer 1 self.abs_max_out: 19117.0\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.427899/  1.720762, val:  45.83%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 102.09 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 81.7153%\n",
      "layer   2  Sparsity: 75.8787%\n",
      "layer   3  Sparsity: 66.5416%\n",
      "total_backward_count 763620 real_backward_count 94019  12.312%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.408601/  1.756289, val:  41.25%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 102.47 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7890%\n",
      "layer   2  Sparsity: 76.1849%\n",
      "layer   3  Sparsity: 66.3236%\n",
      "total_backward_count 773410 real_backward_count 95157  12.304%\n",
      "fc layer 1 self.abs_max_out: 20346.0\n",
      "lif layer 1 self.abs_max_v: 36552.0\n",
      "lif layer 1 self.abs_max_v: 36714.0\n",
      "lif layer 1 self.abs_max_v: 36845.0\n",
      "lif layer 1 self.abs_max_v: 38127.5\n",
      "fc layer 1 self.abs_max_out: 21646.0\n",
      "lif layer 1 self.abs_max_v: 40710.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.443170/  1.716774, val:  57.50%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 101.01 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7027%\n",
      "layer   2  Sparsity: 75.5676%\n",
      "layer   3  Sparsity: 67.1387%\n",
      "total_backward_count 783200 real_backward_count 96305  12.296%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.441361/  1.748504, val:  51.67%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 101.55 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.6991%\n",
      "layer   2  Sparsity: 75.0463%\n",
      "layer   3  Sparsity: 66.5230%\n",
      "total_backward_count 792990 real_backward_count 97475  12.292%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.409771/  1.765229, val:  41.67%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.06 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.6878%\n",
      "layer   2  Sparsity: 75.3740%\n",
      "layer   3  Sparsity: 64.9873%\n",
      "total_backward_count 802780 real_backward_count 98624  12.285%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.407289/  1.725430, val:  50.00%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 101.59 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.6969%\n",
      "layer   2  Sparsity: 75.1713%\n",
      "layer   3  Sparsity: 65.3346%\n",
      "total_backward_count 812570 real_backward_count 99792  12.281%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.395904/  1.784217, val:  40.42%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.31 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6489%\n",
      "layer   2  Sparsity: 76.2079%\n",
      "layer   3  Sparsity: 65.1525%\n",
      "total_backward_count 822360 real_backward_count 100917  12.272%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.397412/  1.693562, val:  59.58%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 101.60 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.6708%\n",
      "layer   2  Sparsity: 76.7158%\n",
      "layer   3  Sparsity: 65.2866%\n",
      "total_backward_count 832150 real_backward_count 102113  12.271%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.415219/  1.791951, val:  42.92%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 102.56 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7582%\n",
      "layer   2  Sparsity: 77.3282%\n",
      "layer   3  Sparsity: 65.0826%\n",
      "total_backward_count 841940 real_backward_count 103314  12.271%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.414823/  1.696773, val:  52.50%, val_best:  62.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 102.41 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7085%\n",
      "layer   2  Sparsity: 77.5401%\n",
      "layer   3  Sparsity: 65.8615%\n",
      "total_backward_count 851730 real_backward_count 104485  12.267%\n",
      "fc layer 2 self.abs_max_out: 5981.0\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.435589/  1.737314, val:  50.42%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 102.86 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.6499%\n",
      "layer   2  Sparsity: 77.0181%\n",
      "layer   3  Sparsity: 66.9630%\n",
      "total_backward_count 861520 real_backward_count 105602  12.258%\n",
      "lif layer 2 self.abs_max_v: 10441.5\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.405257/  1.711138, val:  56.25%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.28 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 81.6576%\n",
      "layer   2  Sparsity: 75.9279%\n",
      "layer   3  Sparsity: 65.3495%\n",
      "total_backward_count 871310 real_backward_count 106753  12.252%\n",
      "lif layer 2 self.abs_max_v: 10668.5\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.383781/  1.720891, val:  51.67%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 102.35 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7231%\n",
      "layer   2  Sparsity: 76.1967%\n",
      "layer   3  Sparsity: 64.1938%\n",
      "total_backward_count 881100 real_backward_count 107895  12.245%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.389042/  1.741250, val:  45.42%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.71 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7120%\n",
      "layer   2  Sparsity: 76.6618%\n",
      "layer   3  Sparsity: 63.9906%\n",
      "total_backward_count 890890 real_backward_count 109046  12.240%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.380580/  1.663942, val:  59.58%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.70 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6523%\n",
      "layer   2  Sparsity: 76.2442%\n",
      "layer   3  Sparsity: 63.4523%\n",
      "total_backward_count 900680 real_backward_count 110168  12.232%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.379155/  1.708634, val:  55.00%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.52 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7233%\n",
      "layer   2  Sparsity: 76.7901%\n",
      "layer   3  Sparsity: 63.6090%\n",
      "total_backward_count 910470 real_backward_count 111262  12.220%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.388504/  1.692648, val:  53.75%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 102.30 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7293%\n",
      "layer   2  Sparsity: 77.0806%\n",
      "layer   3  Sparsity: 64.0386%\n",
      "total_backward_count 920260 real_backward_count 112407  12.215%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.408990/  1.726147, val:  59.17%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.96 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.7447%\n",
      "layer   2  Sparsity: 77.0495%\n",
      "layer   3  Sparsity: 65.8523%\n",
      "total_backward_count 930050 real_backward_count 113547  12.209%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.418546/  1.692025, val:  59.58%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.18 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.7605%\n",
      "layer   2  Sparsity: 76.3009%\n",
      "layer   3  Sparsity: 66.2867%\n",
      "total_backward_count 939840 real_backward_count 114671  12.201%\n",
      "fc layer 2 self.abs_max_out: 6068.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.411080/  1.727820, val:  50.00%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 102.49 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.6794%\n",
      "layer   2  Sparsity: 77.3751%\n",
      "layer   3  Sparsity: 68.2145%\n",
      "total_backward_count 949630 real_backward_count 115852  12.200%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.413697/  1.677581, val:  60.00%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.89 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7301%\n",
      "layer   2  Sparsity: 76.6408%\n",
      "layer   3  Sparsity: 66.1925%\n",
      "total_backward_count 959420 real_backward_count 116983  12.193%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.409354/  1.822187, val:  33.75%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 102.23 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 81.7154%\n",
      "layer   2  Sparsity: 76.0619%\n",
      "layer   3  Sparsity: 66.1740%\n",
      "total_backward_count 969210 real_backward_count 118112  12.186%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.422876/  1.717408, val:  50.42%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 102.12 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 81.6592%\n",
      "layer   2  Sparsity: 75.4332%\n",
      "layer   3  Sparsity: 63.3418%\n",
      "total_backward_count 979000 real_backward_count 119230  12.179%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.417136/  1.757668, val:  48.75%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 101.13 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.6899%\n",
      "layer   2  Sparsity: 76.1297%\n",
      "layer   3  Sparsity: 63.7319%\n",
      "total_backward_count 988790 real_backward_count 120343  12.171%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.360766/  1.657130, val:  57.50%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 101.49 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.6877%\n",
      "layer   2  Sparsity: 76.2131%\n",
      "layer   3  Sparsity: 61.6714%\n",
      "total_backward_count 998580 real_backward_count 121480  12.165%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.371238/  1.700560, val:  49.17%, val_best:  62.92%, tr:  99.28%, tr_best: 100.00%, epoch time: 101.61 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.7558%\n",
      "layer   2  Sparsity: 75.7592%\n",
      "layer   3  Sparsity: 62.4961%\n",
      "total_backward_count 1008370 real_backward_count 122590  12.157%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.377151/  1.696279, val:  49.17%, val_best:  62.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 103.64 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6695%\n",
      "layer   2  Sparsity: 75.4352%\n",
      "layer   3  Sparsity: 63.2950%\n",
      "total_backward_count 1018160 real_backward_count 123772  12.156%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.383870/  1.729647, val:  54.58%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 101.52 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 81.7376%\n",
      "layer   2  Sparsity: 75.2241%\n",
      "layer   3  Sparsity: 63.6418%\n",
      "total_backward_count 1027950 real_backward_count 124849  12.145%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.383722/  1.664633, val:  60.42%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.88 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7334%\n",
      "layer   2  Sparsity: 75.2697%\n",
      "layer   3  Sparsity: 65.2968%\n",
      "total_backward_count 1037740 real_backward_count 126005  12.142%\n",
      "fc layer 1 self.abs_max_out: 22572.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.390916/  1.746505, val:  46.67%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.82 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6754%\n",
      "layer   2  Sparsity: 75.6354%\n",
      "layer   3  Sparsity: 66.2004%\n",
      "total_backward_count 1047530 real_backward_count 127170  12.140%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.385745/  1.741407, val:  46.25%, val_best:  62.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 105.66 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7042%\n",
      "layer   2  Sparsity: 75.7006%\n",
      "layer   3  Sparsity: 66.1097%\n",
      "total_backward_count 1057320 real_backward_count 128337  12.138%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.411003/  1.679199, val:  55.00%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.66 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7253%\n",
      "layer   2  Sparsity: 76.0760%\n",
      "layer   3  Sparsity: 65.2369%\n",
      "total_backward_count 1067110 real_backward_count 129501  12.136%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.407249/  1.734096, val:  47.08%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.59 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 81.6464%\n",
      "layer   2  Sparsity: 76.5742%\n",
      "layer   3  Sparsity: 65.0251%\n",
      "total_backward_count 1076900 real_backward_count 130621  12.129%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.413664/  1.693723, val:  50.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 106.27 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7891%\n",
      "layer   2  Sparsity: 76.4771%\n",
      "layer   3  Sparsity: 64.2827%\n",
      "total_backward_count 1086690 real_backward_count 131776  12.126%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.406729/  1.738497, val:  48.75%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.15 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 81.7297%\n",
      "layer   2  Sparsity: 74.9268%\n",
      "layer   3  Sparsity: 65.4683%\n",
      "total_backward_count 1096480 real_backward_count 132903  12.121%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.407698/  1.717176, val:  55.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.89 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7361%\n",
      "layer   2  Sparsity: 74.7832%\n",
      "layer   3  Sparsity: 65.4909%\n",
      "total_backward_count 1106270 real_backward_count 133994  12.112%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.386531/  1.758859, val:  44.17%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.74 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6009%\n",
      "layer   2  Sparsity: 76.0868%\n",
      "layer   3  Sparsity: 63.6452%\n",
      "total_backward_count 1116060 real_backward_count 135101  12.105%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.380410/  1.745484, val:  45.83%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.28 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.6800%\n",
      "layer   2  Sparsity: 75.0661%\n",
      "layer   3  Sparsity: 65.0601%\n",
      "total_backward_count 1125850 real_backward_count 136255  12.102%\n",
      "lif layer 2 self.abs_max_v: 10913.0\n",
      "lif layer 2 self.abs_max_v: 11281.5\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.417556/  1.743797, val:  54.58%, val_best:  62.92%, tr:  99.28%, tr_best: 100.00%, epoch time: 105.12 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7322%\n",
      "layer   2  Sparsity: 73.8414%\n",
      "layer   3  Sparsity: 67.8661%\n",
      "total_backward_count 1135640 real_backward_count 137399  12.099%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.441699/  1.716894, val:  52.92%, val_best:  62.92%, tr:  99.08%, tr_best: 100.00%, epoch time: 106.27 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.6991%\n",
      "layer   2  Sparsity: 75.0395%\n",
      "layer   3  Sparsity: 68.2920%\n",
      "total_backward_count 1145430 real_backward_count 138609  12.101%\n",
      "fc layer 3 self.abs_max_out: 2543.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.454380/  1.728166, val:  60.42%, val_best:  62.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.79 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7748%\n",
      "layer   2  Sparsity: 74.7710%\n",
      "layer   3  Sparsity: 67.6841%\n",
      "total_backward_count 1155220 real_backward_count 139791  12.101%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.427089/  1.722143, val:  50.83%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.02 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6883%\n",
      "layer   2  Sparsity: 75.9332%\n",
      "layer   3  Sparsity: 66.4893%\n",
      "total_backward_count 1165010 real_backward_count 140939  12.098%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.402587/  1.654743, val:  60.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.78 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6903%\n",
      "layer   2  Sparsity: 75.6011%\n",
      "layer   3  Sparsity: 63.8311%\n",
      "total_backward_count 1174800 real_backward_count 142040  12.091%\n",
      "fc layer 3 self.abs_max_out: 2633.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.399426/  1.700197, val:  55.83%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.73 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7000%\n",
      "layer   2  Sparsity: 75.0691%\n",
      "layer   3  Sparsity: 64.9546%\n",
      "total_backward_count 1184590 real_backward_count 143104  12.080%\n",
      "fc layer 3 self.abs_max_out: 2747.0\n",
      "fc layer 3 self.abs_max_out: 2814.0\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.371118/  1.707797, val:  50.00%, val_best:  62.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.57 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7283%\n",
      "layer   2  Sparsity: 75.5859%\n",
      "layer   3  Sparsity: 63.8042%\n",
      "total_backward_count 1194380 real_backward_count 144220  12.075%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.389225/  1.746697, val:  45.83%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.76 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6817%\n",
      "layer   2  Sparsity: 76.2473%\n",
      "layer   3  Sparsity: 64.3174%\n",
      "total_backward_count 1204170 real_backward_count 145351  12.071%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.410313/  1.740892, val:  44.58%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.01 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.6765%\n",
      "layer   2  Sparsity: 75.5230%\n",
      "layer   3  Sparsity: 63.3420%\n",
      "total_backward_count 1213960 real_backward_count 146482  12.066%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.419053/  1.726843, val:  65.42%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.44 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6811%\n",
      "layer   2  Sparsity: 76.1246%\n",
      "layer   3  Sparsity: 67.3111%\n",
      "total_backward_count 1223750 real_backward_count 147643  12.065%\n",
      "fc layer 1 self.abs_max_out: 23538.0\n",
      "fc layer 1 self.abs_max_out: 24900.0\n",
      "lif layer 1 self.abs_max_v: 44802.5\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.483107/  1.720342, val:  54.17%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.68 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6576%\n",
      "layer   2  Sparsity: 76.6122%\n",
      "layer   3  Sparsity: 70.2236%\n",
      "total_backward_count 1233540 real_backward_count 148825  12.065%\n",
      "fc layer 1 self.abs_max_out: 25519.0\n",
      "lif layer 1 self.abs_max_v: 45949.5\n",
      "lif layer 1 self.abs_max_v: 45968.0\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.442117/  1.763388, val:  49.17%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.25 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7135%\n",
      "layer   2  Sparsity: 75.7140%\n",
      "layer   3  Sparsity: 67.0257%\n",
      "total_backward_count 1243330 real_backward_count 150002  12.065%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.414967/  1.710738, val:  53.75%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.58 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7530%\n",
      "layer   2  Sparsity: 75.5226%\n",
      "layer   3  Sparsity: 65.5840%\n",
      "total_backward_count 1253120 real_backward_count 151144  12.061%\n",
      "lif layer 2 self.abs_max_v: 11440.5\n",
      "lif layer 2 self.abs_max_v: 11730.5\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.368529/  1.701102, val:  49.17%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.76 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6961%\n",
      "layer   2  Sparsity: 75.3025%\n",
      "layer   3  Sparsity: 64.4161%\n",
      "total_backward_count 1262910 real_backward_count 152246  12.055%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.407652/  1.715888, val:  52.92%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.54 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7252%\n",
      "layer   2  Sparsity: 74.9571%\n",
      "layer   3  Sparsity: 66.1852%\n",
      "total_backward_count 1272700 real_backward_count 153319  12.047%\n",
      "fc layer 2 self.abs_max_out: 6138.0\n",
      "lif layer 2 self.abs_max_v: 11762.5\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.417508/  1.736415, val:  44.58%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.63 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6980%\n",
      "layer   2  Sparsity: 73.6576%\n",
      "layer   3  Sparsity: 67.0114%\n",
      "total_backward_count 1282490 real_backward_count 154473  12.045%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.401592/  1.719058, val:  46.25%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.46 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6628%\n",
      "layer   2  Sparsity: 74.6196%\n",
      "layer   3  Sparsity: 66.0690%\n",
      "total_backward_count 1292280 real_backward_count 155614  12.042%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.426321/  1.782848, val:  37.92%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.69 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 81.7180%\n",
      "layer   2  Sparsity: 75.3196%\n",
      "layer   3  Sparsity: 66.6948%\n",
      "total_backward_count 1302070 real_backward_count 156730  12.037%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.407275/  1.703959, val:  51.25%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.39 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.6934%\n",
      "layer   2  Sparsity: 75.1191%\n",
      "layer   3  Sparsity: 65.4166%\n",
      "total_backward_count 1311860 real_backward_count 157897  12.036%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.380888/  1.731427, val:  46.25%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.91 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7012%\n",
      "layer   2  Sparsity: 74.9365%\n",
      "layer   3  Sparsity: 66.1325%\n",
      "total_backward_count 1321650 real_backward_count 159031  12.033%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.381084/  1.664666, val:  57.92%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.75 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6491%\n",
      "layer   2  Sparsity: 74.6369%\n",
      "layer   3  Sparsity: 63.7679%\n",
      "total_backward_count 1331440 real_backward_count 160169  12.030%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.424689/  1.734729, val:  45.00%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.60 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7296%\n",
      "layer   2  Sparsity: 74.8714%\n",
      "layer   3  Sparsity: 66.8028%\n",
      "total_backward_count 1341230 real_backward_count 161316  12.027%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.416468/  1.714913, val:  58.75%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 106.18 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7495%\n",
      "layer   2  Sparsity: 74.7338%\n",
      "layer   3  Sparsity: 65.7134%\n",
      "total_backward_count 1351020 real_backward_count 162455  12.025%\n",
      "fc layer 3 self.abs_max_out: 2891.0\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.412041/  1.698238, val:  57.50%, val_best:  65.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 106.68 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 81.7132%\n",
      "layer   2  Sparsity: 74.1253%\n",
      "layer   3  Sparsity: 65.3953%\n",
      "total_backward_count 1360810 real_backward_count 163643  12.025%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.399853/  1.728233, val:  49.58%, val_best:  65.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 106.41 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7426%\n",
      "layer   2  Sparsity: 74.9386%\n",
      "layer   3  Sparsity: 66.7789%\n",
      "total_backward_count 1370600 real_backward_count 164815  12.025%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.439614/  1.744731, val:  47.08%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.65 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6275%\n",
      "layer   2  Sparsity: 75.7363%\n",
      "layer   3  Sparsity: 67.8573%\n",
      "total_backward_count 1380390 real_backward_count 165997  12.025%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.467525/  1.772082, val:  49.17%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 105.75 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6627%\n",
      "layer   2  Sparsity: 74.8828%\n",
      "layer   3  Sparsity: 68.8534%\n",
      "total_backward_count 1390180 real_backward_count 167166  12.025%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.453570/  1.710071, val:  46.25%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 105.79 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7182%\n",
      "layer   2  Sparsity: 74.6962%\n",
      "layer   3  Sparsity: 67.1505%\n",
      "total_backward_count 1399970 real_backward_count 168351  12.025%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.405470/  1.728055, val:  47.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.82 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7036%\n",
      "layer   2  Sparsity: 73.3936%\n",
      "layer   3  Sparsity: 66.6525%\n",
      "total_backward_count 1409760 real_backward_count 169459  12.020%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.388830/  1.669934, val:  57.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.04 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7041%\n",
      "layer   2  Sparsity: 72.8957%\n",
      "layer   3  Sparsity: 64.9417%\n",
      "total_backward_count 1419550 real_backward_count 170577  12.016%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.438490/  1.699230, val:  55.42%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 103.78 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7737%\n",
      "layer   2  Sparsity: 73.3923%\n",
      "layer   3  Sparsity: 68.2250%\n",
      "total_backward_count 1429340 real_backward_count 171698  12.012%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.411845/  1.747169, val:  50.42%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.20 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7456%\n",
      "layer   2  Sparsity: 73.2828%\n",
      "layer   3  Sparsity: 67.9740%\n",
      "total_backward_count 1439130 real_backward_count 172878  12.013%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.403650/  1.745388, val:  44.17%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.91 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7008%\n",
      "layer   2  Sparsity: 73.0954%\n",
      "layer   3  Sparsity: 66.9150%\n",
      "total_backward_count 1448920 real_backward_count 173975  12.007%\n",
      "fc layer 2 self.abs_max_out: 6165.0\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.428397/  1.735306, val:  49.58%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.87 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6832%\n",
      "layer   2  Sparsity: 72.8596%\n",
      "layer   3  Sparsity: 67.0158%\n",
      "total_backward_count 1458710 real_backward_count 175148  12.007%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.401658/  1.701203, val:  50.42%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.92 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7093%\n",
      "layer   2  Sparsity: 72.7638%\n",
      "layer   3  Sparsity: 66.2919%\n",
      "total_backward_count 1468500 real_backward_count 176266  12.003%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.446403/  1.682856, val:  50.00%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.37 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7676%\n",
      "layer   2  Sparsity: 73.0836%\n",
      "layer   3  Sparsity: 70.0297%\n",
      "total_backward_count 1478290 real_backward_count 177405  12.001%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.414581/  1.692212, val:  55.00%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.02 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7128%\n",
      "layer   2  Sparsity: 74.0776%\n",
      "layer   3  Sparsity: 67.7324%\n",
      "total_backward_count 1488080 real_backward_count 178525  11.997%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.408837/  1.715088, val:  53.75%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.15 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7148%\n",
      "layer   2  Sparsity: 75.1573%\n",
      "layer   3  Sparsity: 68.7350%\n",
      "total_backward_count 1497870 real_backward_count 179672  11.995%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.443818/  1.677465, val:  60.00%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.02 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.6487%\n",
      "layer   2  Sparsity: 74.2829%\n",
      "layer   3  Sparsity: 68.6358%\n",
      "total_backward_count 1507660 real_backward_count 180887  11.998%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.435042/  1.719564, val:  53.33%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.80 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7129%\n",
      "layer   2  Sparsity: 74.3194%\n",
      "layer   3  Sparsity: 68.0166%\n",
      "total_backward_count 1517450 real_backward_count 182020  11.995%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.437995/  1.714302, val:  61.67%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.17 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.7744%\n",
      "layer   2  Sparsity: 76.1945%\n",
      "layer   3  Sparsity: 70.9742%\n",
      "total_backward_count 1527240 real_backward_count 183134  11.991%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.455003/  1.788683, val:  43.33%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.32 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6689%\n",
      "layer   2  Sparsity: 76.2131%\n",
      "layer   3  Sparsity: 69.9135%\n",
      "total_backward_count 1537030 real_backward_count 184325  11.992%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.439638/  1.699876, val:  61.67%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.95 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7329%\n",
      "layer   2  Sparsity: 76.0615%\n",
      "layer   3  Sparsity: 67.3521%\n",
      "total_backward_count 1546820 real_backward_count 185416  11.987%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.400633/  1.724005, val:  53.75%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.69 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7489%\n",
      "layer   2  Sparsity: 76.1793%\n",
      "layer   3  Sparsity: 66.2566%\n",
      "total_backward_count 1556610 real_backward_count 186471  11.979%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.452645/  1.719348, val:  50.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.63 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7501%\n",
      "layer   2  Sparsity: 75.4816%\n",
      "layer   3  Sparsity: 68.8242%\n",
      "total_backward_count 1566400 real_backward_count 187630  11.978%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.414326/  1.719806, val:  60.83%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 106.37 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7584%\n",
      "layer   2  Sparsity: 76.1978%\n",
      "layer   3  Sparsity: 66.9965%\n",
      "total_backward_count 1576190 real_backward_count 188808  11.979%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.451442/  1.774585, val:  54.17%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 107.11 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 81.7056%\n",
      "layer   2  Sparsity: 75.4108%\n",
      "layer   3  Sparsity: 69.4899%\n",
      "total_backward_count 1585980 real_backward_count 189977  11.979%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.504345/  1.729560, val:  56.25%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.91 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.6776%\n",
      "layer   2  Sparsity: 74.4144%\n",
      "layer   3  Sparsity: 69.3933%\n",
      "total_backward_count 1595770 real_backward_count 191187  11.981%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.472824/  1.769562, val:  56.25%, val_best:  65.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 102.59 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.6963%\n",
      "layer   2  Sparsity: 74.2085%\n",
      "layer   3  Sparsity: 70.1427%\n",
      "total_backward_count 1605560 real_backward_count 192367  11.981%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.465426/  1.726031, val:  56.25%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.67 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6872%\n",
      "layer   2  Sparsity: 74.4500%\n",
      "layer   3  Sparsity: 69.0270%\n",
      "total_backward_count 1615350 real_backward_count 193527  11.980%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.449583/  1.700377, val:  56.67%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.59 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7454%\n",
      "layer   2  Sparsity: 74.4170%\n",
      "layer   3  Sparsity: 69.1718%\n",
      "total_backward_count 1625140 real_backward_count 194683  11.979%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.444817/  1.725258, val:  54.58%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 103.02 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.7774%\n",
      "layer   2  Sparsity: 74.1127%\n",
      "layer   3  Sparsity: 69.3339%\n",
      "total_backward_count 1634930 real_backward_count 195847  11.979%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.464776/  1.718926, val:  50.42%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.16 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6269%\n",
      "layer   2  Sparsity: 74.7135%\n",
      "layer   3  Sparsity: 68.0975%\n",
      "total_backward_count 1644720 real_backward_count 196971  11.976%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.419328/  1.815959, val:  35.83%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.83 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7337%\n",
      "layer   2  Sparsity: 74.6443%\n",
      "layer   3  Sparsity: 66.9293%\n",
      "total_backward_count 1654510 real_backward_count 198098  11.973%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.425502/  1.692303, val:  49.17%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.46 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7324%\n",
      "layer   2  Sparsity: 75.9066%\n",
      "layer   3  Sparsity: 67.8247%\n",
      "total_backward_count 1664300 real_backward_count 199181  11.968%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.425667/  1.790717, val:  38.75%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.48 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7194%\n",
      "layer   2  Sparsity: 75.2694%\n",
      "layer   3  Sparsity: 66.9781%\n",
      "total_backward_count 1674090 real_backward_count 200282  11.964%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.454889/  1.769300, val:  44.58%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.00 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6608%\n",
      "layer   2  Sparsity: 74.0621%\n",
      "layer   3  Sparsity: 68.0247%\n",
      "total_backward_count 1683880 real_backward_count 201481  11.965%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.475889/  1.733908, val:  56.67%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 105.34 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.7202%\n",
      "layer   2  Sparsity: 74.1361%\n",
      "layer   3  Sparsity: 68.8513%\n",
      "total_backward_count 1693670 real_backward_count 202706  11.968%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.453660/  1.771769, val:  50.42%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.43 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.6670%\n",
      "layer   2  Sparsity: 74.7158%\n",
      "layer   3  Sparsity: 69.2161%\n",
      "total_backward_count 1703460 real_backward_count 203792  11.963%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.462638/  1.803702, val:  53.33%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.89 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7009%\n",
      "layer   2  Sparsity: 74.5278%\n",
      "layer   3  Sparsity: 71.5197%\n",
      "total_backward_count 1713250 real_backward_count 204958  11.963%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.444055/  1.731081, val:  51.67%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 103.75 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6752%\n",
      "layer   2  Sparsity: 73.9824%\n",
      "layer   3  Sparsity: 67.4070%\n",
      "total_backward_count 1723040 real_backward_count 206107  11.962%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.460039/  1.728008, val:  53.33%, val_best:  65.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 105.23 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7114%\n",
      "layer   2  Sparsity: 74.7399%\n",
      "layer   3  Sparsity: 69.5766%\n",
      "total_backward_count 1732830 real_backward_count 207307  11.963%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.441450/  1.730018, val:  49.58%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.43 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7128%\n",
      "layer   2  Sparsity: 74.6516%\n",
      "layer   3  Sparsity: 66.8281%\n",
      "total_backward_count 1742620 real_backward_count 208434  11.961%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.426337/  1.719852, val:  46.25%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 103.13 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.6998%\n",
      "layer   2  Sparsity: 75.3756%\n",
      "layer   3  Sparsity: 68.0100%\n",
      "total_backward_count 1752410 real_backward_count 209593  11.960%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.401414/  1.675281, val:  54.17%, val_best:  65.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 102.55 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 81.7209%\n",
      "layer   2  Sparsity: 75.5538%\n",
      "layer   3  Sparsity: 66.0749%\n",
      "total_backward_count 1762200 real_backward_count 210709  11.957%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.379111/  1.679320, val:  57.08%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 103.65 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6704%\n",
      "layer   2  Sparsity: 75.3523%\n",
      "layer   3  Sparsity: 67.2167%\n",
      "total_backward_count 1771990 real_backward_count 211783  11.952%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.424704/  1.732334, val:  54.58%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.03 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6895%\n",
      "layer   2  Sparsity: 75.5187%\n",
      "layer   3  Sparsity: 69.2272%\n",
      "total_backward_count 1781780 real_backward_count 212888  11.948%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.446863/  1.701688, val:  58.33%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.96 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6513%\n",
      "layer   2  Sparsity: 76.2902%\n",
      "layer   3  Sparsity: 67.6445%\n",
      "total_backward_count 1791570 real_backward_count 214046  11.947%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.471468/  1.779582, val:  42.92%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.83 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7038%\n",
      "layer   2  Sparsity: 75.1593%\n",
      "layer   3  Sparsity: 68.8457%\n",
      "total_backward_count 1801360 real_backward_count 215173  11.945%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.476391/  1.778426, val:  40.42%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.19 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.7059%\n",
      "layer   2  Sparsity: 75.0792%\n",
      "layer   3  Sparsity: 68.6336%\n",
      "total_backward_count 1811150 real_backward_count 216390  11.948%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.447600/  1.755829, val:  51.67%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.73 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7076%\n",
      "layer   2  Sparsity: 76.3524%\n",
      "layer   3  Sparsity: 67.9772%\n",
      "total_backward_count 1820940 real_backward_count 217583  11.949%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.463656/  1.724945, val:  56.67%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.43 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.6709%\n",
      "layer   2  Sparsity: 77.5183%\n",
      "layer   3  Sparsity: 68.7010%\n",
      "total_backward_count 1830730 real_backward_count 218762  11.949%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.434692/  1.725232, val:  49.58%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.98 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7045%\n",
      "layer   2  Sparsity: 77.3244%\n",
      "layer   3  Sparsity: 67.9219%\n",
      "total_backward_count 1840520 real_backward_count 219969  11.951%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.444126/  1.797614, val:  48.33%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.00 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.6639%\n",
      "layer   2  Sparsity: 76.3113%\n",
      "layer   3  Sparsity: 68.0755%\n",
      "total_backward_count 1850310 real_backward_count 221129  11.951%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.442072/  1.746482, val:  54.17%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.35 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 81.6554%\n",
      "layer   2  Sparsity: 75.9438%\n",
      "layer   3  Sparsity: 68.9142%\n",
      "total_backward_count 1860100 real_backward_count 222247  11.948%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.435897/  1.690284, val:  60.00%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 104.10 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.6220%\n",
      "layer   2  Sparsity: 75.1667%\n",
      "layer   3  Sparsity: 67.8222%\n",
      "total_backward_count 1869890 real_backward_count 223353  11.945%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.405968/  1.800110, val:  38.75%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 103.74 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7138%\n",
      "layer   2  Sparsity: 75.7267%\n",
      "layer   3  Sparsity: 65.0691%\n",
      "total_backward_count 1879680 real_backward_count 224550  11.946%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.430081/  1.775342, val:  51.25%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 104.87 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 81.7009%\n",
      "layer   2  Sparsity: 76.6592%\n",
      "layer   3  Sparsity: 67.7792%\n",
      "total_backward_count 1889470 real_backward_count 225701  11.945%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.484107/  1.784417, val:  44.58%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.98 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.7293%\n",
      "layer   2  Sparsity: 76.7810%\n",
      "layer   3  Sparsity: 69.9593%\n",
      "total_backward_count 1899260 real_backward_count 226882  11.946%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.476671/  1.726052, val:  49.58%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.67 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 81.7173%\n",
      "layer   2  Sparsity: 75.5374%\n",
      "layer   3  Sparsity: 69.6159%\n",
      "total_backward_count 1909050 real_backward_count 228032  11.945%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.486270/  1.804717, val:  48.75%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.37 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 81.6521%\n",
      "layer   2  Sparsity: 75.5507%\n",
      "layer   3  Sparsity: 70.2264%\n",
      "total_backward_count 1918840 real_backward_count 229206  11.945%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.507690/  1.766436, val:  56.25%, val_best:  65.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 106.21 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 81.7315%\n",
      "layer   2  Sparsity: 74.9445%\n",
      "layer   3  Sparsity: 69.6321%\n",
      "total_backward_count 1928630 real_backward_count 230373  11.945%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.460245/  1.742374, val:  53.75%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.93 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 81.8370%\n",
      "layer   2  Sparsity: 76.0405%\n",
      "layer   3  Sparsity: 68.8640%\n",
      "total_backward_count 1938420 real_backward_count 231481  11.942%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.463688/  1.800322, val:  40.42%, val_best:  65.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 100.15 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 81.7028%\n",
      "layer   2  Sparsity: 75.4597%\n",
      "layer   3  Sparsity: 67.2222%\n",
      "total_backward_count 1948210 real_backward_count 232604  11.939%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.444297/  1.726627, val:  50.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 100.92 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 81.7021%\n",
      "layer   2  Sparsity: 76.2843%\n",
      "layer   3  Sparsity: 68.1584%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea55578c773c4431b8d31b031a144a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñá‚ñÑ‚ñá‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñà‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.4443</td></tr><tr><td>val_acc_best</td><td>0.65417</td></tr><tr><td>val_acc_now</td><td>0.50417</td></tr><tr><td>val_loss</td><td>1.72663</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-41</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5eru5fk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y5eru5fk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_144420-y5eru5fk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ozjc3v9l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_202845-ozjc3v9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozjc3v9l' target=\"_blank\">true-sweep-48</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozjc3v9l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozjc3v9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251114_202856_909', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 503.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 924.0\n",
      "fc layer 1 self.abs_max_out: 660.0\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 2 self.abs_max_out: 1609.0\n",
      "lif layer 2 self.abs_max_v: 1944.5\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 2 self.abs_max_v: 2046.5\n",
      "lif layer 1 self.abs_max_v: 1169.0\n",
      "fc layer 1 self.abs_max_out: 1145.0\n",
      "lif layer 2 self.abs_max_v: 2149.5\n",
      "fc layer 3 self.abs_max_out: 997.0\n",
      "fc layer 1 self.abs_max_out: 1633.0\n",
      "lif layer 1 self.abs_max_v: 1633.0\n",
      "lif layer 2 self.abs_max_v: 2404.0\n",
      "fc layer 3 self.abs_max_out: 1235.0\n",
      "fc layer 1 self.abs_max_out: 1953.0\n",
      "lif layer 1 self.abs_max_v: 1953.0\n",
      "fc layer 1 self.abs_max_out: 2208.0\n",
      "lif layer 1 self.abs_max_v: 2208.0\n",
      "fc layer 2 self.abs_max_out: 1612.0\n",
      "lif layer 2 self.abs_max_v: 2516.5\n",
      "fc layer 2 self.abs_max_out: 1628.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "lif layer 2 self.abs_max_v: 2697.0\n",
      "lif layer 2 self.abs_max_v: 2754.5\n",
      "fc layer 2 self.abs_max_out: 1949.0\n",
      "fc layer 1 self.abs_max_out: 2222.0\n",
      "lif layer 1 self.abs_max_v: 2222.0\n",
      "fc layer 2 self.abs_max_out: 2060.0\n",
      "fc layer 1 self.abs_max_out: 2342.0\n",
      "lif layer 1 self.abs_max_v: 2433.0\n",
      "fc layer 1 self.abs_max_out: 2445.0\n",
      "lif layer 1 self.abs_max_v: 2445.0\n",
      "lif layer 2 self.abs_max_v: 3065.0\n",
      "fc layer 3 self.abs_max_out: 1275.0\n",
      "fc layer 3 self.abs_max_out: 1358.0\n",
      "fc layer 3 self.abs_max_out: 1432.0\n",
      "fc layer 2 self.abs_max_out: 2471.0\n",
      "lif layer 2 self.abs_max_v: 3167.0\n",
      "lif layer 1 self.abs_max_v: 2450.0\n",
      "lif layer 2 self.abs_max_v: 3282.5\n",
      "fc layer 1 self.abs_max_out: 2451.0\n",
      "lif layer 1 self.abs_max_v: 2451.0\n",
      "lif layer 2 self.abs_max_v: 3453.0\n",
      "lif layer 1 self.abs_max_v: 2814.0\n",
      "fc layer 2 self.abs_max_out: 2975.0\n",
      "fc layer 1 self.abs_max_out: 2486.0\n",
      "lif layer 1 self.abs_max_v: 3123.5\n",
      "lif layer 2 self.abs_max_v: 3595.0\n",
      "lif layer 2 self.abs_max_v: 3705.5\n",
      "lif layer 2 self.abs_max_v: 3710.5\n",
      "fc layer 1 self.abs_max_out: 3270.0\n",
      "lif layer 1 self.abs_max_v: 3270.0\n",
      "lif layer 2 self.abs_max_v: 3905.0\n",
      "lif layer 2 self.abs_max_v: 4324.5\n",
      "lif layer 1 self.abs_max_v: 3292.0\n",
      "lif layer 1 self.abs_max_v: 4166.0\n",
      "fc layer 3 self.abs_max_out: 1467.0\n",
      "fc layer 1 self.abs_max_out: 3356.0\n",
      "fc layer 1 self.abs_max_out: 3917.0\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "lif layer 1 self.abs_max_v: 4234.0\n",
      "lif layer 1 self.abs_max_v: 4496.5\n",
      "fc layer 3 self.abs_max_out: 1559.0\n",
      "lif layer 1 self.abs_max_v: 4926.0\n",
      "lif layer 1 self.abs_max_v: 5328.0\n",
      "lif layer 1 self.abs_max_v: 5377.5\n",
      "fc layer 2 self.abs_max_out: 3056.0\n",
      "fc layer 2 self.abs_max_out: 3072.0\n",
      "fc layer 1 self.abs_max_out: 4164.0\n",
      "lif layer 2 self.abs_max_v: 4346.5\n",
      "fc layer 2 self.abs_max_out: 3294.0\n",
      "fc layer 2 self.abs_max_out: 3318.0\n",
      "lif layer 2 self.abs_max_v: 4454.5\n",
      "fc layer 2 self.abs_max_out: 3504.0\n",
      "lif layer 1 self.abs_max_v: 5552.5\n",
      "lif layer 2 self.abs_max_v: 4737.5\n",
      "fc layer 1 self.abs_max_out: 4534.0\n",
      "lif layer 1 self.abs_max_v: 5848.5\n",
      "lif layer 2 self.abs_max_v: 5226.0\n",
      "lif layer 2 self.abs_max_v: 5370.0\n",
      "lif layer 2 self.abs_max_v: 5465.0\n",
      "lif layer 2 self.abs_max_v: 5492.0\n",
      "fc layer 2 self.abs_max_out: 3574.0\n",
      "fc layer 2 self.abs_max_out: 3826.0\n",
      "fc layer 3 self.abs_max_out: 1648.0\n",
      "fc layer 2 self.abs_max_out: 4175.0\n",
      "lif layer 1 self.abs_max_v: 6021.0\n",
      "fc layer 1 self.abs_max_out: 4671.0\n",
      "lif layer 1 self.abs_max_v: 6836.5\n",
      "lif layer 2 self.abs_max_v: 5685.5\n",
      "lif layer 2 self.abs_max_v: 6110.0\n",
      "lif layer 2 self.abs_max_v: 6698.0\n",
      "fc layer 3 self.abs_max_out: 2023.0\n",
      "fc layer 3 self.abs_max_out: 2243.0\n",
      "fc layer 1 self.abs_max_out: 4673.0\n",
      "fc layer 2 self.abs_max_out: 4237.0\n",
      "fc layer 2 self.abs_max_out: 4427.0\n",
      "lif layer 2 self.abs_max_v: 7206.0\n",
      "lif layer 2 self.abs_max_v: 7330.0\n",
      "fc layer 1 self.abs_max_out: 4692.0\n",
      "lif layer 1 self.abs_max_v: 7169.0\n",
      "fc layer 1 self.abs_max_out: 4839.0\n",
      "fc layer 1 self.abs_max_out: 5165.0\n",
      "lif layer 1 self.abs_max_v: 7289.0\n",
      "fc layer 2 self.abs_max_out: 4743.0\n",
      "fc layer 2 self.abs_max_out: 4751.0\n",
      "lif layer 1 self.abs_max_v: 7612.0\n",
      "lif layer 1 self.abs_max_v: 7956.0\n",
      "fc layer 1 self.abs_max_out: 5296.0\n",
      "fc layer 3 self.abs_max_out: 2256.0\n",
      "fc layer 3 self.abs_max_out: 2331.0\n",
      "fc layer 3 self.abs_max_out: 2354.0\n",
      "fc layer 3 self.abs_max_out: 2861.0\n",
      "fc layer 3 self.abs_max_out: 2965.0\n",
      "lif layer 1 self.abs_max_v: 8299.5\n",
      "fc layer 1 self.abs_max_out: 5622.0\n",
      "lif layer 1 self.abs_max_v: 8698.0\n",
      "lif layer 1 self.abs_max_v: 8886.5\n",
      "lif layer 1 self.abs_max_v: 9662.5\n",
      "lif layer 2 self.abs_max_v: 7448.0\n",
      "fc layer 2 self.abs_max_out: 4849.0\n",
      "fc layer 2 self.abs_max_out: 4894.0\n",
      "fc layer 1 self.abs_max_out: 6302.0\n",
      "lif layer 1 self.abs_max_v: 10536.5\n",
      "lif layer 1 self.abs_max_v: 10602.5\n",
      "fc layer 1 self.abs_max_out: 6438.0\n",
      "lif layer 1 self.abs_max_v: 11024.0\n",
      "lif layer 2 self.abs_max_v: 7776.5\n",
      "lif layer 2 self.abs_max_v: 7960.5\n",
      "lif layer 2 self.abs_max_v: 8193.0\n",
      "fc layer 1 self.abs_max_out: 6472.0\n",
      "fc layer 1 self.abs_max_out: 6795.0\n",
      "fc layer 1 self.abs_max_out: 7200.0\n",
      "lif layer 1 self.abs_max_v: 11309.0\n",
      "fc layer 1 self.abs_max_out: 7220.0\n",
      "fc layer 1 self.abs_max_out: 7248.0\n",
      "lif layer 1 self.abs_max_v: 12335.0\n",
      "lif layer 1 self.abs_max_v: 12528.0\n",
      "lif layer 1 self.abs_max_v: 12574.0\n",
      "lif layer 2 self.abs_max_v: 8830.0\n",
      "fc layer 1 self.abs_max_out: 8076.0\n",
      "fc layer 2 self.abs_max_out: 5086.0\n",
      "fc layer 2 self.abs_max_out: 5269.0\n",
      "fc layer 2 self.abs_max_out: 5303.0\n",
      "lif layer 1 self.abs_max_v: 12589.0\n",
      "lif layer 1 self.abs_max_v: 13301.5\n",
      "lif layer 1 self.abs_max_v: 13660.5\n",
      "fc layer 2 self.abs_max_out: 5534.0\n",
      "fc layer 2 self.abs_max_out: 5567.0\n",
      "fc layer 1 self.abs_max_out: 8648.0\n",
      "fc layer 1 self.abs_max_out: 9180.0\n",
      "fc layer 1 self.abs_max_out: 9563.0\n",
      "lif layer 1 self.abs_max_v: 16102.0\n",
      "lif layer 1 self.abs_max_v: 17059.5\n",
      "lif layer 1 self.abs_max_v: 17578.0\n",
      "fc layer 2 self.abs_max_out: 5587.0\n",
      "fc layer 2 self.abs_max_out: 5645.0\n",
      "fc layer 2 self.abs_max_out: 5713.0\n",
      "fc layer 2 self.abs_max_out: 5805.0\n",
      "fc layer 2 self.abs_max_out: 5867.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.345612/  1.883797, val:  29.58%, val_best:  29.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 89.09 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 92.8933%\n",
      "layer   2  Sparsity: 72.9925%\n",
      "layer   3  Sparsity: 62.0244%\n",
      "total_backward_count 9790 real_backward_count 1464  14.954%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 8995.5\n",
      "lif layer 2 self.abs_max_v: 9366.0\n",
      "lif layer 2 self.abs_max_v: 9414.5\n",
      "lif layer 2 self.abs_max_v: 9574.5\n",
      "fc layer 3 self.abs_max_out: 3008.0\n",
      "fc layer 3 self.abs_max_out: 3022.0\n",
      "fc layer 3 self.abs_max_out: 3181.0\n",
      "fc layer 1 self.abs_max_out: 9861.0\n",
      "lif layer 2 self.abs_max_v: 9777.5\n",
      "fc layer 1 self.abs_max_out: 10688.0\n",
      "lif layer 1 self.abs_max_v: 17844.0\n",
      "lif layer 1 self.abs_max_v: 19278.0\n",
      "fc layer 1 self.abs_max_out: 11288.0\n",
      "lif layer 1 self.abs_max_v: 20328.5\n",
      "lif layer 2 self.abs_max_v: 9805.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.213264/  1.757948, val:  34.17%, val_best:  34.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 89.92 seconds, 1.50 minutes\n",
      "layer   1  Sparsity: 92.8881%\n",
      "layer   2  Sparsity: 76.2488%\n",
      "layer   3  Sparsity: 62.8529%\n",
      "total_backward_count 19580 real_backward_count 2793  14.265%\n",
      "fc layer 3 self.abs_max_out: 3215.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.213693/  1.691022, val:  45.00%, val_best:  45.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 88.51 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 92.8908%\n",
      "layer   2  Sparsity: 77.8907%\n",
      "layer   3  Sparsity: 64.2850%\n",
      "total_backward_count 29370 real_backward_count 4189  14.263%\n",
      "lif layer 2 self.abs_max_v: 10000.5\n",
      "fc layer 2 self.abs_max_out: 5891.0\n",
      "lif layer 2 self.abs_max_v: 10122.5\n",
      "fc layer 3 self.abs_max_out: 3261.0\n",
      "fc layer 3 self.abs_max_out: 3265.0\n",
      "lif layer 2 self.abs_max_v: 10413.5\n",
      "lif layer 1 self.abs_max_v: 20561.5\n",
      "fc layer 1 self.abs_max_out: 12305.0\n",
      "lif layer 1 self.abs_max_v: 21262.0\n",
      "fc layer 1 self.abs_max_out: 14080.0\n",
      "lif layer 1 self.abs_max_v: 24711.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.175902/  1.777223, val:  42.08%, val_best:  45.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.37 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 92.8970%\n",
      "layer   2  Sparsity: 76.8071%\n",
      "layer   3  Sparsity: 64.5676%\n",
      "total_backward_count 39160 real_backward_count 5527  14.114%\n",
      "fc layer 3 self.abs_max_out: 3278.0\n",
      "fc layer 3 self.abs_max_out: 3457.0\n",
      "fc layer 3 self.abs_max_out: 3608.0\n",
      "fc layer 2 self.abs_max_out: 6175.0\n",
      "fc layer 1 self.abs_max_out: 14461.0\n",
      "lif layer 1 self.abs_max_v: 24875.5\n",
      "fc layer 1 self.abs_max_out: 15668.0\n",
      "lif layer 1 self.abs_max_v: 28106.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.215903/  1.725491, val:  43.75%, val_best:  45.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 86.44 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 92.8929%\n",
      "layer   2  Sparsity: 76.1017%\n",
      "layer   3  Sparsity: 66.6669%\n",
      "total_backward_count 48950 real_backward_count 6792  13.875%\n",
      "lif layer 2 self.abs_max_v: 10640.0\n",
      "lif layer 2 self.abs_max_v: 10641.5\n",
      "lif layer 2 self.abs_max_v: 11239.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.158473/  1.700450, val:  48.33%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 84.94 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 92.8813%\n",
      "layer   2  Sparsity: 75.8929%\n",
      "layer   3  Sparsity: 66.1734%\n",
      "total_backward_count 58740 real_backward_count 8143  13.863%\n",
      "fc layer 2 self.abs_max_out: 6359.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.216976/  1.838850, val:  38.75%, val_best:  48.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 84.31 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 92.8871%\n",
      "layer   2  Sparsity: 77.2594%\n",
      "layer   3  Sparsity: 69.0226%\n",
      "total_backward_count 68530 real_backward_count 9439  13.774%\n",
      "lif layer 2 self.abs_max_v: 11333.5\n",
      "lif layer 2 self.abs_max_v: 11475.5\n",
      "fc layer 2 self.abs_max_out: 6499.0\n",
      "lif layer 2 self.abs_max_v: 11702.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.283884/  1.733183, val:  43.75%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.88 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 92.8879%\n",
      "layer   2  Sparsity: 76.5653%\n",
      "layer   3  Sparsity: 70.1669%\n",
      "total_backward_count 78320 real_backward_count 10762  13.741%\n",
      "fc layer 2 self.abs_max_out: 6520.0\n",
      "fc layer 2 self.abs_max_out: 6584.0\n",
      "fc layer 2 self.abs_max_out: 6640.0\n",
      "lif layer 2 self.abs_max_v: 11818.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.268832/  1.737884, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 83.54 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 92.8929%\n",
      "layer   2  Sparsity: 76.8083%\n",
      "layer   3  Sparsity: 70.0616%\n",
      "total_backward_count 88110 real_backward_count 12164  13.805%\n",
      "fc layer 2 self.abs_max_out: 6924.0\n",
      "lif layer 2 self.abs_max_v: 12002.5\n",
      "lif layer 2 self.abs_max_v: 12182.0\n",
      "fc layer 2 self.abs_max_out: 7036.0\n",
      "lif layer 2 self.abs_max_v: 12692.5\n",
      "lif layer 2 self.abs_max_v: 12760.5\n",
      "lif layer 2 self.abs_max_v: 12883.5\n",
      "lif layer 2 self.abs_max_v: 13298.0\n",
      "lif layer 2 self.abs_max_v: 13679.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.237980/  1.724507, val:  44.58%, val_best:  49.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 92.8961%\n",
      "layer   2  Sparsity: 76.2789%\n",
      "layer   3  Sparsity: 68.9621%\n",
      "total_backward_count 97900 real_backward_count 13501  13.791%\n",
      "fc layer 2 self.abs_max_out: 7043.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.208694/  1.780158, val:  39.17%, val_best:  49.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 92.9020%\n",
      "layer   2  Sparsity: 76.6093%\n",
      "layer   3  Sparsity: 68.4219%\n",
      "total_backward_count 107690 real_backward_count 14880  13.817%\n",
      "fc layer 2 self.abs_max_out: 7190.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.284956/  1.704032, val:  44.17%, val_best:  49.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 82.18 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 92.9015%\n",
      "layer   2  Sparsity: 75.7890%\n",
      "layer   3  Sparsity: 70.3863%\n",
      "total_backward_count 117480 real_backward_count 16279  13.857%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.265961/  1.747622, val:  33.33%, val_best:  49.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 82.50 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 92.8957%\n",
      "layer   2  Sparsity: 75.6884%\n",
      "layer   3  Sparsity: 70.2050%\n",
      "total_backward_count 127270 real_backward_count 17585  13.817%\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.283520/  1.826673, val:  36.25%, val_best:  49.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8937%\n",
      "layer   2  Sparsity: 75.1721%\n",
      "layer   3  Sparsity: 71.2525%\n",
      "total_backward_count 137060 real_backward_count 18943  13.821%\n",
      "fc layer 2 self.abs_max_out: 7554.0\n",
      "lif layer 2 self.abs_max_v: 13967.5\n",
      "fc layer 2 self.abs_max_out: 7738.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.267275/  1.766126, val:  40.83%, val_best:  49.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.8905%\n",
      "layer   2  Sparsity: 75.6411%\n",
      "layer   3  Sparsity: 71.5630%\n",
      "total_backward_count 146850 real_backward_count 20373  13.873%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.315946/  1.786825, val:  42.08%, val_best:  49.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8936%\n",
      "layer   2  Sparsity: 76.0295%\n",
      "layer   3  Sparsity: 71.5099%\n",
      "total_backward_count 156640 real_backward_count 21806  13.921%\n",
      "fc layer 2 self.abs_max_out: 8183.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.360352/  1.703856, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8766%\n",
      "layer   2  Sparsity: 75.9373%\n",
      "layer   3  Sparsity: 73.1309%\n",
      "total_backward_count 166430 real_backward_count 23200  13.940%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.346459/  1.788350, val:  46.67%, val_best:  49.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8881%\n",
      "layer   2  Sparsity: 76.0082%\n",
      "layer   3  Sparsity: 73.0055%\n",
      "total_backward_count 176220 real_backward_count 24596  13.958%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.412467/  1.852823, val:  40.00%, val_best:  49.58%, tr:  98.88%, tr_best:  99.90%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8928%\n",
      "layer   2  Sparsity: 77.1362%\n",
      "layer   3  Sparsity: 77.8849%\n",
      "total_backward_count 186010 real_backward_count 26173  14.071%\n",
      "lif layer 2 self.abs_max_v: 14770.0\n",
      "lif layer 2 self.abs_max_v: 14837.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.394036/  1.848760, val:  40.83%, val_best:  49.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.8954%\n",
      "layer   2  Sparsity: 76.8019%\n",
      "layer   3  Sparsity: 74.5276%\n",
      "total_backward_count 195800 real_backward_count 27596  14.094%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.416660/  1.839627, val:  37.08%, val_best:  49.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8822%\n",
      "layer   2  Sparsity: 77.2537%\n",
      "layer   3  Sparsity: 76.8566%\n",
      "total_backward_count 205590 real_backward_count 29091  14.150%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.414806/  1.962653, val:  35.42%, val_best:  49.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9104%\n",
      "layer   2  Sparsity: 76.7021%\n",
      "layer   3  Sparsity: 75.5862%\n",
      "total_backward_count 215380 real_backward_count 30576  14.196%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.404524/  1.796188, val:  51.67%, val_best:  51.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9058%\n",
      "layer   2  Sparsity: 75.9802%\n",
      "layer   3  Sparsity: 74.7469%\n",
      "total_backward_count 225170 real_backward_count 32033  14.226%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.475057/  1.791357, val:  51.67%, val_best:  51.67%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8810%\n",
      "layer   2  Sparsity: 77.2185%\n",
      "layer   3  Sparsity: 78.4846%\n",
      "total_backward_count 234960 real_backward_count 33535  14.273%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.458745/  1.813588, val:  42.08%, val_best:  51.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.9053%\n",
      "layer   2  Sparsity: 76.4253%\n",
      "layer   3  Sparsity: 77.4648%\n",
      "total_backward_count 244750 real_backward_count 34968  14.287%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.533858/  1.804676, val:  52.92%, val_best:  52.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.8969%\n",
      "layer   2  Sparsity: 76.2602%\n",
      "layer   3  Sparsity: 80.3458%\n",
      "total_backward_count 254540 real_backward_count 36495  14.338%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.494498/  1.808321, val:  46.25%, val_best:  52.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8861%\n",
      "layer   2  Sparsity: 77.2281%\n",
      "layer   3  Sparsity: 79.0319%\n",
      "total_backward_count 264330 real_backward_count 37941  14.354%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.456564/  1.841154, val:  44.58%, val_best:  52.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 77.0897%\n",
      "layer   3  Sparsity: 75.9275%\n",
      "total_backward_count 274120 real_backward_count 39383  14.367%\n",
      "lif layer 1 self.abs_max_v: 28269.5\n",
      "lif layer 1 self.abs_max_v: 29659.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.442311/  1.964686, val:  32.08%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8886%\n",
      "layer   2  Sparsity: 77.0691%\n",
      "layer   3  Sparsity: 77.0759%\n",
      "total_backward_count 283910 real_backward_count 40817  14.377%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.523822/  1.915887, val:  44.17%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 77.3059%\n",
      "layer   3  Sparsity: 78.7641%\n",
      "total_backward_count 293700 real_backward_count 42266  14.391%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.508265/  1.870020, val:  54.17%, val_best:  54.17%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8917%\n",
      "layer   2  Sparsity: 76.7111%\n",
      "layer   3  Sparsity: 78.7483%\n",
      "total_backward_count 303490 real_backward_count 43797  14.431%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.561019/  1.912460, val:  44.17%, val_best:  54.17%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9162%\n",
      "layer   2  Sparsity: 76.7081%\n",
      "layer   3  Sparsity: 80.8620%\n",
      "total_backward_count 313280 real_backward_count 45315  14.465%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.531805/  1.838102, val:  40.83%, val_best:  54.17%, tr:  98.88%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 76.3294%\n",
      "layer   3  Sparsity: 79.1698%\n",
      "total_backward_count 323070 real_backward_count 46726  14.463%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.529993/  1.976966, val:  39.17%, val_best:  54.17%, tr:  98.57%, tr_best:  99.90%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8915%\n",
      "layer   2  Sparsity: 76.5424%\n",
      "layer   3  Sparsity: 79.2148%\n",
      "total_backward_count 332860 real_backward_count 48217  14.486%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.500203/  1.822566, val:  44.17%, val_best:  54.17%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8871%\n",
      "layer   2  Sparsity: 76.6636%\n",
      "layer   3  Sparsity: 76.6435%\n",
      "total_backward_count 342650 real_backward_count 49644  14.488%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.461762/  1.821444, val:  47.08%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8982%\n",
      "layer   2  Sparsity: 77.5925%\n",
      "layer   3  Sparsity: 77.3953%\n",
      "total_backward_count 352440 real_backward_count 51072  14.491%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.445776/  1.768797, val:  48.75%, val_best:  54.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8930%\n",
      "layer   2  Sparsity: 76.9559%\n",
      "layer   3  Sparsity: 77.2247%\n",
      "total_backward_count 362230 real_backward_count 52457  14.482%\n",
      "fc layer 2 self.abs_max_out: 8327.0\n",
      "lif layer 2 self.abs_max_v: 14972.0\n",
      "lif layer 2 self.abs_max_v: 15028.0\n",
      "lif layer 2 self.abs_max_v: 15818.0\n",
      "lif layer 2 self.abs_max_v: 16144.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.480524/  1.819128, val:  42.92%, val_best:  54.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8961%\n",
      "layer   2  Sparsity: 77.3970%\n",
      "layer   3  Sparsity: 77.9363%\n",
      "total_backward_count 372020 real_backward_count 53857  14.477%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.503612/  1.877807, val:  47.08%, val_best:  54.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8894%\n",
      "layer   2  Sparsity: 77.2128%\n",
      "layer   3  Sparsity: 79.1164%\n",
      "total_backward_count 381810 real_backward_count 55429  14.517%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.477642/  1.795349, val:  46.25%, val_best:  54.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8955%\n",
      "layer   2  Sparsity: 77.3507%\n",
      "layer   3  Sparsity: 78.3361%\n",
      "total_backward_count 391600 real_backward_count 56846  14.516%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.417464/  1.749236, val:  57.08%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9110%\n",
      "layer   2  Sparsity: 77.0620%\n",
      "layer   3  Sparsity: 76.3651%\n",
      "total_backward_count 401390 real_backward_count 58246  14.511%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.426061/  1.842041, val:  40.42%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8849%\n",
      "layer   2  Sparsity: 77.5515%\n",
      "layer   3  Sparsity: 77.3142%\n",
      "total_backward_count 411180 real_backward_count 59646  14.506%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.445991/  1.836205, val:  49.17%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9000%\n",
      "layer   2  Sparsity: 77.0575%\n",
      "layer   3  Sparsity: 77.5373%\n",
      "total_backward_count 420970 real_backward_count 61029  14.497%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.416141/  1.833784, val:  47.50%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8986%\n",
      "layer   2  Sparsity: 76.5589%\n",
      "layer   3  Sparsity: 75.8873%\n",
      "total_backward_count 430760 real_backward_count 62443  14.496%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.409117/  1.767100, val:  45.42%, val_best:  57.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8965%\n",
      "layer   2  Sparsity: 76.6088%\n",
      "layer   3  Sparsity: 75.9861%\n",
      "total_backward_count 440550 real_backward_count 63861  14.496%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.418171/  1.755108, val:  47.50%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8981%\n",
      "layer   2  Sparsity: 77.1549%\n",
      "layer   3  Sparsity: 74.7929%\n",
      "total_backward_count 450340 real_backward_count 65268  14.493%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.401547/  1.846992, val:  39.17%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8898%\n",
      "layer   2  Sparsity: 76.6741%\n",
      "layer   3  Sparsity: 74.4803%\n",
      "total_backward_count 460130 real_backward_count 66715  14.499%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.395805/  1.915466, val:  35.83%, val_best:  57.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8881%\n",
      "layer   2  Sparsity: 76.3049%\n",
      "layer   3  Sparsity: 74.9281%\n",
      "total_backward_count 469920 real_backward_count 68156  14.504%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.414497/  1.883051, val:  42.08%, val_best:  57.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9065%\n",
      "layer   2  Sparsity: 76.6279%\n",
      "layer   3  Sparsity: 75.5469%\n",
      "total_backward_count 479710 real_backward_count 69613  14.511%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.388642/  1.784641, val:  47.92%, val_best:  57.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9066%\n",
      "layer   2  Sparsity: 76.7904%\n",
      "layer   3  Sparsity: 75.7779%\n",
      "total_backward_count 489500 real_backward_count 71066  14.518%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.458728/  1.870426, val:  40.00%, val_best:  57.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9058%\n",
      "layer   2  Sparsity: 75.6639%\n",
      "layer   3  Sparsity: 77.0037%\n",
      "total_backward_count 499290 real_backward_count 72602  14.541%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.428455/  1.781209, val:  40.00%, val_best:  57.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8952%\n",
      "layer   2  Sparsity: 75.7288%\n",
      "layer   3  Sparsity: 75.4737%\n",
      "total_backward_count 509080 real_backward_count 74007  14.537%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.483192/  1.844955, val:  47.50%, val_best:  57.08%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9029%\n",
      "layer   2  Sparsity: 76.8676%\n",
      "layer   3  Sparsity: 78.1365%\n",
      "total_backward_count 518870 real_backward_count 75484  14.548%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.553697/  1.891206, val:  49.17%, val_best:  57.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.8852%\n",
      "layer   2  Sparsity: 76.5420%\n",
      "layer   3  Sparsity: 78.8928%\n",
      "total_backward_count 528660 real_backward_count 77002  14.566%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.490297/  1.795036, val:  51.25%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8811%\n",
      "layer   2  Sparsity: 76.3740%\n",
      "layer   3  Sparsity: 76.9149%\n",
      "total_backward_count 538450 real_backward_count 78354  14.552%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.423533/  1.778194, val:  42.50%, val_best:  57.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8877%\n",
      "layer   2  Sparsity: 76.0742%\n",
      "layer   3  Sparsity: 76.2198%\n",
      "total_backward_count 548240 real_backward_count 79787  14.553%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.386174/  1.936837, val:  35.00%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8989%\n",
      "layer   2  Sparsity: 76.6759%\n",
      "layer   3  Sparsity: 75.9925%\n",
      "total_backward_count 558030 real_backward_count 81145  14.541%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.412875/  1.780722, val:  45.83%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9079%\n",
      "layer   2  Sparsity: 77.6729%\n",
      "layer   3  Sparsity: 76.4172%\n",
      "total_backward_count 567820 real_backward_count 82542  14.537%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.446669/  1.830889, val:  50.00%, val_best:  57.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9073%\n",
      "layer   2  Sparsity: 78.3740%\n",
      "layer   3  Sparsity: 78.3700%\n",
      "total_backward_count 577610 real_backward_count 83927  14.530%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.385751/  1.769227, val:  41.25%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9073%\n",
      "layer   2  Sparsity: 76.9897%\n",
      "layer   3  Sparsity: 75.0388%\n",
      "total_backward_count 587400 real_backward_count 85239  14.511%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.366809/  1.808252, val:  51.25%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8891%\n",
      "layer   2  Sparsity: 76.6985%\n",
      "layer   3  Sparsity: 74.8151%\n",
      "total_backward_count 597190 real_backward_count 86526  14.489%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.380636/  1.806864, val:  42.08%, val_best:  57.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9000%\n",
      "layer   2  Sparsity: 76.5934%\n",
      "layer   3  Sparsity: 76.1500%\n",
      "total_backward_count 606980 real_backward_count 87874  14.477%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.417094/  1.840662, val:  40.83%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8790%\n",
      "layer   2  Sparsity: 77.1769%\n",
      "layer   3  Sparsity: 75.7963%\n",
      "total_backward_count 616770 real_backward_count 89363  14.489%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.406101/  1.898818, val:  37.50%, val_best:  57.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8845%\n",
      "layer   2  Sparsity: 76.3451%\n",
      "layer   3  Sparsity: 76.3956%\n",
      "total_backward_count 626560 real_backward_count 90752  14.484%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.444017/  1.779788, val:  39.58%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9032%\n",
      "layer   2  Sparsity: 76.7758%\n",
      "layer   3  Sparsity: 75.9234%\n",
      "total_backward_count 636350 real_backward_count 92194  14.488%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.387801/  1.847615, val:  30.00%, val_best:  57.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8875%\n",
      "layer   2  Sparsity: 76.8096%\n",
      "layer   3  Sparsity: 74.3222%\n",
      "total_backward_count 646140 real_backward_count 93594  14.485%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.341631/  1.698720, val:  44.58%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8880%\n",
      "layer   2  Sparsity: 76.0116%\n",
      "layer   3  Sparsity: 72.9497%\n",
      "total_backward_count 655930 real_backward_count 94896  14.467%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.395497/  1.789038, val:  41.67%, val_best:  57.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8905%\n",
      "layer   2  Sparsity: 76.4295%\n",
      "layer   3  Sparsity: 76.5466%\n",
      "total_backward_count 665720 real_backward_count 96270  14.461%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.379890/  1.746655, val:  57.92%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8996%\n",
      "layer   2  Sparsity: 76.1863%\n",
      "layer   3  Sparsity: 75.5033%\n",
      "total_backward_count 675510 real_backward_count 97614  14.450%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.364858/  1.774386, val:  55.42%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8872%\n",
      "layer   2  Sparsity: 76.4256%\n",
      "layer   3  Sparsity: 74.5919%\n",
      "total_backward_count 685300 real_backward_count 98906  14.433%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.387267/  1.818486, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8888%\n",
      "layer   2  Sparsity: 77.3158%\n",
      "layer   3  Sparsity: 76.6477%\n",
      "total_backward_count 695090 real_backward_count 100228  14.419%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.512062/  1.837053, val:  42.92%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 77.0544%\n",
      "layer   3  Sparsity: 77.1575%\n",
      "total_backward_count 704880 real_backward_count 101633  14.418%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.454378/  1.766623, val:  53.33%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8972%\n",
      "layer   2  Sparsity: 76.0834%\n",
      "layer   3  Sparsity: 76.9018%\n",
      "total_backward_count 714670 real_backward_count 103027  14.416%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.449209/  1.815707, val:  48.75%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8745%\n",
      "layer   2  Sparsity: 76.0704%\n",
      "layer   3  Sparsity: 76.9701%\n",
      "total_backward_count 724460 real_backward_count 104410  14.412%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.399891/  1.793983, val:  45.83%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8828%\n",
      "layer   2  Sparsity: 76.5553%\n",
      "layer   3  Sparsity: 75.5189%\n",
      "total_backward_count 734250 real_backward_count 105820  14.412%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.392333/  1.773497, val:  49.58%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8816%\n",
      "layer   2  Sparsity: 76.1236%\n",
      "layer   3  Sparsity: 75.0501%\n",
      "total_backward_count 744040 real_backward_count 107213  14.410%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.405450/  1.781580, val:  55.00%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8992%\n",
      "layer   2  Sparsity: 76.4739%\n",
      "layer   3  Sparsity: 75.7338%\n",
      "total_backward_count 753830 real_backward_count 108645  14.412%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.438119/  1.932060, val:  34.17%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8970%\n",
      "layer   2  Sparsity: 76.3603%\n",
      "layer   3  Sparsity: 76.7009%\n",
      "total_backward_count 763620 real_backward_count 110073  14.415%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.428601/  1.774532, val:  43.33%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9002%\n",
      "layer   2  Sparsity: 76.2838%\n",
      "layer   3  Sparsity: 76.6404%\n",
      "total_backward_count 773410 real_backward_count 111468  14.413%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.468219/  1.803321, val:  51.67%, val_best:  57.92%, tr:  98.57%, tr_best:  99.90%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8889%\n",
      "layer   2  Sparsity: 76.3396%\n",
      "layer   3  Sparsity: 77.6921%\n",
      "total_backward_count 783200 real_backward_count 112956  14.422%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.468235/  1.782905, val:  46.25%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8960%\n",
      "layer   2  Sparsity: 76.3264%\n",
      "layer   3  Sparsity: 78.4633%\n",
      "total_backward_count 792990 real_backward_count 114399  14.426%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.470601/  1.853019, val:  35.00%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9009%\n",
      "layer   2  Sparsity: 75.9861%\n",
      "layer   3  Sparsity: 76.5816%\n",
      "total_backward_count 802780 real_backward_count 115794  14.424%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.375390/  1.870546, val:  44.58%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8896%\n",
      "layer   2  Sparsity: 76.6340%\n",
      "layer   3  Sparsity: 73.2700%\n",
      "total_backward_count 812570 real_backward_count 117165  14.419%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.384092/  1.902356, val:  39.17%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 76.4051%\n",
      "layer   3  Sparsity: 74.1820%\n",
      "total_backward_count 822360 real_backward_count 118566  14.418%\n",
      "fc layer 3 self.abs_max_out: 3913.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.386007/  1.777984, val:  51.67%, val_best:  57.92%, tr:  98.67%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8921%\n",
      "layer   2  Sparsity: 75.9369%\n",
      "layer   3  Sparsity: 74.3697%\n",
      "total_backward_count 832150 real_backward_count 119998  14.420%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.432285/  1.886648, val:  41.67%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9133%\n",
      "layer   2  Sparsity: 76.7461%\n",
      "layer   3  Sparsity: 75.7729%\n",
      "total_backward_count 841940 real_backward_count 121414  14.421%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.468939/  1.851844, val:  45.83%, val_best:  57.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8732%\n",
      "layer   2  Sparsity: 76.1526%\n",
      "layer   3  Sparsity: 77.6707%\n",
      "total_backward_count 851730 real_backward_count 122767  14.414%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.467376/  1.774655, val:  45.83%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9033%\n",
      "layer   2  Sparsity: 76.4956%\n",
      "layer   3  Sparsity: 77.5672%\n",
      "total_backward_count 861520 real_backward_count 124146  14.410%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.382177/  1.833038, val:  46.25%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9131%\n",
      "layer   2  Sparsity: 76.1167%\n",
      "layer   3  Sparsity: 76.3858%\n",
      "total_backward_count 871310 real_backward_count 125491  14.403%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.411787/  1.709926, val:  52.50%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8892%\n",
      "layer   2  Sparsity: 76.3062%\n",
      "layer   3  Sparsity: 76.0280%\n",
      "total_backward_count 881100 real_backward_count 126880  14.400%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.456538/  1.804659, val:  48.33%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8884%\n",
      "layer   2  Sparsity: 76.5484%\n",
      "layer   3  Sparsity: 77.8947%\n",
      "total_backward_count 890890 real_backward_count 128325  14.404%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.417149/  1.746827, val:  52.92%, val_best:  57.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8757%\n",
      "layer   2  Sparsity: 76.6787%\n",
      "layer   3  Sparsity: 76.8116%\n",
      "total_backward_count 900680 real_backward_count 129719  14.402%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.437286/  1.875342, val:  40.42%, val_best:  57.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8878%\n",
      "layer   2  Sparsity: 75.7156%\n",
      "layer   3  Sparsity: 77.7778%\n",
      "total_backward_count 910470 real_backward_count 131185  14.408%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.491287/  1.866848, val:  46.25%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8896%\n",
      "layer   2  Sparsity: 76.4228%\n",
      "layer   3  Sparsity: 78.0468%\n",
      "total_backward_count 920260 real_backward_count 132654  14.415%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.446738/  1.821150, val:  51.25%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8918%\n",
      "layer   2  Sparsity: 76.3945%\n",
      "layer   3  Sparsity: 76.9929%\n",
      "total_backward_count 930050 real_backward_count 134032  14.411%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.402609/  1.854272, val:  49.58%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8702%\n",
      "layer   2  Sparsity: 76.8968%\n",
      "layer   3  Sparsity: 76.8671%\n",
      "total_backward_count 939840 real_backward_count 135409  14.408%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.479687/  1.877933, val:  43.33%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8789%\n",
      "layer   2  Sparsity: 76.4287%\n",
      "layer   3  Sparsity: 77.6388%\n",
      "total_backward_count 949630 real_backward_count 136811  14.407%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.446829/  1.810377, val:  39.17%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8824%\n",
      "layer   2  Sparsity: 76.7432%\n",
      "layer   3  Sparsity: 77.3883%\n",
      "total_backward_count 959420 real_backward_count 138232  14.408%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.488391/  1.883075, val:  35.83%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8773%\n",
      "layer   2  Sparsity: 76.8633%\n",
      "layer   3  Sparsity: 79.4906%\n",
      "total_backward_count 969210 real_backward_count 139625  14.406%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.414915/  1.764252, val:  55.00%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8926%\n",
      "layer   2  Sparsity: 77.0627%\n",
      "layer   3  Sparsity: 75.6442%\n",
      "total_backward_count 979000 real_backward_count 140976  14.400%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.395784/  1.771765, val:  42.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9030%\n",
      "layer   2  Sparsity: 76.7584%\n",
      "layer   3  Sparsity: 76.1462%\n",
      "total_backward_count 988790 real_backward_count 142321  14.393%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.400038/  1.776226, val:  50.00%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8891%\n",
      "layer   2  Sparsity: 76.8304%\n",
      "layer   3  Sparsity: 75.7973%\n",
      "total_backward_count 998580 real_backward_count 143673  14.388%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.367755/  1.807718, val:  36.67%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8901%\n",
      "layer   2  Sparsity: 76.9630%\n",
      "layer   3  Sparsity: 75.2782%\n",
      "total_backward_count 1008370 real_backward_count 145007  14.380%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.356811/  1.729224, val:  49.17%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8898%\n",
      "layer   2  Sparsity: 76.6518%\n",
      "layer   3  Sparsity: 74.5924%\n",
      "total_backward_count 1018160 real_backward_count 146334  14.372%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.412118/  1.808975, val:  56.67%, val_best:  57.92%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9064%\n",
      "layer   2  Sparsity: 76.0642%\n",
      "layer   3  Sparsity: 76.8860%\n",
      "total_backward_count 1027950 real_backward_count 147748  14.373%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.412408/  1.759902, val:  45.42%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9005%\n",
      "layer   2  Sparsity: 76.2474%\n",
      "layer   3  Sparsity: 76.0434%\n",
      "total_backward_count 1037740 real_backward_count 149168  14.374%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.297583/  1.786872, val:  43.75%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8856%\n",
      "layer   2  Sparsity: 77.0074%\n",
      "layer   3  Sparsity: 74.4805%\n",
      "total_backward_count 1047530 real_backward_count 150438  14.361%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.362617/  1.736893, val:  42.08%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8936%\n",
      "layer   2  Sparsity: 77.2999%\n",
      "layer   3  Sparsity: 74.6439%\n",
      "total_backward_count 1057320 real_backward_count 151817  14.359%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.357473/  1.849688, val:  42.08%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8701%\n",
      "layer   2  Sparsity: 76.4214%\n",
      "layer   3  Sparsity: 75.6037%\n",
      "total_backward_count 1067110 real_backward_count 153288  14.365%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.423257/  1.819730, val:  48.33%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8832%\n",
      "layer   2  Sparsity: 76.2557%\n",
      "layer   3  Sparsity: 78.6340%\n",
      "total_backward_count 1076900 real_backward_count 154695  14.365%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.494631/  1.777105, val:  47.92%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9084%\n",
      "layer   2  Sparsity: 77.1076%\n",
      "layer   3  Sparsity: 79.3710%\n",
      "total_backward_count 1086690 real_backward_count 156138  14.368%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.413826/  1.810565, val:  45.42%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8821%\n",
      "layer   2  Sparsity: 76.6316%\n",
      "layer   3  Sparsity: 77.0149%\n",
      "total_backward_count 1096480 real_backward_count 157490  14.363%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.418115/  1.766779, val:  55.83%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9114%\n",
      "layer   2  Sparsity: 76.6690%\n",
      "layer   3  Sparsity: 76.6161%\n",
      "total_backward_count 1106270 real_backward_count 158905  14.364%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.436449/  1.852438, val:  36.67%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8882%\n",
      "layer   2  Sparsity: 77.4117%\n",
      "layer   3  Sparsity: 77.5795%\n",
      "total_backward_count 1116060 real_backward_count 160220  14.356%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.441431/  1.899428, val:  35.00%, val_best:  57.92%, tr:  98.67%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8910%\n",
      "layer   2  Sparsity: 75.7041%\n",
      "layer   3  Sparsity: 75.7831%\n",
      "total_backward_count 1125850 real_backward_count 161606  14.354%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.437505/  1.849669, val:  39.58%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8843%\n",
      "layer   2  Sparsity: 76.0731%\n",
      "layer   3  Sparsity: 77.1909%\n",
      "total_backward_count 1135640 real_backward_count 163041  14.357%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.439306/  1.900573, val:  42.92%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8775%\n",
      "layer   2  Sparsity: 75.8723%\n",
      "layer   3  Sparsity: 78.0626%\n",
      "total_backward_count 1145430 real_backward_count 164431  14.355%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.489033/  1.793675, val:  51.25%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8981%\n",
      "layer   2  Sparsity: 76.8069%\n",
      "layer   3  Sparsity: 78.5490%\n",
      "total_backward_count 1155220 real_backward_count 165850  14.357%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.429534/  1.822108, val:  49.17%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8941%\n",
      "layer   2  Sparsity: 76.8578%\n",
      "layer   3  Sparsity: 76.8038%\n",
      "total_backward_count 1165010 real_backward_count 167193  14.351%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.440435/  1.860704, val:  49.58%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9110%\n",
      "layer   2  Sparsity: 76.8333%\n",
      "layer   3  Sparsity: 78.3338%\n",
      "total_backward_count 1174800 real_backward_count 168560  14.348%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.443297/  1.777508, val:  54.17%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8851%\n",
      "layer   2  Sparsity: 76.4467%\n",
      "layer   3  Sparsity: 76.5492%\n",
      "total_backward_count 1184590 real_backward_count 169927  14.345%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.466615/  1.840335, val:  50.00%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8969%\n",
      "layer   2  Sparsity: 76.3836%\n",
      "layer   3  Sparsity: 78.2503%\n",
      "total_backward_count 1194380 real_backward_count 171388  14.350%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.381908/  1.761747, val:  46.25%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9013%\n",
      "layer   2  Sparsity: 76.1181%\n",
      "layer   3  Sparsity: 74.9751%\n",
      "total_backward_count 1204170 real_backward_count 172774  14.348%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.461749/  1.783326, val:  51.67%, val_best:  57.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8864%\n",
      "layer   2  Sparsity: 76.2510%\n",
      "layer   3  Sparsity: 77.8938%\n",
      "total_backward_count 1213960 real_backward_count 174216  14.351%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.471932/  1.734367, val:  50.00%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8981%\n",
      "layer   2  Sparsity: 75.9640%\n",
      "layer   3  Sparsity: 77.1381%\n",
      "total_backward_count 1223750 real_backward_count 175640  14.353%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.451347/  1.752067, val:  45.00%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9018%\n",
      "layer   2  Sparsity: 75.8275%\n",
      "layer   3  Sparsity: 76.6083%\n",
      "total_backward_count 1233540 real_backward_count 177033  14.352%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.374058/  1.824206, val:  44.17%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9125%\n",
      "layer   2  Sparsity: 77.1654%\n",
      "layer   3  Sparsity: 75.0741%\n",
      "total_backward_count 1243330 real_backward_count 178312  14.341%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.426965/  1.760336, val:  51.67%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8893%\n",
      "layer   2  Sparsity: 76.5764%\n",
      "layer   3  Sparsity: 76.5886%\n",
      "total_backward_count 1253120 real_backward_count 179603  14.332%\n",
      "fc layer 3 self.abs_max_out: 3960.0\n",
      "fc layer 3 self.abs_max_out: 4018.0\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.431767/  1.813030, val:  45.83%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8987%\n",
      "layer   2  Sparsity: 76.0407%\n",
      "layer   3  Sparsity: 77.6769%\n",
      "total_backward_count 1262910 real_backward_count 180961  14.329%\n",
      "fc layer 3 self.abs_max_out: 4039.0\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.400095/  1.829024, val:  44.17%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8811%\n",
      "layer   2  Sparsity: 76.2779%\n",
      "layer   3  Sparsity: 76.7257%\n",
      "total_backward_count 1272700 real_backward_count 182360  14.329%\n",
      "fc layer 2 self.abs_max_out: 8341.0\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.347758/  1.737964, val:  44.17%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9015%\n",
      "layer   2  Sparsity: 75.8548%\n",
      "layer   3  Sparsity: 76.7584%\n",
      "total_backward_count 1282490 real_backward_count 183698  14.324%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.397508/  1.816011, val:  50.00%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9064%\n",
      "layer   2  Sparsity: 76.4186%\n",
      "layer   3  Sparsity: 76.6703%\n",
      "total_backward_count 1292280 real_backward_count 185103  14.324%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.386100/  1.718255, val:  55.42%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8896%\n",
      "layer   2  Sparsity: 76.4322%\n",
      "layer   3  Sparsity: 75.1111%\n",
      "total_backward_count 1302070 real_backward_count 186390  14.315%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.418158/  1.864746, val:  43.75%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8884%\n",
      "layer   2  Sparsity: 75.9470%\n",
      "layer   3  Sparsity: 76.2790%\n",
      "total_backward_count 1311860 real_backward_count 187776  14.314%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.418402/  1.836575, val:  43.75%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9042%\n",
      "layer   2  Sparsity: 76.0020%\n",
      "layer   3  Sparsity: 75.9251%\n",
      "total_backward_count 1321650 real_backward_count 189192  14.315%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.361275/  1.742573, val:  57.08%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8963%\n",
      "layer   2  Sparsity: 75.7690%\n",
      "layer   3  Sparsity: 73.5235%\n",
      "total_backward_count 1331440 real_backward_count 190637  14.318%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.435062/  1.852458, val:  45.00%, val_best:  57.92%, tr:  98.77%, tr_best:  99.90%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9058%\n",
      "layer   2  Sparsity: 76.5518%\n",
      "layer   3  Sparsity: 78.3616%\n",
      "total_backward_count 1341230 real_backward_count 192069  14.320%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.432893/  1.770855, val:  45.00%, val_best:  57.92%, tr:  98.57%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 76.2861%\n",
      "layer   3  Sparsity: 75.6750%\n",
      "total_backward_count 1351020 real_backward_count 193458  14.319%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.416766/  1.745590, val:  47.92%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8999%\n",
      "layer   2  Sparsity: 77.1454%\n",
      "layer   3  Sparsity: 77.0469%\n",
      "total_backward_count 1360810 real_backward_count 194873  14.320%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.459549/  1.836431, val:  45.83%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9020%\n",
      "layer   2  Sparsity: 76.0313%\n",
      "layer   3  Sparsity: 78.0738%\n",
      "total_backward_count 1370600 real_backward_count 196256  14.319%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.461024/  1.781628, val:  49.58%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8870%\n",
      "layer   2  Sparsity: 75.6498%\n",
      "layer   3  Sparsity: 77.5534%\n",
      "total_backward_count 1380390 real_backward_count 197683  14.321%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.463231/  1.780568, val:  56.25%, val_best:  57.92%, tr:  98.88%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9032%\n",
      "layer   2  Sparsity: 75.6025%\n",
      "layer   3  Sparsity: 76.7050%\n",
      "total_backward_count 1390180 real_backward_count 199132  14.324%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.450963/  1.789413, val:  56.25%, val_best:  57.92%, tr:  98.77%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8980%\n",
      "layer   2  Sparsity: 76.0654%\n",
      "layer   3  Sparsity: 77.3165%\n",
      "total_backward_count 1399970 real_backward_count 200532  14.324%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.413655/  1.880177, val:  34.58%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8760%\n",
      "layer   2  Sparsity: 75.4968%\n",
      "layer   3  Sparsity: 76.3751%\n",
      "total_backward_count 1409760 real_backward_count 201928  14.324%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.381461/  1.710120, val:  52.50%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 76.0237%\n",
      "layer   3  Sparsity: 75.9905%\n",
      "total_backward_count 1419550 real_backward_count 203321  14.323%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.447250/  1.799940, val:  39.58%, val_best:  57.92%, tr:  98.67%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8979%\n",
      "layer   2  Sparsity: 76.4231%\n",
      "layer   3  Sparsity: 76.7763%\n",
      "total_backward_count 1429340 real_backward_count 204763  14.326%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.462243/  1.877079, val:  42.92%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8902%\n",
      "layer   2  Sparsity: 76.1840%\n",
      "layer   3  Sparsity: 76.4497%\n",
      "total_backward_count 1439130 real_backward_count 206157  14.325%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.437846/  1.881710, val:  40.42%, val_best:  57.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8936%\n",
      "layer   2  Sparsity: 76.2144%\n",
      "layer   3  Sparsity: 75.9189%\n",
      "total_backward_count 1448920 real_backward_count 207539  14.324%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.422557/  1.786966, val:  48.33%, val_best:  57.92%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8954%\n",
      "layer   2  Sparsity: 75.8299%\n",
      "layer   3  Sparsity: 75.0849%\n",
      "total_backward_count 1458710 real_backward_count 208972  14.326%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.451119/  1.774773, val:  49.17%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8980%\n",
      "layer   2  Sparsity: 76.4232%\n",
      "layer   3  Sparsity: 76.1240%\n",
      "total_backward_count 1468500 real_backward_count 210339  14.323%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.365885/  1.692564, val:  49.17%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8930%\n",
      "layer   2  Sparsity: 76.8945%\n",
      "layer   3  Sparsity: 74.6374%\n",
      "total_backward_count 1478290 real_backward_count 211644  14.317%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.383152/  1.839722, val:  32.50%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 75.7258%\n",
      "layer   3  Sparsity: 75.2501%\n",
      "total_backward_count 1488080 real_backward_count 213072  14.319%\n",
      "fc layer 2 self.abs_max_out: 8357.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.404013/  1.855473, val:  49.17%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8876%\n",
      "layer   2  Sparsity: 75.8928%\n",
      "layer   3  Sparsity: 76.2898%\n",
      "total_backward_count 1497870 real_backward_count 214495  14.320%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.391643/  1.700604, val:  59.58%, val_best:  59.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9041%\n",
      "layer   2  Sparsity: 75.7321%\n",
      "layer   3  Sparsity: 75.5618%\n",
      "total_backward_count 1507660 real_backward_count 215890  14.320%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.365446/  1.790613, val:  53.33%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9013%\n",
      "layer   2  Sparsity: 75.3032%\n",
      "layer   3  Sparsity: 75.0945%\n",
      "total_backward_count 1517450 real_backward_count 217194  14.313%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.445894/  1.814212, val:  54.17%, val_best:  59.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8789%\n",
      "layer   2  Sparsity: 76.6174%\n",
      "layer   3  Sparsity: 76.5779%\n",
      "total_backward_count 1527240 real_backward_count 218541  14.310%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.430954/  1.794327, val:  39.58%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9048%\n",
      "layer   2  Sparsity: 76.6853%\n",
      "layer   3  Sparsity: 75.7958%\n",
      "total_backward_count 1537030 real_backward_count 219874  14.305%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.371722/  1.746032, val:  50.00%, val_best:  59.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8799%\n",
      "layer   2  Sparsity: 75.6546%\n",
      "layer   3  Sparsity: 74.8653%\n",
      "total_backward_count 1546820 real_backward_count 221233  14.302%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.374759/  1.906482, val:  36.67%, val_best:  59.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8899%\n",
      "layer   2  Sparsity: 75.2802%\n",
      "layer   3  Sparsity: 75.6413%\n",
      "total_backward_count 1556610 real_backward_count 222574  14.299%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.386688/  1.736550, val:  54.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8941%\n",
      "layer   2  Sparsity: 75.7575%\n",
      "layer   3  Sparsity: 76.1288%\n",
      "total_backward_count 1566400 real_backward_count 223949  14.297%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.367568/  1.784837, val:  57.08%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8932%\n",
      "layer   2  Sparsity: 76.3347%\n",
      "layer   3  Sparsity: 74.7326%\n",
      "total_backward_count 1576190 real_backward_count 225304  14.294%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.386474/  1.787718, val:  52.50%, val_best:  59.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8840%\n",
      "layer   2  Sparsity: 75.6409%\n",
      "layer   3  Sparsity: 74.3791%\n",
      "total_backward_count 1585980 real_backward_count 226687  14.293%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.421301/  1.770722, val:  42.08%, val_best:  59.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8919%\n",
      "layer   2  Sparsity: 75.8107%\n",
      "layer   3  Sparsity: 75.2684%\n",
      "total_backward_count 1595770 real_backward_count 228081  14.293%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.360617/  1.883852, val:  41.67%, val_best:  59.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8944%\n",
      "layer   2  Sparsity: 76.1870%\n",
      "layer   3  Sparsity: 75.0281%\n",
      "total_backward_count 1605560 real_backward_count 229367  14.286%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.389876/  1.812709, val:  45.00%, val_best:  59.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8724%\n",
      "layer   2  Sparsity: 75.5224%\n",
      "layer   3  Sparsity: 76.0508%\n",
      "total_backward_count 1615350 real_backward_count 230799  14.288%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.348724/  1.776073, val:  47.92%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8830%\n",
      "layer   2  Sparsity: 75.9115%\n",
      "layer   3  Sparsity: 74.3524%\n",
      "total_backward_count 1625140 real_backward_count 232102  14.282%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.377634/  1.812771, val:  46.67%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8795%\n",
      "layer   2  Sparsity: 76.0733%\n",
      "layer   3  Sparsity: 75.4942%\n",
      "total_backward_count 1634930 real_backward_count 233451  14.279%\n",
      "fc layer 2 self.abs_max_out: 8457.0\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.376293/  1.901515, val:  38.33%, val_best:  59.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8969%\n",
      "layer   2  Sparsity: 74.9260%\n",
      "layer   3  Sparsity: 74.1097%\n",
      "total_backward_count 1644720 real_backward_count 234770  14.274%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.433956/  1.841711, val:  39.17%, val_best:  59.58%, tr:  98.88%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9165%\n",
      "layer   2  Sparsity: 75.5616%\n",
      "layer   3  Sparsity: 77.0834%\n",
      "total_backward_count 1654510 real_backward_count 236189  14.275%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.400415/  1.681460, val:  53.75%, val_best:  59.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9079%\n",
      "layer   2  Sparsity: 74.9339%\n",
      "layer   3  Sparsity: 75.7180%\n",
      "total_backward_count 1664300 real_backward_count 237532  14.272%\n",
      "fc layer 2 self.abs_max_out: 8543.0\n",
      "fc layer 2 self.abs_max_out: 8598.0\n",
      "lif layer 2 self.abs_max_v: 16161.0\n",
      "lif layer 2 self.abs_max_v: 16351.5\n",
      "lif layer 2 self.abs_max_v: 16623.0\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.458490/  1.769693, val:  44.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9060%\n",
      "layer   2  Sparsity: 75.7089%\n",
      "layer   3  Sparsity: 76.4936%\n",
      "total_backward_count 1674090 real_backward_count 238859  14.268%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.410954/  1.808983, val:  46.25%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9159%\n",
      "layer   2  Sparsity: 76.4441%\n",
      "layer   3  Sparsity: 76.4993%\n",
      "total_backward_count 1683880 real_backward_count 240225  14.266%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.460436/  1.760960, val:  49.17%, val_best:  59.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9124%\n",
      "layer   2  Sparsity: 75.8993%\n",
      "layer   3  Sparsity: 77.1498%\n",
      "total_backward_count 1693670 real_backward_count 241637  14.267%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.434541/  1.852084, val:  45.00%, val_best:  59.58%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8983%\n",
      "layer   2  Sparsity: 76.0418%\n",
      "layer   3  Sparsity: 76.5919%\n",
      "total_backward_count 1703460 real_backward_count 242951  14.262%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.382061/  1.744857, val:  51.25%, val_best:  59.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8894%\n",
      "layer   2  Sparsity: 74.8110%\n",
      "layer   3  Sparsity: 74.3612%\n",
      "total_backward_count 1713250 real_backward_count 244201  14.254%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.406286/  1.760105, val:  60.42%, val_best:  60.42%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8764%\n",
      "layer   2  Sparsity: 75.2185%\n",
      "layer   3  Sparsity: 75.2559%\n",
      "total_backward_count 1723040 real_backward_count 245629  14.256%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.409182/  1.777694, val:  43.33%, val_best:  60.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9005%\n",
      "layer   2  Sparsity: 76.2117%\n",
      "layer   3  Sparsity: 75.9676%\n",
      "total_backward_count 1732830 real_backward_count 246983  14.253%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.441024/  1.837481, val:  37.50%, val_best:  60.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8924%\n",
      "layer   2  Sparsity: 75.6740%\n",
      "layer   3  Sparsity: 76.5840%\n",
      "total_backward_count 1742620 real_backward_count 248349  14.251%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.379648/  1.903929, val:  36.67%, val_best:  60.42%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9066%\n",
      "layer   2  Sparsity: 75.1866%\n",
      "layer   3  Sparsity: 74.8146%\n",
      "total_backward_count 1752410 real_backward_count 249714  14.250%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.356546/  1.806732, val:  47.92%, val_best:  60.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8976%\n",
      "layer   2  Sparsity: 75.6018%\n",
      "layer   3  Sparsity: 75.1814%\n",
      "total_backward_count 1762200 real_backward_count 251047  14.246%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.368417/  1.806373, val:  46.25%, val_best:  60.42%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9007%\n",
      "layer   2  Sparsity: 75.3600%\n",
      "layer   3  Sparsity: 73.8954%\n",
      "total_backward_count 1771990 real_backward_count 252402  14.244%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.383415/  1.780982, val:  56.25%, val_best:  60.42%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8914%\n",
      "layer   2  Sparsity: 75.7374%\n",
      "layer   3  Sparsity: 75.0414%\n",
      "total_backward_count 1781780 real_backward_count 253732  14.240%\n",
      "fc layer 2 self.abs_max_out: 8701.0\n",
      "fc layer 3 self.abs_max_out: 4123.0\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.395316/  1.804933, val:  41.25%, val_best:  60.42%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8847%\n",
      "layer   2  Sparsity: 75.5471%\n",
      "layer   3  Sparsity: 75.7441%\n",
      "total_backward_count 1791570 real_backward_count 255052  14.236%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.413725/  1.950052, val:  34.17%, val_best:  60.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9005%\n",
      "layer   2  Sparsity: 75.4113%\n",
      "layer   3  Sparsity: 77.0249%\n",
      "total_backward_count 1801360 real_backward_count 256441  14.236%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.415932/  1.837951, val:  42.92%, val_best:  60.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9051%\n",
      "layer   2  Sparsity: 75.0698%\n",
      "layer   3  Sparsity: 76.7161%\n",
      "total_backward_count 1811150 real_backward_count 257809  14.235%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.444779/  1.800634, val:  39.58%, val_best:  60.42%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8876%\n",
      "layer   2  Sparsity: 75.8057%\n",
      "layer   3  Sparsity: 77.8900%\n",
      "total_backward_count 1820940 real_backward_count 259207  14.235%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.498024/  1.760370, val:  52.92%, val_best:  60.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8968%\n",
      "layer   2  Sparsity: 75.4877%\n",
      "layer   3  Sparsity: 79.1576%\n",
      "total_backward_count 1830730 real_backward_count 260624  14.236%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.470480/  1.866959, val:  33.75%, val_best:  60.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8977%\n",
      "layer   2  Sparsity: 75.1701%\n",
      "layer   3  Sparsity: 77.4729%\n",
      "total_backward_count 1840520 real_backward_count 262035  14.237%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.464622/  1.793440, val:  60.83%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8861%\n",
      "layer   2  Sparsity: 75.2489%\n",
      "layer   3  Sparsity: 75.7205%\n",
      "total_backward_count 1850310 real_backward_count 263373  14.234%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.369344/  1.807019, val:  40.00%, val_best:  60.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9014%\n",
      "layer   2  Sparsity: 74.6038%\n",
      "layer   3  Sparsity: 74.4434%\n",
      "total_backward_count 1860100 real_backward_count 264677  14.229%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.397944/  1.709723, val:  52.92%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9021%\n",
      "layer   2  Sparsity: 74.9269%\n",
      "layer   3  Sparsity: 75.1376%\n",
      "total_backward_count 1869890 real_backward_count 266029  14.227%\n",
      "fc layer 3 self.abs_max_out: 4148.0\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.351961/  1.824136, val:  41.67%, val_best:  60.83%, tr:  98.67%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8909%\n",
      "layer   2  Sparsity: 74.9911%\n",
      "layer   3  Sparsity: 74.2402%\n",
      "total_backward_count 1879680 real_backward_count 267404  14.226%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.418106/  1.786273, val:  44.58%, val_best:  60.83%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9125%\n",
      "layer   2  Sparsity: 75.0728%\n",
      "layer   3  Sparsity: 76.9860%\n",
      "total_backward_count 1889470 real_backward_count 268796  14.226%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.388163/  1.810133, val:  45.00%, val_best:  60.83%, tr:  98.47%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8877%\n",
      "layer   2  Sparsity: 75.0256%\n",
      "layer   3  Sparsity: 77.0024%\n",
      "total_backward_count 1899260 real_backward_count 270113  14.222%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.460347/  1.865133, val:  41.25%, val_best:  60.83%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8836%\n",
      "layer   2  Sparsity: 74.8728%\n",
      "layer   3  Sparsity: 78.9290%\n",
      "total_backward_count 1909050 real_backward_count 271511  14.222%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.399887/  1.834152, val:  44.58%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8839%\n",
      "layer   2  Sparsity: 74.5804%\n",
      "layer   3  Sparsity: 77.0630%\n",
      "total_backward_count 1918840 real_backward_count 272931  14.224%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.388731/  1.711381, val:  50.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8905%\n",
      "layer   2  Sparsity: 75.1847%\n",
      "layer   3  Sparsity: 74.2988%\n",
      "total_backward_count 1928630 real_backward_count 274294  14.222%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.385566/  1.755025, val:  49.58%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9073%\n",
      "layer   2  Sparsity: 75.3005%\n",
      "layer   3  Sparsity: 75.7170%\n",
      "total_backward_count 1938420 real_backward_count 275560  14.216%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.399121/  1.822218, val:  37.92%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8858%\n",
      "layer   2  Sparsity: 75.7984%\n",
      "layer   3  Sparsity: 75.8046%\n",
      "total_backward_count 1948210 real_backward_count 276863  14.211%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.326780/  1.725329, val:  46.25%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8973%\n",
      "layer   2  Sparsity: 75.0858%\n",
      "layer   3  Sparsity: 73.2373%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640f078c5bca4455b88fd5f7dd30d9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÑ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÑ</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99489</td></tr><tr><td>tr_epoch_loss</td><td>1.32678</td></tr><tr><td>val_acc_best</td><td>0.60833</td></tr><tr><td>val_acc_now</td><td>0.4625</td></tr><tr><td>val_loss</td><td>1.72533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-48</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozjc3v9l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozjc3v9l</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_202845-ozjc3v9l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gx72vp7e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_004815-gx72vp7e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gx72vp7e' target=\"_blank\">tough-sweep-55</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gx72vp7e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gx72vp7e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251115_004824_923', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 111.0\n",
      "lif layer 1 self.abs_max_v: 111.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 206.0\n",
      "lif layer 2 self.abs_max_v: 206.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 1 self.abs_max_out: 148.0\n",
      "lif layer 1 self.abs_max_v: 184.0\n",
      "fc layer 2 self.abs_max_out: 429.0\n",
      "lif layer 2 self.abs_max_v: 455.5\n",
      "fc layer 1 self.abs_max_out: 162.0\n",
      "lif layer 1 self.abs_max_v: 252.0\n",
      "lif layer 2 self.abs_max_v: 462.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 1 self.abs_max_out: 174.0\n",
      "lif layer 2 self.abs_max_v: 474.0\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "lif layer 2 self.abs_max_v: 520.0\n",
      "fc layer 1 self.abs_max_out: 196.0\n",
      "fc layer 1 self.abs_max_out: 262.0\n",
      "lif layer 1 self.abs_max_v: 281.5\n",
      "lif layer 2 self.abs_max_v: 547.0\n",
      "lif layer 1 self.abs_max_v: 329.5\n",
      "lif layer 2 self.abs_max_v: 610.5\n",
      "fc layer 2 self.abs_max_out: 431.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 1 self.abs_max_out: 268.0\n",
      "fc layer 2 self.abs_max_out: 441.0\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 285.0\n",
      "lif layer 1 self.abs_max_v: 381.5\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 387.0\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "lif layer 2 self.abs_max_v: 647.0\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 428.5\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "lif layer 2 self.abs_max_v: 697.5\n",
      "fc layer 1 self.abs_max_out: 455.0\n",
      "lif layer 1 self.abs_max_v: 478.5\n",
      "lif layer 2 self.abs_max_v: 728.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 2 self.abs_max_out: 605.0\n",
      "fc layer 1 self.abs_max_out: 516.0\n",
      "lif layer 1 self.abs_max_v: 516.0\n",
      "lif layer 2 self.abs_max_v: 761.5\n",
      "lif layer 2 self.abs_max_v: 804.5\n",
      "lif layer 2 self.abs_max_v: 865.5\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "fc layer 1 self.abs_max_out: 535.0\n",
      "lif layer 1 self.abs_max_v: 606.5\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "lif layer 1 self.abs_max_v: 618.0\n",
      "lif layer 2 self.abs_max_v: 969.5\n",
      "lif layer 2 self.abs_max_v: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1115.0\n",
      "lif layer 1 self.abs_max_v: 636.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 1 self.abs_max_out: 641.0\n",
      "lif layer 1 self.abs_max_v: 662.5\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "lif layer 1 self.abs_max_v: 685.5\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 1 self.abs_max_out: 659.0\n",
      "lif layer 2 self.abs_max_v: 1147.0\n",
      "fc layer 1 self.abs_max_out: 718.0\n",
      "lif layer 1 self.abs_max_v: 718.0\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "lif layer 1 self.abs_max_v: 752.0\n",
      "fc layer 1 self.abs_max_out: 802.0\n",
      "lif layer 1 self.abs_max_v: 802.0\n",
      "lif layer 1 self.abs_max_v: 838.0\n",
      "lif layer 2 self.abs_max_v: 1159.5\n",
      "lif layer 2 self.abs_max_v: 1304.0\n",
      "lif layer 1 self.abs_max_v: 889.5\n",
      "fc layer 2 self.abs_max_out: 844.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 1355.0\n",
      "lif layer 2 self.abs_max_v: 1372.5\n",
      "lif layer 2 self.abs_max_v: 1415.5\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 1 self.abs_max_out: 838.0\n",
      "lif layer 2 self.abs_max_v: 1488.5\n",
      "lif layer 2 self.abs_max_v: 1572.5\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 2 self.abs_max_out: 938.0\n",
      "lif layer 2 self.abs_max_v: 1654.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 1 self.abs_max_v: 891.5\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "lif layer 1 self.abs_max_v: 914.0\n",
      "fc layer 2 self.abs_max_out: 970.0\n",
      "lif layer 1 self.abs_max_v: 956.5\n",
      "lif layer 2 self.abs_max_v: 1670.5\n",
      "lif layer 1 self.abs_max_v: 1043.0\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "lif layer 2 self.abs_max_v: 1692.5\n",
      "lif layer 2 self.abs_max_v: 1768.0\n",
      "fc layer 1 self.abs_max_out: 841.0\n",
      "fc layer 1 self.abs_max_out: 861.0\n",
      "fc layer 1 self.abs_max_out: 874.0\n",
      "fc layer 1 self.abs_max_out: 877.0\n",
      "fc layer 1 self.abs_max_out: 1080.0\n",
      "lif layer 1 self.abs_max_v: 1080.0\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "fc layer 2 self.abs_max_out: 1047.0\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "fc layer 1 self.abs_max_out: 1191.0\n",
      "lif layer 1 self.abs_max_v: 1191.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 1094.0\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1173.0\n",
      "lif layer 1 self.abs_max_v: 1256.0\n",
      "fc layer 2 self.abs_max_out: 1248.0\n",
      "lif layer 2 self.abs_max_v: 1952.5\n",
      "lif layer 2 self.abs_max_v: 2001.5\n",
      "fc layer 2 self.abs_max_out: 1254.0\n",
      "lif layer 1 self.abs_max_v: 1521.0\n",
      "lif layer 2 self.abs_max_v: 2196.5\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "lif layer 2 self.abs_max_v: 2363.5\n",
      "lif layer 2 self.abs_max_v: 2421.0\n",
      "lif layer 2 self.abs_max_v: 2489.5\n",
      "lif layer 2 self.abs_max_v: 2577.0\n",
      "fc layer 1 self.abs_max_out: 1294.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 1585.5\n",
      "lif layer 1 self.abs_max_v: 1778.0\n",
      "fc layer 2 self.abs_max_out: 1488.0\n",
      "fc layer 1 self.abs_max_out: 1409.0\n",
      "lif layer 1 self.abs_max_v: 1842.0\n",
      "fc layer 2 self.abs_max_out: 1537.0\n",
      "fc layer 2 self.abs_max_out: 1542.0\n",
      "fc layer 1 self.abs_max_out: 1493.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "lif layer 1 self.abs_max_v: 1904.5\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "fc layer 2 self.abs_max_out: 1603.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1681.0\n",
      "fc layer 2 self.abs_max_out: 1721.0\n",
      "lif layer 2 self.abs_max_v: 2587.5\n",
      "lif layer 2 self.abs_max_v: 2636.0\n",
      "fc layer 1 self.abs_max_out: 1601.0\n",
      "lif layer 1 self.abs_max_v: 1999.0\n",
      "lif layer 2 self.abs_max_v: 2644.5\n",
      "lif layer 2 self.abs_max_v: 2654.5\n",
      "lif layer 1 self.abs_max_v: 2189.5\n",
      "lif layer 2 self.abs_max_v: 2679.5\n",
      "fc layer 2 self.abs_max_out: 1746.0\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 1 self.abs_max_out: 1607.0\n",
      "lif layer 2 self.abs_max_v: 2713.5\n",
      "lif layer 2 self.abs_max_v: 2725.5\n",
      "lif layer 2 self.abs_max_v: 2830.0\n",
      "lif layer 2 self.abs_max_v: 2912.5\n",
      "lif layer 2 self.abs_max_v: 2920.5\n",
      "lif layer 2 self.abs_max_v: 2988.5\n",
      "lif layer 2 self.abs_max_v: 3100.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "lif layer 1 self.abs_max_v: 2228.5\n",
      "fc layer 1 self.abs_max_out: 1808.0\n",
      "fc layer 1 self.abs_max_out: 1845.0\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "lif layer 1 self.abs_max_v: 2317.0\n",
      "fc layer 2 self.abs_max_out: 1830.0\n",
      "lif layer 1 self.abs_max_v: 2504.0\n",
      "fc layer 1 self.abs_max_out: 1967.0\n",
      "lif layer 1 self.abs_max_v: 2612.0\n",
      "fc layer 1 self.abs_max_out: 2026.0\n",
      "lif layer 1 self.abs_max_v: 3136.0\n",
      "fc layer 1 self.abs_max_out: 2037.0\n",
      "lif layer 1 self.abs_max_v: 3384.0\n",
      "fc layer 1 self.abs_max_out: 2227.0\n",
      "lif layer 1 self.abs_max_v: 3770.0\n",
      "lif layer 1 self.abs_max_v: 3897.0\n",
      "lif layer 1 self.abs_max_v: 3920.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.696410/  2.068415, val:  30.83%, val_best:  30.83%, tr:  98.47%, tr_best:  98.47%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 68.9794%\n",
      "layer   3  Sparsity: 61.2672%\n",
      "total_backward_count 9790 real_backward_count 1861  19.009%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 459.0\n",
      "fc layer 2 self.abs_max_out: 1851.0\n",
      "fc layer 2 self.abs_max_out: 1859.0\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "fc layer 2 self.abs_max_out: 1927.0\n",
      "fc layer 2 self.abs_max_out: 1948.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 2 self.abs_max_out: 2015.0\n",
      "lif layer 2 self.abs_max_v: 3108.0\n",
      "lif layer 2 self.abs_max_v: 3211.0\n",
      "lif layer 2 self.abs_max_v: 3313.5\n",
      "lif layer 2 self.abs_max_v: 3389.0\n",
      "lif layer 2 self.abs_max_v: 3542.5\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 3652.5\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "fc layer 1 self.abs_max_out: 2336.0\n",
      "lif layer 1 self.abs_max_v: 4150.0\n",
      "lif layer 1 self.abs_max_v: 4314.0\n",
      "fc layer 1 self.abs_max_out: 2425.0\n",
      "lif layer 1 self.abs_max_v: 4350.5\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "fc layer 2 self.abs_max_out: 2096.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.611882/  2.012786, val:  28.33%, val_best:  30.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0042%\n",
      "layer   2  Sparsity: 69.0493%\n",
      "layer   3  Sparsity: 62.5961%\n",
      "total_backward_count 19580 real_backward_count 3406  17.395%\n",
      "fc layer 2 self.abs_max_out: 2160.0\n",
      "lif layer 2 self.abs_max_v: 3723.5\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 2332.0\n",
      "lif layer 2 self.abs_max_v: 3824.5\n",
      "lif layer 2 self.abs_max_v: 3919.5\n",
      "fc layer 2 self.abs_max_out: 2342.0\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "fc layer 1 self.abs_max_out: 2630.0\n",
      "lif layer 1 self.abs_max_v: 4786.0\n",
      "fc layer 1 self.abs_max_out: 2904.0\n",
      "lif layer 1 self.abs_max_v: 5297.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.599866/  1.914091, val:  28.33%, val_best:  30.83%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0101%\n",
      "layer   2  Sparsity: 70.3454%\n",
      "layer   3  Sparsity: 63.1520%\n",
      "total_backward_count 29370 real_backward_count 4907  16.708%\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 3933.0\n",
      "fc layer 2 self.abs_max_out: 2510.0\n",
      "fc layer 2 self.abs_max_out: 2540.0\n",
      "fc layer 2 self.abs_max_out: 2575.0\n",
      "lif layer 2 self.abs_max_v: 4022.0\n",
      "lif layer 2 self.abs_max_v: 4127.0\n",
      "lif layer 2 self.abs_max_v: 4145.5\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "lif layer 2 self.abs_max_v: 4269.5\n",
      "fc layer 2 self.abs_max_out: 2586.0\n",
      "fc layer 1 self.abs_max_out: 3359.0\n",
      "lif layer 1 self.abs_max_v: 5940.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.591045/  1.956981, val:  35.83%, val_best:  35.83%, tr:  99.18%, tr_best:  99.28%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0035%\n",
      "layer   2  Sparsity: 70.0333%\n",
      "layer   3  Sparsity: 62.7868%\n",
      "total_backward_count 39160 real_backward_count 6311  16.116%\n",
      "lif layer 2 self.abs_max_v: 4390.5\n",
      "fc layer 2 self.abs_max_out: 2591.0\n",
      "fc layer 2 self.abs_max_out: 2648.0\n",
      "fc layer 2 self.abs_max_out: 2652.0\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "lif layer 2 self.abs_max_v: 4399.5\n",
      "lif layer 2 self.abs_max_v: 4416.5\n",
      "fc layer 2 self.abs_max_out: 2692.0\n",
      "fc layer 2 self.abs_max_out: 2707.0\n",
      "fc layer 2 self.abs_max_out: 2741.0\n",
      "fc layer 1 self.abs_max_out: 3393.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.581943/  1.921639, val:  30.00%, val_best:  35.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0066%\n",
      "layer   2  Sparsity: 71.1677%\n",
      "layer   3  Sparsity: 62.1123%\n",
      "total_backward_count 48950 real_backward_count 7731  15.794%\n",
      "lif layer 2 self.abs_max_v: 4422.0\n",
      "lif layer 2 self.abs_max_v: 4449.0\n",
      "lif layer 2 self.abs_max_v: 4590.0\n",
      "lif layer 2 self.abs_max_v: 4599.0\n",
      "lif layer 2 self.abs_max_v: 4686.0\n",
      "fc layer 2 self.abs_max_out: 2850.0\n",
      "fc layer 2 self.abs_max_out: 2896.0\n",
      "fc layer 1 self.abs_max_out: 3592.0\n",
      "lif layer 1 self.abs_max_v: 6334.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.570627/  1.922748, val:  41.67%, val_best:  41.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0000%\n",
      "layer   2  Sparsity: 70.7970%\n",
      "layer   3  Sparsity: 60.7971%\n",
      "total_backward_count 58740 real_backward_count 9054  15.414%\n",
      "lif layer 2 self.abs_max_v: 4696.5\n",
      "fc layer 2 self.abs_max_out: 2981.0\n",
      "fc layer 1 self.abs_max_out: 3813.0\n",
      "lif layer 1 self.abs_max_v: 6829.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.559137/  1.944206, val:  33.75%, val_best:  41.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0203%\n",
      "layer   2  Sparsity: 71.5125%\n",
      "layer   3  Sparsity: 61.6284%\n",
      "total_backward_count 68530 real_backward_count 10412  15.193%\n",
      "lif layer 2 self.abs_max_v: 4795.5\n",
      "lif layer 2 self.abs_max_v: 4878.5\n",
      "lif layer 2 self.abs_max_v: 4938.0\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "fc layer 3 self.abs_max_out: 582.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.498971/  1.869064, val:  49.17%, val_best:  49.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0016%\n",
      "layer   2  Sparsity: 70.9031%\n",
      "layer   3  Sparsity: 59.8094%\n",
      "total_backward_count 78320 real_backward_count 11724  14.969%\n",
      "fc layer 2 self.abs_max_out: 3004.0\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 2 self.abs_max_out: 3092.0\n",
      "fc layer 2 self.abs_max_out: 3109.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.519047/  1.811843, val:  51.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0119%\n",
      "layer   2  Sparsity: 71.3161%\n",
      "layer   3  Sparsity: 61.0143%\n",
      "total_backward_count 88110 real_backward_count 13020  14.777%\n",
      "lif layer 2 self.abs_max_v: 4974.5\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 1 self.abs_max_out: 3970.0\n",
      "lif layer 1 self.abs_max_v: 6969.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.487583/  1.885200, val:  41.67%, val_best:  51.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9921%\n",
      "layer   2  Sparsity: 70.6928%\n",
      "layer   3  Sparsity: 60.8994%\n",
      "total_backward_count 97900 real_backward_count 14260  14.566%\n",
      "fc layer 2 self.abs_max_out: 3154.0\n",
      "lif layer 2 self.abs_max_v: 5035.0\n",
      "fc layer 2 self.abs_max_out: 3253.0\n",
      "fc layer 1 self.abs_max_out: 4060.0\n",
      "lif layer 1 self.abs_max_v: 7086.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.480515/  1.880439, val:  40.00%, val_best:  51.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9956%\n",
      "layer   2  Sparsity: 70.6610%\n",
      "layer   3  Sparsity: 60.9957%\n",
      "total_backward_count 107690 real_backward_count 15442  14.339%\n",
      "fc layer 2 self.abs_max_out: 3267.0\n",
      "fc layer 2 self.abs_max_out: 3537.0\n",
      "fc layer 1 self.abs_max_out: 4121.0\n",
      "lif layer 1 self.abs_max_v: 7099.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.484353/  1.804866, val:  50.00%, val_best:  51.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 70.8542%\n",
      "layer   3  Sparsity: 60.0526%\n",
      "total_backward_count 117480 real_backward_count 16677  14.196%\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "lif layer 2 self.abs_max_v: 5069.5\n",
      "lif layer 1 self.abs_max_v: 7184.0\n",
      "fc layer 1 self.abs_max_out: 4598.0\n",
      "lif layer 1 self.abs_max_v: 8078.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.442688/  1.837930, val:  31.67%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0169%\n",
      "layer   2  Sparsity: 70.7646%\n",
      "layer   3  Sparsity: 60.5584%\n",
      "total_backward_count 127270 real_backward_count 17870  14.041%\n",
      "lif layer 2 self.abs_max_v: 5085.5\n",
      "lif layer 2 self.abs_max_v: 5099.5\n",
      "lif layer 2 self.abs_max_v: 5105.0\n",
      "lif layer 2 self.abs_max_v: 5149.5\n",
      "lif layer 2 self.abs_max_v: 5151.5\n",
      "lif layer 2 self.abs_max_v: 5305.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.446044/  1.773906, val:  47.92%, val_best:  51.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0152%\n",
      "layer   2  Sparsity: 70.4045%\n",
      "layer   3  Sparsity: 60.6308%\n",
      "total_backward_count 137060 real_backward_count 19025  13.881%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.397926/  1.767177, val:  47.50%, val_best:  51.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0167%\n",
      "layer   2  Sparsity: 70.0158%\n",
      "layer   3  Sparsity: 60.4578%\n",
      "total_backward_count 146850 real_backward_count 20223  13.771%\n",
      "lif layer 2 self.abs_max_v: 5379.0\n",
      "lif layer 2 self.abs_max_v: 5500.5\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 5626.5\n",
      "lif layer 2 self.abs_max_v: 5650.5\n",
      "fc layer 1 self.abs_max_out: 5043.0\n",
      "lif layer 1 self.abs_max_v: 8907.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.409001/  1.754677, val:  52.92%, val_best:  52.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9920%\n",
      "layer   2  Sparsity: 70.3031%\n",
      "layer   3  Sparsity: 60.8792%\n",
      "total_backward_count 156640 real_backward_count 21403  13.664%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.426077/  1.753468, val:  53.75%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0048%\n",
      "layer   2  Sparsity: 70.4181%\n",
      "layer   3  Sparsity: 62.1804%\n",
      "total_backward_count 166430 real_backward_count 22551  13.550%\n",
      "lif layer 2 self.abs_max_v: 5746.0\n",
      "lif layer 2 self.abs_max_v: 5774.0\n",
      "lif layer 2 self.abs_max_v: 5999.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.418818/  1.719536, val:  70.00%, val_best:  70.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0057%\n",
      "layer   2  Sparsity: 70.3987%\n",
      "layer   3  Sparsity: 62.4815%\n",
      "total_backward_count 176220 real_backward_count 23736  13.470%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.380333/  1.713308, val:  55.83%, val_best:  70.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9975%\n",
      "layer   2  Sparsity: 70.0434%\n",
      "layer   3  Sparsity: 61.4608%\n",
      "total_backward_count 186010 real_backward_count 24906  13.390%\n",
      "fc layer 3 self.abs_max_out: 699.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.344404/  1.801151, val:  36.25%, val_best:  70.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9997%\n",
      "layer   2  Sparsity: 70.0826%\n",
      "layer   3  Sparsity: 62.0564%\n",
      "total_backward_count 195800 real_backward_count 25998  13.278%\n",
      "fc layer 2 self.abs_max_out: 3567.0\n",
      "fc layer 2 self.abs_max_out: 3594.0\n",
      "fc layer 1 self.abs_max_out: 5296.0\n",
      "lif layer 1 self.abs_max_v: 9336.5\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.359428/  1.746783, val:  46.25%, val_best:  70.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9951%\n",
      "layer   2  Sparsity: 69.7598%\n",
      "layer   3  Sparsity: 62.2744%\n",
      "total_backward_count 205590 real_backward_count 27104  13.184%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.367216/  1.755631, val:  43.75%, val_best:  70.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9890%\n",
      "layer   2  Sparsity: 69.5519%\n",
      "layer   3  Sparsity: 62.7629%\n",
      "total_backward_count 215380 real_backward_count 28194  13.090%\n",
      "fc layer 1 self.abs_max_out: 5612.0\n",
      "lif layer 1 self.abs_max_v: 9829.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.381902/  1.666391, val:  67.08%, val_best:  70.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9960%\n",
      "layer   2  Sparsity: 69.3553%\n",
      "layer   3  Sparsity: 62.3951%\n",
      "total_backward_count 225170 real_backward_count 29284  13.005%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.331653/  1.632616, val:  69.58%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0115%\n",
      "layer   2  Sparsity: 69.3932%\n",
      "layer   3  Sparsity: 62.3400%\n",
      "total_backward_count 234960 real_backward_count 30348  12.916%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.328810/  1.600004, val:  68.75%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9852%\n",
      "layer   2  Sparsity: 69.0044%\n",
      "layer   3  Sparsity: 62.5067%\n",
      "total_backward_count 244750 real_backward_count 31350  12.809%\n",
      "fc layer 1 self.abs_max_out: 5634.0\n",
      "lif layer 1 self.abs_max_v: 10058.5\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.312102/  1.578586, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9926%\n",
      "layer   2  Sparsity: 68.8731%\n",
      "layer   3  Sparsity: 62.9731%\n",
      "total_backward_count 254540 real_backward_count 32419  12.736%\n",
      "fc layer 1 self.abs_max_out: 5783.0\n",
      "lif layer 1 self.abs_max_v: 10554.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.310205/  1.625786, val:  67.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9977%\n",
      "layer   2  Sparsity: 69.2929%\n",
      "layer   3  Sparsity: 63.4603%\n",
      "total_backward_count 264330 real_backward_count 33409  12.639%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.300674/  1.577520, val:  76.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 68.8847%\n",
      "layer   3  Sparsity: 62.8319%\n",
      "total_backward_count 274120 real_backward_count 34391  12.546%\n",
      "fc layer 1 self.abs_max_out: 5889.0\n",
      "lif layer 1 self.abs_max_v: 10620.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.266845/  1.622822, val:  59.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0136%\n",
      "layer   2  Sparsity: 69.0304%\n",
      "layer   3  Sparsity: 62.1900%\n",
      "total_backward_count 283910 real_backward_count 35341  12.448%\n",
      "fc layer 1 self.abs_max_out: 6074.0\n",
      "lif layer 1 self.abs_max_v: 10985.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.291752/  1.571559, val:  73.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0134%\n",
      "layer   2  Sparsity: 68.9085%\n",
      "layer   3  Sparsity: 61.9783%\n",
      "total_backward_count 293700 real_backward_count 36263  12.347%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.302181/  1.607845, val:  67.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 68.5882%\n",
      "layer   3  Sparsity: 62.9474%\n",
      "total_backward_count 303490 real_backward_count 37144  12.239%\n",
      "lif layer 2 self.abs_max_v: 6191.5\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.317979/  1.613884, val:  57.92%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0110%\n",
      "layer   2  Sparsity: 68.8469%\n",
      "layer   3  Sparsity: 63.7297%\n",
      "total_backward_count 313280 real_backward_count 38064  12.150%\n",
      "fc layer 3 self.abs_max_out: 725.0\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "fc layer 1 self.abs_max_out: 6136.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.286711/  1.522120, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9918%\n",
      "layer   2  Sparsity: 69.1423%\n",
      "layer   3  Sparsity: 62.9450%\n",
      "total_backward_count 323070 real_backward_count 38950  12.056%\n",
      "fc layer 3 self.abs_max_out: 749.0\n",
      "fc layer 3 self.abs_max_out: 791.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.235677/  1.541173, val:  72.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0060%\n",
      "layer   2  Sparsity: 69.1989%\n",
      "layer   3  Sparsity: 63.0077%\n",
      "total_backward_count 332860 real_backward_count 39864  11.976%\n",
      "lif layer 1 self.abs_max_v: 10996.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.218812/  1.523329, val:  67.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0124%\n",
      "layer   2  Sparsity: 68.6495%\n",
      "layer   3  Sparsity: 62.8178%\n",
      "total_backward_count 342650 real_backward_count 40718  11.883%\n",
      "fc layer 1 self.abs_max_out: 6257.0\n",
      "lif layer 1 self.abs_max_v: 11483.5\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.227769/  1.490396, val:  80.00%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0161%\n",
      "layer   2  Sparsity: 68.4813%\n",
      "layer   3  Sparsity: 62.0353%\n",
      "total_backward_count 352440 real_backward_count 41534  11.785%\n",
      "fc layer 1 self.abs_max_out: 6467.0\n",
      "lif layer 1 self.abs_max_v: 11898.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.199483/  1.503236, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0046%\n",
      "layer   2  Sparsity: 68.1349%\n",
      "layer   3  Sparsity: 62.5263%\n",
      "total_backward_count 362230 real_backward_count 42356  11.693%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.194893/  1.529193, val:  66.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0011%\n",
      "layer   2  Sparsity: 68.3845%\n",
      "layer   3  Sparsity: 63.2734%\n",
      "total_backward_count 372020 real_backward_count 43102  11.586%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.214624/  1.502217, val:  76.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 67.8564%\n",
      "layer   3  Sparsity: 63.7050%\n",
      "total_backward_count 381810 real_backward_count 43959  11.513%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.214886/  1.517237, val:  79.58%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0199%\n",
      "layer   2  Sparsity: 68.3214%\n",
      "layer   3  Sparsity: 63.2942%\n",
      "total_backward_count 391600 real_backward_count 44739  11.425%\n",
      "lif layer 2 self.abs_max_v: 6290.5\n",
      "lif layer 2 self.abs_max_v: 6320.5\n",
      "lif layer 2 self.abs_max_v: 6476.5\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.217080/  1.517873, val:  70.83%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9917%\n",
      "layer   2  Sparsity: 68.0142%\n",
      "layer   3  Sparsity: 63.5856%\n",
      "total_backward_count 401390 real_backward_count 45558  11.350%\n",
      "fc layer 3 self.abs_max_out: 797.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.209628/  1.520309, val:  70.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 68.0901%\n",
      "layer   3  Sparsity: 63.4771%\n",
      "total_backward_count 411180 real_backward_count 46379  11.279%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.215077/  1.494274, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0092%\n",
      "layer   2  Sparsity: 68.3839%\n",
      "layer   3  Sparsity: 63.8282%\n",
      "total_backward_count 420970 real_backward_count 47172  11.206%\n",
      "fc layer 1 self.abs_max_out: 6729.0\n",
      "lif layer 1 self.abs_max_v: 12110.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.210764/  1.530355, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0120%\n",
      "layer   2  Sparsity: 68.2116%\n",
      "layer   3  Sparsity: 63.4799%\n",
      "total_backward_count 430760 real_backward_count 47988  11.140%\n",
      "fc layer 2 self.abs_max_out: 3730.0\n",
      "lif layer 2 self.abs_max_v: 6640.5\n",
      "lif layer 2 self.abs_max_v: 6649.0\n",
      "lif layer 2 self.abs_max_v: 6941.5\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.233207/  1.512927, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0221%\n",
      "layer   2  Sparsity: 68.1513%\n",
      "layer   3  Sparsity: 62.8546%\n",
      "total_backward_count 440550 real_backward_count 48739  11.063%\n",
      "lif layer 2 self.abs_max_v: 6996.0\n",
      "lif layer 2 self.abs_max_v: 7159.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.201103/  1.491557, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 67.8349%\n",
      "layer   3  Sparsity: 62.5223%\n",
      "total_backward_count 450340 real_backward_count 49479  10.987%\n",
      "fc layer 2 self.abs_max_out: 3752.0\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.174024/  1.461261, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9894%\n",
      "layer   2  Sparsity: 68.1724%\n",
      "layer   3  Sparsity: 62.8254%\n",
      "total_backward_count 460130 real_backward_count 50202  10.910%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.152074/  1.485246, val:  69.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0072%\n",
      "layer   2  Sparsity: 68.3036%\n",
      "layer   3  Sparsity: 63.3844%\n",
      "total_backward_count 469920 real_backward_count 50933  10.839%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.161433/  1.431051, val:  86.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9955%\n",
      "layer   2  Sparsity: 67.8383%\n",
      "layer   3  Sparsity: 63.3824%\n",
      "total_backward_count 479710 real_backward_count 51666  10.770%\n",
      "fc layer 1 self.abs_max_out: 6869.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.169040/  1.465634, val:  72.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0036%\n",
      "layer   2  Sparsity: 67.6619%\n",
      "layer   3  Sparsity: 63.9181%\n",
      "total_backward_count 489500 real_backward_count 52423  10.709%\n",
      "fc layer 2 self.abs_max_out: 3760.0\n",
      "fc layer 2 self.abs_max_out: 3876.0\n",
      "fc layer 1 self.abs_max_out: 6961.0\n",
      "lif layer 1 self.abs_max_v: 12677.5\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.149712/  1.486843, val:  65.42%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0045%\n",
      "layer   2  Sparsity: 67.8674%\n",
      "layer   3  Sparsity: 64.3005%\n",
      "total_backward_count 499290 real_backward_count 53091  10.633%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.165222/  1.467059, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0112%\n",
      "layer   2  Sparsity: 67.8178%\n",
      "layer   3  Sparsity: 64.1608%\n",
      "total_backward_count 509080 real_backward_count 53790  10.566%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.159814/  1.409943, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9967%\n",
      "layer   2  Sparsity: 68.0147%\n",
      "layer   3  Sparsity: 64.1997%\n",
      "total_backward_count 518870 real_backward_count 54485  10.501%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.143422/  1.431099, val:  74.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 68.1594%\n",
      "layer   3  Sparsity: 64.0581%\n",
      "total_backward_count 528660 real_backward_count 55191  10.440%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.140176/  1.423563, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0020%\n",
      "layer   2  Sparsity: 67.7850%\n",
      "layer   3  Sparsity: 63.4174%\n",
      "total_backward_count 538450 real_backward_count 55868  10.376%\n",
      "fc layer 1 self.abs_max_out: 6977.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.145908/  1.412508, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9952%\n",
      "layer   2  Sparsity: 67.7233%\n",
      "layer   3  Sparsity: 63.3881%\n",
      "total_backward_count 548240 real_backward_count 56601  10.324%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.133810/  1.402388, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0023%\n",
      "layer   2  Sparsity: 67.6518%\n",
      "layer   3  Sparsity: 64.2689%\n",
      "total_backward_count 558030 real_backward_count 57264  10.262%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.139117/  1.427098, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0143%\n",
      "layer   2  Sparsity: 67.6822%\n",
      "layer   3  Sparsity: 64.6816%\n",
      "total_backward_count 567820 real_backward_count 57917  10.200%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.131958/  1.422188, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 68.0722%\n",
      "layer   3  Sparsity: 64.2578%\n",
      "total_backward_count 577610 real_backward_count 58540  10.135%\n",
      "fc layer 1 self.abs_max_out: 6995.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.150138/  1.474469, val:  67.92%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0128%\n",
      "layer   2  Sparsity: 68.1848%\n",
      "layer   3  Sparsity: 64.5828%\n",
      "total_backward_count 587400 real_backward_count 59219  10.082%\n",
      "fc layer 1 self.abs_max_out: 7042.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.135926/  1.405686, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 68.0042%\n",
      "layer   3  Sparsity: 63.3651%\n",
      "total_backward_count 597190 real_backward_count 59837  10.020%\n",
      "fc layer 3 self.abs_max_out: 839.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.114074/  1.368874, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9995%\n",
      "layer   2  Sparsity: 67.8604%\n",
      "layer   3  Sparsity: 63.1681%\n",
      "total_backward_count 606980 real_backward_count 60470   9.962%\n",
      "fc layer 1 self.abs_max_out: 7189.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.094509/  1.408713, val:  75.83%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9998%\n",
      "layer   2  Sparsity: 67.5498%\n",
      "layer   3  Sparsity: 62.8302%\n",
      "total_backward_count 616770 real_backward_count 61128   9.911%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.101877/  1.399085, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9953%\n",
      "layer   2  Sparsity: 67.3497%\n",
      "layer   3  Sparsity: 63.2590%\n",
      "total_backward_count 626560 real_backward_count 61793   9.862%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.107272/  1.363045, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0188%\n",
      "layer   2  Sparsity: 66.8927%\n",
      "layer   3  Sparsity: 63.5568%\n",
      "total_backward_count 636350 real_backward_count 62422   9.809%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.094857/  1.404998, val:  76.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0034%\n",
      "layer   2  Sparsity: 66.7753%\n",
      "layer   3  Sparsity: 63.8046%\n",
      "total_backward_count 646140 real_backward_count 63036   9.756%\n",
      "fc layer 1 self.abs_max_out: 7201.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.084772/  1.385922, val:  74.58%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 66.9388%\n",
      "layer   3  Sparsity: 63.5401%\n",
      "total_backward_count 655930 real_backward_count 63660   9.705%\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.058863/  1.388865, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 67.1052%\n",
      "layer   3  Sparsity: 63.5501%\n",
      "total_backward_count 665720 real_backward_count 64301   9.659%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.084017/  1.366189, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9912%\n",
      "layer   2  Sparsity: 67.2862%\n",
      "layer   3  Sparsity: 64.4308%\n",
      "total_backward_count 675510 real_backward_count 64876   9.604%\n",
      "fc layer 1 self.abs_max_out: 7264.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.059650/  1.406938, val:  66.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0120%\n",
      "layer   2  Sparsity: 67.2488%\n",
      "layer   3  Sparsity: 63.7469%\n",
      "total_backward_count 685300 real_backward_count 65460   9.552%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.065303/  1.375094, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0206%\n",
      "layer   2  Sparsity: 67.5055%\n",
      "layer   3  Sparsity: 64.3075%\n",
      "total_backward_count 695090 real_backward_count 66013   9.497%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.079785/  1.406699, val:  70.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9999%\n",
      "layer   2  Sparsity: 67.3880%\n",
      "layer   3  Sparsity: 64.3178%\n",
      "total_backward_count 704880 real_backward_count 66583   9.446%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.071476/  1.333634, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0090%\n",
      "layer   2  Sparsity: 67.3218%\n",
      "layer   3  Sparsity: 64.7698%\n",
      "total_backward_count 714670 real_backward_count 67156   9.397%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.067437/  1.361096, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 67.0628%\n",
      "layer   3  Sparsity: 64.1237%\n",
      "total_backward_count 724460 real_backward_count 67692   9.344%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.043299/  1.301325, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0068%\n",
      "layer   2  Sparsity: 67.0320%\n",
      "layer   3  Sparsity: 63.5073%\n",
      "total_backward_count 734250 real_backward_count 68246   9.295%\n",
      "lif layer 2 self.abs_max_v: 7255.5\n",
      "lif layer 2 self.abs_max_v: 7477.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.030444/  1.309683, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 67.2146%\n",
      "layer   3  Sparsity: 63.2528%\n",
      "total_backward_count 744040 real_backward_count 68812   9.248%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.014508/  1.322626, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0127%\n",
      "layer   2  Sparsity: 67.0760%\n",
      "layer   3  Sparsity: 63.1314%\n",
      "total_backward_count 753830 real_backward_count 69368   9.202%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.047787/  1.390834, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9901%\n",
      "layer   2  Sparsity: 66.9543%\n",
      "layer   3  Sparsity: 63.5342%\n",
      "total_backward_count 763620 real_backward_count 69924   9.157%\n",
      "fc layer 2 self.abs_max_out: 3902.0\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.056965/  1.362149, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9943%\n",
      "layer   2  Sparsity: 66.7823%\n",
      "layer   3  Sparsity: 63.7107%\n",
      "total_backward_count 773410 real_backward_count 70461   9.110%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.045673/  1.352140, val:  82.50%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0022%\n",
      "layer   2  Sparsity: 66.7375%\n",
      "layer   3  Sparsity: 63.5704%\n",
      "total_backward_count 783200 real_backward_count 71018   9.068%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.029394/  1.332338, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0038%\n",
      "layer   2  Sparsity: 66.4250%\n",
      "layer   3  Sparsity: 62.8825%\n",
      "total_backward_count 792990 real_backward_count 71563   9.024%\n",
      "fc layer 1 self.abs_max_out: 7356.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.019806/  1.313601, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9940%\n",
      "layer   2  Sparsity: 66.6882%\n",
      "layer   3  Sparsity: 62.7913%\n",
      "total_backward_count 802780 real_backward_count 72122   8.984%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.006716/  1.279773, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 66.5096%\n",
      "layer   3  Sparsity: 63.2163%\n",
      "total_backward_count 812570 real_backward_count 72669   8.943%\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "fc layer 2 self.abs_max_out: 4093.0\n",
      "lif layer 2 self.abs_max_v: 7538.5\n",
      "lif layer 2 self.abs_max_v: 7785.5\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.993811/  1.320076, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9955%\n",
      "layer   2  Sparsity: 66.4017%\n",
      "layer   3  Sparsity: 63.3513%\n",
      "total_backward_count 822360 real_backward_count 73195   8.901%\n",
      "fc layer 1 self.abs_max_out: 7408.0\n",
      "lif layer 1 self.abs_max_v: 12781.5\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.986385/  1.274099, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0139%\n",
      "layer   2  Sparsity: 66.6279%\n",
      "layer   3  Sparsity: 63.1315%\n",
      "total_backward_count 832150 real_backward_count 73691   8.855%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.994785/  1.330006, val:  75.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0189%\n",
      "layer   2  Sparsity: 66.6640%\n",
      "layer   3  Sparsity: 63.3484%\n",
      "total_backward_count 841940 real_backward_count 74202   8.813%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.000512/  1.322050, val:  74.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0089%\n",
      "layer   2  Sparsity: 66.6333%\n",
      "layer   3  Sparsity: 62.0155%\n",
      "total_backward_count 851730 real_backward_count 74710   8.772%\n",
      "fc layer 3 self.abs_max_out: 879.0\n",
      "fc layer 1 self.abs_max_out: 7667.0\n",
      "lif layer 1 self.abs_max_v: 13266.0\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.995146/  1.275142, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0047%\n",
      "layer   2  Sparsity: 66.6401%\n",
      "layer   3  Sparsity: 62.1855%\n",
      "total_backward_count 861520 real_backward_count 75228   8.732%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.990090/  1.321908, val:  74.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0068%\n",
      "layer   2  Sparsity: 66.6842%\n",
      "layer   3  Sparsity: 61.9682%\n",
      "total_backward_count 871310 real_backward_count 75710   8.689%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.983764/  1.248475, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9883%\n",
      "layer   2  Sparsity: 66.3380%\n",
      "layer   3  Sparsity: 61.9677%\n",
      "total_backward_count 881100 real_backward_count 76202   8.649%\n",
      "fc layer 3 self.abs_max_out: 888.0\n",
      "fc layer 3 self.abs_max_out: 907.0\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.961959/  1.282867, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9873%\n",
      "layer   2  Sparsity: 66.4642%\n",
      "layer   3  Sparsity: 62.4443%\n",
      "total_backward_count 890890 real_backward_count 76702   8.610%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.961382/  1.245796, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0023%\n",
      "layer   2  Sparsity: 66.2674%\n",
      "layer   3  Sparsity: 62.4696%\n",
      "total_backward_count 900680 real_backward_count 77221   8.574%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.947531/  1.229499, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9895%\n",
      "layer   2  Sparsity: 66.2013%\n",
      "layer   3  Sparsity: 62.6482%\n",
      "total_backward_count 910470 real_backward_count 77712   8.535%\n",
      "lif layer 1 self.abs_max_v: 13374.5\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.954126/  1.261149, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0218%\n",
      "layer   2  Sparsity: 66.3076%\n",
      "layer   3  Sparsity: 62.3224%\n",
      "total_backward_count 920260 real_backward_count 78232   8.501%\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "fc layer 3 self.abs_max_out: 931.0\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.947064/  1.216742, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0031%\n",
      "layer   2  Sparsity: 66.3225%\n",
      "layer   3  Sparsity: 62.9842%\n",
      "total_backward_count 930050 real_backward_count 78718   8.464%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.940789/  1.209127, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9968%\n",
      "layer   2  Sparsity: 66.4482%\n",
      "layer   3  Sparsity: 62.1670%\n",
      "total_backward_count 939840 real_backward_count 79183   8.425%\n",
      "fc layer 1 self.abs_max_out: 7737.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.943032/  1.278354, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0109%\n",
      "layer   2  Sparsity: 66.5177%\n",
      "layer   3  Sparsity: 62.4843%\n",
      "total_backward_count 949630 real_backward_count 79643   8.387%\n",
      "lif layer 1 self.abs_max_v: 13432.0\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.948950/  1.255622, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0045%\n",
      "layer   2  Sparsity: 66.4877%\n",
      "layer   3  Sparsity: 62.4638%\n",
      "total_backward_count 959420 real_backward_count 80073   8.346%\n",
      "fc layer 1 self.abs_max_out: 7752.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.931518/  1.233155, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9992%\n",
      "layer   2  Sparsity: 66.5275%\n",
      "layer   3  Sparsity: 62.8723%\n",
      "total_backward_count 969210 real_backward_count 80500   8.306%\n",
      "fc layer 3 self.abs_max_out: 934.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "fc layer 1 self.abs_max_out: 7911.0\n",
      "lif layer 1 self.abs_max_v: 13697.0\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.924565/  1.218787, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0047%\n",
      "layer   2  Sparsity: 66.2524%\n",
      "layer   3  Sparsity: 62.3752%\n",
      "total_backward_count 979000 real_backward_count 80889   8.262%\n",
      "fc layer 2 self.abs_max_out: 4176.0\n",
      "fc layer 2 self.abs_max_out: 4180.0\n",
      "lif layer 2 self.abs_max_v: 8053.0\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.907264/  1.197098, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0185%\n",
      "layer   2  Sparsity: 66.2628%\n",
      "layer   3  Sparsity: 62.8364%\n",
      "total_backward_count 988790 real_backward_count 81306   8.223%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.902216/  1.205487, val:  85.42%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9982%\n",
      "layer   2  Sparsity: 66.3643%\n",
      "layer   3  Sparsity: 63.4431%\n",
      "total_backward_count 998580 real_backward_count 81734   8.185%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.923558/  1.196787, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0070%\n",
      "layer   2  Sparsity: 66.3043%\n",
      "layer   3  Sparsity: 63.4749%\n",
      "total_backward_count 1008370 real_backward_count 82168   8.149%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.923310/  1.199137, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9896%\n",
      "layer   2  Sparsity: 66.1631%\n",
      "layer   3  Sparsity: 63.2280%\n",
      "total_backward_count 1018160 real_backward_count 82556   8.108%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.903904/  1.180457, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0007%\n",
      "layer   2  Sparsity: 66.3830%\n",
      "layer   3  Sparsity: 63.3508%\n",
      "total_backward_count 1027950 real_backward_count 83004   8.075%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.904039/  1.189913, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9985%\n",
      "layer   2  Sparsity: 66.5019%\n",
      "layer   3  Sparsity: 62.7748%\n",
      "total_backward_count 1037740 real_backward_count 83412   8.038%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.900340/  1.250360, val:  74.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0040%\n",
      "layer   2  Sparsity: 66.4579%\n",
      "layer   3  Sparsity: 62.9356%\n",
      "total_backward_count 1047530 real_backward_count 83839   8.003%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.889015/  1.194968, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9978%\n",
      "layer   2  Sparsity: 66.3710%\n",
      "layer   3  Sparsity: 63.0982%\n",
      "total_backward_count 1057320 real_backward_count 84320   7.975%\n",
      "fc layer 3 self.abs_max_out: 957.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.884733/  1.184800, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9842%\n",
      "layer   2  Sparsity: 66.1577%\n",
      "layer   3  Sparsity: 63.3108%\n",
      "total_backward_count 1067110 real_backward_count 84729   7.940%\n",
      "fc layer 3 self.abs_max_out: 963.0\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.898367/  1.227169, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0102%\n",
      "layer   2  Sparsity: 66.3565%\n",
      "layer   3  Sparsity: 62.8619%\n",
      "total_backward_count 1076900 real_backward_count 85165   7.908%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.884396/  1.157798, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0078%\n",
      "layer   2  Sparsity: 66.2744%\n",
      "layer   3  Sparsity: 62.8592%\n",
      "total_backward_count 1086690 real_backward_count 85603   7.877%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.873469/  1.192189, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9959%\n",
      "layer   2  Sparsity: 66.3287%\n",
      "layer   3  Sparsity: 63.1131%\n",
      "total_backward_count 1096480 real_backward_count 85978   7.841%\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.861515/  1.166815, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 66.6224%\n",
      "layer   3  Sparsity: 62.6283%\n",
      "total_backward_count 1106270 real_backward_count 86361   7.807%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.867409/  1.185590, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0038%\n",
      "layer   2  Sparsity: 66.3973%\n",
      "layer   3  Sparsity: 63.3699%\n",
      "total_backward_count 1116060 real_backward_count 86741   7.772%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.863028/  1.175359, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0104%\n",
      "layer   2  Sparsity: 66.2860%\n",
      "layer   3  Sparsity: 63.7326%\n",
      "total_backward_count 1125850 real_backward_count 87129   7.739%\n",
      "fc layer 3 self.abs_max_out: 990.0\n",
      "fc layer 3 self.abs_max_out: 998.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.877266/  1.208522, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0156%\n",
      "layer   2  Sparsity: 66.2358%\n",
      "layer   3  Sparsity: 63.5495%\n",
      "total_backward_count 1135640 real_backward_count 87582   7.712%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.874960/  1.184890, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0163%\n",
      "layer   2  Sparsity: 66.0391%\n",
      "layer   3  Sparsity: 63.1273%\n",
      "total_backward_count 1145430 real_backward_count 88013   7.684%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.856334/  1.153187, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0000%\n",
      "layer   2  Sparsity: 66.0756%\n",
      "layer   3  Sparsity: 63.1651%\n",
      "total_backward_count 1155220 real_backward_count 88414   7.653%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.857172/  1.178908, val:  81.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 66.0274%\n",
      "layer   3  Sparsity: 63.2755%\n",
      "total_backward_count 1165010 real_backward_count 88810   7.623%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.872622/  1.184178, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0073%\n",
      "layer   2  Sparsity: 65.9235%\n",
      "layer   3  Sparsity: 62.7356%\n",
      "total_backward_count 1174800 real_backward_count 89233   7.596%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.873952/  1.173911, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0042%\n",
      "layer   2  Sparsity: 66.2617%\n",
      "layer   3  Sparsity: 63.1881%\n",
      "total_backward_count 1184590 real_backward_count 89634   7.567%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.841380/  1.158390, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9971%\n",
      "layer   2  Sparsity: 66.2025%\n",
      "layer   3  Sparsity: 63.2776%\n",
      "total_backward_count 1194380 real_backward_count 90010   7.536%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.839557/  1.199701, val:  78.33%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0114%\n",
      "layer   2  Sparsity: 66.2491%\n",
      "layer   3  Sparsity: 63.1545%\n",
      "total_backward_count 1204170 real_backward_count 90381   7.506%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.849507/  1.168080, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0221%\n",
      "layer   2  Sparsity: 66.2853%\n",
      "layer   3  Sparsity: 63.6191%\n",
      "total_backward_count 1213960 real_backward_count 90728   7.474%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.843480/  1.152295, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0009%\n",
      "layer   2  Sparsity: 66.4342%\n",
      "layer   3  Sparsity: 63.3234%\n",
      "total_backward_count 1223750 real_backward_count 91085   7.443%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.839431/  1.141207, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9957%\n",
      "layer   2  Sparsity: 66.4193%\n",
      "layer   3  Sparsity: 62.8775%\n",
      "total_backward_count 1233540 real_backward_count 91415   7.411%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.842885/  1.133351, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0025%\n",
      "layer   2  Sparsity: 66.2740%\n",
      "layer   3  Sparsity: 63.1971%\n",
      "total_backward_count 1243330 real_backward_count 91810   7.384%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.834903/  1.166091, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9997%\n",
      "layer   2  Sparsity: 66.3145%\n",
      "layer   3  Sparsity: 63.5567%\n",
      "total_backward_count 1253120 real_backward_count 92199   7.358%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.848305/  1.133665, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0015%\n",
      "layer   2  Sparsity: 66.2421%\n",
      "layer   3  Sparsity: 63.2953%\n",
      "total_backward_count 1262910 real_backward_count 92567   7.330%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.849035/  1.133906, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 66.3580%\n",
      "layer   3  Sparsity: 63.1998%\n",
      "total_backward_count 1272700 real_backward_count 92967   7.305%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.844886/  1.146770, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0067%\n",
      "layer   2  Sparsity: 66.2570%\n",
      "layer   3  Sparsity: 63.5288%\n",
      "total_backward_count 1282490 real_backward_count 93355   7.279%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.837752/  1.145508, val:  87.50%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0122%\n",
      "layer   2  Sparsity: 66.1617%\n",
      "layer   3  Sparsity: 63.3568%\n",
      "total_backward_count 1292280 real_backward_count 93723   7.253%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.827863/  1.156899, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0125%\n",
      "layer   2  Sparsity: 66.1347%\n",
      "layer   3  Sparsity: 63.4248%\n",
      "total_backward_count 1302070 real_backward_count 94084   7.226%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.820349/  1.124883, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0041%\n",
      "layer   2  Sparsity: 66.2183%\n",
      "layer   3  Sparsity: 63.5174%\n",
      "total_backward_count 1311860 real_backward_count 94447   7.199%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.825926/  1.158481, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0122%\n",
      "layer   2  Sparsity: 66.2708%\n",
      "layer   3  Sparsity: 62.8733%\n",
      "total_backward_count 1321650 real_backward_count 94800   7.173%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.838037/  1.129348, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9959%\n",
      "layer   2  Sparsity: 66.2830%\n",
      "layer   3  Sparsity: 63.0192%\n",
      "total_backward_count 1331440 real_backward_count 95159   7.147%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.818472/  1.138741, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0077%\n",
      "layer   2  Sparsity: 66.1941%\n",
      "layer   3  Sparsity: 63.6951%\n",
      "total_backward_count 1341230 real_backward_count 95494   7.120%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.808943/  1.159309, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9879%\n",
      "layer   2  Sparsity: 66.0730%\n",
      "layer   3  Sparsity: 63.3385%\n",
      "total_backward_count 1351020 real_backward_count 95824   7.093%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.828038/  1.138567, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9909%\n",
      "layer   2  Sparsity: 66.1341%\n",
      "layer   3  Sparsity: 63.4379%\n",
      "total_backward_count 1360810 real_backward_count 96202   7.069%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.821560/  1.157092, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0127%\n",
      "layer   2  Sparsity: 66.1389%\n",
      "layer   3  Sparsity: 62.7868%\n",
      "total_backward_count 1370600 real_backward_count 96548   7.044%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.823447/  1.149770, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0134%\n",
      "layer   2  Sparsity: 66.0199%\n",
      "layer   3  Sparsity: 62.6224%\n",
      "total_backward_count 1380390 real_backward_count 96906   7.020%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.823560/  1.133756, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 65.7482%\n",
      "layer   3  Sparsity: 62.3350%\n",
      "total_backward_count 1390180 real_backward_count 97285   6.998%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.805523/  1.142492, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0090%\n",
      "layer   2  Sparsity: 65.8130%\n",
      "layer   3  Sparsity: 62.2029%\n",
      "total_backward_count 1399970 real_backward_count 97656   6.976%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.803671/  1.122917, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0244%\n",
      "layer   2  Sparsity: 65.9630%\n",
      "layer   3  Sparsity: 62.1207%\n",
      "total_backward_count 1409760 real_backward_count 98018   6.953%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.817967/  1.137716, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0158%\n",
      "layer   2  Sparsity: 65.7910%\n",
      "layer   3  Sparsity: 62.0938%\n",
      "total_backward_count 1419550 real_backward_count 98387   6.931%\n",
      "fc layer 3 self.abs_max_out: 1019.0\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.823632/  1.120490, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9974%\n",
      "layer   2  Sparsity: 65.8953%\n",
      "layer   3  Sparsity: 63.2720%\n",
      "total_backward_count 1429340 real_backward_count 98745   6.908%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.811910/  1.114301, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 65.8708%\n",
      "layer   3  Sparsity: 63.2936%\n",
      "total_backward_count 1439130 real_backward_count 99090   6.885%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.790101/  1.131096, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0190%\n",
      "layer   2  Sparsity: 65.9444%\n",
      "layer   3  Sparsity: 62.9736%\n",
      "total_backward_count 1448920 real_backward_count 99395   6.860%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.812971/  1.136545, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0169%\n",
      "layer   2  Sparsity: 65.9096%\n",
      "layer   3  Sparsity: 62.7435%\n",
      "total_backward_count 1458710 real_backward_count 99728   6.837%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.806502/  1.135859, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 65.5698%\n",
      "layer   3  Sparsity: 62.7407%\n",
      "total_backward_count 1468500 real_backward_count 100072   6.815%\n",
      "fc layer 3 self.abs_max_out: 1028.0\n",
      "fc layer 3 self.abs_max_out: 1050.0\n",
      "fc layer 3 self.abs_max_out: 1055.0\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.810291/  1.135069, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0057%\n",
      "layer   2  Sparsity: 65.7363%\n",
      "layer   3  Sparsity: 62.5845%\n",
      "total_backward_count 1478290 real_backward_count 100400   6.792%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.798704/  1.117000, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9886%\n",
      "layer   2  Sparsity: 65.8474%\n",
      "layer   3  Sparsity: 62.6039%\n",
      "total_backward_count 1488080 real_backward_count 100740   6.770%\n",
      "fc layer 3 self.abs_max_out: 1058.0\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.798261/  1.121985, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 65.8218%\n",
      "layer   3  Sparsity: 63.2330%\n",
      "total_backward_count 1497870 real_backward_count 101067   6.747%\n",
      "fc layer 1 self.abs_max_out: 8101.0\n",
      "lif layer 1 self.abs_max_v: 14038.0\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.794130/  1.118587, val:  86.67%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9732%\n",
      "layer   2  Sparsity: 65.9747%\n",
      "layer   3  Sparsity: 62.7190%\n",
      "total_backward_count 1507660 real_backward_count 101373   6.724%\n",
      "fc layer 3 self.abs_max_out: 1071.0\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.789487/  1.110589, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9915%\n",
      "layer   2  Sparsity: 65.9971%\n",
      "layer   3  Sparsity: 62.7779%\n",
      "total_backward_count 1517450 real_backward_count 101670   6.700%\n",
      "fc layer 3 self.abs_max_out: 1076.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.793625/  1.143072, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 66.1088%\n",
      "layer   3  Sparsity: 63.3958%\n",
      "total_backward_count 1527240 real_backward_count 102021   6.680%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.791748/  1.112136, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9867%\n",
      "layer   2  Sparsity: 66.1520%\n",
      "layer   3  Sparsity: 64.0463%\n",
      "total_backward_count 1537030 real_backward_count 102356   6.659%\n",
      "fc layer 3 self.abs_max_out: 1092.0\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.792523/  1.112968, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0083%\n",
      "layer   2  Sparsity: 66.1385%\n",
      "layer   3  Sparsity: 63.0879%\n",
      "total_backward_count 1546820 real_backward_count 102674   6.638%\n",
      "fc layer 3 self.abs_max_out: 1106.0\n",
      "fc layer 3 self.abs_max_out: 1114.0\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.785175/  1.118731, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9985%\n",
      "layer   2  Sparsity: 65.9501%\n",
      "layer   3  Sparsity: 63.4560%\n",
      "total_backward_count 1556610 real_backward_count 102999   6.617%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.780212/  1.135405, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 65.8482%\n",
      "layer   3  Sparsity: 63.7048%\n",
      "total_backward_count 1566400 real_backward_count 103324   6.596%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.784384/  1.117340, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 65.8886%\n",
      "layer   3  Sparsity: 63.5778%\n",
      "total_backward_count 1576190 real_backward_count 103640   6.575%\n",
      "fc layer 3 self.abs_max_out: 1136.0\n",
      "fc layer 3 self.abs_max_out: 1147.0\n",
      "fc layer 3 self.abs_max_out: 1183.0\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.785350/  1.115446, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 66.0941%\n",
      "layer   3  Sparsity: 63.3907%\n",
      "total_backward_count 1585980 real_backward_count 103943   6.554%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.783134/  1.120661, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9948%\n",
      "layer   2  Sparsity: 66.0751%\n",
      "layer   3  Sparsity: 63.0464%\n",
      "total_backward_count 1595770 real_backward_count 104259   6.533%\n",
      "fc layer 2 self.abs_max_out: 4196.0\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.780400/  1.101015, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9878%\n",
      "layer   2  Sparsity: 65.9520%\n",
      "layer   3  Sparsity: 63.5186%\n",
      "total_backward_count 1605560 real_backward_count 104584   6.514%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.770473/  1.090801, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9946%\n",
      "layer   2  Sparsity: 65.9205%\n",
      "layer   3  Sparsity: 63.3649%\n",
      "total_backward_count 1615350 real_backward_count 104881   6.493%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.759574/  1.090535, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9989%\n",
      "layer   2  Sparsity: 66.0417%\n",
      "layer   3  Sparsity: 63.2575%\n",
      "total_backward_count 1625140 real_backward_count 105184   6.472%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.771018/  1.089626, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9998%\n",
      "layer   2  Sparsity: 66.2213%\n",
      "layer   3  Sparsity: 63.7932%\n",
      "total_backward_count 1634930 real_backward_count 105489   6.452%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.754713/  1.069130, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0173%\n",
      "layer   2  Sparsity: 66.1011%\n",
      "layer   3  Sparsity: 63.8413%\n",
      "total_backward_count 1644720 real_backward_count 105781   6.432%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.746980/  1.075332, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9902%\n",
      "layer   2  Sparsity: 65.8412%\n",
      "layer   3  Sparsity: 63.6616%\n",
      "total_backward_count 1654510 real_backward_count 106056   6.410%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.757491/  1.103370, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0158%\n",
      "layer   2  Sparsity: 65.7163%\n",
      "layer   3  Sparsity: 63.6132%\n",
      "total_backward_count 1664300 real_backward_count 106346   6.390%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.759548/  1.112624, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9892%\n",
      "layer   2  Sparsity: 65.8370%\n",
      "layer   3  Sparsity: 63.3027%\n",
      "total_backward_count 1674090 real_backward_count 106623   6.369%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.752516/  1.107662, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9933%\n",
      "layer   2  Sparsity: 65.8786%\n",
      "layer   3  Sparsity: 63.6898%\n",
      "total_backward_count 1683880 real_backward_count 106915   6.349%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.752586/  1.092933, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 65.8954%\n",
      "layer   3  Sparsity: 63.7606%\n",
      "total_backward_count 1693670 real_backward_count 107210   6.330%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.766420/  1.114199, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0080%\n",
      "layer   2  Sparsity: 65.9705%\n",
      "layer   3  Sparsity: 63.7359%\n",
      "total_backward_count 1703460 real_backward_count 107519   6.312%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.748750/  1.054060, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0131%\n",
      "layer   2  Sparsity: 66.0852%\n",
      "layer   3  Sparsity: 63.2430%\n",
      "total_backward_count 1713250 real_backward_count 107789   6.291%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.734333/  1.062128, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9779%\n",
      "layer   2  Sparsity: 65.9285%\n",
      "layer   3  Sparsity: 63.4900%\n",
      "total_backward_count 1723040 real_backward_count 108070   6.272%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.733412/  1.062099, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 65.8779%\n",
      "layer   3  Sparsity: 64.3709%\n",
      "total_backward_count 1732830 real_backward_count 108359   6.253%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.724713/  1.074349, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0235%\n",
      "layer   2  Sparsity: 65.9390%\n",
      "layer   3  Sparsity: 64.5737%\n",
      "total_backward_count 1742620 real_backward_count 108648   6.235%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.720826/  1.060932, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0021%\n",
      "layer   2  Sparsity: 65.9666%\n",
      "layer   3  Sparsity: 63.9681%\n",
      "total_backward_count 1752410 real_backward_count 108912   6.215%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.723026/  1.049397, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0031%\n",
      "layer   2  Sparsity: 65.9527%\n",
      "layer   3  Sparsity: 63.9288%\n",
      "total_backward_count 1762200 real_backward_count 109186   6.196%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.731937/  1.065018, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0116%\n",
      "layer   2  Sparsity: 66.1383%\n",
      "layer   3  Sparsity: 63.7646%\n",
      "total_backward_count 1771990 real_backward_count 109494   6.179%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.728249/  1.057559, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9977%\n",
      "layer   2  Sparsity: 66.0457%\n",
      "layer   3  Sparsity: 63.6253%\n",
      "total_backward_count 1781780 real_backward_count 109765   6.160%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.724288/  1.075291, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0041%\n",
      "layer   2  Sparsity: 66.0547%\n",
      "layer   3  Sparsity: 63.8062%\n",
      "total_backward_count 1791570 real_backward_count 110047   6.142%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.727104/  1.071509, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9905%\n",
      "layer   2  Sparsity: 66.0077%\n",
      "layer   3  Sparsity: 63.1076%\n",
      "total_backward_count 1801360 real_backward_count 110326   6.125%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.721142/  1.081099, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0118%\n",
      "layer   2  Sparsity: 65.9463%\n",
      "layer   3  Sparsity: 63.9294%\n",
      "total_backward_count 1811150 real_backward_count 110606   6.107%\n",
      "fc layer 1 self.abs_max_out: 8184.0\n",
      "lif layer 1 self.abs_max_v: 14040.5\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.721492/  1.063207, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 66.0962%\n",
      "layer   3  Sparsity: 63.8948%\n",
      "total_backward_count 1820940 real_backward_count 110897   6.090%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.722426/  1.073295, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0036%\n",
      "layer   2  Sparsity: 66.0304%\n",
      "layer   3  Sparsity: 63.7166%\n",
      "total_backward_count 1830730 real_backward_count 111166   6.072%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.726105/  1.070043, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 66.0850%\n",
      "layer   3  Sparsity: 63.5886%\n",
      "total_backward_count 1840520 real_backward_count 111433   6.054%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.699161/  1.031939, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9926%\n",
      "layer   2  Sparsity: 65.9720%\n",
      "layer   3  Sparsity: 63.6040%\n",
      "total_backward_count 1850310 real_backward_count 111705   6.037%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.700256/  1.015329, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0118%\n",
      "layer   2  Sparsity: 65.9337%\n",
      "layer   3  Sparsity: 63.3414%\n",
      "total_backward_count 1860100 real_backward_count 111982   6.020%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.694832/  1.042251, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9992%\n",
      "layer   2  Sparsity: 65.8708%\n",
      "layer   3  Sparsity: 63.6824%\n",
      "total_backward_count 1869890 real_backward_count 112260   6.004%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.703428/  1.040756, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9890%\n",
      "layer   2  Sparsity: 65.8047%\n",
      "layer   3  Sparsity: 64.4681%\n",
      "total_backward_count 1879680 real_backward_count 112519   5.986%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.704504/  1.056658, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 65.8488%\n",
      "layer   3  Sparsity: 64.1517%\n",
      "total_backward_count 1889470 real_backward_count 112793   5.970%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.714112/  1.084556, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0055%\n",
      "layer   2  Sparsity: 65.6879%\n",
      "layer   3  Sparsity: 64.0563%\n",
      "total_backward_count 1899260 real_backward_count 113054   5.953%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.696568/  1.033920, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9971%\n",
      "layer   2  Sparsity: 65.7418%\n",
      "layer   3  Sparsity: 64.1153%\n",
      "total_backward_count 1909050 real_backward_count 113315   5.936%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.710512/  1.052322, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0184%\n",
      "layer   2  Sparsity: 65.8231%\n",
      "layer   3  Sparsity: 63.8770%\n",
      "total_backward_count 1918840 real_backward_count 113574   5.919%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.711905/  1.044455, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0071%\n",
      "layer   2  Sparsity: 65.6268%\n",
      "layer   3  Sparsity: 64.1445%\n",
      "total_backward_count 1928630 real_backward_count 113822   5.902%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.703085/  1.008785, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9880%\n",
      "layer   2  Sparsity: 65.6831%\n",
      "layer   3  Sparsity: 64.4768%\n",
      "total_backward_count 1938420 real_backward_count 114088   5.886%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.685297/  1.033739, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0133%\n",
      "layer   2  Sparsity: 65.8289%\n",
      "layer   3  Sparsity: 64.4068%\n",
      "total_backward_count 1948210 real_backward_count 114332   5.869%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.719557/  1.031060, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 65.8909%\n",
      "layer   3  Sparsity: 64.7830%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790779a6d7f64ca8a83bf891f461abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.71956</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>1.03106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-55</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gx72vp7e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gx72vp7e</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_004815-gx72vp7e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: au098gzt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_050533-au098gzt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/au098gzt' target=\"_blank\">polished-sweep-60</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/au098gzt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/au098gzt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251115_050542_748', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 749.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 968.0\n",
      "lif layer 1 self.abs_max_v: 1075.0\n",
      "fc layer 2 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 360.0\n",
      "fc layer 1 self.abs_max_out: 1165.0\n",
      "lif layer 1 self.abs_max_v: 1515.5\n",
      "fc layer 2 self.abs_max_out: 541.0\n",
      "lif layer 2 self.abs_max_v: 550.0\n",
      "fc layer 1 self.abs_max_out: 1296.0\n",
      "lif layer 1 self.abs_max_v: 1561.0\n",
      "fc layer 2 self.abs_max_out: 589.0\n",
      "lif layer 2 self.abs_max_v: 674.5\n",
      "fc layer 1 self.abs_max_out: 2411.0\n",
      "lif layer 1 self.abs_max_v: 2546.5\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "lif layer 2 self.abs_max_v: 1094.0\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 1 self.abs_max_out: 2998.0\n",
      "lif layer 1 self.abs_max_v: 3233.5\n",
      "fc layer 2 self.abs_max_out: 1107.0\n",
      "lif layer 2 self.abs_max_v: 1460.5\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "fc layer 1 self.abs_max_out: 4486.0\n",
      "lif layer 1 self.abs_max_v: 4486.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "lif layer 2 self.abs_max_v: 2133.0\n",
      "lif layer 1 self.abs_max_v: 4537.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "lif layer 1 self.abs_max_v: 5132.5\n",
      "lif layer 2 self.abs_max_v: 2282.5\n",
      "lif layer 2 self.abs_max_v: 2287.5\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 1 self.abs_max_out: 4797.0\n",
      "fc layer 2 self.abs_max_out: 1611.0\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "lif layer 2 self.abs_max_v: 2316.5\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 2397.5\n",
      "fc layer 1 self.abs_max_out: 4911.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "lif layer 1 self.abs_max_v: 5211.0\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "fc layer 1 self.abs_max_out: 6112.0\n",
      "lif layer 1 self.abs_max_v: 6112.0\n",
      "fc layer 2 self.abs_max_out: 1667.0\n",
      "lif layer 2 self.abs_max_v: 2573.5\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "fc layer 2 self.abs_max_out: 1694.0\n",
      "fc layer 2 self.abs_max_out: 1701.0\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "fc layer 2 self.abs_max_out: 1869.0\n",
      "lif layer 2 self.abs_max_v: 2713.5\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "lif layer 2 self.abs_max_v: 2982.5\n",
      "fc layer 2 self.abs_max_out: 1956.0\n",
      "fc layer 2 self.abs_max_out: 2040.0\n",
      "fc layer 1 self.abs_max_out: 6548.0\n",
      "lif layer 1 self.abs_max_v: 6548.0\n",
      "fc layer 2 self.abs_max_out: 2504.0\n",
      "lif layer 2 self.abs_max_v: 3089.5\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "lif layer 2 self.abs_max_v: 3171.5\n",
      "fc layer 2 self.abs_max_out: 2741.0\n",
      "lif layer 2 self.abs_max_v: 3648.0\n",
      "lif layer 2 self.abs_max_v: 3971.5\n",
      "lif layer 2 self.abs_max_v: 4152.0\n",
      "lif layer 2 self.abs_max_v: 4376.0\n",
      "fc layer 2 self.abs_max_out: 2785.0\n",
      "fc layer 2 self.abs_max_out: 2968.0\n",
      "fc layer 2 self.abs_max_out: 2976.0\n",
      "fc layer 2 self.abs_max_out: 3098.0\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "fc layer 1 self.abs_max_out: 8352.0\n",
      "lif layer 1 self.abs_max_v: 8352.0\n",
      "fc layer 1 self.abs_max_out: 8463.0\n",
      "lif layer 1 self.abs_max_v: 8463.0\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 2 self.abs_max_out: 3245.0\n",
      "fc layer 2 self.abs_max_out: 3400.0\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 2 self.abs_max_out: 3486.0\n",
      "fc layer 2 self.abs_max_out: 3534.0\n",
      "fc layer 2 self.abs_max_out: 3622.0\n",
      "fc layer 2 self.abs_max_out: 3893.0\n",
      "fc layer 1 self.abs_max_out: 9446.0\n",
      "lif layer 1 self.abs_max_v: 9446.0\n",
      "fc layer 1 self.abs_max_out: 9643.0\n",
      "lif layer 1 self.abs_max_v: 9643.0\n",
      "lif layer 2 self.abs_max_v: 4491.5\n",
      "lif layer 2 self.abs_max_v: 4934.0\n",
      "lif layer 2 self.abs_max_v: 5054.0\n",
      "fc layer 2 self.abs_max_out: 4159.0\n",
      "fc layer 3 self.abs_max_out: 765.0\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "lif layer 2 self.abs_max_v: 5307.5\n",
      "lif layer 2 self.abs_max_v: 5407.0\n",
      "fc layer 2 self.abs_max_out: 4244.0\n",
      "fc layer 2 self.abs_max_out: 4558.0\n",
      "fc layer 2 self.abs_max_out: 4871.0\n",
      "fc layer 2 self.abs_max_out: 4971.0\n",
      "lif layer 2 self.abs_max_v: 5691.5\n",
      "lif layer 2 self.abs_max_v: 5743.5\n",
      "lif layer 1 self.abs_max_v: 10130.0\n",
      "lif layer 2 self.abs_max_v: 5794.0\n",
      "lif layer 2 self.abs_max_v: 5908.5\n",
      "lif layer 1 self.abs_max_v: 10324.5\n",
      "fc layer 2 self.abs_max_out: 5136.0\n",
      "lif layer 2 self.abs_max_v: 6233.5\n",
      "lif layer 2 self.abs_max_v: 6305.5\n",
      "lif layer 2 self.abs_max_v: 6491.0\n",
      "lif layer 2 self.abs_max_v: 6549.0\n",
      "fc layer 3 self.abs_max_out: 993.0\n",
      "fc layer 3 self.abs_max_out: 1075.0\n",
      "lif layer 2 self.abs_max_v: 6817.5\n",
      "lif layer 2 self.abs_max_v: 7060.0\n",
      "lif layer 2 self.abs_max_v: 7112.0\n",
      "lif layer 2 self.abs_max_v: 7364.0\n",
      "lif layer 1 self.abs_max_v: 10541.0\n",
      "lif layer 2 self.abs_max_v: 8011.0\n",
      "lif layer 2 self.abs_max_v: 8270.0\n",
      "lif layer 2 self.abs_max_v: 8471.0\n",
      "fc layer 2 self.abs_max_out: 5641.0\n",
      "fc layer 2 self.abs_max_out: 5642.0\n",
      "fc layer 3 self.abs_max_out: 1122.0\n",
      "lif layer 1 self.abs_max_v: 10551.5\n",
      "fc layer 3 self.abs_max_out: 1175.0\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "fc layer 3 self.abs_max_out: 1189.0\n",
      "lif layer 1 self.abs_max_v: 10994.0\n",
      "lif layer 1 self.abs_max_v: 11467.5\n",
      "lif layer 1 self.abs_max_v: 11912.0\n",
      "fc layer 3 self.abs_max_out: 1345.0\n",
      "fc layer 2 self.abs_max_out: 5930.0\n",
      "fc layer 2 self.abs_max_out: 6121.0\n",
      "fc layer 2 self.abs_max_out: 6313.0\n",
      "fc layer 3 self.abs_max_out: 1385.0\n",
      "fc layer 3 self.abs_max_out: 1398.0\n",
      "fc layer 3 self.abs_max_out: 1611.0\n",
      "lif layer 1 self.abs_max_v: 12841.0\n",
      "lif layer 1 self.abs_max_v: 13155.5\n",
      "fc layer 2 self.abs_max_out: 6483.0\n",
      "fc layer 1 self.abs_max_out: 9950.0\n",
      "lif layer 1 self.abs_max_v: 14760.0\n",
      "fc layer 1 self.abs_max_out: 11098.0\n",
      "fc layer 1 self.abs_max_out: 11981.0\n",
      "lif layer 1 self.abs_max_v: 15720.0\n",
      "lif layer 1 self.abs_max_v: 16611.0\n",
      "lif layer 2 self.abs_max_v: 8507.0\n",
      "lif layer 2 self.abs_max_v: 8699.0\n",
      "lif layer 2 self.abs_max_v: 8868.5\n",
      "lif layer 1 self.abs_max_v: 16682.0\n",
      "lif layer 2 self.abs_max_v: 9120.5\n",
      "lif layer 2 self.abs_max_v: 9362.0\n",
      "fc layer 1 self.abs_max_out: 12376.0\n",
      "lif layer 1 self.abs_max_v: 17472.0\n",
      "lif layer 2 self.abs_max_v: 9568.0\n",
      "lif layer 2 self.abs_max_v: 9736.0\n",
      "fc layer 2 self.abs_max_out: 6635.0\n",
      "lif layer 1 self.abs_max_v: 17744.0\n",
      "lif layer 1 self.abs_max_v: 17780.0\n",
      "lif layer 2 self.abs_max_v: 9770.0\n",
      "lif layer 1 self.abs_max_v: 18545.0\n",
      "fc layer 1 self.abs_max_out: 12803.0\n",
      "lif layer 1 self.abs_max_v: 20507.5\n",
      "lif layer 1 self.abs_max_v: 21124.0\n",
      "lif layer 1 self.abs_max_v: 21528.0\n",
      "fc layer 1 self.abs_max_out: 13108.0\n",
      "lif layer 1 self.abs_max_v: 23872.0\n",
      "fc layer 1 self.abs_max_out: 13166.0\n",
      "lif layer 2 self.abs_max_v: 9980.0\n",
      "lif layer 2 self.abs_max_v: 10238.5\n",
      "lif layer 1 self.abs_max_v: 23979.5\n",
      "lif layer 1 self.abs_max_v: 24243.0\n",
      "lif layer 2 self.abs_max_v: 10332.0\n",
      "fc layer 1 self.abs_max_out: 14755.0\n",
      "fc layer 2 self.abs_max_out: 6806.0\n",
      "lif layer 2 self.abs_max_v: 10630.5\n",
      "lif layer 2 self.abs_max_v: 10951.5\n",
      "lif layer 2 self.abs_max_v: 11220.0\n",
      "lif layer 2 self.abs_max_v: 11275.0\n",
      "fc layer 2 self.abs_max_out: 6813.0\n",
      "fc layer 2 self.abs_max_out: 6825.0\n",
      "fc layer 2 self.abs_max_out: 6953.0\n",
      "fc layer 2 self.abs_max_out: 6992.0\n",
      "fc layer 2 self.abs_max_out: 7197.0\n",
      "lif layer 2 self.abs_max_v: 11393.5\n",
      "lif layer 2 self.abs_max_v: 12069.0\n",
      "fc layer 2 self.abs_max_out: 7242.0\n",
      "fc layer 2 self.abs_max_out: 7270.0\n",
      "fc layer 2 self.abs_max_out: 7438.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.784076/  2.027771, val:  32.92%, val_best:  32.92%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6244%\n",
      "layer   2  Sparsity: 71.8853%\n",
      "layer   3  Sparsity: 81.7386%\n",
      "total_backward_count 9790 real_backward_count 2018  20.613%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 15259.0\n",
      "fc layer 2 self.abs_max_out: 7632.0\n",
      "fc layer 2 self.abs_max_out: 7753.0\n",
      "fc layer 1 self.abs_max_out: 15990.0\n",
      "lif layer 2 self.abs_max_v: 12209.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.810088/  2.051942, val:  43.33%, val_best:  43.33%, tr:  98.06%, tr_best:  98.88%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6101%\n",
      "layer   2  Sparsity: 75.1900%\n",
      "layer   3  Sparsity: 84.9797%\n",
      "total_backward_count 19580 real_backward_count 3874  19.785%\n",
      "lif layer 1 self.abs_max_v: 25727.0\n",
      "lif layer 2 self.abs_max_v: 12278.0\n",
      "fc layer 2 self.abs_max_out: 7776.0\n",
      "lif layer 1 self.abs_max_v: 25875.0\n",
      "lif layer 1 self.abs_max_v: 27455.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.883670/  2.099307, val:  39.58%, val_best:  43.33%, tr:  97.96%, tr_best:  98.88%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6030%\n",
      "layer   2  Sparsity: 77.8749%\n",
      "layer   3  Sparsity: 86.4271%\n",
      "total_backward_count 29370 real_backward_count 5838  19.877%\n",
      "fc layer 1 self.abs_max_out: 16485.0\n",
      "lif layer 2 self.abs_max_v: 12359.5\n",
      "lif layer 2 self.abs_max_v: 12627.5\n",
      "lif layer 2 self.abs_max_v: 12688.0\n",
      "lif layer 2 self.abs_max_v: 12825.0\n",
      "lif layer 2 self.abs_max_v: 12855.0\n",
      "lif layer 2 self.abs_max_v: 12867.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.845618/  2.095026, val:  30.00%, val_best:  43.33%, tr:  98.37%, tr_best:  98.88%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6162%\n",
      "layer   2  Sparsity: 77.2472%\n",
      "layer   3  Sparsity: 84.8936%\n",
      "total_backward_count 39160 real_backward_count 7697  19.655%\n",
      "lif layer 2 self.abs_max_v: 13066.0\n",
      "lif layer 2 self.abs_max_v: 13186.0\n",
      "lif layer 1 self.abs_max_v: 28236.5\n",
      "lif layer 1 self.abs_max_v: 28688.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.883921/  2.058891, val:  37.92%, val_best:  43.33%, tr:  97.85%, tr_best:  98.88%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6104%\n",
      "layer   2  Sparsity: 77.7414%\n",
      "layer   3  Sparsity: 85.9874%\n",
      "total_backward_count 48950 real_backward_count 9539  19.487%\n",
      "lif layer 2 self.abs_max_v: 13187.5\n",
      "fc layer 1 self.abs_max_out: 16692.0\n",
      "lif layer 1 self.abs_max_v: 30612.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.870118/  2.108750, val:  35.42%, val_best:  43.33%, tr:  98.06%, tr_best:  98.88%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5970%\n",
      "layer   2  Sparsity: 77.5582%\n",
      "layer   3  Sparsity: 86.5164%\n",
      "total_backward_count 58740 real_backward_count 11321  19.273%\n",
      "fc layer 1 self.abs_max_out: 17539.0\n",
      "lif layer 2 self.abs_max_v: 13614.5\n",
      "lif layer 2 self.abs_max_v: 13682.5\n",
      "lif layer 2 self.abs_max_v: 13687.5\n",
      "lif layer 2 self.abs_max_v: 13797.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.935514/  2.094934, val:  41.25%, val_best:  43.33%, tr:  97.45%, tr_best:  98.88%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5876%\n",
      "layer   2  Sparsity: 78.3214%\n",
      "layer   3  Sparsity: 87.4636%\n",
      "total_backward_count 68530 real_backward_count 13188  19.244%\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.929352/  2.126390, val:  43.75%, val_best:  43.75%, tr:  98.26%, tr_best:  98.88%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5989%\n",
      "layer   2  Sparsity: 79.7728%\n",
      "layer   3  Sparsity: 87.5873%\n",
      "total_backward_count 78320 real_backward_count 15050  19.216%\n",
      "lif layer 2 self.abs_max_v: 14258.0\n",
      "lif layer 2 self.abs_max_v: 14636.0\n",
      "fc layer 2 self.abs_max_out: 7925.0\n",
      "lif layer 2 self.abs_max_v: 15243.0\n",
      "lif layer 2 self.abs_max_v: 15422.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.934875/  2.118495, val:  37.92%, val_best:  43.75%, tr:  97.34%, tr_best:  98.88%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6154%\n",
      "layer   2  Sparsity: 79.7447%\n",
      "layer   3  Sparsity: 87.4964%\n",
      "total_backward_count 88110 real_backward_count 17117  19.427%\n",
      "fc layer 2 self.abs_max_out: 8119.0\n",
      "fc layer 2 self.abs_max_out: 8694.0\n",
      "lif layer 2 self.abs_max_v: 15464.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.883440/  2.146654, val:  35.42%, val_best:  43.75%, tr:  97.34%, tr_best:  98.88%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5997%\n",
      "layer   2  Sparsity: 77.7185%\n",
      "layer   3  Sparsity: 86.2822%\n",
      "total_backward_count 97900 real_backward_count 19029  19.437%\n",
      "lif layer 2 self.abs_max_v: 15727.5\n",
      "fc layer 2 self.abs_max_out: 8950.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.895139/  2.117703, val:  29.58%, val_best:  43.75%, tr:  98.06%, tr_best:  98.88%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6256%\n",
      "layer   2  Sparsity: 76.7115%\n",
      "layer   3  Sparsity: 85.4349%\n",
      "total_backward_count 107690 real_backward_count 20823  19.336%\n",
      "fc layer 1 self.abs_max_out: 17608.0\n",
      "lif layer 1 self.abs_max_v: 30816.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.881732/  2.079413, val:  30.42%, val_best:  43.75%, tr:  98.37%, tr_best:  98.88%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6302%\n",
      "layer   2  Sparsity: 78.7098%\n",
      "layer   3  Sparsity: 85.9176%\n",
      "total_backward_count 117480 real_backward_count 22665  19.293%\n",
      "fc layer 1 self.abs_max_out: 18284.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.880043/  2.099188, val:  22.50%, val_best:  43.75%, tr:  98.37%, tr_best:  98.88%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6097%\n",
      "layer   2  Sparsity: 79.5053%\n",
      "layer   3  Sparsity: 86.0678%\n",
      "total_backward_count 127270 real_backward_count 24419  19.187%\n",
      "fc layer 3 self.abs_max_out: 1640.0\n",
      "lif layer 1 self.abs_max_v: 31233.5\n",
      "lif layer 1 self.abs_max_v: 31398.0\n",
      "lif layer 1 self.abs_max_v: 31741.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.906549/  2.085052, val:  29.58%, val_best:  43.75%, tr:  97.55%, tr_best:  98.88%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6175%\n",
      "layer   2  Sparsity: 78.2537%\n",
      "layer   3  Sparsity: 86.0008%\n",
      "total_backward_count 137060 real_backward_count 26251  19.153%\n",
      "lif layer 1 self.abs_max_v: 31822.0\n",
      "lif layer 2 self.abs_max_v: 15927.0\n",
      "lif layer 1 self.abs_max_v: 32359.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.876985/  2.126755, val:  38.33%, val_best:  43.75%, tr:  98.26%, tr_best:  98.88%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6124%\n",
      "layer   2  Sparsity: 78.4253%\n",
      "layer   3  Sparsity: 85.7584%\n",
      "total_backward_count 146850 real_backward_count 28028  19.086%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.910987/  2.131111, val:  35.83%, val_best:  43.75%, tr:  98.57%, tr_best:  98.88%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6059%\n",
      "layer   2  Sparsity: 79.8184%\n",
      "layer   3  Sparsity: 86.0111%\n",
      "total_backward_count 156640 real_backward_count 29839  19.049%\n",
      "fc layer 1 self.abs_max_out: 18361.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.902856/  2.049014, val:  43.75%, val_best:  43.75%, tr:  98.77%, tr_best:  98.88%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5619%\n",
      "layer   2  Sparsity: 78.5576%\n",
      "layer   3  Sparsity: 85.5383%\n",
      "total_backward_count 166430 real_backward_count 31618  18.998%\n",
      "fc layer 1 self.abs_max_out: 18772.0\n",
      "lif layer 1 self.abs_max_v: 32744.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.900340/  2.142920, val:  22.50%, val_best:  43.75%, tr:  97.34%, tr_best:  98.88%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6007%\n",
      "layer   2  Sparsity: 78.9181%\n",
      "layer   3  Sparsity: 85.8620%\n",
      "total_backward_count 176220 real_backward_count 33554  19.041%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.914652/  2.090631, val:  38.33%, val_best:  43.75%, tr:  97.75%, tr_best:  98.88%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6095%\n",
      "layer   2  Sparsity: 78.1623%\n",
      "layer   3  Sparsity: 86.2364%\n",
      "total_backward_count 186010 real_backward_count 35380  19.020%\n",
      "fc layer 1 self.abs_max_out: 19092.0\n",
      "lif layer 1 self.abs_max_v: 33255.0\n",
      "lif layer 1 self.abs_max_v: 33578.0\n",
      "lif layer 2 self.abs_max_v: 16068.5\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.904778/  2.175061, val:  30.42%, val_best:  43.75%, tr:  98.26%, tr_best:  98.88%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6064%\n",
      "layer   2  Sparsity: 77.4214%\n",
      "layer   3  Sparsity: 85.5466%\n",
      "total_backward_count 195800 real_backward_count 37127  18.962%\n",
      "fc layer 1 self.abs_max_out: 19273.0\n",
      "lif layer 1 self.abs_max_v: 33738.0\n",
      "lif layer 1 self.abs_max_v: 34071.5\n",
      "lif layer 1 self.abs_max_v: 34531.0\n",
      "lif layer 2 self.abs_max_v: 16130.5\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.865540/  2.166350, val:  20.00%, val_best:  43.75%, tr:  98.26%, tr_best:  98.88%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5890%\n",
      "layer   2  Sparsity: 77.9326%\n",
      "layer   3  Sparsity: 84.6940%\n",
      "total_backward_count 205590 real_backward_count 38771  18.858%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.849592/  2.154441, val:  25.83%, val_best:  43.75%, tr:  98.26%, tr_best:  98.88%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6408%\n",
      "layer   2  Sparsity: 78.9685%\n",
      "layer   3  Sparsity: 84.5538%\n",
      "total_backward_count 215380 real_backward_count 40630  18.864%\n",
      "fc layer 1 self.abs_max_out: 19630.0\n",
      "fc layer 2 self.abs_max_out: 9203.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.833557/  2.041426, val:  43.33%, val_best:  43.75%, tr:  98.26%, tr_best:  98.88%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6182%\n",
      "layer   2  Sparsity: 77.5944%\n",
      "layer   3  Sparsity: 83.1655%\n",
      "total_backward_count 225170 real_backward_count 42381  18.822%\n",
      "lif layer 1 self.abs_max_v: 34769.5\n",
      "lif layer 1 self.abs_max_v: 35233.0\n",
      "lif layer 2 self.abs_max_v: 16670.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.777097/  2.051420, val:  36.67%, val_best:  43.75%, tr:  98.67%, tr_best:  98.88%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5987%\n",
      "layer   2  Sparsity: 78.0749%\n",
      "layer   3  Sparsity: 82.5627%\n",
      "total_backward_count 234960 real_backward_count 44064  18.754%\n",
      "fc layer 1 self.abs_max_out: 20015.0\n",
      "lif layer 1 self.abs_max_v: 35528.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.814031/  2.040028, val:  50.42%, val_best:  50.42%, tr:  98.57%, tr_best:  98.88%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6368%\n",
      "layer   2  Sparsity: 78.0397%\n",
      "layer   3  Sparsity: 83.0583%\n",
      "total_backward_count 244750 real_backward_count 45783  18.706%\n",
      "fc layer 3 self.abs_max_out: 1710.0\n",
      "fc layer 3 self.abs_max_out: 1795.0\n",
      "fc layer 2 self.abs_max_out: 9568.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.828441/  2.044783, val:  38.33%, val_best:  50.42%, tr:  97.96%, tr_best:  98.88%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6221%\n",
      "layer   2  Sparsity: 77.7102%\n",
      "layer   3  Sparsity: 82.9034%\n",
      "total_backward_count 254540 real_backward_count 47662  18.725%\n",
      "lif layer 1 self.abs_max_v: 35765.0\n",
      "lif layer 1 self.abs_max_v: 36201.5\n",
      "fc layer 1 self.abs_max_out: 20136.0\n",
      "lif layer 2 self.abs_max_v: 16849.0\n",
      "lif layer 2 self.abs_max_v: 16886.5\n",
      "lif layer 2 self.abs_max_v: 17091.5\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.791774/  2.014793, val:  42.08%, val_best:  50.42%, tr:  98.77%, tr_best:  98.88%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6099%\n",
      "layer   2  Sparsity: 77.6806%\n",
      "layer   3  Sparsity: 83.0910%\n",
      "total_backward_count 264330 real_backward_count 49454  18.709%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.774441/  1.969445, val:  42.50%, val_best:  50.42%, tr:  98.57%, tr_best:  98.88%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6090%\n",
      "layer   2  Sparsity: 77.7316%\n",
      "layer   3  Sparsity: 82.2716%\n",
      "total_backward_count 274120 real_backward_count 51175  18.669%\n",
      "lif layer 1 self.abs_max_v: 36346.5\n",
      "fc layer 1 self.abs_max_out: 20188.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.790526/  2.034682, val:  36.25%, val_best:  50.42%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5984%\n",
      "layer   2  Sparsity: 78.4018%\n",
      "layer   3  Sparsity: 83.1206%\n",
      "total_backward_count 283910 real_backward_count 52884  18.627%\n",
      "lif layer 1 self.abs_max_v: 36510.0\n",
      "fc layer 3 self.abs_max_out: 1796.0\n",
      "fc layer 3 self.abs_max_out: 1803.0\n",
      "fc layer 3 self.abs_max_out: 1820.0\n",
      "fc layer 1 self.abs_max_out: 20314.0\n",
      "fc layer 1 self.abs_max_out: 20683.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.779681/  2.064869, val:  26.25%, val_best:  50.42%, tr:  98.67%, tr_best:  98.88%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6150%\n",
      "layer   2  Sparsity: 77.4756%\n",
      "layer   3  Sparsity: 82.4854%\n",
      "total_backward_count 293700 real_backward_count 54618  18.597%\n",
      "lif layer 1 self.abs_max_v: 36636.5\n",
      "fc layer 1 self.abs_max_out: 20704.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.798400/  2.003102, val:  46.25%, val_best:  50.42%, tr:  98.37%, tr_best:  98.88%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6102%\n",
      "layer   2  Sparsity: 79.0301%\n",
      "layer   3  Sparsity: 82.7034%\n",
      "total_backward_count 303490 real_backward_count 56436  18.596%\n",
      "lif layer 1 self.abs_max_v: 36975.5\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.804454/  2.013772, val:  41.25%, val_best:  50.42%, tr:  98.16%, tr_best:  98.88%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6481%\n",
      "layer   2  Sparsity: 78.4309%\n",
      "layer   3  Sparsity: 82.7266%\n",
      "total_backward_count 313280 real_backward_count 58296  18.608%\n",
      "fc layer 1 self.abs_max_out: 21057.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.820280/  2.098445, val:  26.25%, val_best:  50.42%, tr:  98.06%, tr_best:  98.88%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5873%\n",
      "layer   2  Sparsity: 77.5839%\n",
      "layer   3  Sparsity: 81.8112%\n",
      "total_backward_count 323070 real_backward_count 59996  18.571%\n",
      "fc layer 1 self.abs_max_out: 21117.0\n",
      "lif layer 1 self.abs_max_v: 36998.5\n",
      "lif layer 1 self.abs_max_v: 37493.5\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.774022/  1.998712, val:  31.25%, val_best:  50.42%, tr:  98.67%, tr_best:  98.88%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6096%\n",
      "layer   2  Sparsity: 77.7662%\n",
      "layer   3  Sparsity: 82.2005%\n",
      "total_backward_count 332860 real_backward_count 61786  18.562%\n",
      "fc layer 2 self.abs_max_out: 9810.0\n",
      "lif layer 2 self.abs_max_v: 17220.5\n",
      "lif layer 2 self.abs_max_v: 17589.5\n",
      "fc layer 2 self.abs_max_out: 9923.0\n",
      "fc layer 2 self.abs_max_out: 9953.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.767532/  2.040281, val:  35.42%, val_best:  50.42%, tr:  98.77%, tr_best:  98.88%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5919%\n",
      "layer   2  Sparsity: 76.8199%\n",
      "layer   3  Sparsity: 81.9957%\n",
      "total_backward_count 342650 real_backward_count 63575  18.554%\n",
      "lif layer 2 self.abs_max_v: 17893.5\n",
      "lif layer 2 self.abs_max_v: 18012.5\n",
      "fc layer 2 self.abs_max_out: 10290.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.789584/  2.055482, val:  48.33%, val_best:  50.42%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6276%\n",
      "layer   2  Sparsity: 77.5733%\n",
      "layer   3  Sparsity: 81.9449%\n",
      "total_backward_count 352440 real_backward_count 65256  18.515%\n",
      "fc layer 1 self.abs_max_out: 21449.0\n",
      "lif layer 1 self.abs_max_v: 37517.0\n",
      "fc layer 3 self.abs_max_out: 1868.0\n",
      "lif layer 1 self.abs_max_v: 37584.0\n",
      "lif layer 1 self.abs_max_v: 38114.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.753999/  1.990307, val:  47.50%, val_best:  50.42%, tr:  97.96%, tr_best:  99.39%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 88.6126%\n",
      "layer   2  Sparsity: 78.1383%\n",
      "layer   3  Sparsity: 81.3936%\n",
      "total_backward_count 362230 real_backward_count 66965  18.487%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.751155/  2.074859, val:  37.92%, val_best:  50.42%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6091%\n",
      "layer   2  Sparsity: 78.0939%\n",
      "layer   3  Sparsity: 82.1772%\n",
      "total_backward_count 372020 real_backward_count 68588  18.437%\n",
      "fc layer 1 self.abs_max_out: 21581.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.763790/  2.022847, val:  34.58%, val_best:  50.42%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6280%\n",
      "layer   2  Sparsity: 77.4307%\n",
      "layer   3  Sparsity: 81.4903%\n",
      "total_backward_count 381810 real_backward_count 70315  18.416%\n",
      "lif layer 1 self.abs_max_v: 38494.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.780952/  2.077823, val:  27.50%, val_best:  50.42%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6257%\n",
      "layer   2  Sparsity: 76.9560%\n",
      "layer   3  Sparsity: 82.5734%\n",
      "total_backward_count 391600 real_backward_count 72062  18.402%\n",
      "lif layer 2 self.abs_max_v: 18040.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.767842/  2.042650, val:  40.42%, val_best:  50.42%, tr:  98.77%, tr_best:  99.39%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6322%\n",
      "layer   2  Sparsity: 76.2174%\n",
      "layer   3  Sparsity: 81.6842%\n",
      "total_backward_count 401390 real_backward_count 73745  18.372%\n",
      "lif layer 2 self.abs_max_v: 18357.0\n",
      "fc layer 1 self.abs_max_out: 21817.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.785460/  2.053474, val:  41.67%, val_best:  50.42%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6016%\n",
      "layer   2  Sparsity: 78.2396%\n",
      "layer   3  Sparsity: 83.0121%\n",
      "total_backward_count 411180 real_backward_count 75497  18.361%\n",
      "fc layer 1 self.abs_max_out: 21889.0\n",
      "lif layer 1 self.abs_max_v: 38833.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.807562/  2.020326, val:  37.08%, val_best:  50.42%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5992%\n",
      "layer   2  Sparsity: 77.5023%\n",
      "layer   3  Sparsity: 82.5609%\n",
      "total_backward_count 420970 real_backward_count 77207  18.340%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.774687/  2.009752, val:  48.75%, val_best:  50.42%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6253%\n",
      "layer   2  Sparsity: 78.7050%\n",
      "layer   3  Sparsity: 82.3481%\n",
      "total_backward_count 430760 real_backward_count 78956  18.329%\n",
      "fc layer 1 self.abs_max_out: 21922.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.749809/  2.004985, val:  33.33%, val_best:  50.42%, tr:  99.18%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5952%\n",
      "layer   2  Sparsity: 77.2217%\n",
      "layer   3  Sparsity: 80.4283%\n",
      "total_backward_count 440550 real_backward_count 80644  18.305%\n",
      "fc layer 1 self.abs_max_out: 21959.0\n",
      "lif layer 1 self.abs_max_v: 39093.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.728521/  1.994347, val:  40.00%, val_best:  50.42%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6032%\n",
      "layer   2  Sparsity: 77.7218%\n",
      "layer   3  Sparsity: 80.2418%\n",
      "total_backward_count 450340 real_backward_count 82353  18.287%\n",
      "fc layer 3 self.abs_max_out: 1876.0\n",
      "fc layer 1 self.abs_max_out: 22134.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.760761/  2.040221, val:  37.50%, val_best:  50.42%, tr:  97.75%, tr_best:  99.39%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6183%\n",
      "layer   2  Sparsity: 77.5505%\n",
      "layer   3  Sparsity: 80.9816%\n",
      "total_backward_count 460130 real_backward_count 84099  18.277%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.752924/  2.031003, val:  37.92%, val_best:  50.42%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6086%\n",
      "layer   2  Sparsity: 78.1874%\n",
      "layer   3  Sparsity: 81.4898%\n",
      "total_backward_count 469920 real_backward_count 85779  18.254%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.779807/  2.019218, val:  37.08%, val_best:  50.42%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6284%\n",
      "layer   2  Sparsity: 77.2889%\n",
      "layer   3  Sparsity: 81.8187%\n",
      "total_backward_count 479710 real_backward_count 87417  18.223%\n",
      "lif layer 2 self.abs_max_v: 18679.0\n",
      "fc layer 1 self.abs_max_out: 22328.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.798468/  2.000426, val:  50.42%, val_best:  50.42%, tr:  97.96%, tr_best:  99.39%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6175%\n",
      "layer   2  Sparsity: 76.7890%\n",
      "layer   3  Sparsity: 82.7960%\n",
      "total_backward_count 489500 real_backward_count 89149  18.212%\n",
      "fc layer 2 self.abs_max_out: 10362.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.769317/  2.090197, val:  26.25%, val_best:  50.42%, tr:  98.77%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6121%\n",
      "layer   2  Sparsity: 78.0730%\n",
      "layer   3  Sparsity: 81.4374%\n",
      "total_backward_count 499290 real_backward_count 90842  18.194%\n",
      "lif layer 2 self.abs_max_v: 18778.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.757445/  1.991108, val:  40.83%, val_best:  50.42%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6414%\n",
      "layer   2  Sparsity: 78.0111%\n",
      "layer   3  Sparsity: 81.2281%\n",
      "total_backward_count 509080 real_backward_count 92519  18.174%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.798953/  2.003696, val:  41.67%, val_best:  50.42%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6165%\n",
      "layer   2  Sparsity: 79.1754%\n",
      "layer   3  Sparsity: 82.0079%\n",
      "total_backward_count 518870 real_backward_count 94243  18.163%\n",
      "fc layer 1 self.abs_max_out: 22620.0\n",
      "lif layer 1 self.abs_max_v: 39358.0\n",
      "lif layer 1 self.abs_max_v: 39660.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.782615/  2.033132, val:  35.83%, val_best:  50.42%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5805%\n",
      "layer   2  Sparsity: 78.0026%\n",
      "layer   3  Sparsity: 81.9895%\n",
      "total_backward_count 528660 real_backward_count 96026  18.164%\n",
      "fc layer 2 self.abs_max_out: 10370.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.789155/  2.029867, val:  38.75%, val_best:  50.42%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6317%\n",
      "layer   2  Sparsity: 79.0540%\n",
      "layer   3  Sparsity: 81.9558%\n",
      "total_backward_count 538450 real_backward_count 97793  18.162%\n",
      "fc layer 3 self.abs_max_out: 2032.0\n",
      "lif layer 2 self.abs_max_v: 19182.5\n",
      "lif layer 2 self.abs_max_v: 19796.5\n",
      "lif layer 1 self.abs_max_v: 39783.0\n",
      "lif layer 1 self.abs_max_v: 40142.0\n",
      "lif layer 1 self.abs_max_v: 40753.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.714323/  1.964324, val:  40.00%, val_best:  50.42%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6189%\n",
      "layer   2  Sparsity: 77.8386%\n",
      "layer   3  Sparsity: 79.4623%\n",
      "total_backward_count 548240 real_backward_count 99488  18.147%\n",
      "lif layer 1 self.abs_max_v: 40803.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.733497/  2.075840, val:  17.50%, val_best:  50.42%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6058%\n",
      "layer   2  Sparsity: 79.0715%\n",
      "layer   3  Sparsity: 80.3550%\n",
      "total_backward_count 558030 real_backward_count 101145  18.125%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.742244/  2.045078, val:  37.08%, val_best:  50.42%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6231%\n",
      "layer   2  Sparsity: 79.6535%\n",
      "layer   3  Sparsity: 80.7698%\n",
      "total_backward_count 567820 real_backward_count 102863  18.115%\n",
      "lif layer 1 self.abs_max_v: 41034.0\n",
      "fc layer 1 self.abs_max_out: 22772.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.775826/  2.067701, val:  36.67%, val_best:  50.42%, tr:  98.06%, tr_best:  99.39%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6038%\n",
      "layer   2  Sparsity: 78.7774%\n",
      "layer   3  Sparsity: 82.0617%\n",
      "total_backward_count 577610 real_backward_count 104511  18.094%\n",
      "fc layer 1 self.abs_max_out: 22947.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.815218/  2.098910, val:  36.67%, val_best:  50.42%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6038%\n",
      "layer   2  Sparsity: 78.8232%\n",
      "layer   3  Sparsity: 83.3618%\n",
      "total_backward_count 587400 real_backward_count 106240  18.086%\n",
      "lif layer 1 self.abs_max_v: 41655.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.785936/  2.055513, val:  37.92%, val_best:  50.42%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6136%\n",
      "layer   2  Sparsity: 77.9640%\n",
      "layer   3  Sparsity: 81.8033%\n",
      "total_backward_count 597190 real_backward_count 107930  18.073%\n",
      "lif layer 1 self.abs_max_v: 41704.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.732125/  2.030921, val:  36.25%, val_best:  50.42%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6167%\n",
      "layer   2  Sparsity: 78.0883%\n",
      "layer   3  Sparsity: 80.7787%\n",
      "total_backward_count 606980 real_backward_count 109689  18.071%\n",
      "lif layer 1 self.abs_max_v: 41875.5\n",
      "fc layer 1 self.abs_max_out: 23249.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.779957/  2.071148, val:  26.25%, val_best:  50.42%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5870%\n",
      "layer   2  Sparsity: 78.2380%\n",
      "layer   3  Sparsity: 81.4057%\n",
      "total_backward_count 616770 real_backward_count 111308  18.047%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.771723/  2.072032, val:  32.08%, val_best:  50.42%, tr:  98.16%, tr_best:  99.39%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5885%\n",
      "layer   2  Sparsity: 77.2710%\n",
      "layer   3  Sparsity: 81.7297%\n",
      "total_backward_count 626560 real_backward_count 112969  18.030%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.800629/  2.031310, val:  34.17%, val_best:  50.42%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6201%\n",
      "layer   2  Sparsity: 77.6406%\n",
      "layer   3  Sparsity: 81.8917%\n",
      "total_backward_count 636350 real_backward_count 114655  18.018%\n",
      "fc layer 2 self.abs_max_out: 10836.0\n",
      "lif layer 2 self.abs_max_v: 20069.5\n",
      "lif layer 2 self.abs_max_v: 20142.5\n",
      "lif layer 2 self.abs_max_v: 20583.5\n",
      "fc layer 2 self.abs_max_out: 11564.0\n",
      "lif layer 2 self.abs_max_v: 21421.5\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.799172/  2.095285, val:  21.25%, val_best:  50.42%, tr:  98.57%, tr_best:  99.39%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5681%\n",
      "layer   2  Sparsity: 76.0563%\n",
      "layer   3  Sparsity: 82.1293%\n",
      "total_backward_count 646140 real_backward_count 116369  18.010%\n",
      "lif layer 1 self.abs_max_v: 42150.5\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.806587/  2.050259, val:  35.42%, val_best:  50.42%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5928%\n",
      "layer   2  Sparsity: 76.1895%\n",
      "layer   3  Sparsity: 82.4317%\n",
      "total_backward_count 655930 real_backward_count 118064  17.999%\n",
      "fc layer 2 self.abs_max_out: 11798.0\n",
      "lif layer 2 self.abs_max_v: 21896.0\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.746378/  2.063971, val:  34.58%, val_best:  50.42%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6291%\n",
      "layer   2  Sparsity: 77.6020%\n",
      "layer   3  Sparsity: 81.4080%\n",
      "total_backward_count 665720 real_backward_count 119750  17.988%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.779786/  2.002026, val:  44.17%, val_best:  50.42%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5957%\n",
      "layer   2  Sparsity: 77.2075%\n",
      "layer   3  Sparsity: 81.2971%\n",
      "total_backward_count 675510 real_backward_count 121439  17.977%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.739871/  2.065568, val:  30.00%, val_best:  50.42%, tr:  98.67%, tr_best:  99.39%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5864%\n",
      "layer   2  Sparsity: 77.2263%\n",
      "layer   3  Sparsity: 80.3813%\n",
      "total_backward_count 685300 real_backward_count 123107  17.964%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.726740/  1.996651, val:  50.83%, val_best:  50.83%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6073%\n",
      "layer   2  Sparsity: 78.1420%\n",
      "layer   3  Sparsity: 80.6957%\n",
      "total_backward_count 695090 real_backward_count 124849  17.962%\n",
      "fc layer 1 self.abs_max_out: 23301.0\n",
      "lif layer 1 self.abs_max_v: 42304.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.787877/  2.092588, val:  30.83%, val_best:  50.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5963%\n",
      "layer   2  Sparsity: 77.6132%\n",
      "layer   3  Sparsity: 82.1406%\n",
      "total_backward_count 704880 real_backward_count 126615  17.963%\n",
      "fc layer 1 self.abs_max_out: 23380.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.771568/  1.985474, val:  34.17%, val_best:  50.83%, tr:  98.47%, tr_best:  99.39%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5922%\n",
      "layer   2  Sparsity: 78.6295%\n",
      "layer   3  Sparsity: 81.2575%\n",
      "total_backward_count 714670 real_backward_count 128451  17.973%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.769119/  2.065235, val:  37.92%, val_best:  50.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5805%\n",
      "layer   2  Sparsity: 79.2290%\n",
      "layer   3  Sparsity: 82.1719%\n",
      "total_backward_count 724460 real_backward_count 130160  17.966%\n",
      "lif layer 1 self.abs_max_v: 42462.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.764038/  2.066574, val:  39.17%, val_best:  50.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5901%\n",
      "layer   2  Sparsity: 79.0566%\n",
      "layer   3  Sparsity: 81.5064%\n",
      "total_backward_count 734250 real_backward_count 131908  17.965%\n",
      "fc layer 1 self.abs_max_out: 23438.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.702829/  1.951241, val:  42.92%, val_best:  50.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6051%\n",
      "layer   2  Sparsity: 77.8506%\n",
      "layer   3  Sparsity: 80.5826%\n",
      "total_backward_count 744040 real_backward_count 133533  17.947%\n",
      "lif layer 1 self.abs_max_v: 42611.0\n",
      "fc layer 1 self.abs_max_out: 23534.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.760713/  1.983575, val:  51.67%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6273%\n",
      "layer   2  Sparsity: 79.4315%\n",
      "layer   3  Sparsity: 81.7462%\n",
      "total_backward_count 753830 real_backward_count 135316  17.950%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.756745/  2.053614, val:  26.25%, val_best:  51.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5972%\n",
      "layer   2  Sparsity: 78.8264%\n",
      "layer   3  Sparsity: 81.0970%\n",
      "total_backward_count 763620 real_backward_count 137059  17.949%\n",
      "fc layer 1 self.abs_max_out: 23782.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.755382/  1.972152, val:  34.58%, val_best:  51.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5945%\n",
      "layer   2  Sparsity: 78.6180%\n",
      "layer   3  Sparsity: 80.8831%\n",
      "total_backward_count 773410 real_backward_count 138769  17.942%\n",
      "lif layer 1 self.abs_max_v: 43253.5\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.735457/  1.982814, val:  39.17%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6065%\n",
      "layer   2  Sparsity: 76.6614%\n",
      "layer   3  Sparsity: 80.6509%\n",
      "total_backward_count 783200 real_backward_count 140444  17.932%\n",
      "fc layer 1 self.abs_max_out: 23941.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.734354/  2.000492, val:  37.92%, val_best:  51.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6109%\n",
      "layer   2  Sparsity: 78.4524%\n",
      "layer   3  Sparsity: 80.6894%\n",
      "total_backward_count 792990 real_backward_count 142106  17.920%\n",
      "lif layer 1 self.abs_max_v: 43508.5\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.781525/  2.032331, val:  26.67%, val_best:  51.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5936%\n",
      "layer   2  Sparsity: 77.6001%\n",
      "layer   3  Sparsity: 82.0101%\n",
      "total_backward_count 802780 real_backward_count 143873  17.922%\n",
      "lif layer 1 self.abs_max_v: 43686.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.742627/  2.061133, val:  38.75%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6110%\n",
      "layer   2  Sparsity: 77.7749%\n",
      "layer   3  Sparsity: 80.1504%\n",
      "total_backward_count 812570 real_backward_count 145594  17.918%\n",
      "fc layer 1 self.abs_max_out: 24100.0\n",
      "lif layer 1 self.abs_max_v: 43717.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.698250/  2.114852, val:  30.83%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6370%\n",
      "layer   2  Sparsity: 77.8457%\n",
      "layer   3  Sparsity: 80.6077%\n",
      "total_backward_count 822360 real_backward_count 147329  17.915%\n",
      "lif layer 1 self.abs_max_v: 43798.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.758953/  2.083852, val:  20.83%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6233%\n",
      "layer   2  Sparsity: 77.2153%\n",
      "layer   3  Sparsity: 80.9801%\n",
      "total_backward_count 832150 real_backward_count 149062  17.913%\n",
      "lif layer 1 self.abs_max_v: 43900.5\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.785099/  2.102383, val:  32.50%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.5982%\n",
      "layer   2  Sparsity: 77.8952%\n",
      "layer   3  Sparsity: 82.7060%\n",
      "total_backward_count 841940 real_backward_count 150801  17.911%\n",
      "lif layer 1 self.abs_max_v: 44023.5\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.785985/  2.032468, val:  28.33%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6026%\n",
      "layer   2  Sparsity: 76.9489%\n",
      "layer   3  Sparsity: 82.0375%\n",
      "total_backward_count 851730 real_backward_count 152410  17.894%\n",
      "lif layer 1 self.abs_max_v: 44027.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.799121/  2.079117, val:  30.83%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6139%\n",
      "layer   2  Sparsity: 78.1707%\n",
      "layer   3  Sparsity: 82.2598%\n",
      "total_backward_count 861520 real_backward_count 154140  17.892%\n",
      "fc layer 3 self.abs_max_out: 2046.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.747820/  2.013283, val:  40.00%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6044%\n",
      "layer   2  Sparsity: 77.3919%\n",
      "layer   3  Sparsity: 80.6736%\n",
      "total_backward_count 871310 real_backward_count 155765  17.877%\n",
      "fc layer 1 self.abs_max_out: 24258.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.761774/  2.013733, val:  32.50%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6239%\n",
      "layer   2  Sparsity: 77.5410%\n",
      "layer   3  Sparsity: 81.2542%\n",
      "total_backward_count 881100 real_backward_count 157464  17.871%\n",
      "lif layer 1 self.abs_max_v: 44177.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.815893/  2.022402, val:  42.92%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6010%\n",
      "layer   2  Sparsity: 78.2192%\n",
      "layer   3  Sparsity: 83.1368%\n",
      "total_backward_count 890890 real_backward_count 159235  17.874%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.791136/  2.066006, val:  45.00%, val_best:  51.67%, tr:  97.75%, tr_best:  99.59%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6131%\n",
      "layer   2  Sparsity: 77.5742%\n",
      "layer   3  Sparsity: 82.3870%\n",
      "total_backward_count 900680 real_backward_count 161003  17.876%\n",
      "lif layer 1 self.abs_max_v: 44508.5\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.795333/  2.051733, val:  31.67%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6238%\n",
      "layer   2  Sparsity: 78.3914%\n",
      "layer   3  Sparsity: 82.7108%\n",
      "total_backward_count 910470 real_backward_count 162795  17.880%\n",
      "fc layer 2 self.abs_max_out: 11992.0\n",
      "fc layer 1 self.abs_max_out: 24349.0\n",
      "lif layer 1 self.abs_max_v: 44579.5\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.783893/  2.067022, val:  37.50%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5924%\n",
      "layer   2  Sparsity: 78.3075%\n",
      "layer   3  Sparsity: 82.3748%\n",
      "total_backward_count 920260 real_backward_count 164536  17.879%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.742107/  1.987442, val:  36.67%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5803%\n",
      "layer   2  Sparsity: 77.2067%\n",
      "layer   3  Sparsity: 81.3118%\n",
      "total_backward_count 930050 real_backward_count 166262  17.877%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.787991/  2.045691, val:  37.50%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6137%\n",
      "layer   2  Sparsity: 77.8766%\n",
      "layer   3  Sparsity: 82.8301%\n",
      "total_backward_count 939840 real_backward_count 167992  17.875%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.790910/  2.119732, val:  28.33%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6122%\n",
      "layer   2  Sparsity: 78.4739%\n",
      "layer   3  Sparsity: 83.1636%\n",
      "total_backward_count 949630 real_backward_count 169658  17.866%\n",
      "lif layer 1 self.abs_max_v: 44623.5\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.818174/  2.033311, val:  36.67%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5908%\n",
      "layer   2  Sparsity: 78.4882%\n",
      "layer   3  Sparsity: 82.7953%\n",
      "total_backward_count 959420 real_backward_count 171355  17.860%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.786159/  2.078917, val:  26.67%, val_best:  51.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5902%\n",
      "layer   2  Sparsity: 77.8680%\n",
      "layer   3  Sparsity: 82.5926%\n",
      "total_backward_count 969210 real_backward_count 173044  17.854%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.788973/  2.036784, val:  37.08%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6221%\n",
      "layer   2  Sparsity: 78.2380%\n",
      "layer   3  Sparsity: 82.3965%\n",
      "total_backward_count 979000 real_backward_count 174675  17.842%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.809182/  2.059520, val:  26.67%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5996%\n",
      "layer   2  Sparsity: 78.9989%\n",
      "layer   3  Sparsity: 83.0584%\n",
      "total_backward_count 988790 real_backward_count 176327  17.833%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.798936/  2.056808, val:  34.17%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5944%\n",
      "layer   2  Sparsity: 76.5619%\n",
      "layer   3  Sparsity: 81.7143%\n",
      "total_backward_count 998580 real_backward_count 177959  17.821%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.740320/  2.022357, val:  30.00%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6057%\n",
      "layer   2  Sparsity: 77.5800%\n",
      "layer   3  Sparsity: 80.5210%\n",
      "total_backward_count 1008370 real_backward_count 179607  17.812%\n",
      "lif layer 1 self.abs_max_v: 44696.5\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.814642/  2.046502, val:  42.08%, val_best:  51.67%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6303%\n",
      "layer   2  Sparsity: 78.4182%\n",
      "layer   3  Sparsity: 82.7960%\n",
      "total_backward_count 1018160 real_backward_count 181363  17.813%\n",
      "lif layer 1 self.abs_max_v: 44731.5\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.808628/  2.060007, val:  31.67%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6141%\n",
      "layer   2  Sparsity: 78.7811%\n",
      "layer   3  Sparsity: 81.9965%\n",
      "total_backward_count 1027950 real_backward_count 183075  17.810%\n",
      "fc layer 1 self.abs_max_out: 24369.0\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.826296/  2.080413, val:  31.67%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6239%\n",
      "layer   2  Sparsity: 78.1181%\n",
      "layer   3  Sparsity: 82.7998%\n",
      "total_backward_count 1037740 real_backward_count 184763  17.804%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.822357/  2.102264, val:  27.08%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5698%\n",
      "layer   2  Sparsity: 78.8836%\n",
      "layer   3  Sparsity: 83.2551%\n",
      "total_backward_count 1047530 real_backward_count 186537  17.807%\n",
      "lif layer 1 self.abs_max_v: 44861.0\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.821010/  2.035143, val:  36.25%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5739%\n",
      "layer   2  Sparsity: 77.2819%\n",
      "layer   3  Sparsity: 82.4343%\n",
      "total_backward_count 1057320 real_backward_count 188200  17.800%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.799842/  2.055196, val:  37.92%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5822%\n",
      "layer   2  Sparsity: 77.7346%\n",
      "layer   3  Sparsity: 82.2626%\n",
      "total_backward_count 1067110 real_backward_count 189884  17.794%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.770189/  2.003428, val:  27.08%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6276%\n",
      "layer   2  Sparsity: 78.0156%\n",
      "layer   3  Sparsity: 81.8685%\n",
      "total_backward_count 1076900 real_backward_count 191594  17.791%\n",
      "fc layer 1 self.abs_max_out: 24441.0\n",
      "lif layer 1 self.abs_max_v: 45139.0\n",
      "fc layer 1 self.abs_max_out: 24486.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.782747/  2.019884, val:  38.33%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5991%\n",
      "layer   2  Sparsity: 78.2633%\n",
      "layer   3  Sparsity: 82.4090%\n",
      "total_backward_count 1086690 real_backward_count 193370  17.794%\n",
      "lif layer 1 self.abs_max_v: 45149.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.828796/  2.001776, val:  41.67%, val_best:  51.67%, tr:  97.75%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5910%\n",
      "layer   2  Sparsity: 78.5186%\n",
      "layer   3  Sparsity: 83.0237%\n",
      "total_backward_count 1096480 real_backward_count 195144  17.797%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.809896/  2.028331, val:  30.83%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6218%\n",
      "layer   2  Sparsity: 78.4744%\n",
      "layer   3  Sparsity: 82.5344%\n",
      "total_backward_count 1106270 real_backward_count 196991  17.807%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.781424/  2.039443, val:  29.17%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.6187%\n",
      "layer   2  Sparsity: 78.3192%\n",
      "layer   3  Sparsity: 81.3903%\n",
      "total_backward_count 1116060 real_backward_count 198720  17.805%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.752707/  2.095881, val:  31.67%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6122%\n",
      "layer   2  Sparsity: 78.2553%\n",
      "layer   3  Sparsity: 81.2219%\n",
      "total_backward_count 1125850 real_backward_count 200463  17.805%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.764378/  2.121486, val:  30.00%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5782%\n",
      "layer   2  Sparsity: 78.0534%\n",
      "layer   3  Sparsity: 81.6259%\n",
      "total_backward_count 1135640 real_backward_count 202194  17.804%\n",
      "lif layer 1 self.abs_max_v: 45175.0\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.764094/  2.100036, val:  32.50%, val_best:  51.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5829%\n",
      "layer   2  Sparsity: 78.8274%\n",
      "layer   3  Sparsity: 81.0565%\n",
      "total_backward_count 1145430 real_backward_count 203835  17.796%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.828702/  2.047207, val:  36.67%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6307%\n",
      "layer   2  Sparsity: 79.0342%\n",
      "layer   3  Sparsity: 83.0932%\n",
      "total_backward_count 1155220 real_backward_count 205680  17.804%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.780689/  2.041495, val:  34.17%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6053%\n",
      "layer   2  Sparsity: 77.9843%\n",
      "layer   3  Sparsity: 81.0096%\n",
      "total_backward_count 1165010 real_backward_count 207465  17.808%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.795081/  2.063420, val:  36.25%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6201%\n",
      "layer   2  Sparsity: 76.7829%\n",
      "layer   3  Sparsity: 81.1753%\n",
      "total_backward_count 1174800 real_backward_count 209222  17.809%\n",
      "fc layer 1 self.abs_max_out: 24691.0\n",
      "lif layer 1 self.abs_max_v: 45447.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.808598/  2.066744, val:  33.33%, val_best:  51.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5581%\n",
      "layer   2  Sparsity: 77.7072%\n",
      "layer   3  Sparsity: 82.2342%\n",
      "total_backward_count 1184590 real_backward_count 210996  17.812%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.804792/  2.070570, val:  37.92%, val_best:  51.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6088%\n",
      "layer   2  Sparsity: 78.3187%\n",
      "layer   3  Sparsity: 83.0063%\n",
      "total_backward_count 1194380 real_backward_count 212707  17.809%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.820789/  2.054981, val:  30.00%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5937%\n",
      "layer   2  Sparsity: 76.7753%\n",
      "layer   3  Sparsity: 82.4899%\n",
      "total_backward_count 1204170 real_backward_count 214382  17.803%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.861771/  2.009538, val:  37.92%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6143%\n",
      "layer   2  Sparsity: 77.6831%\n",
      "layer   3  Sparsity: 83.0790%\n",
      "total_backward_count 1213960 real_backward_count 216104  17.802%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.876621/  2.077366, val:  46.25%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6139%\n",
      "layer   2  Sparsity: 79.0137%\n",
      "layer   3  Sparsity: 84.3618%\n",
      "total_backward_count 1223750 real_backward_count 217883  17.805%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.806128/  2.065466, val:  32.50%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6041%\n",
      "layer   2  Sparsity: 77.2371%\n",
      "layer   3  Sparsity: 81.9493%\n",
      "total_backward_count 1233540 real_backward_count 219599  17.802%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.753857/  2.102457, val:  30.00%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6618%\n",
      "layer   2  Sparsity: 76.7333%\n",
      "layer   3  Sparsity: 80.9692%\n",
      "total_backward_count 1243330 real_backward_count 221266  17.796%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.812771/  2.104680, val:  36.67%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6137%\n",
      "layer   2  Sparsity: 78.3200%\n",
      "layer   3  Sparsity: 82.3689%\n",
      "total_backward_count 1253120 real_backward_count 223004  17.796%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.832580/  2.130217, val:  36.25%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6312%\n",
      "layer   2  Sparsity: 79.0656%\n",
      "layer   3  Sparsity: 83.6220%\n",
      "total_backward_count 1262910 real_backward_count 224683  17.791%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.812415/  2.087925, val:  40.00%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6221%\n",
      "layer   2  Sparsity: 78.0641%\n",
      "layer   3  Sparsity: 83.0676%\n",
      "total_backward_count 1272700 real_backward_count 226413  17.790%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.832713/  2.119904, val:  21.67%, val_best:  51.67%, tr:  97.45%, tr_best:  99.59%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5891%\n",
      "layer   2  Sparsity: 77.8033%\n",
      "layer   3  Sparsity: 83.1686%\n",
      "total_backward_count 1282490 real_backward_count 228274  17.799%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.781993/  2.003644, val:  43.33%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6082%\n",
      "layer   2  Sparsity: 77.2193%\n",
      "layer   3  Sparsity: 81.9921%\n",
      "total_backward_count 1292280 real_backward_count 230064  17.803%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.797826/  2.057821, val:  41.25%, val_best:  51.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6132%\n",
      "layer   2  Sparsity: 78.3030%\n",
      "layer   3  Sparsity: 81.6327%\n",
      "total_backward_count 1302070 real_backward_count 231710  17.796%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.792293/  2.103419, val:  26.25%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6040%\n",
      "layer   2  Sparsity: 78.1387%\n",
      "layer   3  Sparsity: 82.2909%\n",
      "total_backward_count 1311860 real_backward_count 233379  17.790%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.771746/  2.072051, val:  32.08%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5921%\n",
      "layer   2  Sparsity: 77.3789%\n",
      "layer   3  Sparsity: 81.3320%\n",
      "total_backward_count 1321650 real_backward_count 235160  17.793%\n",
      "fc layer 1 self.abs_max_out: 24733.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.756244/  1.971401, val:  44.17%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6083%\n",
      "layer   2  Sparsity: 78.4182%\n",
      "layer   3  Sparsity: 80.9203%\n",
      "total_backward_count 1331440 real_backward_count 236884  17.792%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.776765/  2.010575, val:  31.25%, val_best:  51.67%, tr:  97.45%, tr_best:  99.59%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6255%\n",
      "layer   2  Sparsity: 78.1282%\n",
      "layer   3  Sparsity: 81.7728%\n",
      "total_backward_count 1341230 real_backward_count 238608  17.790%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.716343/  1.955991, val:  35.42%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6298%\n",
      "layer   2  Sparsity: 77.2447%\n",
      "layer   3  Sparsity: 79.0795%\n",
      "total_backward_count 1351020 real_backward_count 240246  17.783%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.741691/  1.992271, val:  34.17%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6096%\n",
      "layer   2  Sparsity: 77.0099%\n",
      "layer   3  Sparsity: 81.1160%\n",
      "total_backward_count 1360810 real_backward_count 241982  17.782%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.741090/  2.057933, val:  35.83%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5872%\n",
      "layer   2  Sparsity: 78.7064%\n",
      "layer   3  Sparsity: 81.4218%\n",
      "total_backward_count 1370600 real_backward_count 243730  17.783%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.767622/  2.014344, val:  42.08%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6258%\n",
      "layer   2  Sparsity: 79.1386%\n",
      "layer   3  Sparsity: 81.6831%\n",
      "total_backward_count 1380390 real_backward_count 245523  17.786%\n",
      "fc layer 1 self.abs_max_out: 25244.0\n",
      "lif layer 1 self.abs_max_v: 45591.5\n",
      "lif layer 1 self.abs_max_v: 45808.0\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.787073/  2.050452, val:  44.17%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6191%\n",
      "layer   2  Sparsity: 77.4763%\n",
      "layer   3  Sparsity: 82.4962%\n",
      "total_backward_count 1390180 real_backward_count 247298  17.789%\n",
      "fc layer 1 self.abs_max_out: 25314.0\n",
      "lif layer 1 self.abs_max_v: 46008.5\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.827588/  2.085833, val:  28.75%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6026%\n",
      "layer   2  Sparsity: 77.9556%\n",
      "layer   3  Sparsity: 82.6742%\n",
      "total_backward_count 1399970 real_backward_count 249152  17.797%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.762822/  2.034250, val:  35.42%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6085%\n",
      "layer   2  Sparsity: 78.3427%\n",
      "layer   3  Sparsity: 81.5596%\n",
      "total_backward_count 1409760 real_backward_count 250839  17.793%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.766658/  2.065700, val:  31.67%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5701%\n",
      "layer   2  Sparsity: 77.6224%\n",
      "layer   3  Sparsity: 81.2911%\n",
      "total_backward_count 1419550 real_backward_count 252492  17.787%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.770799/  2.036731, val:  35.42%, val_best:  51.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.5979%\n",
      "layer   2  Sparsity: 78.0872%\n",
      "layer   3  Sparsity: 81.3108%\n",
      "total_backward_count 1429340 real_backward_count 254219  17.786%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.763745/  2.040533, val:  40.42%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6018%\n",
      "layer   2  Sparsity: 77.6747%\n",
      "layer   3  Sparsity: 81.4459%\n",
      "total_backward_count 1439130 real_backward_count 255983  17.787%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.738737/  2.058364, val:  28.75%, val_best:  51.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6163%\n",
      "layer   2  Sparsity: 78.3217%\n",
      "layer   3  Sparsity: 80.4116%\n",
      "total_backward_count 1448920 real_backward_count 257678  17.784%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.739320/  1.990259, val:  45.83%, val_best:  51.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5908%\n",
      "layer   2  Sparsity: 78.4126%\n",
      "layer   3  Sparsity: 80.9618%\n",
      "total_backward_count 1458710 real_backward_count 259385  17.782%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.732923/  2.021558, val:  50.42%, val_best:  51.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5864%\n",
      "layer   2  Sparsity: 78.6728%\n",
      "layer   3  Sparsity: 81.7874%\n",
      "total_backward_count 1468500 real_backward_count 261163  17.784%\n",
      "fc layer 3 self.abs_max_out: 2057.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.775037/  2.078750, val:  31.67%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5978%\n",
      "layer   2  Sparsity: 78.8961%\n",
      "layer   3  Sparsity: 82.1140%\n",
      "total_backward_count 1478290 real_backward_count 262876  17.782%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.763185/  2.066041, val:  38.33%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6146%\n",
      "layer   2  Sparsity: 78.5576%\n",
      "layer   3  Sparsity: 81.6210%\n",
      "total_backward_count 1488080 real_backward_count 264556  17.778%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.783100/  2.095478, val:  34.58%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6139%\n",
      "layer   2  Sparsity: 78.0678%\n",
      "layer   3  Sparsity: 82.7833%\n",
      "total_backward_count 1497870 real_backward_count 266271  17.777%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.772623/  2.045660, val:  37.08%, val_best:  51.67%, tr:  97.65%, tr_best:  99.59%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6322%\n",
      "layer   2  Sparsity: 77.4301%\n",
      "layer   3  Sparsity: 81.4197%\n",
      "total_backward_count 1507660 real_backward_count 268044  17.779%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.814626/  2.070509, val:  44.58%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6085%\n",
      "layer   2  Sparsity: 78.9898%\n",
      "layer   3  Sparsity: 82.2855%\n",
      "total_backward_count 1517450 real_backward_count 269781  17.779%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.799747/  2.056526, val:  32.08%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5983%\n",
      "layer   2  Sparsity: 77.8570%\n",
      "layer   3  Sparsity: 81.3431%\n",
      "total_backward_count 1527240 real_backward_count 271554  17.781%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.781989/  2.090824, val:  37.08%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6425%\n",
      "layer   2  Sparsity: 78.1106%\n",
      "layer   3  Sparsity: 82.2077%\n",
      "total_backward_count 1537030 real_backward_count 273261  17.779%\n",
      "fc layer 3 self.abs_max_out: 2161.0\n",
      "fc layer 3 self.abs_max_out: 2234.0\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.780078/  2.010283, val:  40.83%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5996%\n",
      "layer   2  Sparsity: 77.2558%\n",
      "layer   3  Sparsity: 81.6695%\n",
      "total_backward_count 1546820 real_backward_count 275014  17.779%\n",
      "fc layer 1 self.abs_max_out: 25354.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.735546/  2.106732, val:  32.50%, val_best:  51.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5876%\n",
      "layer   2  Sparsity: 77.4427%\n",
      "layer   3  Sparsity: 80.5670%\n",
      "total_backward_count 1556610 real_backward_count 276746  17.779%\n",
      "fc layer 1 self.abs_max_out: 25552.0\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.716076/  1.982251, val:  35.42%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6225%\n",
      "layer   2  Sparsity: 77.8918%\n",
      "layer   3  Sparsity: 79.6929%\n",
      "total_backward_count 1566400 real_backward_count 278458  17.777%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.718726/  1.976016, val:  45.42%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5782%\n",
      "layer   2  Sparsity: 78.1916%\n",
      "layer   3  Sparsity: 79.3153%\n",
      "total_backward_count 1576190 real_backward_count 280196  17.777%\n",
      "lif layer 1 self.abs_max_v: 46098.0\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.738431/  1.988914, val:  42.50%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6003%\n",
      "layer   2  Sparsity: 78.9295%\n",
      "layer   3  Sparsity: 80.3072%\n",
      "total_backward_count 1585980 real_backward_count 281883  17.773%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.760128/  2.045207, val:  40.00%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5913%\n",
      "layer   2  Sparsity: 78.9166%\n",
      "layer   3  Sparsity: 81.2866%\n",
      "total_backward_count 1595770 real_backward_count 283675  17.777%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.782666/  2.092107, val:  34.58%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5679%\n",
      "layer   2  Sparsity: 78.8013%\n",
      "layer   3  Sparsity: 81.0662%\n",
      "total_backward_count 1605560 real_backward_count 285370  17.774%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.794698/  2.062281, val:  38.75%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5945%\n",
      "layer   2  Sparsity: 79.2007%\n",
      "layer   3  Sparsity: 82.5303%\n",
      "total_backward_count 1615350 real_backward_count 287165  17.777%\n",
      "fc layer 1 self.abs_max_out: 25566.0\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.813731/  2.050557, val:  39.17%, val_best:  51.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5942%\n",
      "layer   2  Sparsity: 78.3215%\n",
      "layer   3  Sparsity: 82.6225%\n",
      "total_backward_count 1625140 real_backward_count 289054  17.786%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.765382/  2.040409, val:  36.25%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6270%\n",
      "layer   2  Sparsity: 77.2929%\n",
      "layer   3  Sparsity: 81.1961%\n",
      "total_backward_count 1634930 real_backward_count 290790  17.786%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.809800/  2.021543, val:  33.33%, val_best:  51.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6062%\n",
      "layer   2  Sparsity: 77.8669%\n",
      "layer   3  Sparsity: 82.4016%\n",
      "total_backward_count 1644720 real_backward_count 292582  17.789%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.801202/  2.092573, val:  26.25%, val_best:  51.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6078%\n",
      "layer   2  Sparsity: 77.8116%\n",
      "layer   3  Sparsity: 82.0148%\n",
      "total_backward_count 1654510 real_backward_count 294261  17.785%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.768790/  2.118103, val:  19.58%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6187%\n",
      "layer   2  Sparsity: 78.6461%\n",
      "layer   3  Sparsity: 81.1575%\n",
      "total_backward_count 1664300 real_backward_count 295948  17.782%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.758266/  2.105900, val:  26.25%, val_best:  51.67%, tr:  97.65%, tr_best:  99.59%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6143%\n",
      "layer   2  Sparsity: 77.6516%\n",
      "layer   3  Sparsity: 82.1482%\n",
      "total_backward_count 1674090 real_backward_count 297721  17.784%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.777928/  2.056415, val:  39.17%, val_best:  51.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6063%\n",
      "layer   2  Sparsity: 77.8251%\n",
      "layer   3  Sparsity: 80.9541%\n",
      "total_backward_count 1683880 real_backward_count 299484  17.785%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.750097/  2.038493, val:  40.42%, val_best:  51.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6371%\n",
      "layer   2  Sparsity: 76.7221%\n",
      "layer   3  Sparsity: 79.9588%\n",
      "total_backward_count 1693670 real_backward_count 301220  17.785%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.731138/  2.035561, val:  29.17%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6418%\n",
      "layer   2  Sparsity: 76.7046%\n",
      "layer   3  Sparsity: 79.9297%\n",
      "total_backward_count 1703460 real_backward_count 302753  17.773%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.754255/  2.038287, val:  34.17%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6276%\n",
      "layer   2  Sparsity: 78.6212%\n",
      "layer   3  Sparsity: 80.4249%\n",
      "total_backward_count 1713250 real_backward_count 304498  17.773%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.718709/  2.010203, val:  25.83%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5905%\n",
      "layer   2  Sparsity: 76.5018%\n",
      "layer   3  Sparsity: 79.0916%\n",
      "total_backward_count 1723040 real_backward_count 306243  17.773%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.741828/  2.012927, val:  35.42%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5921%\n",
      "layer   2  Sparsity: 77.9030%\n",
      "layer   3  Sparsity: 79.8498%\n",
      "total_backward_count 1732830 real_backward_count 308016  17.775%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.752274/  2.022515, val:  41.25%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6172%\n",
      "layer   2  Sparsity: 78.3841%\n",
      "layer   3  Sparsity: 81.1744%\n",
      "total_backward_count 1742620 real_backward_count 309783  17.777%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.764371/  2.004402, val:  37.50%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6053%\n",
      "layer   2  Sparsity: 78.0274%\n",
      "layer   3  Sparsity: 81.9632%\n",
      "total_backward_count 1752410 real_backward_count 311446  17.772%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.752933/  2.062732, val:  35.00%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6304%\n",
      "layer   2  Sparsity: 76.4727%\n",
      "layer   3  Sparsity: 81.2297%\n",
      "total_backward_count 1762200 real_backward_count 313183  17.772%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.786724/  1.971995, val:  41.67%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.6307%\n",
      "layer   2  Sparsity: 77.4288%\n",
      "layer   3  Sparsity: 81.6284%\n",
      "total_backward_count 1771990 real_backward_count 314916  17.772%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.805731/  2.097495, val:  37.50%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5839%\n",
      "layer   2  Sparsity: 78.9290%\n",
      "layer   3  Sparsity: 83.5164%\n",
      "total_backward_count 1781780 real_backward_count 316682  17.773%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.790907/  2.016771, val:  40.83%, val_best:  51.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6248%\n",
      "layer   2  Sparsity: 78.4485%\n",
      "layer   3  Sparsity: 82.7377%\n",
      "total_backward_count 1791570 real_backward_count 318347  17.769%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.797169/  2.049269, val:  27.08%, val_best:  51.67%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6333%\n",
      "layer   2  Sparsity: 78.0896%\n",
      "layer   3  Sparsity: 81.9960%\n",
      "total_backward_count 1801360 real_backward_count 320209  17.776%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.774431/  2.094782, val:  29.58%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5882%\n",
      "layer   2  Sparsity: 77.8687%\n",
      "layer   3  Sparsity: 81.4814%\n",
      "total_backward_count 1811150 real_backward_count 321931  17.775%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.779242/  2.071590, val:  42.50%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5922%\n",
      "layer   2  Sparsity: 78.6331%\n",
      "layer   3  Sparsity: 81.5763%\n",
      "total_backward_count 1820940 real_backward_count 323617  17.772%\n",
      "fc layer 2 self.abs_max_out: 12093.0\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.801553/  2.013017, val:  37.92%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5843%\n",
      "layer   2  Sparsity: 78.0982%\n",
      "layer   3  Sparsity: 82.0736%\n",
      "total_backward_count 1830730 real_backward_count 325401  17.774%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.777362/  2.102305, val:  29.17%, val_best:  51.67%, tr:  97.65%, tr_best:  99.59%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.6187%\n",
      "layer   2  Sparsity: 77.6055%\n",
      "layer   3  Sparsity: 82.1596%\n",
      "total_backward_count 1840520 real_backward_count 327192  17.777%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.816977/  2.077542, val:  45.83%, val_best:  51.67%, tr:  97.75%, tr_best:  99.59%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6133%\n",
      "layer   2  Sparsity: 78.6706%\n",
      "layer   3  Sparsity: 82.7989%\n",
      "total_backward_count 1850310 real_backward_count 328841  17.772%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.767047/  1.947256, val:  34.58%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6120%\n",
      "layer   2  Sparsity: 77.4435%\n",
      "layer   3  Sparsity: 81.0606%\n",
      "total_backward_count 1860100 real_backward_count 330514  17.769%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.758763/  2.005833, val:  44.58%, val_best:  51.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6236%\n",
      "layer   2  Sparsity: 78.7187%\n",
      "layer   3  Sparsity: 81.6074%\n",
      "total_backward_count 1869890 real_backward_count 332218  17.767%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.763232/  2.007955, val:  32.50%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6237%\n",
      "layer   2  Sparsity: 78.3423%\n",
      "layer   3  Sparsity: 82.1929%\n",
      "total_backward_count 1879680 real_backward_count 333970  17.767%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.803784/  2.107134, val:  31.67%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5805%\n",
      "layer   2  Sparsity: 77.6784%\n",
      "layer   3  Sparsity: 82.6722%\n",
      "total_backward_count 1889470 real_backward_count 335702  17.767%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.792254/  2.066472, val:  30.00%, val_best:  51.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6076%\n",
      "layer   2  Sparsity: 77.4136%\n",
      "layer   3  Sparsity: 82.3338%\n",
      "total_backward_count 1899260 real_backward_count 337419  17.766%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.820445/  2.065033, val:  45.00%, val_best:  51.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5984%\n",
      "layer   2  Sparsity: 78.6609%\n",
      "layer   3  Sparsity: 82.2714%\n",
      "total_backward_count 1909050 real_backward_count 339160  17.766%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.816337/  2.078305, val:  44.17%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.6284%\n",
      "layer   2  Sparsity: 78.3365%\n",
      "layer   3  Sparsity: 83.4367%\n",
      "total_backward_count 1918840 real_backward_count 340958  17.769%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.829870/  2.128399, val:  35.42%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.6158%\n",
      "layer   2  Sparsity: 78.2036%\n",
      "layer   3  Sparsity: 83.4523%\n",
      "total_backward_count 1928630 real_backward_count 342774  17.773%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.825852/  2.066495, val:  31.25%, val_best:  51.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.5767%\n",
      "layer   2  Sparsity: 77.3456%\n",
      "layer   3  Sparsity: 83.3290%\n",
      "total_backward_count 1938420 real_backward_count 344536  17.774%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.845666/  2.063749, val:  33.75%, val_best:  51.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5859%\n",
      "layer   2  Sparsity: 78.5625%\n",
      "layer   3  Sparsity: 83.5354%\n",
      "total_backward_count 1948210 real_backward_count 346308  17.776%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.819073/  2.070654, val:  44.58%, val_best:  51.67%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5919%\n",
      "layer   2  Sparsity: 78.4158%\n",
      "layer   3  Sparsity: 83.2911%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7af34babad4c66b81155aca20953c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÑ‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>1.81907</td></tr><tr><td>val_acc_best</td><td>0.51667</td></tr><tr><td>val_acc_now</td><td>0.44583</td></tr><tr><td>val_loss</td><td>2.07065</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-sweep-60</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/au098gzt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/au098gzt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_050533-au098gzt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1o5sl4rx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_092250-1o5sl4rx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1o5sl4rx' target=\"_blank\">golden-sweep-66</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1o5sl4rx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1o5sl4rx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251115_092300_228', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 778.0\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1059.5\n",
      "fc layer 1 self.abs_max_out: 1028.0\n",
      "lif layer 1 self.abs_max_v: 1393.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 596.0\n",
      "fc layer 1 self.abs_max_out: 1029.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 1349.0\n",
      "lif layer 1 self.abs_max_v: 1442.0\n",
      "lif layer 1 self.abs_max_v: 1653.5\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "lif layer 2 self.abs_max_v: 1057.0\n",
      "fc layer 1 self.abs_max_out: 1376.0\n",
      "lif layer 1 self.abs_max_v: 1755.5\n",
      "fc layer 1 self.abs_max_out: 1474.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "lif layer 1 self.abs_max_v: 1984.5\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "lif layer 1 self.abs_max_v: 2274.5\n",
      "fc layer 2 self.abs_max_out: 989.0\n",
      "lif layer 2 self.abs_max_v: 1268.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 1674.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 1 self.abs_max_out: 2375.0\n",
      "lif layer 1 self.abs_max_v: 2746.5\n",
      "lif layer 2 self.abs_max_v: 1422.0\n",
      "fc layer 1 self.abs_max_out: 2821.0\n",
      "lif layer 1 self.abs_max_v: 2821.0\n",
      "fc layer 2 self.abs_max_out: 1173.0\n",
      "lif layer 2 self.abs_max_v: 1568.5\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 2 self.abs_max_out: 1365.0\n",
      "lif layer 2 self.abs_max_v: 1606.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "lif layer 1 self.abs_max_v: 2999.5\n",
      "lif layer 2 self.abs_max_v: 1618.0\n",
      "lif layer 1 self.abs_max_v: 3007.0\n",
      "lif layer 1 self.abs_max_v: 3115.0\n",
      "lif layer 2 self.abs_max_v: 1701.5\n",
      "lif layer 2 self.abs_max_v: 1703.0\n",
      "lif layer 2 self.abs_max_v: 2116.5\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "fc layer 1 self.abs_max_out: 2830.0\n",
      "fc layer 1 self.abs_max_out: 2969.0\n",
      "lif layer 1 self.abs_max_v: 3337.0\n",
      "fc layer 1 self.abs_max_out: 3291.0\n",
      "lif layer 2 self.abs_max_v: 2340.5\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "lif layer 1 self.abs_max_v: 3360.0\n",
      "fc layer 2 self.abs_max_out: 1630.0\n",
      "lif layer 2 self.abs_max_v: 2580.5\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 3 self.abs_max_out: 365.0\n",
      "fc layer 1 self.abs_max_out: 3455.0\n",
      "lif layer 1 self.abs_max_v: 3455.0\n",
      "fc layer 1 self.abs_max_out: 3911.0\n",
      "lif layer 1 self.abs_max_v: 3911.0\n",
      "fc layer 1 self.abs_max_out: 4075.0\n",
      "lif layer 1 self.abs_max_v: 4075.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 1 self.abs_max_out: 4146.0\n",
      "lif layer 1 self.abs_max_v: 4146.0\n",
      "fc layer 2 self.abs_max_out: 1635.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "lif layer 2 self.abs_max_v: 2587.5\n",
      "lif layer 2 self.abs_max_v: 2626.0\n",
      "lif layer 2 self.abs_max_v: 2684.0\n",
      "fc layer 1 self.abs_max_out: 4689.0\n",
      "lif layer 1 self.abs_max_v: 4689.0\n",
      "lif layer 2 self.abs_max_v: 2708.0\n",
      "lif layer 2 self.abs_max_v: 2730.5\n",
      "fc layer 2 self.abs_max_out: 1988.0\n",
      "lif layer 2 self.abs_max_v: 2803.5\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "lif layer 2 self.abs_max_v: 2870.5\n",
      "lif layer 2 self.abs_max_v: 2886.5\n",
      "fc layer 3 self.abs_max_out: 524.0\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "lif layer 2 self.abs_max_v: 2893.0\n",
      "lif layer 2 self.abs_max_v: 3061.5\n",
      "fc layer 2 self.abs_max_out: 2423.0\n",
      "fc layer 2 self.abs_max_out: 2659.0\n",
      "fc layer 1 self.abs_max_out: 4826.0\n",
      "lif layer 1 self.abs_max_v: 4826.0\n",
      "fc layer 1 self.abs_max_out: 5143.0\n",
      "lif layer 1 self.abs_max_v: 5143.0\n",
      "fc layer 1 self.abs_max_out: 5219.0\n",
      "lif layer 1 self.abs_max_v: 5219.0\n",
      "fc layer 2 self.abs_max_out: 2692.0\n",
      "fc layer 2 self.abs_max_out: 3021.0\n",
      "lif layer 2 self.abs_max_v: 3198.0\n",
      "lif layer 2 self.abs_max_v: 3213.0\n",
      "lif layer 2 self.abs_max_v: 3236.0\n",
      "fc layer 1 self.abs_max_out: 5228.0\n",
      "lif layer 1 self.abs_max_v: 5228.0\n",
      "lif layer 2 self.abs_max_v: 3331.5\n",
      "fc layer 1 self.abs_max_out: 5394.0\n",
      "lif layer 1 self.abs_max_v: 5394.0\n",
      "lif layer 2 self.abs_max_v: 3729.5\n",
      "fc layer 3 self.abs_max_out: 535.0\n",
      "fc layer 1 self.abs_max_out: 5478.0\n",
      "lif layer 1 self.abs_max_v: 5478.0\n",
      "fc layer 1 self.abs_max_out: 5505.0\n",
      "lif layer 1 self.abs_max_v: 5505.0\n",
      "fc layer 1 self.abs_max_out: 5552.0\n",
      "lif layer 1 self.abs_max_v: 5552.0\n",
      "fc layer 2 self.abs_max_out: 3296.0\n",
      "fc layer 1 self.abs_max_out: 6236.0\n",
      "lif layer 1 self.abs_max_v: 6236.0\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "lif layer 1 self.abs_max_v: 6302.5\n",
      "fc layer 1 self.abs_max_out: 6408.0\n",
      "lif layer 1 self.abs_max_v: 6408.0\n",
      "fc layer 1 self.abs_max_out: 7261.0\n",
      "lif layer 1 self.abs_max_v: 7261.0\n",
      "lif layer 2 self.abs_max_v: 4025.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.194183/  2.188564, val:  34.58%, val_best:  34.58%, tr:  59.45%, tr_best:  59.45%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 85.3464%\n",
      "layer   3  Sparsity: 92.5388%\n",
      "total_backward_count 9790 real_backward_count 5609  57.293%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3298.0\n",
      "fc layer 2 self.abs_max_out: 3417.0\n",
      "fc layer 2 self.abs_max_out: 3427.0\n",
      "fc layer 2 self.abs_max_out: 3534.0\n",
      "fc layer 1 self.abs_max_out: 7485.0\n",
      "lif layer 1 self.abs_max_v: 7485.0\n",
      "fc layer 1 self.abs_max_out: 7493.0\n",
      "lif layer 1 self.abs_max_v: 7493.0\n",
      "fc layer 2 self.abs_max_out: 3607.0\n",
      "fc layer 1 self.abs_max_out: 7788.0\n",
      "lif layer 1 self.abs_max_v: 7788.0\n",
      "lif layer 1 self.abs_max_v: 7887.0\n",
      "lif layer 1 self.abs_max_v: 7959.0\n",
      "fc layer 2 self.abs_max_out: 3615.0\n",
      "lif layer 1 self.abs_max_v: 9512.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.149757/  2.179621, val:  47.08%, val_best:  47.08%, tr:  84.47%, tr_best:  84.47%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4660%\n",
      "layer   2  Sparsity: 84.3036%\n",
      "layer   3  Sparsity: 90.3488%\n",
      "total_backward_count 19580 real_backward_count 9344  47.722%\n",
      "fc layer 1 self.abs_max_out: 7859.0\n",
      "fc layer 1 self.abs_max_out: 8476.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.159127/  2.192969, val:  53.75%, val_best:  53.75%, tr:  90.30%, tr_best:  90.30%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 83.4949%\n",
      "layer   3  Sparsity: 89.5546%\n",
      "total_backward_count 29370 real_backward_count 12522  42.635%\n",
      "fc layer 1 self.abs_max_out: 8775.0\n",
      "fc layer 1 self.abs_max_out: 8856.0\n",
      "fc layer 2 self.abs_max_out: 3626.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.168286/  2.203895, val:  46.25%, val_best:  53.75%, tr:  93.77%, tr_best:  93.77%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4286%\n",
      "layer   2  Sparsity: 83.6550%\n",
      "layer   3  Sparsity: 89.0853%\n",
      "total_backward_count 39160 real_backward_count 15307  39.088%\n",
      "fc layer 1 self.abs_max_out: 9195.0\n",
      "lif layer 2 self.abs_max_v: 4109.5\n",
      "fc layer 1 self.abs_max_out: 9446.0\n",
      "lif layer 1 self.abs_max_v: 10436.0\n",
      "lif layer 1 self.abs_max_v: 10812.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.172764/  2.197562, val:  47.92%, val_best:  53.75%, tr:  96.02%, tr_best:  96.02%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 83.6588%\n",
      "layer   3  Sparsity: 88.4103%\n",
      "total_backward_count 48950 real_backward_count 17909  36.586%\n",
      "fc layer 2 self.abs_max_out: 3654.0\n",
      "fc layer 2 self.abs_max_out: 3672.0\n",
      "fc layer 1 self.abs_max_out: 9497.0\n",
      "fc layer 1 self.abs_max_out: 9618.0\n",
      "fc layer 2 self.abs_max_out: 3688.0\n",
      "lif layer 1 self.abs_max_v: 11412.0\n",
      "fc layer 2 self.abs_max_out: 4025.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.176063/  2.213074, val:  52.50%, val_best:  53.75%, tr:  96.22%, tr_best:  96.22%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 83.3911%\n",
      "layer   3  Sparsity: 88.0904%\n",
      "total_backward_count 58740 real_backward_count 20344  34.634%\n",
      "fc layer 1 self.abs_max_out: 9899.0\n",
      "lif layer 1 self.abs_max_v: 11995.0\n",
      "fc layer 2 self.abs_max_out: 4031.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.187199/  2.222803, val:  53.33%, val_best:  53.75%, tr:  96.83%, tr_best:  96.83%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 83.1514%\n",
      "layer   3  Sparsity: 87.7884%\n",
      "total_backward_count 68530 real_backward_count 22601  32.980%\n",
      "fc layer 1 self.abs_max_out: 10237.0\n",
      "fc layer 1 self.abs_max_out: 10241.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.189965/  2.213984, val:  50.42%, val_best:  53.75%, tr:  97.65%, tr_best:  97.65%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 83.2703%\n",
      "layer   3  Sparsity: 87.7029%\n",
      "total_backward_count 78320 real_backward_count 24754  31.606%\n",
      "fc layer 1 self.abs_max_out: 10381.0\n",
      "fc layer 2 self.abs_max_out: 4263.0\n",
      "lif layer 2 self.abs_max_v: 4263.0\n",
      "fc layer 1 self.abs_max_out: 10690.0\n",
      "lif layer 1 self.abs_max_v: 12422.5\n",
      "lif layer 2 self.abs_max_v: 4278.0\n",
      "lif layer 2 self.abs_max_v: 4638.5\n",
      "lif layer 2 self.abs_max_v: 4797.5\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.191483/  2.215787, val:  46.67%, val_best:  53.75%, tr:  98.06%, tr_best:  98.06%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4599%\n",
      "layer   2  Sparsity: 82.9341%\n",
      "layer   3  Sparsity: 87.3188%\n",
      "total_backward_count 88110 real_backward_count 26895  30.524%\n",
      "fc layer 1 self.abs_max_out: 10759.0\n",
      "lif layer 1 self.abs_max_v: 13124.5\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.189358/  2.226718, val:  48.75%, val_best:  53.75%, tr:  98.06%, tr_best:  98.06%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4528%\n",
      "layer   2  Sparsity: 83.0077%\n",
      "layer   3  Sparsity: 87.0306%\n",
      "total_backward_count 97900 real_backward_count 28931  29.552%\n",
      "fc layer 1 self.abs_max_out: 10761.0\n",
      "fc layer 1 self.abs_max_out: 11040.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.191112/  2.224714, val:  46.67%, val_best:  53.75%, tr:  98.67%, tr_best:  98.67%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4366%\n",
      "layer   2  Sparsity: 82.6839%\n",
      "layer   3  Sparsity: 86.5386%\n",
      "total_backward_count 107690 real_backward_count 30916  28.708%\n",
      "fc layer 1 self.abs_max_out: 11133.0\n",
      "fc layer 1 self.abs_max_out: 11237.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.194593/  2.215240, val:  54.58%, val_best:  54.58%, tr:  98.67%, tr_best:  98.67%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4093%\n",
      "layer   2  Sparsity: 82.1342%\n",
      "layer   3  Sparsity: 86.6474%\n",
      "total_backward_count 117480 real_backward_count 32850  27.962%\n",
      "lif layer 1 self.abs_max_v: 14163.0\n",
      "fc layer 1 self.abs_max_out: 11310.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.195757/  2.227143, val:  39.17%, val_best:  54.58%, tr:  99.08%, tr_best:  99.08%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4209%\n",
      "layer   2  Sparsity: 82.1129%\n",
      "layer   3  Sparsity: 87.3113%\n",
      "total_backward_count 127270 real_backward_count 34765  27.316%\n",
      "lif layer 1 self.abs_max_v: 14508.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.196827/  2.224459, val:  50.83%, val_best:  54.58%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 81.9301%\n",
      "layer   3  Sparsity: 87.2673%\n",
      "total_backward_count 137060 real_backward_count 36696  26.774%\n",
      "fc layer 1 self.abs_max_out: 11588.0\n",
      "lif layer 1 self.abs_max_v: 14748.0\n",
      "lif layer 1 self.abs_max_v: 14909.0\n",
      "lif layer 1 self.abs_max_v: 15144.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.199013/  2.214985, val:  58.75%, val_best:  58.75%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 81.9830%\n",
      "layer   3  Sparsity: 87.7623%\n",
      "total_backward_count 146850 real_backward_count 38549  26.251%\n",
      "fc layer 1 self.abs_max_out: 12022.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.197146/  2.227757, val:  55.00%, val_best:  58.75%, tr:  98.57%, tr_best:  99.28%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 81.8682%\n",
      "layer   3  Sparsity: 87.6452%\n",
      "total_backward_count 156640 real_backward_count 40468  25.835%\n",
      "lif layer 2 self.abs_max_v: 4818.5\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.197360/  2.221719, val:  56.25%, val_best:  58.75%, tr:  98.77%, tr_best:  99.28%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 81.8669%\n",
      "layer   3  Sparsity: 87.4758%\n",
      "total_backward_count 166430 real_backward_count 42321  25.429%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.199262/  2.226990, val:  61.25%, val_best:  61.25%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 81.4843%\n",
      "layer   3  Sparsity: 87.6331%\n",
      "total_backward_count 176220 real_backward_count 44195  25.079%\n",
      "fc layer 1 self.abs_max_out: 12164.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.192673/  2.216310, val:  59.58%, val_best:  61.25%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4203%\n",
      "layer   2  Sparsity: 81.4419%\n",
      "layer   3  Sparsity: 86.9265%\n",
      "total_backward_count 186010 real_backward_count 46017  24.739%\n",
      "lif layer 1 self.abs_max_v: 15524.5\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.189695/  2.213274, val:  38.33%, val_best:  61.25%, tr:  98.88%, tr_best:  99.28%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4281%\n",
      "layer   2  Sparsity: 81.5460%\n",
      "layer   3  Sparsity: 86.9767%\n",
      "total_backward_count 195800 real_backward_count 47833  24.430%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.188146/  2.223449, val:  50.00%, val_best:  61.25%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 81.8785%\n",
      "layer   3  Sparsity: 87.2747%\n",
      "total_backward_count 205590 real_backward_count 49614  24.132%\n",
      "fc layer 1 self.abs_max_out: 12357.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.188765/  2.212951, val:  52.08%, val_best:  61.25%, tr:  98.98%, tr_best:  99.28%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4473%\n",
      "layer   2  Sparsity: 81.8143%\n",
      "layer   3  Sparsity: 87.1959%\n",
      "total_backward_count 215380 real_backward_count 51427  23.877%\n",
      "fc layer 1 self.abs_max_out: 12624.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.190497/  2.219285, val:  60.83%, val_best:  61.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4432%\n",
      "layer   2  Sparsity: 81.9647%\n",
      "layer   3  Sparsity: 87.2489%\n",
      "total_backward_count 225170 real_backward_count 53187  23.621%\n",
      "fc layer 1 self.abs_max_out: 12671.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.196403/  2.221683, val:  55.83%, val_best:  61.25%, tr:  98.98%, tr_best:  99.49%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3963%\n",
      "layer   2  Sparsity: 81.9529%\n",
      "layer   3  Sparsity: 87.4148%\n",
      "total_backward_count 234960 real_backward_count 55040  23.425%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.194550/  2.225209, val:  52.92%, val_best:  61.25%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4588%\n",
      "layer   2  Sparsity: 82.2050%\n",
      "layer   3  Sparsity: 87.1994%\n",
      "total_backward_count 244750 real_backward_count 56826  23.218%\n",
      "lif layer 1 self.abs_max_v: 16794.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.198717/  2.220533, val:  57.50%, val_best:  61.25%, tr:  98.37%, tr_best:  99.49%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4221%\n",
      "layer   2  Sparsity: 82.3985%\n",
      "layer   3  Sparsity: 87.3987%\n",
      "total_backward_count 254540 real_backward_count 58699  23.061%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.197594/  2.221397, val:  58.33%, val_best:  61.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 82.1038%\n",
      "layer   3  Sparsity: 87.2875%\n",
      "total_backward_count 264330 real_backward_count 60442  22.866%\n",
      "fc layer 1 self.abs_max_out: 12685.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.196802/  2.219825, val:  65.83%, val_best:  65.83%, tr:  98.57%, tr_best:  99.49%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.3871%\n",
      "layer   2  Sparsity: 81.9197%\n",
      "layer   3  Sparsity: 87.4428%\n",
      "total_backward_count 274120 real_backward_count 62227  22.701%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.200292/  2.228734, val:  48.33%, val_best:  65.83%, tr:  99.28%, tr_best:  99.49%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4363%\n",
      "layer   2  Sparsity: 81.7622%\n",
      "layer   3  Sparsity: 87.4616%\n",
      "total_backward_count 283910 real_backward_count 63937  22.520%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.203367/  2.221870, val:  55.00%, val_best:  65.83%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 81.5162%\n",
      "layer   3  Sparsity: 87.3333%\n",
      "total_backward_count 293700 real_backward_count 65674  22.361%\n",
      "fc layer 2 self.abs_max_out: 4367.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.203231/  2.225971, val:  57.08%, val_best:  65.83%, tr:  99.39%, tr_best:  99.49%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4707%\n",
      "layer   2  Sparsity: 81.8118%\n",
      "layer   3  Sparsity: 87.0722%\n",
      "total_backward_count 303490 real_backward_count 67362  22.196%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.203189/  2.231791, val:  45.00%, val_best:  65.83%, tr:  99.08%, tr_best:  99.49%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 81.7148%\n",
      "layer   3  Sparsity: 87.2669%\n",
      "total_backward_count 313280 real_backward_count 69058  22.044%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.201852/  2.229535, val:  52.92%, val_best:  65.83%, tr:  99.28%, tr_best:  99.49%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4215%\n",
      "layer   2  Sparsity: 81.6243%\n",
      "layer   3  Sparsity: 87.1320%\n",
      "total_backward_count 323070 real_backward_count 70735  21.895%\n",
      "fc layer 1 self.abs_max_out: 12745.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.199808/  2.226969, val:  47.08%, val_best:  65.83%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4620%\n",
      "layer   2  Sparsity: 81.6039%\n",
      "layer   3  Sparsity: 87.3795%\n",
      "total_backward_count 332860 real_backward_count 72419  21.757%\n",
      "fc layer 1 self.abs_max_out: 12981.0\n",
      "lif layer 1 self.abs_max_v: 16893.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.195700/  2.220564, val:  57.08%, val_best:  65.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4190%\n",
      "layer   2  Sparsity: 81.6655%\n",
      "layer   3  Sparsity: 87.1553%\n",
      "total_backward_count 342650 real_backward_count 74088  21.622%\n",
      "lif layer 1 self.abs_max_v: 17414.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.193392/  2.225960, val:  51.25%, val_best:  65.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4137%\n",
      "layer   2  Sparsity: 81.5702%\n",
      "layer   3  Sparsity: 87.0233%\n",
      "total_backward_count 352440 real_backward_count 75751  21.493%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.194811/  2.220828, val:  55.83%, val_best:  65.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4756%\n",
      "layer   2  Sparsity: 81.4425%\n",
      "layer   3  Sparsity: 87.0598%\n",
      "total_backward_count 362230 real_backward_count 77397  21.367%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.197519/  2.224179, val:  53.75%, val_best:  65.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4652%\n",
      "layer   2  Sparsity: 81.4199%\n",
      "layer   3  Sparsity: 87.1867%\n",
      "total_backward_count 372020 real_backward_count 78945  21.221%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.200981/  2.221431, val:  68.75%, val_best:  68.75%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3838%\n",
      "layer   2  Sparsity: 81.4864%\n",
      "layer   3  Sparsity: 87.1429%\n",
      "total_backward_count 381810 real_backward_count 80633  21.119%\n",
      "fc layer 1 self.abs_max_out: 13514.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.195788/  2.222957, val:  52.50%, val_best:  68.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 81.7196%\n",
      "layer   3  Sparsity: 86.9932%\n",
      "total_backward_count 391600 real_backward_count 82245  21.002%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.198112/  2.221977, val:  59.58%, val_best:  68.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 88.4060%\n",
      "layer   2  Sparsity: 81.7080%\n",
      "layer   3  Sparsity: 87.0900%\n",
      "total_backward_count 401390 real_backward_count 83813  20.881%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.197421/  2.226227, val:  60.83%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4285%\n",
      "layer   2  Sparsity: 81.6919%\n",
      "layer   3  Sparsity: 87.3425%\n",
      "total_backward_count 411180 real_backward_count 85439  20.779%\n",
      "fc layer 1 self.abs_max_out: 13565.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.196784/  2.224373, val:  62.92%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4793%\n",
      "layer   2  Sparsity: 81.4667%\n",
      "layer   3  Sparsity: 87.2150%\n",
      "total_backward_count 420970 real_backward_count 87034  20.675%\n",
      "fc layer 1 self.abs_max_out: 13624.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.198021/  2.227881, val:  65.83%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4836%\n",
      "layer   2  Sparsity: 81.6918%\n",
      "layer   3  Sparsity: 87.4414%\n",
      "total_backward_count 430760 real_backward_count 88562  20.559%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.202948/  2.226563, val:  65.42%, val_best:  68.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4907%\n",
      "layer   2  Sparsity: 81.5072%\n",
      "layer   3  Sparsity: 87.4575%\n",
      "total_backward_count 440550 real_backward_count 90204  20.475%\n",
      "lif layer 1 self.abs_max_v: 17610.5\n",
      "lif layer 2 self.abs_max_v: 4870.5\n",
      "lif layer 2 self.abs_max_v: 4871.5\n",
      "lif layer 2 self.abs_max_v: 5083.5\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.202296/  2.222971, val:  66.67%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 81.4849%\n",
      "layer   3  Sparsity: 87.4877%\n",
      "total_backward_count 450340 real_backward_count 91819  20.389%\n",
      "lif layer 2 self.abs_max_v: 5106.0\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.200072/  2.225781, val:  59.58%, val_best:  68.75%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4026%\n",
      "layer   2  Sparsity: 81.5643%\n",
      "layer   3  Sparsity: 87.5221%\n",
      "total_backward_count 460130 real_backward_count 93400  20.299%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.200075/  2.232520, val:  48.75%, val_best:  68.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4800%\n",
      "layer   2  Sparsity: 81.4940%\n",
      "layer   3  Sparsity: 87.7805%\n",
      "total_backward_count 469920 real_backward_count 94999  20.216%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.204163/  2.234293, val:  55.83%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3908%\n",
      "layer   2  Sparsity: 81.6530%\n",
      "layer   3  Sparsity: 87.7560%\n",
      "total_backward_count 479710 real_backward_count 96509  20.118%\n",
      "fc layer 1 self.abs_max_out: 13685.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.203126/  2.223556, val:  61.67%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 81.6009%\n",
      "layer   3  Sparsity: 87.8495%\n",
      "total_backward_count 489500 real_backward_count 98079  20.037%\n",
      "lif layer 2 self.abs_max_v: 5376.0\n",
      "fc layer 1 self.abs_max_out: 13720.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.201707/  2.223450, val:  61.25%, val_best:  68.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4516%\n",
      "layer   2  Sparsity: 81.3770%\n",
      "layer   3  Sparsity: 87.7934%\n",
      "total_backward_count 499290 real_backward_count 99670  19.962%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.203230/  2.227984, val:  66.67%, val_best:  68.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4048%\n",
      "layer   2  Sparsity: 81.3783%\n",
      "layer   3  Sparsity: 88.0866%\n",
      "total_backward_count 509080 real_backward_count 101175  19.874%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.205095/  2.232122, val:  62.92%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4455%\n",
      "layer   2  Sparsity: 81.5079%\n",
      "layer   3  Sparsity: 87.8952%\n",
      "total_backward_count 518870 real_backward_count 102737  19.800%\n",
      "lif layer 1 self.abs_max_v: 18206.5\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.202453/  2.226069, val:  67.92%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 81.7687%\n",
      "layer   3  Sparsity: 87.8741%\n",
      "total_backward_count 528660 real_backward_count 104330  19.735%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.205969/  2.224875, val:  67.92%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 81.6856%\n",
      "layer   3  Sparsity: 87.7699%\n",
      "total_backward_count 538450 real_backward_count 105883  19.664%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.204304/  2.228909, val:  56.67%, val_best:  68.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4457%\n",
      "layer   2  Sparsity: 81.5907%\n",
      "layer   3  Sparsity: 87.4560%\n",
      "total_backward_count 548240 real_backward_count 107469  19.603%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.200612/  2.226164, val:  47.08%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4068%\n",
      "layer   2  Sparsity: 81.4573%\n",
      "layer   3  Sparsity: 87.3187%\n",
      "total_backward_count 558030 real_backward_count 109027  19.538%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.198835/  2.227715, val:  61.67%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4583%\n",
      "layer   2  Sparsity: 81.5475%\n",
      "layer   3  Sparsity: 87.1907%\n",
      "total_backward_count 567820 real_backward_count 110530  19.466%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.200937/  2.228626, val:  59.58%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 81.6665%\n",
      "layer   3  Sparsity: 87.9159%\n",
      "total_backward_count 577610 real_backward_count 111977  19.386%\n",
      "lif layer 2 self.abs_max_v: 5418.0\n",
      "fc layer 2 self.abs_max_out: 4437.0\n",
      "fc layer 1 self.abs_max_out: 13744.0\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.202936/  2.226325, val:  53.33%, val_best:  68.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4405%\n",
      "layer   2  Sparsity: 81.5610%\n",
      "layer   3  Sparsity: 87.7270%\n",
      "total_backward_count 587400 real_backward_count 113510  19.324%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.201640/  2.227465, val:  57.50%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4515%\n",
      "layer   2  Sparsity: 81.6586%\n",
      "layer   3  Sparsity: 87.7586%\n",
      "total_backward_count 597190 real_backward_count 115025  19.261%\n",
      "lif layer 2 self.abs_max_v: 5436.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.203512/  2.226294, val:  66.25%, val_best:  68.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 81.4530%\n",
      "layer   3  Sparsity: 88.1442%\n",
      "total_backward_count 606980 real_backward_count 116585  19.207%\n",
      "lif layer 2 self.abs_max_v: 5570.5\n",
      "lif layer 2 self.abs_max_v: 5622.5\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.200125/  2.230898, val:  60.00%, val_best:  68.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4370%\n",
      "layer   2  Sparsity: 81.5103%\n",
      "layer   3  Sparsity: 87.6109%\n",
      "total_backward_count 616770 real_backward_count 118058  19.141%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.200283/  2.222841, val:  56.25%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4600%\n",
      "layer   2  Sparsity: 81.5327%\n",
      "layer   3  Sparsity: 87.7962%\n",
      "total_backward_count 626560 real_backward_count 119494  19.071%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.204208/  2.229867, val:  55.42%, val_best:  68.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4439%\n",
      "layer   2  Sparsity: 81.5833%\n",
      "layer   3  Sparsity: 87.9349%\n",
      "total_backward_count 636350 real_backward_count 120987  19.013%\n",
      "fc layer 2 self.abs_max_out: 4477.0\n",
      "fc layer 1 self.abs_max_out: 13824.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.201549/  2.226435, val:  64.58%, val_best:  68.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 81.5622%\n",
      "layer   3  Sparsity: 87.4455%\n",
      "total_backward_count 646140 real_backward_count 122452  18.951%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.198234/  2.223489, val:  65.83%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4253%\n",
      "layer   2  Sparsity: 81.5516%\n",
      "layer   3  Sparsity: 87.3996%\n",
      "total_backward_count 655930 real_backward_count 123910  18.891%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.197517/  2.224501, val:  57.92%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4125%\n",
      "layer   2  Sparsity: 81.3699%\n",
      "layer   3  Sparsity: 87.2521%\n",
      "total_backward_count 665720 real_backward_count 125378  18.833%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.196047/  2.218040, val:  72.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 81.5137%\n",
      "layer   3  Sparsity: 87.4661%\n",
      "total_backward_count 675510 real_backward_count 126887  18.784%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.196472/  2.222427, val:  56.25%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 81.5317%\n",
      "layer   3  Sparsity: 87.3168%\n",
      "total_backward_count 685300 real_backward_count 128408  18.737%\n",
      "fc layer 1 self.abs_max_out: 13996.0\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.192778/  2.222817, val:  68.33%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4428%\n",
      "layer   2  Sparsity: 81.4785%\n",
      "layer   3  Sparsity: 87.1381%\n",
      "total_backward_count 695090 real_backward_count 129911  18.690%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.193507/  2.222737, val:  59.17%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4284%\n",
      "layer   2  Sparsity: 81.5080%\n",
      "layer   3  Sparsity: 87.0716%\n",
      "total_backward_count 704880 real_backward_count 131342  18.633%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.192448/  2.213304, val:  71.67%, val_best:  72.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4367%\n",
      "layer   2  Sparsity: 81.4938%\n",
      "layer   3  Sparsity: 87.5733%\n",
      "total_backward_count 714670 real_backward_count 132805  18.583%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.182800/  2.210957, val:  62.92%, val_best:  72.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   2  Sparsity: 81.2958%\n",
      "layer   3  Sparsity: 87.1174%\n",
      "total_backward_count 724460 real_backward_count 134308  18.539%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.185952/  2.217351, val:  62.08%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 81.4020%\n",
      "layer   3  Sparsity: 86.9045%\n",
      "total_backward_count 734250 real_backward_count 135842  18.501%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.187058/  2.215586, val:  69.58%, val_best:  72.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4329%\n",
      "layer   2  Sparsity: 81.1712%\n",
      "layer   3  Sparsity: 86.9729%\n",
      "total_backward_count 744040 real_backward_count 137280  18.451%\n",
      "fc layer 1 self.abs_max_out: 14125.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.191073/  2.218086, val:  60.42%, val_best:  72.50%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 81.3932%\n",
      "layer   3  Sparsity: 87.2396%\n",
      "total_backward_count 753830 real_backward_count 138801  18.413%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.191099/  2.223940, val:  56.25%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4431%\n",
      "layer   2  Sparsity: 81.4095%\n",
      "layer   3  Sparsity: 87.3988%\n",
      "total_backward_count 763620 real_backward_count 140237  18.365%\n",
      "fc layer 1 self.abs_max_out: 14362.0\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.192739/  2.218998, val:  72.50%, val_best:  72.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4742%\n",
      "layer   2  Sparsity: 81.5313%\n",
      "layer   3  Sparsity: 87.4016%\n",
      "total_backward_count 773410 real_backward_count 141666  18.317%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.193804/  2.220867, val:  63.33%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4255%\n",
      "layer   2  Sparsity: 81.5807%\n",
      "layer   3  Sparsity: 87.3308%\n",
      "total_backward_count 783200 real_backward_count 143014  18.260%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.195491/  2.219750, val:  65.83%, val_best:  72.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4084%\n",
      "layer   2  Sparsity: 81.4250%\n",
      "layer   3  Sparsity: 87.1173%\n",
      "total_backward_count 792990 real_backward_count 144459  18.217%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.193703/  2.221663, val:  60.00%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 81.4001%\n",
      "layer   3  Sparsity: 87.1833%\n",
      "total_backward_count 802780 real_backward_count 145853  18.168%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.195076/  2.219721, val:  56.67%, val_best:  72.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4212%\n",
      "layer   2  Sparsity: 81.5489%\n",
      "layer   3  Sparsity: 87.2495%\n",
      "total_backward_count 812570 real_backward_count 147353  18.134%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.194863/  2.220665, val:  62.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3966%\n",
      "layer   2  Sparsity: 81.6037%\n",
      "layer   3  Sparsity: 87.3324%\n",
      "total_backward_count 822360 real_backward_count 148773  18.091%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.194392/  2.222846, val:  70.83%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4206%\n",
      "layer   2  Sparsity: 81.6400%\n",
      "layer   3  Sparsity: 87.3734%\n",
      "total_backward_count 832150 real_backward_count 150157  18.044%\n",
      "lif layer 1 self.abs_max_v: 18617.5\n",
      "lif layer 1 self.abs_max_v: 18797.5\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.195559/  2.223251, val:  68.75%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4530%\n",
      "layer   2  Sparsity: 81.3770%\n",
      "layer   3  Sparsity: 87.1603%\n",
      "total_backward_count 841940 real_backward_count 151525  17.997%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.194107/  2.220392, val:  66.25%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4248%\n",
      "layer   2  Sparsity: 81.5107%\n",
      "layer   3  Sparsity: 86.8386%\n",
      "total_backward_count 851730 real_backward_count 152904  17.952%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.191588/  2.216155, val:  70.83%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3822%\n",
      "layer   2  Sparsity: 81.6210%\n",
      "layer   3  Sparsity: 86.9997%\n",
      "total_backward_count 861520 real_backward_count 154313  17.912%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.193803/  2.220428, val:  70.83%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3983%\n",
      "layer   2  Sparsity: 81.6304%\n",
      "layer   3  Sparsity: 87.0644%\n",
      "total_backward_count 871310 real_backward_count 155706  17.870%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.193700/  2.221487, val:  69.58%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4472%\n",
      "layer   2  Sparsity: 81.4382%\n",
      "layer   3  Sparsity: 86.9653%\n",
      "total_backward_count 881100 real_backward_count 157121  17.832%\n",
      "lif layer 1 self.abs_max_v: 18832.5\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.194208/  2.220276, val:  67.08%, val_best:  72.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 81.4132%\n",
      "layer   3  Sparsity: 87.0437%\n",
      "total_backward_count 890890 real_backward_count 158590  17.801%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.190134/  2.218558, val:  75.83%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 81.3908%\n",
      "layer   3  Sparsity: 86.7078%\n",
      "total_backward_count 900680 real_backward_count 160002  17.765%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.193066/  2.221011, val:  68.33%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4502%\n",
      "layer   2  Sparsity: 81.4943%\n",
      "layer   3  Sparsity: 87.2151%\n",
      "total_backward_count 910470 real_backward_count 161402  17.727%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.193185/  2.219808, val:  72.92%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4451%\n",
      "layer   2  Sparsity: 81.3745%\n",
      "layer   3  Sparsity: 87.2635%\n",
      "total_backward_count 920260 real_backward_count 162777  17.688%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.194034/  2.219275, val:  69.17%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4670%\n",
      "layer   2  Sparsity: 81.4637%\n",
      "layer   3  Sparsity: 87.2708%\n",
      "total_backward_count 930050 real_backward_count 164143  17.649%\n",
      "lif layer 1 self.abs_max_v: 19197.0\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.190799/  2.214312, val:  69.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4638%\n",
      "layer   2  Sparsity: 81.3437%\n",
      "layer   3  Sparsity: 87.0788%\n",
      "total_backward_count 939840 real_backward_count 165501  17.609%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.189485/  2.218965, val:  75.42%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4033%\n",
      "layer   2  Sparsity: 81.2788%\n",
      "layer   3  Sparsity: 86.7513%\n",
      "total_backward_count 949630 real_backward_count 166890  17.574%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.191861/  2.212860, val:  70.42%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4446%\n",
      "layer   2  Sparsity: 81.2886%\n",
      "layer   3  Sparsity: 86.9314%\n",
      "total_backward_count 959420 real_backward_count 168298  17.542%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.188738/  2.224848, val:  47.50%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 81.5125%\n",
      "layer   3  Sparsity: 87.2728%\n",
      "total_backward_count 969210 real_backward_count 169652  17.504%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.192886/  2.218993, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4208%\n",
      "layer   2  Sparsity: 81.1958%\n",
      "layer   3  Sparsity: 87.5482%\n",
      "total_backward_count 979000 real_backward_count 171064  17.473%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.193027/  2.221328, val:  61.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4256%\n",
      "layer   2  Sparsity: 81.2448%\n",
      "layer   3  Sparsity: 87.3891%\n",
      "total_backward_count 988790 real_backward_count 172375  17.433%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.192119/  2.218991, val:  71.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4261%\n",
      "layer   2  Sparsity: 81.1350%\n",
      "layer   3  Sparsity: 87.0988%\n",
      "total_backward_count 998580 real_backward_count 173692  17.394%\n",
      "lif layer 1 self.abs_max_v: 19753.5\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.194376/  2.219884, val:  67.08%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4798%\n",
      "layer   2  Sparsity: 81.2460%\n",
      "layer   3  Sparsity: 87.3475%\n",
      "total_backward_count 1008370 real_backward_count 175009  17.356%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.193084/  2.219478, val:  60.83%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4214%\n",
      "layer   2  Sparsity: 81.0922%\n",
      "layer   3  Sparsity: 86.9784%\n",
      "total_backward_count 1018160 real_backward_count 176340  17.319%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.191503/  2.217800, val:  71.25%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4603%\n",
      "layer   2  Sparsity: 80.8470%\n",
      "layer   3  Sparsity: 87.0908%\n",
      "total_backward_count 1027950 real_backward_count 177665  17.283%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.192145/  2.216760, val:  73.33%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4458%\n",
      "layer   2  Sparsity: 81.0263%\n",
      "layer   3  Sparsity: 87.6116%\n",
      "total_backward_count 1037740 real_backward_count 179012  17.250%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.189405/  2.220021, val:  66.25%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4293%\n",
      "layer   2  Sparsity: 81.1093%\n",
      "layer   3  Sparsity: 87.5311%\n",
      "total_backward_count 1047530 real_backward_count 180277  17.210%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.195143/  2.220405, val:  67.92%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 81.0377%\n",
      "layer   3  Sparsity: 87.6339%\n",
      "total_backward_count 1057320 real_backward_count 181637  17.179%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.196211/  2.220444, val:  68.75%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4353%\n",
      "layer   2  Sparsity: 81.0178%\n",
      "layer   3  Sparsity: 87.4824%\n",
      "total_backward_count 1067110 real_backward_count 182953  17.145%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.191602/  2.220397, val:  70.00%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3850%\n",
      "layer   2  Sparsity: 81.0913%\n",
      "layer   3  Sparsity: 87.3243%\n",
      "total_backward_count 1076900 real_backward_count 184273  17.111%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.189833/  2.217910, val:  74.17%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4796%\n",
      "layer   2  Sparsity: 81.1726%\n",
      "layer   3  Sparsity: 87.3134%\n",
      "total_backward_count 1086690 real_backward_count 185538  17.074%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.191381/  2.215377, val:  69.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 81.0930%\n",
      "layer   3  Sparsity: 87.2987%\n",
      "total_backward_count 1096480 real_backward_count 186829  17.039%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.192688/  2.215052, val:  73.33%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4570%\n",
      "layer   2  Sparsity: 81.1305%\n",
      "layer   3  Sparsity: 87.5449%\n",
      "total_backward_count 1106270 real_backward_count 188139  17.007%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.189583/  2.221232, val:  70.00%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3622%\n",
      "layer   2  Sparsity: 80.9924%\n",
      "layer   3  Sparsity: 87.5103%\n",
      "total_backward_count 1116060 real_backward_count 189406  16.971%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.194047/  2.225877, val:  65.42%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4309%\n",
      "layer   2  Sparsity: 80.9032%\n",
      "layer   3  Sparsity: 87.4565%\n",
      "total_backward_count 1125850 real_backward_count 190707  16.939%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.193561/  2.219843, val:  66.25%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 80.8741%\n",
      "layer   3  Sparsity: 87.1856%\n",
      "total_backward_count 1135640 real_backward_count 191968  16.904%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.189140/  2.217950, val:  62.92%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 80.9646%\n",
      "layer   3  Sparsity: 87.0995%\n",
      "total_backward_count 1145430 real_backward_count 193228  16.869%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.188194/  2.216623, val:  72.92%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4663%\n",
      "layer   2  Sparsity: 81.0203%\n",
      "layer   3  Sparsity: 87.3822%\n",
      "total_backward_count 1155220 real_backward_count 194503  16.837%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.186093/  2.215844, val:  72.92%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4193%\n",
      "layer   2  Sparsity: 80.9580%\n",
      "layer   3  Sparsity: 87.0899%\n",
      "total_backward_count 1165010 real_backward_count 195795  16.806%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.188405/  2.217862, val:  77.08%, val_best:  77.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4288%\n",
      "layer   2  Sparsity: 81.0210%\n",
      "layer   3  Sparsity: 87.1920%\n",
      "total_backward_count 1174800 real_backward_count 197063  16.774%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.184795/  2.215165, val:  80.00%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4224%\n",
      "layer   2  Sparsity: 81.1034%\n",
      "layer   3  Sparsity: 87.0224%\n",
      "total_backward_count 1184590 real_backward_count 198331  16.743%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.190766/  2.218306, val:  72.08%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4552%\n",
      "layer   2  Sparsity: 80.9460%\n",
      "layer   3  Sparsity: 87.2274%\n",
      "total_backward_count 1194380 real_backward_count 199617  16.713%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.188598/  2.215370, val:  67.92%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4233%\n",
      "layer   2  Sparsity: 80.8155%\n",
      "layer   3  Sparsity: 87.3345%\n",
      "total_backward_count 1204170 real_backward_count 200933  16.686%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.187087/  2.211161, val:  76.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 80.7671%\n",
      "layer   3  Sparsity: 86.9572%\n",
      "total_backward_count 1213960 real_backward_count 202236  16.659%\n",
      "fc layer 1 self.abs_max_out: 14453.0\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.184014/  2.214704, val:  69.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4392%\n",
      "layer   2  Sparsity: 80.8511%\n",
      "layer   3  Sparsity: 86.9103%\n",
      "total_backward_count 1223750 real_backward_count 203520  16.631%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.187539/  2.218045, val:  70.83%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.3975%\n",
      "layer   2  Sparsity: 81.2189%\n",
      "layer   3  Sparsity: 87.5359%\n",
      "total_backward_count 1233540 real_backward_count 204783  16.601%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.186731/  2.218804, val:  66.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4549%\n",
      "layer   2  Sparsity: 80.9799%\n",
      "layer   3  Sparsity: 87.6655%\n",
      "total_backward_count 1243330 real_backward_count 206071  16.574%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.189056/  2.216186, val:  74.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 80.9336%\n",
      "layer   3  Sparsity: 87.4888%\n",
      "total_backward_count 1253120 real_backward_count 207362  16.548%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.188128/  2.216533, val:  76.25%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4191%\n",
      "layer   2  Sparsity: 80.9616%\n",
      "layer   3  Sparsity: 87.5092%\n",
      "total_backward_count 1262910 real_backward_count 208568  16.515%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.186856/  2.219253, val:  79.17%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4315%\n",
      "layer   2  Sparsity: 81.0408%\n",
      "layer   3  Sparsity: 87.4194%\n",
      "total_backward_count 1272700 real_backward_count 209784  16.483%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.189586/  2.216417, val:  60.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4136%\n",
      "layer   2  Sparsity: 81.0103%\n",
      "layer   3  Sparsity: 87.5292%\n",
      "total_backward_count 1282490 real_backward_count 211049  16.456%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.188065/  2.218069, val:  65.42%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4198%\n",
      "layer   2  Sparsity: 81.0913%\n",
      "layer   3  Sparsity: 87.4123%\n",
      "total_backward_count 1292280 real_backward_count 212314  16.429%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.188715/  2.214343, val:  72.08%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 80.8977%\n",
      "layer   3  Sparsity: 87.4347%\n",
      "total_backward_count 1302070 real_backward_count 213528  16.399%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.185987/  2.215878, val:  71.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 80.9030%\n",
      "layer   3  Sparsity: 87.1504%\n",
      "total_backward_count 1311860 real_backward_count 214774  16.372%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.183109/  2.217669, val:  67.50%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4386%\n",
      "layer   2  Sparsity: 80.8938%\n",
      "layer   3  Sparsity: 87.1319%\n",
      "total_backward_count 1321650 real_backward_count 215969  16.341%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.188535/  2.217961, val:  75.00%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3999%\n",
      "layer   2  Sparsity: 80.8945%\n",
      "layer   3  Sparsity: 87.2320%\n",
      "total_backward_count 1331440 real_backward_count 217258  16.318%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.184981/  2.210628, val:  79.58%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4437%\n",
      "layer   2  Sparsity: 80.8454%\n",
      "layer   3  Sparsity: 86.9606%\n",
      "total_backward_count 1341230 real_backward_count 218536  16.294%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.182449/  2.209630, val:  63.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4640%\n",
      "layer   2  Sparsity: 80.9512%\n",
      "layer   3  Sparsity: 87.2738%\n",
      "total_backward_count 1351020 real_backward_count 219715  16.263%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.186399/  2.213871, val:  80.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 81.0193%\n",
      "layer   3  Sparsity: 87.4680%\n",
      "total_backward_count 1360810 real_backward_count 220976  16.239%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.186403/  2.216803, val:  67.92%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4500%\n",
      "layer   2  Sparsity: 81.0231%\n",
      "layer   3  Sparsity: 87.4737%\n",
      "total_backward_count 1370600 real_backward_count 222270  16.217%\n",
      "fc layer 1 self.abs_max_out: 14517.0\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.187080/  2.213911, val:  69.58%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3876%\n",
      "layer   2  Sparsity: 81.1937%\n",
      "layer   3  Sparsity: 87.5975%\n",
      "total_backward_count 1380390 real_backward_count 223491  16.190%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.182657/  2.209073, val:  75.83%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 81.0726%\n",
      "layer   3  Sparsity: 87.6316%\n",
      "total_backward_count 1390180 real_backward_count 224704  16.164%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.181084/  2.208579, val:  79.58%, val_best:  80.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 80.9057%\n",
      "layer   3  Sparsity: 87.4088%\n",
      "total_backward_count 1399970 real_backward_count 225947  16.139%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.184308/  2.211799, val:  74.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4159%\n",
      "layer   2  Sparsity: 80.8867%\n",
      "layer   3  Sparsity: 87.4666%\n",
      "total_backward_count 1409760 real_backward_count 227158  16.113%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.184272/  2.209920, val:  65.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4419%\n",
      "layer   2  Sparsity: 80.9100%\n",
      "layer   3  Sparsity: 87.7749%\n",
      "total_backward_count 1419550 real_backward_count 228364  16.087%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.188265/  2.215941, val:  63.33%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4641%\n",
      "layer   2  Sparsity: 81.0274%\n",
      "layer   3  Sparsity: 87.7635%\n",
      "total_backward_count 1429340 real_backward_count 229575  16.062%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.186553/  2.219689, val:  67.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4555%\n",
      "layer   2  Sparsity: 81.0183%\n",
      "layer   3  Sparsity: 87.6283%\n",
      "total_backward_count 1439130 real_backward_count 230887  16.044%\n",
      "fc layer 1 self.abs_max_out: 14700.0\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.187032/  2.217041, val:  69.17%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4359%\n",
      "layer   2  Sparsity: 81.0388%\n",
      "layer   3  Sparsity: 87.4970%\n",
      "total_backward_count 1448920 real_backward_count 232060  16.016%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.188107/  2.218645, val:  69.58%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4338%\n",
      "layer   2  Sparsity: 80.8613%\n",
      "layer   3  Sparsity: 87.1000%\n",
      "total_backward_count 1458710 real_backward_count 233294  15.993%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.189216/  2.215862, val:  70.83%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4263%\n",
      "layer   2  Sparsity: 80.9185%\n",
      "layer   3  Sparsity: 87.5062%\n",
      "total_backward_count 1468500 real_backward_count 234507  15.969%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.186626/  2.214934, val:  82.08%, val_best:  82.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4890%\n",
      "layer   2  Sparsity: 80.8782%\n",
      "layer   3  Sparsity: 87.6408%\n",
      "total_backward_count 1478290 real_backward_count 235745  15.947%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.184088/  2.210404, val:  72.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4407%\n",
      "layer   2  Sparsity: 80.9002%\n",
      "layer   3  Sparsity: 87.8121%\n",
      "total_backward_count 1488080 real_backward_count 236953  15.923%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.183394/  2.214497, val:  63.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4509%\n",
      "layer   2  Sparsity: 80.9904%\n",
      "layer   3  Sparsity: 87.5119%\n",
      "total_backward_count 1497870 real_backward_count 238142  15.899%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.187916/  2.211392, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4051%\n",
      "layer   2  Sparsity: 80.8605%\n",
      "layer   3  Sparsity: 87.5249%\n",
      "total_backward_count 1507660 real_backward_count 239376  15.877%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.186573/  2.214797, val:  77.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 80.8199%\n",
      "layer   3  Sparsity: 87.4822%\n",
      "total_backward_count 1517450 real_backward_count 240530  15.851%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.180738/  2.211044, val:  70.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4632%\n",
      "layer   2  Sparsity: 80.9739%\n",
      "layer   3  Sparsity: 87.2034%\n",
      "total_backward_count 1527240 real_backward_count 241680  15.825%\n",
      "fc layer 1 self.abs_max_out: 14758.0\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.183687/  2.213705, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4156%\n",
      "layer   2  Sparsity: 80.8357%\n",
      "layer   3  Sparsity: 87.4874%\n",
      "total_backward_count 1537030 real_backward_count 242852  15.800%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.183437/  2.214333, val:  72.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4274%\n",
      "layer   2  Sparsity: 81.0294%\n",
      "layer   3  Sparsity: 87.6390%\n",
      "total_backward_count 1546820 real_backward_count 244026  15.776%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.182581/  2.210568, val:  78.33%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4578%\n",
      "layer   2  Sparsity: 80.8747%\n",
      "layer   3  Sparsity: 87.5578%\n",
      "total_backward_count 1556610 real_backward_count 245252  15.756%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.182057/  2.208030, val:  74.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4591%\n",
      "layer   2  Sparsity: 81.0112%\n",
      "layer   3  Sparsity: 87.5608%\n",
      "total_backward_count 1566400 real_backward_count 246446  15.733%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.182862/  2.210132, val:  71.25%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4628%\n",
      "layer   2  Sparsity: 81.0156%\n",
      "layer   3  Sparsity: 87.6122%\n",
      "total_backward_count 1576190 real_backward_count 247610  15.709%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.186764/  2.219399, val:  78.75%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4337%\n",
      "layer   2  Sparsity: 81.0840%\n",
      "layer   3  Sparsity: 87.5057%\n",
      "total_backward_count 1585980 real_backward_count 248778  15.686%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.188290/  2.212941, val:  74.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 80.9354%\n",
      "layer   3  Sparsity: 87.5742%\n",
      "total_backward_count 1595770 real_backward_count 249978  15.665%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.181038/  2.214151, val:  75.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4257%\n",
      "layer   2  Sparsity: 80.8655%\n",
      "layer   3  Sparsity: 87.6963%\n",
      "total_backward_count 1605560 real_backward_count 251173  15.644%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.186807/  2.215294, val:  72.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4210%\n",
      "layer   2  Sparsity: 81.0007%\n",
      "layer   3  Sparsity: 87.5659%\n",
      "total_backward_count 1615350 real_backward_count 252326  15.621%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.186655/  2.215591, val:  73.33%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4491%\n",
      "layer   2  Sparsity: 81.0142%\n",
      "layer   3  Sparsity: 87.6116%\n",
      "total_backward_count 1625140 real_backward_count 253456  15.596%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.187580/  2.215691, val:  72.50%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4731%\n",
      "layer   2  Sparsity: 81.1048%\n",
      "layer   3  Sparsity: 87.4892%\n",
      "total_backward_count 1634930 real_backward_count 254623  15.574%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.185955/  2.217718, val:  71.25%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3998%\n",
      "layer   2  Sparsity: 80.9372%\n",
      "layer   3  Sparsity: 87.5463%\n",
      "total_backward_count 1644720 real_backward_count 255750  15.550%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.186860/  2.213982, val:  71.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4671%\n",
      "layer   2  Sparsity: 80.7719%\n",
      "layer   3  Sparsity: 87.2785%\n",
      "total_backward_count 1654510 real_backward_count 256878  15.526%\n",
      "lif layer 2 self.abs_max_v: 5870.0\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.186728/  2.215938, val:  75.83%, val_best:  82.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4466%\n",
      "layer   2  Sparsity: 80.9384%\n",
      "layer   3  Sparsity: 87.6266%\n",
      "total_backward_count 1664300 real_backward_count 258053  15.505%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.186168/  2.214232, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4414%\n",
      "layer   2  Sparsity: 80.9832%\n",
      "layer   3  Sparsity: 87.6540%\n",
      "total_backward_count 1674090 real_backward_count 259188  15.482%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.185883/  2.214451, val:  68.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.3944%\n",
      "layer   2  Sparsity: 80.9336%\n",
      "layer   3  Sparsity: 87.4602%\n",
      "total_backward_count 1683880 real_backward_count 260355  15.462%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.184726/  2.212590, val:  69.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 80.8811%\n",
      "layer   3  Sparsity: 87.3677%\n",
      "total_backward_count 1693670 real_backward_count 261520  15.441%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.183330/  2.213797, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 81.1269%\n",
      "layer   3  Sparsity: 87.4148%\n",
      "total_backward_count 1703460 real_backward_count 262614  15.417%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.184308/  2.210511, val:  81.67%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4142%\n",
      "layer   2  Sparsity: 81.0046%\n",
      "layer   3  Sparsity: 87.4685%\n",
      "total_backward_count 1713250 real_backward_count 263768  15.396%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.181954/  2.211680, val:  86.67%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3973%\n",
      "layer   2  Sparsity: 81.0425%\n",
      "layer   3  Sparsity: 87.5537%\n",
      "total_backward_count 1723040 real_backward_count 264885  15.373%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.183800/  2.209985, val:  74.17%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4291%\n",
      "layer   2  Sparsity: 80.7252%\n",
      "layer   3  Sparsity: 87.4726%\n",
      "total_backward_count 1732830 real_backward_count 266001  15.351%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.185891/  2.214002, val:  72.08%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 80.8084%\n",
      "layer   3  Sparsity: 87.4418%\n",
      "total_backward_count 1742620 real_backward_count 267145  15.330%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.186529/  2.216589, val:  67.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 80.8446%\n",
      "layer   3  Sparsity: 87.1933%\n",
      "total_backward_count 1752410 real_backward_count 268265  15.308%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.185825/  2.212924, val:  70.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4441%\n",
      "layer   2  Sparsity: 80.8279%\n",
      "layer   3  Sparsity: 87.0648%\n",
      "total_backward_count 1762200 real_backward_count 269436  15.290%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.180173/  2.205319, val:  75.00%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4087%\n",
      "layer   2  Sparsity: 80.7550%\n",
      "layer   3  Sparsity: 86.8243%\n",
      "total_backward_count 1771990 real_backward_count 270591  15.270%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.179264/  2.211974, val:  70.00%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 80.7825%\n",
      "layer   3  Sparsity: 87.1571%\n",
      "total_backward_count 1781780 real_backward_count 271723  15.250%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.178926/  2.207766, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 80.9910%\n",
      "layer   3  Sparsity: 87.1459%\n",
      "total_backward_count 1791570 real_backward_count 272842  15.229%\n",
      "fc layer 1 self.abs_max_out: 14806.0\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.180062/  2.205259, val:  64.17%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4260%\n",
      "layer   2  Sparsity: 80.7780%\n",
      "layer   3  Sparsity: 87.2126%\n",
      "total_backward_count 1801360 real_backward_count 273929  15.207%\n",
      "fc layer 1 self.abs_max_out: 14810.0\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.176550/  2.203336, val:  69.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4424%\n",
      "layer   2  Sparsity: 81.0175%\n",
      "layer   3  Sparsity: 87.2132%\n",
      "total_backward_count 1811150 real_backward_count 274992  15.183%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.176995/  2.210383, val:  72.50%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 81.0890%\n",
      "layer   3  Sparsity: 87.5846%\n",
      "total_backward_count 1820940 real_backward_count 276168  15.166%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.179089/  2.209384, val:  79.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4092%\n",
      "layer   2  Sparsity: 80.9163%\n",
      "layer   3  Sparsity: 87.5979%\n",
      "total_backward_count 1830730 real_backward_count 277237  15.144%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.177674/  2.210872, val:  73.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4323%\n",
      "layer   2  Sparsity: 80.8628%\n",
      "layer   3  Sparsity: 87.2128%\n",
      "total_backward_count 1840520 real_backward_count 278340  15.123%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.179432/  2.209499, val:  69.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4132%\n",
      "layer   2  Sparsity: 80.9565%\n",
      "layer   3  Sparsity: 87.2644%\n",
      "total_backward_count 1850310 real_backward_count 279435  15.102%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.178500/  2.207259, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 80.8328%\n",
      "layer   3  Sparsity: 87.5473%\n",
      "total_backward_count 1860100 real_backward_count 280507  15.080%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.178736/  2.205418, val:  78.33%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3947%\n",
      "layer   2  Sparsity: 80.8360%\n",
      "layer   3  Sparsity: 87.1702%\n",
      "total_backward_count 1869890 real_backward_count 281600  15.060%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.176275/  2.205003, val:  68.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4354%\n",
      "layer   2  Sparsity: 81.0758%\n",
      "layer   3  Sparsity: 86.8334%\n",
      "total_backward_count 1879680 real_backward_count 282666  15.038%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.171691/  2.210799, val:  79.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 81.0016%\n",
      "layer   3  Sparsity: 86.6766%\n",
      "total_backward_count 1889470 real_backward_count 283752  15.018%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.175982/  2.212710, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4487%\n",
      "layer   2  Sparsity: 80.9762%\n",
      "layer   3  Sparsity: 87.2020%\n",
      "total_backward_count 1899260 real_backward_count 284818  14.996%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.178030/  2.206890, val:  79.58%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4382%\n",
      "layer   2  Sparsity: 80.9820%\n",
      "layer   3  Sparsity: 87.1183%\n",
      "total_backward_count 1909050 real_backward_count 285896  14.976%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.177263/  2.208137, val:  66.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.3872%\n",
      "layer   2  Sparsity: 80.9976%\n",
      "layer   3  Sparsity: 86.9930%\n",
      "total_backward_count 1918840 real_backward_count 287018  14.958%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.174257/  2.206087, val:  83.75%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4464%\n",
      "layer   2  Sparsity: 80.9516%\n",
      "layer   3  Sparsity: 86.9286%\n",
      "total_backward_count 1928630 real_backward_count 288106  14.938%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.175225/  2.207584, val:  75.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.5142%\n",
      "layer   2  Sparsity: 80.9861%\n",
      "layer   3  Sparsity: 87.0037%\n",
      "total_backward_count 1938420 real_backward_count 289213  14.920%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.176559/  2.204973, val:  75.42%, val_best:  86.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 80.8754%\n",
      "layer   3  Sparsity: 87.0263%\n",
      "total_backward_count 1948210 real_backward_count 290273  14.899%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.174473/  2.209616, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4394%\n",
      "layer   2  Sparsity: 80.9169%\n",
      "layer   3  Sparsity: 87.3154%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a7fffdd92942b8a582f8c6c92bce69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>2.17447</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.74167</td></tr><tr><td>val_loss</td><td>2.20962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-sweep-66</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1o5sl4rx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1o5sl4rx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_092250-1o5sl4rx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 54n8knmg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_134007-54n8knmg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54n8knmg' target=\"_blank\">flowing-sweep-72</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54n8knmg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54n8knmg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251115_134016_072', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 467.0\n",
      "lif layer 1 self.abs_max_v: 467.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 839.0\n",
      "lif layer 2 self.abs_max_v: 839.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "fc layer 1 self.abs_max_out: 588.0\n",
      "lif layer 1 self.abs_max_v: 747.5\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "lif layer 2 self.abs_max_v: 1518.5\n",
      "fc layer 1 self.abs_max_out: 846.0\n",
      "lif layer 1 self.abs_max_v: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 1760.5\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 1790.0\n",
      "lif layer 2 self.abs_max_v: 1861.5\n",
      "lif layer 2 self.abs_max_v: 1882.0\n",
      "lif layer 1 self.abs_max_v: 1278.5\n",
      "lif layer 2 self.abs_max_v: 2133.0\n",
      "lif layer 2 self.abs_max_v: 2253.5\n",
      "fc layer 1 self.abs_max_out: 987.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "lif layer 1 self.abs_max_v: 1330.0\n",
      "fc layer 1 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 1620.0\n",
      "lif layer 2 self.abs_max_v: 2265.5\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "lif layer 2 self.abs_max_v: 2660.5\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "lif layer 1 self.abs_max_v: 1677.0\n",
      "fc layer 1 self.abs_max_out: 1464.0\n",
      "lif layer 1 self.abs_max_v: 1703.5\n",
      "fc layer 3 self.abs_max_out: 1012.0\n",
      "fc layer 3 self.abs_max_out: 1262.0\n",
      "fc layer 1 self.abs_max_out: 1568.0\n",
      "lif layer 1 self.abs_max_v: 1858.0\n",
      "fc layer 2 self.abs_max_out: 2018.0\n",
      "lif layer 2 self.abs_max_v: 2712.5\n",
      "fc layer 2 self.abs_max_out: 2106.0\n",
      "lif layer 2 self.abs_max_v: 2775.5\n",
      "lif layer 2 self.abs_max_v: 3088.0\n",
      "lif layer 1 self.abs_max_v: 2074.0\n",
      "lif layer 2 self.abs_max_v: 3401.0\n",
      "fc layer 1 self.abs_max_out: 1612.0\n",
      "lif layer 1 self.abs_max_v: 2085.0\n",
      "fc layer 2 self.abs_max_out: 2422.0\n",
      "lif layer 2 self.abs_max_v: 4012.0\n",
      "lif layer 1 self.abs_max_v: 2285.5\n",
      "fc layer 1 self.abs_max_out: 1726.0\n",
      "fc layer 2 self.abs_max_out: 2470.0\n",
      "fc layer 1 self.abs_max_out: 1920.0\n",
      "fc layer 2 self.abs_max_out: 2559.0\n",
      "lif layer 1 self.abs_max_v: 2499.0\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "fc layer 1 self.abs_max_out: 2034.0\n",
      "fc layer 1 self.abs_max_out: 2658.0\n",
      "lif layer 1 self.abs_max_v: 2658.0\n",
      "fc layer 2 self.abs_max_out: 2650.0\n",
      "lif layer 1 self.abs_max_v: 2810.5\n",
      "fc layer 2 self.abs_max_out: 2684.0\n",
      "fc layer 2 self.abs_max_out: 2687.0\n",
      "lif layer 2 self.abs_max_v: 4142.0\n",
      "fc layer 2 self.abs_max_out: 2806.0\n",
      "lif layer 2 self.abs_max_v: 4273.5\n",
      "lif layer 1 self.abs_max_v: 2986.5\n",
      "lif layer 1 self.abs_max_v: 3022.5\n",
      "lif layer 1 self.abs_max_v: 3314.5\n",
      "fc layer 1 self.abs_max_out: 2672.0\n",
      "lif layer 1 self.abs_max_v: 3571.5\n",
      "fc layer 1 self.abs_max_out: 2881.0\n",
      "lif layer 2 self.abs_max_v: 4330.5\n",
      "fc layer 1 self.abs_max_out: 3004.0\n",
      "fc layer 1 self.abs_max_out: 3064.0\n",
      "fc layer 1 self.abs_max_out: 3124.0\n",
      "fc layer 2 self.abs_max_out: 2907.0\n",
      "fc layer 1 self.abs_max_out: 3682.0\n",
      "lif layer 1 self.abs_max_v: 3682.0\n",
      "lif layer 1 self.abs_max_v: 4057.5\n",
      "lif layer 2 self.abs_max_v: 4359.5\n",
      "lif layer 2 self.abs_max_v: 4616.0\n",
      "lif layer 1 self.abs_max_v: 4153.0\n",
      "fc layer 2 self.abs_max_out: 3020.0\n",
      "lif layer 1 self.abs_max_v: 4354.0\n",
      "fc layer 2 self.abs_max_out: 3051.0\n",
      "fc layer 1 self.abs_max_out: 3900.0\n",
      "fc layer 1 self.abs_max_out: 3957.0\n",
      "lif layer 1 self.abs_max_v: 4541.5\n",
      "fc layer 2 self.abs_max_out: 3059.0\n",
      "fc layer 2 self.abs_max_out: 3111.0\n",
      "lif layer 1 self.abs_max_v: 4555.5\n",
      "lif layer 2 self.abs_max_v: 4752.0\n",
      "lif layer 2 self.abs_max_v: 4934.0\n",
      "fc layer 2 self.abs_max_out: 3186.0\n",
      "lif layer 2 self.abs_max_v: 5083.5\n",
      "lif layer 2 self.abs_max_v: 5158.0\n",
      "lif layer 2 self.abs_max_v: 5276.0\n",
      "fc layer 2 self.abs_max_out: 3229.0\n",
      "fc layer 1 self.abs_max_out: 3999.0\n",
      "fc layer 1 self.abs_max_out: 4015.0\n",
      "fc layer 1 self.abs_max_out: 4033.0\n",
      "fc layer 1 self.abs_max_out: 4134.0\n",
      "fc layer 1 self.abs_max_out: 4220.0\n",
      "fc layer 1 self.abs_max_out: 4527.0\n",
      "lif layer 1 self.abs_max_v: 4651.0\n",
      "lif layer 2 self.abs_max_v: 5415.5\n",
      "lif layer 2 self.abs_max_v: 5592.0\n",
      "fc layer 2 self.abs_max_out: 3344.0\n",
      "fc layer 3 self.abs_max_out: 1271.0\n",
      "lif layer 1 self.abs_max_v: 4720.5\n",
      "lif layer 1 self.abs_max_v: 5024.0\n",
      "lif layer 1 self.abs_max_v: 5322.0\n",
      "lif layer 1 self.abs_max_v: 5910.5\n",
      "lif layer 1 self.abs_max_v: 6360.5\n",
      "lif layer 1 self.abs_max_v: 6849.5\n",
      "lif layer 1 self.abs_max_v: 6963.0\n",
      "lif layer 1 self.abs_max_v: 7106.5\n",
      "fc layer 1 self.abs_max_out: 4882.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.882535/  2.042487, val:  34.17%, val_best:  34.17%, tr:  92.13%, tr_best:  92.13%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2308%\n",
      "layer   2  Sparsity: 74.8630%\n",
      "layer   3  Sparsity: 70.9418%\n",
      "total_backward_count 9790 real_backward_count 2780  28.396%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1378.0\n",
      "fc layer 2 self.abs_max_out: 3384.0\n",
      "fc layer 3 self.abs_max_out: 1454.0\n",
      "fc layer 2 self.abs_max_out: 3395.0\n",
      "lif layer 2 self.abs_max_v: 5879.0\n",
      "fc layer 2 self.abs_max_out: 3456.0\n",
      "fc layer 2 self.abs_max_out: 3662.0\n",
      "fc layer 1 self.abs_max_out: 4910.0\n",
      "fc layer 1 self.abs_max_out: 5244.0\n",
      "fc layer 1 self.abs_max_out: 5297.0\n",
      "fc layer 1 self.abs_max_out: 5321.0\n",
      "fc layer 1 self.abs_max_out: 5404.0\n",
      "lif layer 1 self.abs_max_v: 7107.5\n",
      "fc layer 1 self.abs_max_out: 5731.0\n",
      "fc layer 2 self.abs_max_out: 3730.0\n",
      "fc layer 2 self.abs_max_out: 3790.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.845140/  2.022284, val:  42.08%, val_best:  42.08%, tr:  98.37%, tr_best:  98.37%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2299%\n",
      "layer   2  Sparsity: 73.2630%\n",
      "layer   3  Sparsity: 68.6945%\n",
      "total_backward_count 19580 real_backward_count 4622  23.606%\n",
      "lif layer 2 self.abs_max_v: 5917.0\n",
      "lif layer 2 self.abs_max_v: 5998.0\n",
      "lif layer 2 self.abs_max_v: 6021.0\n",
      "fc layer 2 self.abs_max_out: 3888.0\n",
      "lif layer 2 self.abs_max_v: 6030.0\n",
      "fc layer 1 self.abs_max_out: 5745.0\n",
      "fc layer 1 self.abs_max_out: 5799.0\n",
      "fc layer 1 self.abs_max_out: 5815.0\n",
      "lif layer 1 self.abs_max_v: 7323.5\n",
      "lif layer 2 self.abs_max_v: 6181.0\n",
      "lif layer 1 self.abs_max_v: 7355.0\n",
      "fc layer 1 self.abs_max_out: 5880.0\n",
      "fc layer 1 self.abs_max_out: 6115.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.858027/  2.003943, val:  42.92%, val_best:  42.92%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2338%\n",
      "layer   2  Sparsity: 72.1103%\n",
      "layer   3  Sparsity: 69.0407%\n",
      "total_backward_count 29370 real_backward_count 6340  21.587%\n",
      "lif layer 2 self.abs_max_v: 6582.0\n",
      "fc layer 1 self.abs_max_out: 6221.0\n",
      "fc layer 1 self.abs_max_out: 6339.0\n",
      "lif layer 1 self.abs_max_v: 7436.5\n",
      "fc layer 1 self.abs_max_out: 6422.0\n",
      "lif layer 1 self.abs_max_v: 8356.5\n",
      "fc layer 1 self.abs_max_out: 6443.0\n",
      "lif layer 1 self.abs_max_v: 8723.0\n",
      "fc layer 1 self.abs_max_out: 6593.0\n",
      "fc layer 2 self.abs_max_out: 3969.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.847028/  2.013592, val:  49.58%, val_best:  49.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2318%\n",
      "layer   2  Sparsity: 72.1112%\n",
      "layer   3  Sparsity: 69.4450%\n",
      "total_backward_count 39160 real_backward_count 7915  20.212%\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.853992/  2.004556, val:  42.50%, val_best:  49.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2365%\n",
      "layer   2  Sparsity: 71.7058%\n",
      "layer   3  Sparsity: 69.4268%\n",
      "total_backward_count 48950 real_backward_count 9450  19.305%\n",
      "fc layer 1 self.abs_max_out: 6770.0\n",
      "fc layer 1 self.abs_max_out: 6844.0\n",
      "lif layer 1 self.abs_max_v: 8760.0\n",
      "lif layer 1 self.abs_max_v: 8879.0\n",
      "lif layer 1 self.abs_max_v: 9311.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.837501/  1.996306, val:  52.08%, val_best:  52.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2262%\n",
      "layer   2  Sparsity: 71.5979%\n",
      "layer   3  Sparsity: 68.8357%\n",
      "total_backward_count 58740 real_backward_count 10925  18.599%\n",
      "lif layer 1 self.abs_max_v: 9411.0\n",
      "lif layer 1 self.abs_max_v: 10419.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.847328/  1.989415, val:  54.17%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2493%\n",
      "layer   2  Sparsity: 71.3523%\n",
      "layer   3  Sparsity: 68.8751%\n",
      "total_backward_count 68530 real_backward_count 12331  17.994%\n",
      "fc layer 2 self.abs_max_out: 4107.0\n",
      "lif layer 2 self.abs_max_v: 7135.0\n",
      "fc layer 1 self.abs_max_out: 6886.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.842678/  1.993141, val:  51.25%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2254%\n",
      "layer   2  Sparsity: 71.3001%\n",
      "layer   3  Sparsity: 69.9330%\n",
      "total_backward_count 78320 real_backward_count 13622  17.393%\n",
      "lif layer 2 self.abs_max_v: 7298.0\n",
      "fc layer 2 self.abs_max_out: 4392.0\n",
      "lif layer 2 self.abs_max_v: 7627.5\n",
      "lif layer 2 self.abs_max_v: 7674.0\n",
      "fc layer 1 self.abs_max_out: 7081.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.862489/  2.002290, val:  55.42%, val_best:  55.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2383%\n",
      "layer   2  Sparsity: 71.2047%\n",
      "layer   3  Sparsity: 71.0685%\n",
      "total_backward_count 88110 real_backward_count 15028  17.056%\n",
      "fc layer 1 self.abs_max_out: 7373.0\n",
      "lif layer 1 self.abs_max_v: 10766.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.860132/  2.008179, val:  41.67%, val_best:  55.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2190%\n",
      "layer   2  Sparsity: 70.8279%\n",
      "layer   3  Sparsity: 71.4619%\n",
      "total_backward_count 97900 real_backward_count 16307  16.657%\n",
      "fc layer 1 self.abs_max_out: 7450.0\n",
      "lif layer 1 self.abs_max_v: 10823.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.836994/  1.997986, val:  51.67%, val_best:  55.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2223%\n",
      "layer   2  Sparsity: 70.7915%\n",
      "layer   3  Sparsity: 71.4549%\n",
      "total_backward_count 107690 real_backward_count 17569  16.314%\n",
      "lif layer 1 self.abs_max_v: 11124.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.847182/  1.956379, val:  59.17%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2306%\n",
      "layer   2  Sparsity: 70.3582%\n",
      "layer   3  Sparsity: 71.5719%\n",
      "total_backward_count 117480 real_backward_count 18853  16.048%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.831678/  1.961573, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2460%\n",
      "layer   2  Sparsity: 70.6869%\n",
      "layer   3  Sparsity: 71.8226%\n",
      "total_backward_count 127270 real_backward_count 20028  15.737%\n",
      "fc layer 1 self.abs_max_out: 7616.0\n",
      "lif layer 1 self.abs_max_v: 11715.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.830794/  1.974858, val:  54.17%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2465%\n",
      "layer   2  Sparsity: 70.6749%\n",
      "layer   3  Sparsity: 71.9558%\n",
      "total_backward_count 137060 real_backward_count 21182  15.455%\n",
      "fc layer 1 self.abs_max_out: 7623.0\n",
      "lif layer 1 self.abs_max_v: 12103.5\n",
      "lif layer 1 self.abs_max_v: 12167.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.830209/  1.945240, val:  65.00%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2494%\n",
      "layer   2  Sparsity: 70.2025%\n",
      "layer   3  Sparsity: 72.0918%\n",
      "total_backward_count 146850 real_backward_count 22324  15.202%\n",
      "lif layer 2 self.abs_max_v: 7920.5\n",
      "fc layer 1 self.abs_max_out: 7916.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.824348/  1.945305, val:  57.50%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2177%\n",
      "layer   2  Sparsity: 70.1584%\n",
      "layer   3  Sparsity: 72.4133%\n",
      "total_backward_count 156640 real_backward_count 23474  14.986%\n",
      "fc layer 1 self.abs_max_out: 7984.0\n",
      "lif layer 1 self.abs_max_v: 12200.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.823363/  1.954064, val:  62.08%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2316%\n",
      "layer   2  Sparsity: 70.4341%\n",
      "layer   3  Sparsity: 73.1841%\n",
      "total_backward_count 166430 real_backward_count 24554  14.753%\n",
      "lif layer 1 self.abs_max_v: 12300.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.823254/  1.942256, val:  72.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2345%\n",
      "layer   2  Sparsity: 69.9990%\n",
      "layer   3  Sparsity: 72.6811%\n",
      "total_backward_count 176220 real_backward_count 25635  14.547%\n",
      "fc layer 2 self.abs_max_out: 4751.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.812624/  1.928559, val:  57.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2272%\n",
      "layer   2  Sparsity: 70.3392%\n",
      "layer   3  Sparsity: 71.9582%\n",
      "total_backward_count 186010 real_backward_count 26703  14.356%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.806738/  1.950988, val:  42.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2292%\n",
      "layer   2  Sparsity: 69.8707%\n",
      "layer   3  Sparsity: 71.4663%\n",
      "total_backward_count 195800 real_backward_count 27706  14.150%\n",
      "lif layer 1 self.abs_max_v: 12549.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.805644/  1.949445, val:  55.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2225%\n",
      "layer   2  Sparsity: 70.0933%\n",
      "layer   3  Sparsity: 71.6186%\n",
      "total_backward_count 205590 real_backward_count 28681  13.951%\n",
      "fc layer 1 self.abs_max_out: 8091.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.799921/  1.928802, val:  57.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2124%\n",
      "layer   2  Sparsity: 69.8457%\n",
      "layer   3  Sparsity: 72.0826%\n",
      "total_backward_count 215380 real_backward_count 29700  13.790%\n",
      "fc layer 1 self.abs_max_out: 8256.0\n",
      "lif layer 1 self.abs_max_v: 12704.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.797146/  1.916912, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2182%\n",
      "layer   2  Sparsity: 69.8921%\n",
      "layer   3  Sparsity: 72.0148%\n",
      "total_backward_count 225170 real_backward_count 30696  13.632%\n",
      "fc layer 1 self.abs_max_out: 8373.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.800020/  1.916488, val:  73.75%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2402%\n",
      "layer   2  Sparsity: 69.8084%\n",
      "layer   3  Sparsity: 72.3739%\n",
      "total_backward_count 234960 real_backward_count 31711  13.496%\n",
      "fc layer 1 self.abs_max_out: 8480.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.798919/  1.925421, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2143%\n",
      "layer   2  Sparsity: 69.3970%\n",
      "layer   3  Sparsity: 72.7594%\n",
      "total_backward_count 244750 real_backward_count 32669  13.348%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.794940/  1.890617, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2231%\n",
      "layer   2  Sparsity: 69.4321%\n",
      "layer   3  Sparsity: 72.0464%\n",
      "total_backward_count 254540 real_backward_count 33642  13.217%\n",
      "fc layer 1 self.abs_max_out: 8491.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.783093/  1.902926, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2207%\n",
      "layer   2  Sparsity: 69.3421%\n",
      "layer   3  Sparsity: 72.5650%\n",
      "total_backward_count 264330 real_backward_count 34556  13.073%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.793420/  1.908549, val:  75.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2390%\n",
      "layer   2  Sparsity: 69.3469%\n",
      "layer   3  Sparsity: 72.4434%\n",
      "total_backward_count 274120 real_backward_count 35501  12.951%\n",
      "fc layer 3 self.abs_max_out: 1500.0\n",
      "lif layer 1 self.abs_max_v: 13305.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.774155/  1.907727, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2442%\n",
      "layer   2  Sparsity: 69.5353%\n",
      "layer   3  Sparsity: 72.1091%\n",
      "total_backward_count 283910 real_backward_count 36391  12.818%\n",
      "lif layer 1 self.abs_max_v: 13315.5\n",
      "lif layer 1 self.abs_max_v: 13732.0\n",
      "lif layer 1 self.abs_max_v: 14930.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.786867/  1.895790, val:  75.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2427%\n",
      "layer   2  Sparsity: 69.4312%\n",
      "layer   3  Sparsity: 72.2742%\n",
      "total_backward_count 293700 real_backward_count 37276  12.692%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.775704/  1.896822, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2407%\n",
      "layer   2  Sparsity: 68.9976%\n",
      "layer   3  Sparsity: 72.5483%\n",
      "total_backward_count 303490 real_backward_count 38168  12.576%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.765831/  1.882359, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2438%\n",
      "layer   2  Sparsity: 68.8972%\n",
      "layer   3  Sparsity: 72.4158%\n",
      "total_backward_count 313280 real_backward_count 39022  12.456%\n",
      "lif layer 1 self.abs_max_v: 14945.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.768987/  1.894332, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2200%\n",
      "layer   2  Sparsity: 68.9178%\n",
      "layer   3  Sparsity: 72.3036%\n",
      "total_backward_count 323070 real_backward_count 39909  12.353%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.761179/  1.881645, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2356%\n",
      "layer   2  Sparsity: 69.1849%\n",
      "layer   3  Sparsity: 72.0119%\n",
      "total_backward_count 332860 real_backward_count 40768  12.248%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.741892/  1.880982, val:  67.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2452%\n",
      "layer   2  Sparsity: 69.3105%\n",
      "layer   3  Sparsity: 71.9089%\n",
      "total_backward_count 342650 real_backward_count 41602  12.141%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.758994/  1.871679, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2479%\n",
      "layer   2  Sparsity: 69.0695%\n",
      "layer   3  Sparsity: 71.8804%\n",
      "total_backward_count 352440 real_backward_count 42375  12.023%\n",
      "fc layer 1 self.abs_max_out: 8517.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.738524/  1.854491, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2313%\n",
      "layer   2  Sparsity: 68.9471%\n",
      "layer   3  Sparsity: 70.6290%\n",
      "total_backward_count 362230 real_backward_count 43185  11.922%\n",
      "fc layer 1 self.abs_max_out: 8612.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.732895/  1.891924, val:  72.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2326%\n",
      "layer   2  Sparsity: 69.1444%\n",
      "layer   3  Sparsity: 71.8349%\n",
      "total_backward_count 372020 real_backward_count 43959  11.816%\n",
      "fc layer 1 self.abs_max_out: 8660.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.744656/  1.865248, val:  68.75%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2435%\n",
      "layer   2  Sparsity: 68.7061%\n",
      "layer   3  Sparsity: 70.9470%\n",
      "total_backward_count 381810 real_backward_count 44795  11.732%\n",
      "lif layer 1 self.abs_max_v: 14970.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.733187/  1.865093, val:  78.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2491%\n",
      "layer   2  Sparsity: 68.8773%\n",
      "layer   3  Sparsity: 70.6678%\n",
      "total_backward_count 391600 real_backward_count 45575  11.638%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.730042/  1.852192, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2166%\n",
      "layer   2  Sparsity: 68.7087%\n",
      "layer   3  Sparsity: 71.0329%\n",
      "total_backward_count 401390 real_backward_count 46354  11.548%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.737852/  1.850480, val:  72.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2375%\n",
      "layer   2  Sparsity: 68.8444%\n",
      "layer   3  Sparsity: 71.6041%\n",
      "total_backward_count 411180 real_backward_count 47193  11.477%\n",
      "lif layer 1 self.abs_max_v: 15558.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.730539/  1.854295, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2357%\n",
      "layer   2  Sparsity: 68.8972%\n",
      "layer   3  Sparsity: 71.1643%\n",
      "total_backward_count 420970 real_backward_count 47983  11.398%\n",
      "lif layer 1 self.abs_max_v: 15695.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.730099/  1.858764, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 94.2397%\n",
      "layer   2  Sparsity: 68.8322%\n",
      "layer   3  Sparsity: 71.4777%\n",
      "total_backward_count 430760 real_backward_count 48774  11.323%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.724185/  1.837314, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2491%\n",
      "layer   2  Sparsity: 69.0134%\n",
      "layer   3  Sparsity: 71.1824%\n",
      "total_backward_count 440550 real_backward_count 49580  11.254%\n",
      "lif layer 2 self.abs_max_v: 7962.5\n",
      "lif layer 2 self.abs_max_v: 8220.5\n",
      "lif layer 2 self.abs_max_v: 8561.5\n",
      "lif layer 1 self.abs_max_v: 15751.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.721082/  1.842900, val:  81.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2373%\n",
      "layer   2  Sparsity: 68.7559%\n",
      "layer   3  Sparsity: 71.5429%\n",
      "total_backward_count 450340 real_backward_count 50351  11.181%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.720003/  1.846896, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2162%\n",
      "layer   2  Sparsity: 68.8905%\n",
      "layer   3  Sparsity: 71.1227%\n",
      "total_backward_count 460130 real_backward_count 51119  11.110%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.715405/  1.863688, val:  74.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2375%\n",
      "layer   2  Sparsity: 68.6781%\n",
      "layer   3  Sparsity: 71.3702%\n",
      "total_backward_count 469920 real_backward_count 51865  11.037%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.727030/  1.851528, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2170%\n",
      "layer   2  Sparsity: 68.6226%\n",
      "layer   3  Sparsity: 72.3096%\n",
      "total_backward_count 479710 real_backward_count 52640  10.973%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.720067/  1.863321, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2270%\n",
      "layer   2  Sparsity: 68.8322%\n",
      "layer   3  Sparsity: 71.6276%\n",
      "total_backward_count 489500 real_backward_count 53393  10.908%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.726334/  1.844816, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2299%\n",
      "layer   2  Sparsity: 68.7476%\n",
      "layer   3  Sparsity: 71.8138%\n",
      "total_backward_count 499290 real_backward_count 54133  10.842%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.719576/  1.847795, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2432%\n",
      "layer   2  Sparsity: 68.7344%\n",
      "layer   3  Sparsity: 72.8600%\n",
      "total_backward_count 509080 real_backward_count 54817  10.768%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.717882/  1.835165, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2252%\n",
      "layer   2  Sparsity: 68.8287%\n",
      "layer   3  Sparsity: 71.9596%\n",
      "total_backward_count 518870 real_backward_count 55549  10.706%\n",
      "fc layer 2 self.abs_max_out: 4765.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.714664/  1.836042, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2293%\n",
      "layer   2  Sparsity: 68.8153%\n",
      "layer   3  Sparsity: 72.4368%\n",
      "total_backward_count 528660 real_backward_count 56264  10.643%\n",
      "fc layer 2 self.abs_max_out: 4769.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.724440/  1.838016, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2246%\n",
      "layer   2  Sparsity: 68.7225%\n",
      "layer   3  Sparsity: 73.1002%\n",
      "total_backward_count 538450 real_backward_count 56978  10.582%\n",
      "fc layer 2 self.abs_max_out: 4784.0\n",
      "fc layer 2 self.abs_max_out: 4799.0\n",
      "fc layer 2 self.abs_max_out: 4843.0\n",
      "lif layer 1 self.abs_max_v: 15881.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.725507/  1.838524, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2251%\n",
      "layer   2  Sparsity: 68.6482%\n",
      "layer   3  Sparsity: 72.9300%\n",
      "total_backward_count 548240 real_backward_count 57755  10.535%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.723440/  1.854548, val:  79.58%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2275%\n",
      "layer   2  Sparsity: 68.7473%\n",
      "layer   3  Sparsity: 72.6644%\n",
      "total_backward_count 558030 real_backward_count 58438  10.472%\n",
      "fc layer 3 self.abs_max_out: 1523.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.722760/  1.845025, val:  73.33%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2452%\n",
      "layer   2  Sparsity: 68.7588%\n",
      "layer   3  Sparsity: 72.8528%\n",
      "total_backward_count 567820 real_backward_count 59135  10.414%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.712620/  1.824279, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2244%\n",
      "layer   2  Sparsity: 68.6197%\n",
      "layer   3  Sparsity: 72.9602%\n",
      "total_backward_count 577610 real_backward_count 59796  10.352%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.724222/  1.845182, val:  76.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2407%\n",
      "layer   2  Sparsity: 69.0116%\n",
      "layer   3  Sparsity: 72.9748%\n",
      "total_backward_count 587400 real_backward_count 60482  10.297%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.705122/  1.818303, val:  77.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2197%\n",
      "layer   2  Sparsity: 69.0648%\n",
      "layer   3  Sparsity: 72.6089%\n",
      "total_backward_count 597190 real_backward_count 61204  10.249%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.707742/  1.826995, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2273%\n",
      "layer   2  Sparsity: 68.8450%\n",
      "layer   3  Sparsity: 72.2651%\n",
      "total_backward_count 606980 real_backward_count 61930  10.203%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.715879/  1.846476, val:  77.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2289%\n",
      "layer   2  Sparsity: 68.8684%\n",
      "layer   3  Sparsity: 72.4198%\n",
      "total_backward_count 616770 real_backward_count 62595  10.149%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.710442/  1.837712, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2206%\n",
      "layer   2  Sparsity: 68.8932%\n",
      "layer   3  Sparsity: 72.0565%\n",
      "total_backward_count 626560 real_backward_count 63260  10.096%\n",
      "fc layer 1 self.abs_max_out: 8678.0\n",
      "lif layer 1 self.abs_max_v: 15984.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.705408/  1.824026, val:  81.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2484%\n",
      "layer   2  Sparsity: 68.8017%\n",
      "layer   3  Sparsity: 72.0902%\n",
      "total_backward_count 636350 real_backward_count 63922  10.045%\n",
      "fc layer 1 self.abs_max_out: 8731.0\n",
      "lif layer 1 self.abs_max_v: 16068.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.705441/  1.831710, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2300%\n",
      "layer   2  Sparsity: 68.6582%\n",
      "layer   3  Sparsity: 72.5959%\n",
      "total_backward_count 646140 real_backward_count 64585   9.996%\n",
      "fc layer 2 self.abs_max_out: 4844.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.694177/  1.816750, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2319%\n",
      "layer   2  Sparsity: 68.5779%\n",
      "layer   3  Sparsity: 72.2428%\n",
      "total_backward_count 655930 real_backward_count 65231   9.945%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.695983/  1.815719, val:  83.75%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2229%\n",
      "layer   2  Sparsity: 68.8307%\n",
      "layer   3  Sparsity: 72.7253%\n",
      "total_backward_count 665720 real_backward_count 65902   9.899%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.704922/  1.803112, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2156%\n",
      "layer   2  Sparsity: 69.0188%\n",
      "layer   3  Sparsity: 72.0575%\n",
      "total_backward_count 675510 real_backward_count 66541   9.850%\n",
      "fc layer 1 self.abs_max_out: 8785.0\n",
      "lif layer 1 self.abs_max_v: 16162.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.686592/  1.825605, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2411%\n",
      "layer   2  Sparsity: 68.8682%\n",
      "layer   3  Sparsity: 72.5117%\n",
      "total_backward_count 685300 real_backward_count 67179   9.803%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.697437/  1.824848, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2523%\n",
      "layer   2  Sparsity: 68.8883%\n",
      "layer   3  Sparsity: 73.1820%\n",
      "total_backward_count 695090 real_backward_count 67836   9.759%\n",
      "fc layer 1 self.abs_max_out: 8864.0\n",
      "lif layer 1 self.abs_max_v: 16274.5\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.699673/  1.837385, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2287%\n",
      "layer   2  Sparsity: 68.7305%\n",
      "layer   3  Sparsity: 73.1552%\n",
      "total_backward_count 704880 real_backward_count 68503   9.718%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.698566/  1.815743, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2369%\n",
      "layer   2  Sparsity: 68.9749%\n",
      "layer   3  Sparsity: 73.1891%\n",
      "total_backward_count 714670 real_backward_count 69130   9.673%\n",
      "fc layer 2 self.abs_max_out: 5034.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.689008/  1.819577, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 68.8069%\n",
      "layer   3  Sparsity: 73.2674%\n",
      "total_backward_count 724460 real_backward_count 69752   9.628%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.695632/  1.816027, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2316%\n",
      "layer   2  Sparsity: 68.8836%\n",
      "layer   3  Sparsity: 73.3653%\n",
      "total_backward_count 734250 real_backward_count 70394   9.587%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.704075/  1.812231, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2131%\n",
      "layer   2  Sparsity: 68.7788%\n",
      "layer   3  Sparsity: 73.0012%\n",
      "total_backward_count 744040 real_backward_count 70997   9.542%\n",
      "lif layer 1 self.abs_max_v: 16301.5\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.700861/  1.808565, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2438%\n",
      "layer   2  Sparsity: 68.4424%\n",
      "layer   3  Sparsity: 73.0266%\n",
      "total_backward_count 753830 real_backward_count 71616   9.500%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.687681/  1.825471, val:  71.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2160%\n",
      "layer   2  Sparsity: 68.6179%\n",
      "layer   3  Sparsity: 73.6554%\n",
      "total_backward_count 763620 real_backward_count 72240   9.460%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.708598/  1.827723, val:  73.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2165%\n",
      "layer   2  Sparsity: 68.6151%\n",
      "layer   3  Sparsity: 73.9961%\n",
      "total_backward_count 773410 real_backward_count 72870   9.422%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.691526/  1.803879, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 68.8674%\n",
      "layer   3  Sparsity: 73.9260%\n",
      "total_backward_count 783200 real_backward_count 73446   9.378%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.701630/  1.816701, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2357%\n",
      "layer   2  Sparsity: 68.6140%\n",
      "layer   3  Sparsity: 74.3274%\n",
      "total_backward_count 792990 real_backward_count 74067   9.340%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.695530/  1.824517, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2204%\n",
      "layer   2  Sparsity: 68.7059%\n",
      "layer   3  Sparsity: 74.2148%\n",
      "total_backward_count 802780 real_backward_count 74645   9.298%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.692713/  1.811527, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2419%\n",
      "layer   2  Sparsity: 69.0076%\n",
      "layer   3  Sparsity: 73.0511%\n",
      "total_backward_count 812570 real_backward_count 75240   9.260%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.684409/  1.807749, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2234%\n",
      "layer   2  Sparsity: 68.6621%\n",
      "layer   3  Sparsity: 73.0269%\n",
      "total_backward_count 822360 real_backward_count 75854   9.224%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.688852/  1.803743, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2412%\n",
      "layer   2  Sparsity: 68.7023%\n",
      "layer   3  Sparsity: 73.5788%\n",
      "total_backward_count 832150 real_backward_count 76424   9.184%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.691884/  1.814995, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2490%\n",
      "layer   2  Sparsity: 68.6806%\n",
      "layer   3  Sparsity: 73.3450%\n",
      "total_backward_count 841940 real_backward_count 77020   9.148%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.688343/  1.820764, val:  75.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2378%\n",
      "layer   2  Sparsity: 68.2965%\n",
      "layer   3  Sparsity: 73.4591%\n",
      "total_backward_count 851730 real_backward_count 77605   9.111%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.691315/  1.823211, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2334%\n",
      "layer   2  Sparsity: 68.1020%\n",
      "layer   3  Sparsity: 74.1113%\n",
      "total_backward_count 861520 real_backward_count 78194   9.076%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.692762/  1.807343, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2380%\n",
      "layer   2  Sparsity: 68.4906%\n",
      "layer   3  Sparsity: 74.1379%\n",
      "total_backward_count 871310 real_backward_count 78766   9.040%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.689591/  1.802459, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2143%\n",
      "layer   2  Sparsity: 68.5110%\n",
      "layer   3  Sparsity: 74.2510%\n",
      "total_backward_count 881100 real_backward_count 79312   9.001%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.686351/  1.815170, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2123%\n",
      "layer   2  Sparsity: 68.3098%\n",
      "layer   3  Sparsity: 74.0728%\n",
      "total_backward_count 890890 real_backward_count 79879   8.966%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.696723/  1.806087, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2335%\n",
      "layer   2  Sparsity: 68.6525%\n",
      "layer   3  Sparsity: 74.3884%\n",
      "total_backward_count 900680 real_backward_count 80426   8.929%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.691826/  1.831817, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2167%\n",
      "layer   2  Sparsity: 68.7357%\n",
      "layer   3  Sparsity: 73.7486%\n",
      "total_backward_count 910470 real_backward_count 80991   8.896%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.700790/  1.835567, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2489%\n",
      "layer   2  Sparsity: 68.6859%\n",
      "layer   3  Sparsity: 73.1080%\n",
      "total_backward_count 920260 real_backward_count 81543   8.861%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.702100/  1.823889, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2312%\n",
      "layer   2  Sparsity: 68.4794%\n",
      "layer   3  Sparsity: 74.0739%\n",
      "total_backward_count 930050 real_backward_count 82142   8.832%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.693870/  1.822761, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2224%\n",
      "layer   2  Sparsity: 68.5805%\n",
      "layer   3  Sparsity: 74.2135%\n",
      "total_backward_count 939840 real_backward_count 82674   8.797%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.694295/  1.823220, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2362%\n",
      "layer   2  Sparsity: 68.6842%\n",
      "layer   3  Sparsity: 74.3013%\n",
      "total_backward_count 949630 real_backward_count 83213   8.763%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.697060/  1.815128, val:  85.00%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2336%\n",
      "layer   2  Sparsity: 68.7896%\n",
      "layer   3  Sparsity: 73.9678%\n",
      "total_backward_count 959420 real_backward_count 83764   8.731%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.696497/  1.814975, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2253%\n",
      "layer   2  Sparsity: 68.7540%\n",
      "layer   3  Sparsity: 74.3276%\n",
      "total_backward_count 969210 real_backward_count 84307   8.699%\n",
      "fc layer 1 self.abs_max_out: 8927.0\n",
      "fc layer 2 self.abs_max_out: 5062.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.679592/  1.794482, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2326%\n",
      "layer   2  Sparsity: 68.6785%\n",
      "layer   3  Sparsity: 74.1914%\n",
      "total_backward_count 979000 real_backward_count 84861   8.668%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.668000/  1.800314, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2469%\n",
      "layer   2  Sparsity: 68.8078%\n",
      "layer   3  Sparsity: 74.1706%\n",
      "total_backward_count 988790 real_backward_count 85400   8.637%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.678041/  1.800036, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2259%\n",
      "layer   2  Sparsity: 68.8817%\n",
      "layer   3  Sparsity: 74.7241%\n",
      "total_backward_count 998580 real_backward_count 85915   8.604%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.690968/  1.812997, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2341%\n",
      "layer   2  Sparsity: 68.5334%\n",
      "layer   3  Sparsity: 74.8861%\n",
      "total_backward_count 1008370 real_backward_count 86439   8.572%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.688382/  1.806712, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2187%\n",
      "layer   2  Sparsity: 68.5973%\n",
      "layer   3  Sparsity: 75.0374%\n",
      "total_backward_count 1018160 real_backward_count 86959   8.541%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.700094/  1.813797, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2295%\n",
      "layer   2  Sparsity: 68.9386%\n",
      "layer   3  Sparsity: 74.7140%\n",
      "total_backward_count 1027950 real_backward_count 87496   8.512%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.693315/  1.820202, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2274%\n",
      "layer   2  Sparsity: 68.9342%\n",
      "layer   3  Sparsity: 74.8715%\n",
      "total_backward_count 1037740 real_backward_count 88015   8.481%\n",
      "fc layer 1 self.abs_max_out: 8945.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.690603/  1.816112, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2325%\n",
      "layer   2  Sparsity: 68.8691%\n",
      "layer   3  Sparsity: 74.6460%\n",
      "total_backward_count 1047530 real_backward_count 88537   8.452%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.690805/  1.807735, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2253%\n",
      "layer   2  Sparsity: 68.7454%\n",
      "layer   3  Sparsity: 74.8754%\n",
      "total_backward_count 1057320 real_backward_count 89057   8.423%\n",
      "fc layer 1 self.abs_max_out: 8947.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.674828/  1.813330, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2109%\n",
      "layer   2  Sparsity: 68.6540%\n",
      "layer   3  Sparsity: 74.5100%\n",
      "total_backward_count 1067110 real_backward_count 89508   8.388%\n",
      "fc layer 1 self.abs_max_out: 8989.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.689553/  1.798663, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2424%\n",
      "layer   2  Sparsity: 68.8166%\n",
      "layer   3  Sparsity: 74.1151%\n",
      "total_backward_count 1076900 real_backward_count 90054   8.362%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.681409/  1.798156, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2308%\n",
      "layer   2  Sparsity: 68.6457%\n",
      "layer   3  Sparsity: 74.2670%\n",
      "total_backward_count 1086690 real_backward_count 90598   8.337%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.671574/  1.789156, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2208%\n",
      "layer   2  Sparsity: 68.8324%\n",
      "layer   3  Sparsity: 74.9541%\n",
      "total_backward_count 1096480 real_backward_count 91103   8.309%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.672959/  1.794613, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2221%\n",
      "layer   2  Sparsity: 68.8415%\n",
      "layer   3  Sparsity: 75.4494%\n",
      "total_backward_count 1106270 real_backward_count 91592   8.279%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.671471/  1.810288, val:  84.17%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 68.8665%\n",
      "layer   3  Sparsity: 75.0795%\n",
      "total_backward_count 1116060 real_backward_count 92052   8.248%\n",
      "fc layer 1 self.abs_max_out: 9050.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.673581/  1.795148, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2447%\n",
      "layer   2  Sparsity: 68.5461%\n",
      "layer   3  Sparsity: 74.9674%\n",
      "total_backward_count 1125850 real_backward_count 92543   8.220%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.674732/  1.786736, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2437%\n",
      "layer   2  Sparsity: 68.7738%\n",
      "layer   3  Sparsity: 75.1703%\n",
      "total_backward_count 1135640 real_backward_count 93060   8.194%\n",
      "fc layer 2 self.abs_max_out: 5138.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.677153/  1.807123, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2476%\n",
      "layer   2  Sparsity: 68.9780%\n",
      "layer   3  Sparsity: 75.4633%\n",
      "total_backward_count 1145430 real_backward_count 93542   8.167%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.675926/  1.796937, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2270%\n",
      "layer   2  Sparsity: 68.8380%\n",
      "layer   3  Sparsity: 75.3556%\n",
      "total_backward_count 1155220 real_backward_count 94029   8.139%\n",
      "fc layer 2 self.abs_max_out: 5184.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.675098/  1.804472, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2333%\n",
      "layer   2  Sparsity: 68.8057%\n",
      "layer   3  Sparsity: 75.7516%\n",
      "total_backward_count 1165010 real_backward_count 94523   8.113%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.671463/  1.802830, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2396%\n",
      "layer   2  Sparsity: 68.7384%\n",
      "layer   3  Sparsity: 75.4425%\n",
      "total_backward_count 1174800 real_backward_count 95002   8.087%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.679562/  1.797299, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2318%\n",
      "layer   2  Sparsity: 68.7910%\n",
      "layer   3  Sparsity: 74.8858%\n",
      "total_backward_count 1184590 real_backward_count 95512   8.063%\n",
      "fc layer 2 self.abs_max_out: 5304.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.673724/  1.799907, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2263%\n",
      "layer   2  Sparsity: 68.6642%\n",
      "layer   3  Sparsity: 75.3763%\n",
      "total_backward_count 1194380 real_backward_count 96018   8.039%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.679698/  1.809728, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2400%\n",
      "layer   2  Sparsity: 68.7732%\n",
      "layer   3  Sparsity: 75.2226%\n",
      "total_backward_count 1204170 real_backward_count 96497   8.014%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.668958/  1.787896, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2516%\n",
      "layer   2  Sparsity: 68.9918%\n",
      "layer   3  Sparsity: 75.2917%\n",
      "total_backward_count 1213960 real_backward_count 96980   7.989%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.678005/  1.794970, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2292%\n",
      "layer   2  Sparsity: 68.8523%\n",
      "layer   3  Sparsity: 75.0092%\n",
      "total_backward_count 1223750 real_backward_count 97493   7.967%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.680921/  1.800019, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2238%\n",
      "layer   2  Sparsity: 68.7109%\n",
      "layer   3  Sparsity: 74.5813%\n",
      "total_backward_count 1233540 real_backward_count 97962   7.942%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.673698/  1.804661, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2290%\n",
      "layer   2  Sparsity: 68.7764%\n",
      "layer   3  Sparsity: 74.5616%\n",
      "total_backward_count 1243330 real_backward_count 98388   7.913%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.667861/  1.802393, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2234%\n",
      "layer   2  Sparsity: 68.9458%\n",
      "layer   3  Sparsity: 74.3196%\n",
      "total_backward_count 1253120 real_backward_count 98889   7.891%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.672286/  1.787432, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2296%\n",
      "layer   2  Sparsity: 68.8631%\n",
      "layer   3  Sparsity: 74.0373%\n",
      "total_backward_count 1262910 real_backward_count 99367   7.868%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.676434/  1.783049, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2410%\n",
      "layer   2  Sparsity: 68.5715%\n",
      "layer   3  Sparsity: 74.7720%\n",
      "total_backward_count 1272700 real_backward_count 99869   7.847%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.671738/  1.788176, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2378%\n",
      "layer   2  Sparsity: 68.7133%\n",
      "layer   3  Sparsity: 75.0918%\n",
      "total_backward_count 1282490 real_backward_count 100322   7.822%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.671163/  1.778154, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2440%\n",
      "layer   2  Sparsity: 68.4917%\n",
      "layer   3  Sparsity: 74.7203%\n",
      "total_backward_count 1292280 real_backward_count 100778   7.798%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.653959/  1.789300, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2463%\n",
      "layer   2  Sparsity: 68.8847%\n",
      "layer   3  Sparsity: 74.9550%\n",
      "total_backward_count 1302070 real_backward_count 101215   7.773%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.656397/  1.798832, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2287%\n",
      "layer   2  Sparsity: 68.8643%\n",
      "layer   3  Sparsity: 74.7989%\n",
      "total_backward_count 1311860 real_backward_count 101706   7.753%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.659253/  1.777843, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2439%\n",
      "layer   2  Sparsity: 68.8268%\n",
      "layer   3  Sparsity: 75.2265%\n",
      "total_backward_count 1321650 real_backward_count 102162   7.730%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.661456/  1.781189, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2205%\n",
      "layer   2  Sparsity: 68.7578%\n",
      "layer   3  Sparsity: 74.9831%\n",
      "total_backward_count 1331440 real_backward_count 102608   7.707%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.666740/  1.790536, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2382%\n",
      "layer   2  Sparsity: 68.8368%\n",
      "layer   3  Sparsity: 74.8880%\n",
      "total_backward_count 1341230 real_backward_count 103030   7.682%\n",
      "fc layer 1 self.abs_max_out: 9091.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.669788/  1.799975, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2127%\n",
      "layer   2  Sparsity: 68.8411%\n",
      "layer   3  Sparsity: 75.5812%\n",
      "total_backward_count 1351020 real_backward_count 103473   7.659%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.679960/  1.784364, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2182%\n",
      "layer   2  Sparsity: 68.6911%\n",
      "layer   3  Sparsity: 75.6136%\n",
      "total_backward_count 1360810 real_backward_count 103932   7.638%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.671810/  1.789498, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2426%\n",
      "layer   2  Sparsity: 68.8692%\n",
      "layer   3  Sparsity: 75.2428%\n",
      "total_backward_count 1370600 real_backward_count 104402   7.617%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.659996/  1.789561, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2462%\n",
      "layer   2  Sparsity: 68.4258%\n",
      "layer   3  Sparsity: 75.4683%\n",
      "total_backward_count 1380390 real_backward_count 104887   7.598%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.664607/  1.785889, val:  77.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2207%\n",
      "layer   2  Sparsity: 68.4674%\n",
      "layer   3  Sparsity: 75.0772%\n",
      "total_backward_count 1390180 real_backward_count 105335   7.577%\n",
      "fc layer 1 self.abs_max_out: 9119.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.653584/  1.773827, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2415%\n",
      "layer   2  Sparsity: 68.5986%\n",
      "layer   3  Sparsity: 75.5963%\n",
      "total_backward_count 1399970 real_backward_count 105794   7.557%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.643447/  1.787313, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2603%\n",
      "layer   2  Sparsity: 68.4923%\n",
      "layer   3  Sparsity: 75.0402%\n",
      "total_backward_count 1409760 real_backward_count 106227   7.535%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.656177/  1.777093, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2483%\n",
      "layer   2  Sparsity: 68.2942%\n",
      "layer   3  Sparsity: 75.3906%\n",
      "total_backward_count 1419550 real_backward_count 106675   7.515%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.656221/  1.779351, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2226%\n",
      "layer   2  Sparsity: 68.4985%\n",
      "layer   3  Sparsity: 75.4607%\n",
      "total_backward_count 1429340 real_backward_count 107128   7.495%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.666178/  1.804174, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2399%\n",
      "layer   2  Sparsity: 68.6953%\n",
      "layer   3  Sparsity: 75.0893%\n",
      "total_backward_count 1439130 real_backward_count 107594   7.476%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.662045/  1.782942, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2447%\n",
      "layer   2  Sparsity: 68.8329%\n",
      "layer   3  Sparsity: 75.1501%\n",
      "total_backward_count 1448920 real_backward_count 108014   7.455%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.649469/  1.769107, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2523%\n",
      "layer   2  Sparsity: 68.7711%\n",
      "layer   3  Sparsity: 75.3604%\n",
      "total_backward_count 1458710 real_backward_count 108419   7.433%\n",
      "fc layer 2 self.abs_max_out: 5354.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.655043/  1.772851, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2405%\n",
      "layer   2  Sparsity: 68.4145%\n",
      "layer   3  Sparsity: 74.9375%\n",
      "total_backward_count 1468500 real_backward_count 108874   7.414%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.659958/  1.768932, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2303%\n",
      "layer   2  Sparsity: 68.7263%\n",
      "layer   3  Sparsity: 75.5987%\n",
      "total_backward_count 1478290 real_backward_count 109283   7.393%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.666306/  1.783900, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2145%\n",
      "layer   2  Sparsity: 68.6764%\n",
      "layer   3  Sparsity: 76.2757%\n",
      "total_backward_count 1488080 real_backward_count 109678   7.370%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.654337/  1.775946, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2355%\n",
      "layer   2  Sparsity: 68.6711%\n",
      "layer   3  Sparsity: 75.2624%\n",
      "total_backward_count 1497870 real_backward_count 110125   7.352%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.656126/  1.761312, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1911%\n",
      "layer   2  Sparsity: 68.5098%\n",
      "layer   3  Sparsity: 76.2285%\n",
      "total_backward_count 1507660 real_backward_count 110577   7.334%\n",
      "fc layer 1 self.abs_max_out: 9143.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.653573/  1.780096, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2192%\n",
      "layer   2  Sparsity: 68.5515%\n",
      "layer   3  Sparsity: 77.5153%\n",
      "total_backward_count 1517450 real_backward_count 110982   7.314%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.651656/  1.773441, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2332%\n",
      "layer   2  Sparsity: 68.5755%\n",
      "layer   3  Sparsity: 76.7166%\n",
      "total_backward_count 1527240 real_backward_count 111407   7.295%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.658881/  1.776596, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2150%\n",
      "layer   2  Sparsity: 68.5262%\n",
      "layer   3  Sparsity: 76.0400%\n",
      "total_backward_count 1537030 real_backward_count 111840   7.276%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.656613/  1.771945, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2385%\n",
      "layer   2  Sparsity: 68.4007%\n",
      "layer   3  Sparsity: 76.0989%\n",
      "total_backward_count 1546820 real_backward_count 112247   7.257%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.657202/  1.787722, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2281%\n",
      "layer   2  Sparsity: 68.7143%\n",
      "layer   3  Sparsity: 76.3578%\n",
      "total_backward_count 1556610 real_backward_count 112710   7.241%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.650877/  1.779067, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2301%\n",
      "layer   2  Sparsity: 68.6072%\n",
      "layer   3  Sparsity: 75.6135%\n",
      "total_backward_count 1566400 real_backward_count 113134   7.223%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.656776/  1.777120, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2335%\n",
      "layer   2  Sparsity: 68.5860%\n",
      "layer   3  Sparsity: 75.9772%\n",
      "total_backward_count 1576190 real_backward_count 113553   7.204%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.653345/  1.769775, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2238%\n",
      "layer   2  Sparsity: 68.6276%\n",
      "layer   3  Sparsity: 74.9970%\n",
      "total_backward_count 1585980 real_backward_count 114021   7.189%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.656532/  1.791610, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2208%\n",
      "layer   2  Sparsity: 68.5829%\n",
      "layer   3  Sparsity: 75.7195%\n",
      "total_backward_count 1595770 real_backward_count 114463   7.173%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.654617/  1.787890, val:  80.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2141%\n",
      "layer   2  Sparsity: 68.4258%\n",
      "layer   3  Sparsity: 75.7747%\n",
      "total_backward_count 1605560 real_backward_count 114899   7.156%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.654698/  1.783764, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 94.2184%\n",
      "layer   2  Sparsity: 68.5158%\n",
      "layer   3  Sparsity: 76.2859%\n",
      "total_backward_count 1615350 real_backward_count 115302   7.138%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.659363/  1.793722, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2271%\n",
      "layer   2  Sparsity: 68.6780%\n",
      "layer   3  Sparsity: 75.8567%\n",
      "total_backward_count 1625140 real_backward_count 115705   7.120%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.655723/  1.777144, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2303%\n",
      "layer   2  Sparsity: 68.5639%\n",
      "layer   3  Sparsity: 75.9497%\n",
      "total_backward_count 1634930 real_backward_count 116119   7.102%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.648164/  1.767774, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2478%\n",
      "layer   2  Sparsity: 68.5356%\n",
      "layer   3  Sparsity: 75.8537%\n",
      "total_backward_count 1644720 real_backward_count 116514   7.084%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.645073/  1.780501, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2205%\n",
      "layer   2  Sparsity: 68.4441%\n",
      "layer   3  Sparsity: 75.7597%\n",
      "total_backward_count 1654510 real_backward_count 116927   7.067%\n",
      "fc layer 3 self.abs_max_out: 1562.0\n",
      "lif layer 1 self.abs_max_v: 16317.0\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.641780/  1.769999, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2437%\n",
      "layer   2  Sparsity: 68.6176%\n",
      "layer   3  Sparsity: 76.3863%\n",
      "total_backward_count 1664300 real_backward_count 117319   7.049%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.647041/  1.761241, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2186%\n",
      "layer   2  Sparsity: 68.8140%\n",
      "layer   3  Sparsity: 76.3509%\n",
      "total_backward_count 1674090 real_backward_count 117734   7.033%\n",
      "fc layer 1 self.abs_max_out: 9148.0\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.639845/  1.782116, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2178%\n",
      "layer   2  Sparsity: 68.7820%\n",
      "layer   3  Sparsity: 75.8507%\n",
      "total_backward_count 1683880 real_backward_count 118159   7.017%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.659493/  1.777924, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2432%\n",
      "layer   2  Sparsity: 68.6669%\n",
      "layer   3  Sparsity: 75.9662%\n",
      "total_backward_count 1693670 real_backward_count 118545   6.999%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.656908/  1.785517, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2368%\n",
      "layer   2  Sparsity: 68.6208%\n",
      "layer   3  Sparsity: 75.7945%\n",
      "total_backward_count 1703460 real_backward_count 118978   6.984%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.667461/  1.792176, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2424%\n",
      "layer   2  Sparsity: 68.5187%\n",
      "layer   3  Sparsity: 75.8403%\n",
      "total_backward_count 1713250 real_backward_count 119403   6.969%\n",
      "fc layer 1 self.abs_max_out: 9174.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.660349/  1.769342, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2027%\n",
      "layer   2  Sparsity: 68.4222%\n",
      "layer   3  Sparsity: 75.3925%\n",
      "total_backward_count 1723040 real_backward_count 119813   6.954%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.653191/  1.784798, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2395%\n",
      "layer   2  Sparsity: 68.4148%\n",
      "layer   3  Sparsity: 75.4438%\n",
      "total_backward_count 1732830 real_backward_count 120190   6.936%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.661290/  1.772820, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2490%\n",
      "layer   2  Sparsity: 68.5172%\n",
      "layer   3  Sparsity: 75.3487%\n",
      "total_backward_count 1742620 real_backward_count 120591   6.920%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.655754/  1.770286, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2315%\n",
      "layer   2  Sparsity: 68.6002%\n",
      "layer   3  Sparsity: 75.5702%\n",
      "total_backward_count 1752410 real_backward_count 120968   6.903%\n",
      "fc layer 2 self.abs_max_out: 5393.0\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.647043/  1.771416, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2272%\n",
      "layer   2  Sparsity: 68.4683%\n",
      "layer   3  Sparsity: 76.0228%\n",
      "total_backward_count 1762200 real_backward_count 121340   6.886%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.655970/  1.777048, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2414%\n",
      "layer   2  Sparsity: 68.5611%\n",
      "layer   3  Sparsity: 75.7986%\n",
      "total_backward_count 1771990 real_backward_count 121765   6.872%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.640978/  1.771767, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2198%\n",
      "layer   2  Sparsity: 68.7871%\n",
      "layer   3  Sparsity: 75.6984%\n",
      "total_backward_count 1781780 real_backward_count 122195   6.858%\n",
      "fc layer 1 self.abs_max_out: 9192.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.642534/  1.782027, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2313%\n",
      "layer   2  Sparsity: 68.7283%\n",
      "layer   3  Sparsity: 75.7400%\n",
      "total_backward_count 1791570 real_backward_count 122543   6.840%\n",
      "fc layer 1 self.abs_max_out: 9210.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.636494/  1.763504, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2179%\n",
      "layer   2  Sparsity: 68.7405%\n",
      "layer   3  Sparsity: 76.2950%\n",
      "total_backward_count 1801360 real_backward_count 122903   6.823%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.637820/  1.790374, val:  77.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2371%\n",
      "layer   2  Sparsity: 68.7206%\n",
      "layer   3  Sparsity: 76.0936%\n",
      "total_backward_count 1811150 real_backward_count 123266   6.806%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.645818/  1.772831, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2237%\n",
      "layer   2  Sparsity: 68.7500%\n",
      "layer   3  Sparsity: 75.7263%\n",
      "total_backward_count 1820940 real_backward_count 123664   6.791%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.648717/  1.764665, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2310%\n",
      "layer   2  Sparsity: 68.7145%\n",
      "layer   3  Sparsity: 75.4548%\n",
      "total_backward_count 1830730 real_backward_count 124067   6.777%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.645669/  1.777883, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2163%\n",
      "layer   2  Sparsity: 68.7553%\n",
      "layer   3  Sparsity: 75.9614%\n",
      "total_backward_count 1840520 real_backward_count 124437   6.761%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.639010/  1.770088, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2224%\n",
      "layer   2  Sparsity: 68.7633%\n",
      "layer   3  Sparsity: 75.1935%\n",
      "total_backward_count 1850310 real_backward_count 124830   6.746%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.642321/  1.754285, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2401%\n",
      "layer   2  Sparsity: 68.5514%\n",
      "layer   3  Sparsity: 75.3322%\n",
      "total_backward_count 1860100 real_backward_count 125216   6.732%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.643429/  1.773029, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2284%\n",
      "layer   2  Sparsity: 68.4697%\n",
      "layer   3  Sparsity: 75.6688%\n",
      "total_backward_count 1869890 real_backward_count 125647   6.719%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.653531/  1.775138, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2173%\n",
      "layer   2  Sparsity: 68.3905%\n",
      "layer   3  Sparsity: 75.7096%\n",
      "total_backward_count 1879680 real_backward_count 126020   6.704%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.640214/  1.764881, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2240%\n",
      "layer   2  Sparsity: 68.6157%\n",
      "layer   3  Sparsity: 75.8098%\n",
      "total_backward_count 1889470 real_backward_count 126389   6.689%\n",
      "fc layer 1 self.abs_max_out: 9299.0\n",
      "lif layer 1 self.abs_max_v: 16324.5\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.631668/  1.766005, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2333%\n",
      "layer   2  Sparsity: 68.6610%\n",
      "layer   3  Sparsity: 76.0052%\n",
      "total_backward_count 1899260 real_backward_count 126802   6.676%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.634231/  1.758601, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.2260%\n",
      "layer   2  Sparsity: 68.4971%\n",
      "layer   3  Sparsity: 75.7752%\n",
      "total_backward_count 1909050 real_backward_count 127191   6.663%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.639904/  1.771107, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2506%\n",
      "layer   2  Sparsity: 68.4964%\n",
      "layer   3  Sparsity: 75.4733%\n",
      "total_backward_count 1918840 real_backward_count 127607   6.650%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.646517/  1.781933, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.2357%\n",
      "layer   2  Sparsity: 68.7170%\n",
      "layer   3  Sparsity: 75.8620%\n",
      "total_backward_count 1928630 real_backward_count 128003   6.637%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.642734/  1.778629, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2141%\n",
      "layer   2  Sparsity: 68.8345%\n",
      "layer   3  Sparsity: 75.5038%\n",
      "total_backward_count 1938420 real_backward_count 128395   6.624%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.633404/  1.771283, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.2436%\n",
      "layer   2  Sparsity: 68.7423%\n",
      "layer   3  Sparsity: 75.7362%\n",
      "total_backward_count 1948210 real_backward_count 128779   6.610%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.638570/  1.776068, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.2374%\n",
      "layer   2  Sparsity: 68.6452%\n",
      "layer   3  Sparsity: 75.0845%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693887fc47db4a2c96b8444bb2a9c360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.63857</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.84167</td></tr><tr><td>val_loss</td><td>1.77607</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-sweep-72</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54n8knmg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54n8knmg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_134007-54n8knmg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9fzru9dg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_175616-9fzru9dg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9fzru9dg' target=\"_blank\">hopeful-sweep-79</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9fzru9dg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9fzru9dg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251115_175625_262', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 236.0\n",
      "lif layer 1 self.abs_max_v: 236.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 499.0\n",
      "lif layer 2 self.abs_max_v: 499.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 1 self.abs_max_out: 302.0\n",
      "lif layer 1 self.abs_max_v: 319.5\n",
      "lif layer 2 self.abs_max_v: 613.0\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "lif layer 1 self.abs_max_v: 365.0\n",
      "fc layer 2 self.abs_max_out: 595.0\n",
      "lif layer 2 self.abs_max_v: 833.0\n",
      "fc layer 1 self.abs_max_out: 366.0\n",
      "lif layer 1 self.abs_max_v: 415.0\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 927.5\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "fc layer 1 self.abs_max_out: 639.0\n",
      "lif layer 1 self.abs_max_v: 656.5\n",
      "fc layer 2 self.abs_max_out: 648.0\n",
      "fc layer 1 self.abs_max_out: 677.0\n",
      "lif layer 1 self.abs_max_v: 677.0\n",
      "lif layer 1 self.abs_max_v: 833.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 2 self.abs_max_out: 771.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "lif layer 1 self.abs_max_v: 874.0\n",
      "lif layer 2 self.abs_max_v: 1003.5\n",
      "lif layer 1 self.abs_max_v: 911.5\n",
      "fc layer 1 self.abs_max_out: 684.0\n",
      "lif layer 1 self.abs_max_v: 933.0\n",
      "lif layer 1 self.abs_max_v: 993.5\n",
      "lif layer 2 self.abs_max_v: 1085.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 1 self.abs_max_out: 762.0\n",
      "lif layer 2 self.abs_max_v: 1252.5\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "fc layer 2 self.abs_max_out: 784.0\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "lif layer 1 self.abs_max_v: 1046.0\n",
      "fc layer 1 self.abs_max_out: 1346.0\n",
      "lif layer 1 self.abs_max_v: 1346.0\n",
      "fc layer 1 self.abs_max_out: 1502.0\n",
      "lif layer 1 self.abs_max_v: 1502.0\n",
      "lif layer 1 self.abs_max_v: 1520.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "lif layer 1 self.abs_max_v: 1550.5\n",
      "fc layer 1 self.abs_max_out: 1623.0\n",
      "lif layer 1 self.abs_max_v: 1623.0\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "lif layer 1 self.abs_max_v: 1681.5\n",
      "lif layer 1 self.abs_max_v: 1694.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 877.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "fc layer 1 self.abs_max_out: 1648.0\n",
      "lif layer 1 self.abs_max_v: 1719.0\n",
      "lif layer 2 self.abs_max_v: 1317.0\n",
      "lif layer 1 self.abs_max_v: 1831.5\n",
      "lif layer 2 self.abs_max_v: 1348.5\n",
      "lif layer 2 self.abs_max_v: 1368.5\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "fc layer 2 self.abs_max_out: 1089.0\n",
      "lif layer 2 self.abs_max_v: 1461.0\n",
      "lif layer 2 self.abs_max_v: 1667.5\n",
      "lif layer 2 self.abs_max_v: 1707.5\n",
      "lif layer 2 self.abs_max_v: 1721.0\n",
      "lif layer 2 self.abs_max_v: 1743.5\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "lif layer 1 self.abs_max_v: 1869.5\n",
      "lif layer 1 self.abs_max_v: 2163.5\n",
      "fc layer 1 self.abs_max_out: 1687.0\n",
      "lif layer 1 self.abs_max_v: 2573.5\n",
      "fc layer 1 self.abs_max_out: 1720.0\n",
      "lif layer 1 self.abs_max_v: 2818.0\n",
      "lif layer 1 self.abs_max_v: 2914.0\n",
      "lif layer 1 self.abs_max_v: 2976.0\n",
      "fc layer 1 self.abs_max_out: 1765.0\n",
      "fc layer 2 self.abs_max_out: 1149.0\n",
      "lif layer 2 self.abs_max_v: 1784.5\n",
      "lif layer 2 self.abs_max_v: 1852.0\n",
      "lif layer 2 self.abs_max_v: 1894.0\n",
      "lif layer 2 self.abs_max_v: 2004.5\n",
      "fc layer 1 self.abs_max_out: 1862.0\n",
      "fc layer 1 self.abs_max_out: 1952.0\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "lif layer 2 self.abs_max_v: 2021.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 2042.5\n",
      "lif layer 2 self.abs_max_v: 2043.0\n",
      "lif layer 2 self.abs_max_v: 2048.0\n",
      "fc layer 2 self.abs_max_out: 1315.0\n",
      "fc layer 2 self.abs_max_out: 1323.0\n",
      "fc layer 2 self.abs_max_out: 1356.0\n",
      "lif layer 2 self.abs_max_v: 2080.5\n",
      "lif layer 2 self.abs_max_v: 2096.5\n",
      "lif layer 2 self.abs_max_v: 2108.5\n",
      "lif layer 2 self.abs_max_v: 2129.5\n",
      "lif layer 2 self.abs_max_v: 2152.0\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "fc layer 2 self.abs_max_out: 1388.0\n",
      "lif layer 2 self.abs_max_v: 2236.5\n",
      "lif layer 2 self.abs_max_v: 2268.5\n",
      "lif layer 2 self.abs_max_v: 2275.5\n",
      "lif layer 2 self.abs_max_v: 2383.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "fc layer 2 self.abs_max_out: 1389.0\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 1443.0\n",
      "lif layer 2 self.abs_max_v: 2415.5\n",
      "lif layer 2 self.abs_max_v: 2417.5\n",
      "lif layer 2 self.abs_max_v: 2452.0\n",
      "lif layer 2 self.abs_max_v: 2482.0\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "lif layer 2 self.abs_max_v: 2503.5\n",
      "fc layer 2 self.abs_max_out: 1544.0\n",
      "lif layer 2 self.abs_max_v: 2567.0\n",
      "lif layer 2 self.abs_max_v: 2657.5\n",
      "lif layer 2 self.abs_max_v: 2733.0\n",
      "lif layer 1 self.abs_max_v: 3062.5\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "fc layer 1 self.abs_max_out: 2016.0\n",
      "lif layer 1 self.abs_max_v: 3147.0\n",
      "lif layer 1 self.abs_max_v: 3187.5\n",
      "lif layer 1 self.abs_max_v: 3247.0\n",
      "lif layer 1 self.abs_max_v: 3554.5\n",
      "fc layer 2 self.abs_max_out: 1590.0\n",
      "fc layer 2 self.abs_max_out: 1633.0\n",
      "fc layer 2 self.abs_max_out: 1648.0\n",
      "fc layer 2 self.abs_max_out: 1652.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 1 self.abs_max_out: 2078.0\n",
      "fc layer 1 self.abs_max_out: 2422.0\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "fc layer 2 self.abs_max_out: 1795.0\n",
      "fc layer 1 self.abs_max_out: 2469.0\n",
      "lif layer 1 self.abs_max_v: 3557.0\n",
      "fc layer 1 self.abs_max_out: 2682.0\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 1 self.abs_max_v: 3692.5\n",
      "lif layer 1 self.abs_max_v: 4081.0\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 1811.0\n",
      "fc layer 2 self.abs_max_out: 1819.0\n",
      "lif layer 2 self.abs_max_v: 2834.0\n",
      "lif layer 2 self.abs_max_v: 2856.0\n",
      "lif layer 2 self.abs_max_v: 2909.0\n",
      "lif layer 2 self.abs_max_v: 2912.5\n",
      "fc layer 2 self.abs_max_out: 1876.0\n",
      "lif layer 1 self.abs_max_v: 4180.0\n",
      "lif layer 2 self.abs_max_v: 2982.0\n",
      "lif layer 2 self.abs_max_v: 3062.5\n",
      "lif layer 2 self.abs_max_v: 3178.5\n",
      "lif layer 2 self.abs_max_v: 3217.5\n",
      "lif layer 2 self.abs_max_v: 3281.0\n",
      "fc layer 2 self.abs_max_out: 1892.0\n",
      "lif layer 1 self.abs_max_v: 4265.0\n",
      "lif layer 1 self.abs_max_v: 4441.5\n",
      "lif layer 1 self.abs_max_v: 4515.0\n",
      "lif layer 1 self.abs_max_v: 4625.5\n",
      "lif layer 1 self.abs_max_v: 4689.0\n",
      "lif layer 1 self.abs_max_v: 4932.0\n",
      "fc layer 1 self.abs_max_out: 2758.0\n",
      "fc layer 1 self.abs_max_out: 2830.0\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "fc layer 1 self.abs_max_out: 3003.0\n",
      "lif layer 1 self.abs_max_v: 4982.5\n",
      "lif layer 1 self.abs_max_v: 5130.0\n",
      "lif layer 1 self.abs_max_v: 5221.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "lif layer 1 self.abs_max_v: 5414.5\n",
      "lif layer 1 self.abs_max_v: 5476.5\n",
      "lif layer 1 self.abs_max_v: 5511.5\n",
      "fc layer 1 self.abs_max_out: 3254.0\n",
      "fc layer 1 self.abs_max_out: 3393.0\n",
      "lif layer 1 self.abs_max_v: 5834.5\n",
      "fc layer 2 self.abs_max_out: 1984.0\n",
      "fc layer 2 self.abs_max_out: 2014.0\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "lif layer 1 self.abs_max_v: 5910.5\n",
      "lif layer 1 self.abs_max_v: 5932.5\n",
      "lif layer 1 self.abs_max_v: 6086.5\n",
      "fc layer 1 self.abs_max_out: 3471.0\n",
      "lif layer 1 self.abs_max_v: 6245.5\n",
      "fc layer 1 self.abs_max_out: 3876.0\n",
      "lif layer 1 self.abs_max_v: 6943.0\n",
      "lif layer 1 self.abs_max_v: 6986.0\n",
      "lif layer 1 self.abs_max_v: 7258.0\n",
      "lif layer 2 self.abs_max_v: 3431.5\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "fc layer 1 self.abs_max_out: 4379.0\n",
      "lif layer 1 self.abs_max_v: 7646.5\n",
      "fc layer 2 self.abs_max_out: 2074.0\n",
      "fc layer 2 self.abs_max_out: 2096.0\n",
      "fc layer 1 self.abs_max_out: 4415.0\n",
      "lif layer 1 self.abs_max_v: 7723.5\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "lif layer 2 self.abs_max_v: 3526.0\n",
      "lif layer 2 self.abs_max_v: 3545.0\n",
      "lif layer 2 self.abs_max_v: 3561.5\n",
      "lif layer 2 self.abs_max_v: 3741.0\n",
      "fc layer 3 self.abs_max_out: 808.0\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "lif layer 2 self.abs_max_v: 3827.0\n",
      "lif layer 2 self.abs_max_v: 4009.5\n",
      "lif layer 2 self.abs_max_v: 4117.0\n",
      "lif layer 2 self.abs_max_v: 4200.5\n",
      "fc layer 2 self.abs_max_out: 2298.0\n",
      "fc layer 2 self.abs_max_out: 2313.0\n",
      "lif layer 2 self.abs_max_v: 4234.5\n",
      "lif layer 2 self.abs_max_v: 4276.5\n",
      "lif layer 1 self.abs_max_v: 7905.5\n",
      "fc layer 1 self.abs_max_out: 4551.0\n",
      "lif layer 1 self.abs_max_v: 8182.5\n",
      "lif layer 1 self.abs_max_v: 8193.5\n",
      "lif layer 2 self.abs_max_v: 4279.5\n",
      "lif layer 2 self.abs_max_v: 4380.0\n",
      "fc layer 1 self.abs_max_out: 4675.0\n",
      "lif layer 1 self.abs_max_v: 8605.5\n",
      "lif layer 1 self.abs_max_v: 8849.0\n",
      "lif layer 1 self.abs_max_v: 9034.5\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "fc layer 3 self.abs_max_out: 860.0\n",
      "fc layer 2 self.abs_max_out: 2322.0\n",
      "fc layer 2 self.abs_max_out: 2451.0\n",
      "lif layer 2 self.abs_max_v: 4456.0\n",
      "fc layer 2 self.abs_max_out: 2479.0\n",
      "lif layer 2 self.abs_max_v: 4627.0\n",
      "lif layer 2 self.abs_max_v: 4653.0\n",
      "lif layer 2 self.abs_max_v: 4719.5\n",
      "lif layer 2 self.abs_max_v: 4743.0\n",
      "fc layer 2 self.abs_max_out: 2488.0\n",
      "lif layer 2 self.abs_max_v: 4765.5\n",
      "lif layer 2 self.abs_max_v: 4851.0\n",
      "lif layer 2 self.abs_max_v: 4906.5\n",
      "lif layer 2 self.abs_max_v: 4938.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.291620/  1.919077, val:  27.92%, val_best:  27.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3599%\n",
      "layer   2  Sparsity: 66.3393%\n",
      "layer   3  Sparsity: 54.3155%\n",
      "total_backward_count 9790 real_backward_count 1320  13.483%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "fc layer 1 self.abs_max_out: 5155.0\n",
      "fc layer 1 self.abs_max_out: 5161.0\n",
      "lif layer 1 self.abs_max_v: 9404.0\n",
      "fc layer 3 self.abs_max_out: 875.0\n",
      "fc layer 1 self.abs_max_out: 5305.0\n",
      "lif layer 1 self.abs_max_v: 9525.5\n",
      "lif layer 1 self.abs_max_v: 9651.0\n",
      "fc layer 1 self.abs_max_out: 5468.0\n",
      "lif layer 1 self.abs_max_v: 9711.0\n",
      "fc layer 3 self.abs_max_out: 894.0\n",
      "fc layer 3 self.abs_max_out: 912.0\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 3 self.abs_max_out: 968.0\n",
      "lif layer 1 self.abs_max_v: 9799.5\n",
      "fc layer 1 self.abs_max_out: 5509.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.126887/  1.665657, val:  46.67%, val_best:  46.67%, tr:  99.39%, tr_best:  99.49%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3392%\n",
      "layer   2  Sparsity: 73.1912%\n",
      "layer   3  Sparsity: 60.0576%\n",
      "total_backward_count 19580 real_backward_count 2543  12.988%\n",
      "fc layer 1 self.abs_max_out: 5616.0\n",
      "fc layer 2 self.abs_max_out: 2510.0\n",
      "fc layer 2 self.abs_max_out: 2518.0\n",
      "lif layer 1 self.abs_max_v: 9833.0\n",
      "fc layer 1 self.abs_max_out: 5648.0\n",
      "lif layer 1 self.abs_max_v: 10158.5\n",
      "lif layer 1 self.abs_max_v: 10299.5\n",
      "fc layer 2 self.abs_max_out: 2520.0\n",
      "fc layer 2 self.abs_max_out: 2605.0\n",
      "fc layer 2 self.abs_max_out: 2629.0\n",
      "fc layer 2 self.abs_max_out: 2732.0\n",
      "fc layer 2 self.abs_max_out: 2742.0\n",
      "fc layer 2 self.abs_max_out: 2800.0\n",
      "lif layer 1 self.abs_max_v: 10417.5\n",
      "fc layer 1 self.abs_max_out: 6055.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.175778/  1.683647, val:  43.75%, val_best:  46.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 76.7757%\n",
      "layer   3  Sparsity: 63.7240%\n",
      "total_backward_count 29370 real_backward_count 3769  12.833%\n",
      "lif layer 1 self.abs_max_v: 10615.5\n",
      "fc layer 1 self.abs_max_out: 6236.0\n",
      "fc layer 2 self.abs_max_out: 2856.0\n",
      "fc layer 1 self.abs_max_out: 6872.0\n",
      "lif layer 1 self.abs_max_v: 11620.0\n",
      "lif layer 1 self.abs_max_v: 12570.0\n",
      "lif layer 2 self.abs_max_v: 5090.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.130222/  1.757216, val:  36.67%, val_best:  46.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3457%\n",
      "layer   2  Sparsity: 76.3621%\n",
      "layer   3  Sparsity: 64.3244%\n",
      "total_backward_count 39160 real_backward_count 4989  12.740%\n",
      "fc layer 2 self.abs_max_out: 2888.0\n",
      "fc layer 2 self.abs_max_out: 2910.0\n",
      "fc layer 2 self.abs_max_out: 2931.0\n",
      "fc layer 2 self.abs_max_out: 3119.0\n",
      "fc layer 1 self.abs_max_out: 6957.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.075047/  1.616090, val:  47.08%, val_best:  47.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 76.2205%\n",
      "layer   3  Sparsity: 64.4626%\n",
      "total_backward_count 48950 real_backward_count 6195  12.656%\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "fc layer 3 self.abs_max_out: 988.0\n",
      "fc layer 3 self.abs_max_out: 1024.0\n",
      "lif layer 2 self.abs_max_v: 5155.5\n",
      "lif layer 2 self.abs_max_v: 5340.0\n",
      "lif layer 2 self.abs_max_v: 5355.0\n",
      "lif layer 2 self.abs_max_v: 5389.5\n",
      "fc layer 1 self.abs_max_out: 7095.0\n",
      "lif layer 1 self.abs_max_v: 13312.5\n",
      "fc layer 1 self.abs_max_out: 7424.0\n",
      "fc layer 1 self.abs_max_out: 7670.0\n",
      "lif layer 1 self.abs_max_v: 13423.5\n",
      "lif layer 1 self.abs_max_v: 13465.0\n",
      "fc layer 1 self.abs_max_out: 7827.0\n",
      "lif layer 1 self.abs_max_v: 14061.0\n",
      "lif layer 1 self.abs_max_v: 14310.5\n",
      "lif layer 1 self.abs_max_v: 14316.0\n",
      "lif layer 1 self.abs_max_v: 14889.0\n",
      "lif layer 1 self.abs_max_v: 14896.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.047829/  1.612619, val:  47.92%, val_best:  47.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 75.1218%\n",
      "layer   3  Sparsity: 64.2874%\n",
      "total_backward_count 58740 real_backward_count 7391  12.583%\n",
      "fc layer 3 self.abs_max_out: 1041.0\n",
      "fc layer 1 self.abs_max_out: 7863.0\n",
      "fc layer 1 self.abs_max_out: 7955.0\n",
      "fc layer 1 self.abs_max_out: 8022.0\n",
      "lif layer 2 self.abs_max_v: 5421.5\n",
      "lif layer 2 self.abs_max_v: 5459.5\n",
      "lif layer 2 self.abs_max_v: 5520.5\n",
      "lif layer 2 self.abs_max_v: 5707.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.070509/  1.609423, val:  46.67%, val_best:  47.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3125%\n",
      "layer   2  Sparsity: 75.0451%\n",
      "layer   3  Sparsity: 65.5447%\n",
      "total_backward_count 68530 real_backward_count 8577  12.516%\n",
      "fc layer 2 self.abs_max_out: 3152.0\n",
      "fc layer 2 self.abs_max_out: 3166.0\n",
      "lif layer 2 self.abs_max_v: 5816.0\n",
      "lif layer 2 self.abs_max_v: 5932.5\n",
      "lif layer 2 self.abs_max_v: 6041.5\n",
      "fc layer 2 self.abs_max_out: 3179.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.059422/  1.609591, val:  43.33%, val_best:  47.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3242%\n",
      "layer   2  Sparsity: 74.9098%\n",
      "layer   3  Sparsity: 66.0711%\n",
      "total_backward_count 78320 real_backward_count 9680  12.360%\n",
      "fc layer 2 self.abs_max_out: 3258.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.042488/  1.517932, val:  48.75%, val_best:  48.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 76.5404%\n",
      "layer   3  Sparsity: 66.4648%\n",
      "total_backward_count 88110 real_backward_count 10890  12.360%\n",
      "fc layer 1 self.abs_max_out: 8062.0\n",
      "fc layer 3 self.abs_max_out: 1087.0\n",
      "fc layer 3 self.abs_max_out: 1107.0\n",
      "fc layer 3 self.abs_max_out: 1109.0\n",
      "fc layer 2 self.abs_max_out: 3260.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.006917/  1.550799, val:  51.67%, val_best:  51.67%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 76.3462%\n",
      "layer   3  Sparsity: 67.0363%\n",
      "total_backward_count 97900 real_backward_count 12021  12.279%\n",
      "fc layer 2 self.abs_max_out: 3305.0\n",
      "fc layer 2 self.abs_max_out: 3320.0\n",
      "fc layer 1 self.abs_max_out: 8250.0\n",
      "fc layer 1 self.abs_max_out: 8581.0\n",
      "lif layer 1 self.abs_max_v: 15074.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.990564/  1.510155, val:  50.42%, val_best:  51.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3585%\n",
      "layer   2  Sparsity: 76.1809%\n",
      "layer   3  Sparsity: 67.6295%\n",
      "total_backward_count 107690 real_backward_count 13175  12.234%\n",
      "fc layer 1 self.abs_max_out: 8808.0\n",
      "lif layer 1 self.abs_max_v: 16028.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.974277/  1.556583, val:  38.33%, val_best:  51.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 75.8334%\n",
      "layer   3  Sparsity: 66.6123%\n",
      "total_backward_count 117480 real_backward_count 14253  12.132%\n",
      "fc layer 1 self.abs_max_out: 8889.0\n",
      "fc layer 1 self.abs_max_out: 8935.0\n",
      "fc layer 1 self.abs_max_out: 8959.0\n",
      "lif layer 1 self.abs_max_v: 16179.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.938842/  1.489769, val:  49.17%, val_best:  51.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3343%\n",
      "layer   2  Sparsity: 76.7571%\n",
      "layer   3  Sparsity: 67.6142%\n",
      "total_backward_count 127270 real_backward_count 15289  12.013%\n",
      "fc layer 2 self.abs_max_out: 3375.0\n",
      "lif layer 2 self.abs_max_v: 6132.0\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "lif layer 2 self.abs_max_v: 6238.0\n",
      "fc layer 1 self.abs_max_out: 9069.0\n",
      "lif layer 2 self.abs_max_v: 6356.5\n",
      "lif layer 2 self.abs_max_v: 6558.5\n",
      "fc layer 2 self.abs_max_out: 3431.0\n",
      "lif layer 2 self.abs_max_v: 6648.5\n",
      "fc layer 2 self.abs_max_out: 3584.0\n",
      "lif layer 2 self.abs_max_v: 6723.0\n",
      "lif layer 2 self.abs_max_v: 6753.0\n",
      "lif layer 2 self.abs_max_v: 6811.5\n",
      "lif layer 2 self.abs_max_v: 6913.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.942053/  1.464200, val:  48.75%, val_best:  51.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3433%\n",
      "layer   2  Sparsity: 75.8153%\n",
      "layer   3  Sparsity: 67.8133%\n",
      "total_backward_count 137060 real_backward_count 16312  11.901%\n",
      "fc layer 2 self.abs_max_out: 3591.0\n",
      "fc layer 2 self.abs_max_out: 3709.0\n",
      "fc layer 2 self.abs_max_out: 3879.0\n",
      "lif layer 2 self.abs_max_v: 6961.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.887255/  1.463621, val:  54.17%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3454%\n",
      "layer   2  Sparsity: 75.1614%\n",
      "layer   3  Sparsity: 67.6799%\n",
      "total_backward_count 146850 real_backward_count 17349  11.814%\n",
      "lif layer 2 self.abs_max_v: 7041.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.928540/  1.382184, val:  55.00%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3256%\n",
      "layer   2  Sparsity: 74.8097%\n",
      "layer   3  Sparsity: 67.8692%\n",
      "total_backward_count 156640 real_backward_count 18384  11.736%\n",
      "fc layer 3 self.abs_max_out: 1146.0\n",
      "fc layer 1 self.abs_max_out: 9083.0\n",
      "fc layer 1 self.abs_max_out: 9236.0\n",
      "fc layer 1 self.abs_max_out: 9873.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.870558/  1.371370, val:  54.58%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2795%\n",
      "layer   2  Sparsity: 75.2474%\n",
      "layer   3  Sparsity: 67.6856%\n",
      "total_backward_count 166430 real_backward_count 19402  11.658%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.869221/  1.324795, val:  61.25%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 75.3550%\n",
      "layer   3  Sparsity: 66.6659%\n",
      "total_backward_count 176220 real_backward_count 20405  11.579%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.853944/  1.317970, val:  59.17%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3354%\n",
      "layer   2  Sparsity: 75.2675%\n",
      "layer   3  Sparsity: 66.2677%\n",
      "total_backward_count 186010 real_backward_count 21446  11.529%\n",
      "lif layer 1 self.abs_max_v: 16412.5\n",
      "fc layer 3 self.abs_max_out: 1173.0\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "fc layer 3 self.abs_max_out: 1202.0\n",
      "fc layer 2 self.abs_max_out: 3939.0\n",
      "fc layer 2 self.abs_max_out: 3949.0\n",
      "lif layer 2 self.abs_max_v: 7273.5\n",
      "lif layer 1 self.abs_max_v: 16709.0\n",
      "lif layer 1 self.abs_max_v: 17499.5\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.820244/  1.563586, val:  42.08%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3349%\n",
      "layer   2  Sparsity: 74.1904%\n",
      "layer   3  Sparsity: 65.3104%\n",
      "total_backward_count 195800 real_backward_count 22411  11.446%\n",
      "fc layer 2 self.abs_max_out: 3999.0\n",
      "lif layer 2 self.abs_max_v: 7354.5\n",
      "lif layer 1 self.abs_max_v: 17768.0\n",
      "fc layer 1 self.abs_max_out: 10717.0\n",
      "lif layer 1 self.abs_max_v: 17828.5\n",
      "lif layer 1 self.abs_max_v: 18171.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.794940/  1.285347, val:  60.42%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3081%\n",
      "layer   2  Sparsity: 74.3234%\n",
      "layer   3  Sparsity: 66.4667%\n",
      "total_backward_count 205590 real_backward_count 23337  11.351%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.803424/  1.416537, val:  52.50%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3738%\n",
      "layer   2  Sparsity: 73.6768%\n",
      "layer   3  Sparsity: 67.3153%\n",
      "total_backward_count 215380 real_backward_count 24347  11.304%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.840486/  1.278236, val:  63.33%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3379%\n",
      "layer   2  Sparsity: 73.7126%\n",
      "layer   3  Sparsity: 67.9352%\n",
      "total_backward_count 225170 real_backward_count 25316  11.243%\n",
      "fc layer 3 self.abs_max_out: 1203.0\n",
      "fc layer 3 self.abs_max_out: 1204.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.743717/  1.294637, val:  60.42%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 74.5935%\n",
      "layer   3  Sparsity: 67.2682%\n",
      "total_backward_count 234960 real_backward_count 26233  11.165%\n",
      "fc layer 3 self.abs_max_out: 1208.0\n",
      "fc layer 3 self.abs_max_out: 1238.0\n",
      "lif layer 1 self.abs_max_v: 18656.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.746148/  1.221443, val:  60.83%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3679%\n",
      "layer   2  Sparsity: 73.8044%\n",
      "layer   3  Sparsity: 67.0923%\n",
      "total_backward_count 244750 real_backward_count 27158  11.096%\n",
      "fc layer 1 self.abs_max_out: 10874.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.759726/  1.300293, val:  57.08%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3476%\n",
      "layer   2  Sparsity: 73.5673%\n",
      "layer   3  Sparsity: 67.8009%\n",
      "total_backward_count 254540 real_backward_count 28101  11.040%\n",
      "fc layer 1 self.abs_max_out: 11006.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.757651/  1.247967, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3467%\n",
      "layer   2  Sparsity: 73.0353%\n",
      "layer   3  Sparsity: 67.9849%\n",
      "total_backward_count 264330 real_backward_count 29025  10.981%\n",
      "fc layer 3 self.abs_max_out: 1284.0\n",
      "fc layer 3 self.abs_max_out: 1324.0\n",
      "fc layer 3 self.abs_max_out: 1339.0\n",
      "fc layer 1 self.abs_max_out: 11020.0\n",
      "lif layer 1 self.abs_max_v: 18902.5\n",
      "lif layer 1 self.abs_max_v: 19142.5\n",
      "fc layer 1 self.abs_max_out: 11227.0\n",
      "lif layer 1 self.abs_max_v: 20376.5\n",
      "lif layer 1 self.abs_max_v: 20702.5\n",
      "lif layer 1 self.abs_max_v: 21069.0\n",
      "lif layer 1 self.abs_max_v: 21089.5\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.726539/  1.245082, val:  59.58%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3408%\n",
      "layer   2  Sparsity: 72.9470%\n",
      "layer   3  Sparsity: 66.6531%\n",
      "total_backward_count 274120 real_backward_count 29937  10.921%\n",
      "fc layer 3 self.abs_max_out: 1410.0\n",
      "fc layer 1 self.abs_max_out: 11496.0\n",
      "fc layer 1 self.abs_max_out: 11858.0\n",
      "lif layer 1 self.abs_max_v: 21587.0\n",
      "lif layer 1 self.abs_max_v: 21992.5\n",
      "lif layer 1 self.abs_max_v: 22421.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.677340/  1.230330, val:  60.83%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3173%\n",
      "layer   2  Sparsity: 73.1798%\n",
      "layer   3  Sparsity: 67.5486%\n",
      "total_backward_count 283910 real_backward_count 30762  10.835%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.663020/  1.291220, val:  55.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3423%\n",
      "layer   2  Sparsity: 71.9950%\n",
      "layer   3  Sparsity: 65.2817%\n",
      "total_backward_count 293700 real_backward_count 31621  10.766%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.638622/  1.127401, val:  70.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 72.1836%\n",
      "layer   3  Sparsity: 65.8240%\n",
      "total_backward_count 303490 real_backward_count 32464  10.697%\n",
      "fc layer 1 self.abs_max_out: 12098.0\n",
      "lif layer 1 self.abs_max_v: 22822.0\n",
      "lif layer 1 self.abs_max_v: 22981.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.608978/  1.220480, val:  58.33%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3777%\n",
      "layer   2  Sparsity: 72.2133%\n",
      "layer   3  Sparsity: 65.4160%\n",
      "total_backward_count 313280 real_backward_count 33315  10.634%\n",
      "fc layer 2 self.abs_max_out: 4013.0\n",
      "lif layer 2 self.abs_max_v: 7428.5\n",
      "lif layer 2 self.abs_max_v: 7571.5\n",
      "lif layer 2 self.abs_max_v: 7724.0\n",
      "fc layer 2 self.abs_max_out: 4132.0\n",
      "lif layer 1 self.abs_max_v: 23038.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.595131/  1.161838, val:  62.08%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3057%\n",
      "layer   2  Sparsity: 72.7110%\n",
      "layer   3  Sparsity: 66.7212%\n",
      "total_backward_count 323070 real_backward_count 34111  10.558%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.593606/  1.108973, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3417%\n",
      "layer   2  Sparsity: 72.8139%\n",
      "layer   3  Sparsity: 66.6213%\n",
      "total_backward_count 332860 real_backward_count 34864  10.474%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.595729/  1.151134, val:  69.17%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3143%\n",
      "layer   2  Sparsity: 72.9055%\n",
      "layer   3  Sparsity: 66.3479%\n",
      "total_backward_count 342650 real_backward_count 35663  10.408%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.565059/  1.107885, val:  65.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3625%\n",
      "layer   2  Sparsity: 72.9104%\n",
      "layer   3  Sparsity: 67.2026%\n",
      "total_backward_count 352440 real_backward_count 36364  10.318%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.556118/  1.060607, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3480%\n",
      "layer   2  Sparsity: 72.4397%\n",
      "layer   3  Sparsity: 66.9115%\n",
      "total_backward_count 362230 real_backward_count 37117  10.247%\n",
      "fc layer 1 self.abs_max_out: 12160.0\n",
      "lif layer 1 self.abs_max_v: 23162.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.550870/  1.246480, val:  57.50%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 71.8594%\n",
      "layer   3  Sparsity: 67.2279%\n",
      "total_backward_count 372020 real_backward_count 37848  10.174%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.557714/  1.100896, val:  69.17%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3158%\n",
      "layer   2  Sparsity: 71.6905%\n",
      "layer   3  Sparsity: 67.4350%\n",
      "total_backward_count 381810 real_backward_count 38619  10.115%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.534746/  1.225449, val:  66.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3330%\n",
      "layer   2  Sparsity: 71.9682%\n",
      "layer   3  Sparsity: 67.5615%\n",
      "total_backward_count 391600 real_backward_count 39375  10.055%\n",
      "fc layer 3 self.abs_max_out: 1447.0\n",
      "fc layer 3 self.abs_max_out: 1481.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.528393/  1.066979, val:  72.92%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3621%\n",
      "layer   2  Sparsity: 72.0272%\n",
      "layer   3  Sparsity: 67.6822%\n",
      "total_backward_count 401390 real_backward_count 40109   9.993%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.510829/  1.083148, val:  73.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3357%\n",
      "layer   2  Sparsity: 72.4463%\n",
      "layer   3  Sparsity: 67.7544%\n",
      "total_backward_count 411180 real_backward_count 40835   9.931%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.497985/  1.041255, val:  68.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 72.0313%\n",
      "layer   3  Sparsity: 67.6585%\n",
      "total_backward_count 420970 real_backward_count 41532   9.866%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.509187/  1.142346, val:  64.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3415%\n",
      "layer   2  Sparsity: 71.7477%\n",
      "layer   3  Sparsity: 67.5469%\n",
      "total_backward_count 430760 real_backward_count 42253   9.809%\n",
      "fc layer 3 self.abs_max_out: 1491.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.501372/  1.068958, val:  70.42%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3327%\n",
      "layer   2  Sparsity: 72.1123%\n",
      "layer   3  Sparsity: 66.1934%\n",
      "total_backward_count 440550 real_backward_count 43002   9.761%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.496773/  1.021610, val:  72.92%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 71.8719%\n",
      "layer   3  Sparsity: 66.7813%\n",
      "total_backward_count 450340 real_backward_count 43666   9.696%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.476767/  1.107976, val:  67.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3312%\n",
      "layer   2  Sparsity: 71.9202%\n",
      "layer   3  Sparsity: 67.0808%\n",
      "total_backward_count 460130 real_backward_count 44369   9.643%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.465969/  1.100625, val:  69.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 71.6624%\n",
      "layer   3  Sparsity: 66.0568%\n",
      "total_backward_count 469920 real_backward_count 45042   9.585%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.471574/  1.015435, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3594%\n",
      "layer   2  Sparsity: 71.5557%\n",
      "layer   3  Sparsity: 66.8569%\n",
      "total_backward_count 479710 real_backward_count 45695   9.526%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.463218/  0.994212, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3317%\n",
      "layer   2  Sparsity: 71.3284%\n",
      "layer   3  Sparsity: 67.6460%\n",
      "total_backward_count 489500 real_backward_count 46305   9.460%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.442400/  1.167735, val:  59.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 71.5144%\n",
      "layer   3  Sparsity: 67.6130%\n",
      "total_backward_count 499290 real_backward_count 46879   9.389%\n",
      "fc layer 3 self.abs_max_out: 1513.0\n",
      "fc layer 3 self.abs_max_out: 1533.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.439569/  1.032636, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3353%\n",
      "layer   2  Sparsity: 71.3449%\n",
      "layer   3  Sparsity: 67.2410%\n",
      "total_backward_count 509080 real_backward_count 47464   9.323%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.446022/  1.103728, val:  67.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 71.0909%\n",
      "layer   3  Sparsity: 67.1729%\n",
      "total_backward_count 518870 real_backward_count 48088   9.268%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.444355/  1.004402, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3163%\n",
      "layer   2  Sparsity: 71.7455%\n",
      "layer   3  Sparsity: 67.2354%\n",
      "total_backward_count 528660 real_backward_count 48667   9.206%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.446595/  0.994694, val:  70.83%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3340%\n",
      "layer   2  Sparsity: 71.9225%\n",
      "layer   3  Sparsity: 67.9033%\n",
      "total_backward_count 538450 real_backward_count 49235   9.144%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.434879/  0.975198, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3274%\n",
      "layer   2  Sparsity: 72.2469%\n",
      "layer   3  Sparsity: 68.2402%\n",
      "total_backward_count 548240 real_backward_count 49797   9.083%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.422162/  1.145823, val:  63.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3386%\n",
      "layer   2  Sparsity: 72.1248%\n",
      "layer   3  Sparsity: 68.4461%\n",
      "total_backward_count 558030 real_backward_count 50370   9.026%\n",
      "fc layer 3 self.abs_max_out: 1553.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.400057/  1.061464, val:  71.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3535%\n",
      "layer   2  Sparsity: 71.8558%\n",
      "layer   3  Sparsity: 67.2115%\n",
      "total_backward_count 567820 real_backward_count 50870   8.959%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.408038/  1.011351, val:  72.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3322%\n",
      "layer   2  Sparsity: 71.6451%\n",
      "layer   3  Sparsity: 65.6007%\n",
      "total_backward_count 577610 real_backward_count 51383   8.896%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.400201/  1.089147, val:  67.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 71.2264%\n",
      "layer   3  Sparsity: 67.5869%\n",
      "total_backward_count 587400 real_backward_count 51914   8.838%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.418934/  1.090939, val:  68.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3436%\n",
      "layer   2  Sparsity: 71.6276%\n",
      "layer   3  Sparsity: 67.9292%\n",
      "total_backward_count 597190 real_backward_count 52439   8.781%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.406868/  0.970019, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3487%\n",
      "layer   2  Sparsity: 71.7866%\n",
      "layer   3  Sparsity: 67.0430%\n",
      "total_backward_count 606980 real_backward_count 52974   8.727%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.392639/  1.068484, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2906%\n",
      "layer   2  Sparsity: 71.3066%\n",
      "layer   3  Sparsity: 67.8091%\n",
      "total_backward_count 616770 real_backward_count 53491   8.673%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.402663/  1.068020, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 71.0189%\n",
      "layer   3  Sparsity: 70.0168%\n",
      "total_backward_count 626560 real_backward_count 53977   8.615%\n",
      "fc layer 3 self.abs_max_out: 1585.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.384932/  1.028848, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3683%\n",
      "layer   2  Sparsity: 71.3002%\n",
      "layer   3  Sparsity: 69.2866%\n",
      "total_backward_count 636350 real_backward_count 54492   8.563%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.369330/  1.024791, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3233%\n",
      "layer   2  Sparsity: 71.6863%\n",
      "layer   3  Sparsity: 68.7086%\n",
      "total_backward_count 646140 real_backward_count 54972   8.508%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.379799/  0.954430, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3300%\n",
      "layer   2  Sparsity: 72.0991%\n",
      "layer   3  Sparsity: 68.9468%\n",
      "total_backward_count 655930 real_backward_count 55438   8.452%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.376178/  1.017622, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3041%\n",
      "layer   2  Sparsity: 72.3728%\n",
      "layer   3  Sparsity: 69.5048%\n",
      "total_backward_count 665720 real_backward_count 55957   8.405%\n",
      "fc layer 3 self.abs_max_out: 1587.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.374567/  0.946286, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3483%\n",
      "layer   2  Sparsity: 72.3108%\n",
      "layer   3  Sparsity: 68.7052%\n",
      "total_backward_count 675510 real_backward_count 56415   8.351%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.358870/  1.110261, val:  62.50%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3217%\n",
      "layer   2  Sparsity: 72.3958%\n",
      "layer   3  Sparsity: 70.2641%\n",
      "total_backward_count 685300 real_backward_count 56880   8.300%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.373428/  1.032164, val:  72.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 72.2091%\n",
      "layer   3  Sparsity: 70.3378%\n",
      "total_backward_count 695090 real_backward_count 57346   8.250%\n",
      "fc layer 3 self.abs_max_out: 1633.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.368146/  0.997236, val:  70.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3388%\n",
      "layer   2  Sparsity: 72.3304%\n",
      "layer   3  Sparsity: 69.3065%\n",
      "total_backward_count 704880 real_backward_count 57806   8.201%\n",
      "fc layer 3 self.abs_max_out: 1637.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.343888/  0.960156, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3409%\n",
      "layer   2  Sparsity: 72.5208%\n",
      "layer   3  Sparsity: 69.4688%\n",
      "total_backward_count 714670 real_backward_count 58242   8.149%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.351820/  1.046103, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3058%\n",
      "layer   2  Sparsity: 72.6420%\n",
      "layer   3  Sparsity: 69.2612%\n",
      "total_backward_count 724460 real_backward_count 58688   8.101%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.343546/  0.998649, val:  74.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3068%\n",
      "layer   2  Sparsity: 72.3634%\n",
      "layer   3  Sparsity: 68.6194%\n",
      "total_backward_count 734250 real_backward_count 59158   8.057%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.346733/  0.926315, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2863%\n",
      "layer   2  Sparsity: 72.2977%\n",
      "layer   3  Sparsity: 68.4711%\n",
      "total_backward_count 744040 real_backward_count 59590   8.009%\n",
      "fc layer 3 self.abs_max_out: 1652.0\n",
      "fc layer 3 self.abs_max_out: 1658.0\n",
      "fc layer 3 self.abs_max_out: 1708.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.320403/  0.951013, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 72.5176%\n",
      "layer   3  Sparsity: 68.2540%\n",
      "total_backward_count 753830 real_backward_count 60021   7.962%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.322268/  0.996817, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3268%\n",
      "layer   2  Sparsity: 72.8211%\n",
      "layer   3  Sparsity: 67.4979%\n",
      "total_backward_count 763620 real_backward_count 60439   7.915%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.312927/  0.946596, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3537%\n",
      "layer   2  Sparsity: 72.5385%\n",
      "layer   3  Sparsity: 67.2464%\n",
      "total_backward_count 773410 real_backward_count 60824   7.864%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.299632/  0.931134, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3138%\n",
      "layer   2  Sparsity: 72.6510%\n",
      "layer   3  Sparsity: 67.8251%\n",
      "total_backward_count 783200 real_backward_count 61205   7.815%\n",
      "fc layer 3 self.abs_max_out: 1725.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.308881/  0.969529, val:  72.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 72.5671%\n",
      "layer   3  Sparsity: 67.0221%\n",
      "total_backward_count 792990 real_backward_count 61615   7.770%\n",
      "fc layer 3 self.abs_max_out: 1728.0\n",
      "fc layer 3 self.abs_max_out: 1740.0\n",
      "fc layer 3 self.abs_max_out: 1798.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.302147/  0.983819, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 72.1201%\n",
      "layer   3  Sparsity: 67.3040%\n",
      "total_backward_count 802780 real_backward_count 61995   7.723%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.282265/  0.898287, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3336%\n",
      "layer   2  Sparsity: 72.1880%\n",
      "layer   3  Sparsity: 67.4747%\n",
      "total_backward_count 812570 real_backward_count 62367   7.675%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.284170/  1.031944, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3339%\n",
      "layer   2  Sparsity: 72.2877%\n",
      "layer   3  Sparsity: 66.6415%\n",
      "total_backward_count 822360 real_backward_count 62748   7.630%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.287864/  0.897421, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3401%\n",
      "layer   2  Sparsity: 72.2285%\n",
      "layer   3  Sparsity: 66.7146%\n",
      "total_backward_count 832150 real_backward_count 63147   7.588%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.278342/  0.903931, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3605%\n",
      "layer   2  Sparsity: 72.0978%\n",
      "layer   3  Sparsity: 68.3892%\n",
      "total_backward_count 841940 real_backward_count 63501   7.542%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.284031/  0.989145, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3079%\n",
      "layer   2  Sparsity: 71.7641%\n",
      "layer   3  Sparsity: 69.1526%\n",
      "total_backward_count 851730 real_backward_count 63850   7.497%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.285033/  0.955178, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3506%\n",
      "layer   2  Sparsity: 71.6508%\n",
      "layer   3  Sparsity: 68.4385%\n",
      "total_backward_count 861520 real_backward_count 64214   7.454%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.289141/  0.894824, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3556%\n",
      "layer   2  Sparsity: 71.6890%\n",
      "layer   3  Sparsity: 67.2569%\n",
      "total_backward_count 871310 real_backward_count 64626   7.417%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.276548/  0.859261, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3377%\n",
      "layer   2  Sparsity: 71.8379%\n",
      "layer   3  Sparsity: 68.8425%\n",
      "total_backward_count 881100 real_backward_count 64976   7.374%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.281646/  0.965135, val:  75.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 71.6350%\n",
      "layer   3  Sparsity: 68.2797%\n",
      "total_backward_count 890890 real_backward_count 65367   7.337%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.259277/  1.016976, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2931%\n",
      "layer   2  Sparsity: 71.9861%\n",
      "layer   3  Sparsity: 67.9939%\n",
      "total_backward_count 900680 real_backward_count 65682   7.292%\n",
      "fc layer 3 self.abs_max_out: 1802.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.256328/  0.913627, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3298%\n",
      "layer   2  Sparsity: 72.5189%\n",
      "layer   3  Sparsity: 68.0913%\n",
      "total_backward_count 910470 real_backward_count 66009   7.250%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.252455/  0.999033, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 72.5379%\n",
      "layer   3  Sparsity: 67.1276%\n",
      "total_backward_count 920260 real_backward_count 66283   7.203%\n",
      "fc layer 3 self.abs_max_out: 1821.0\n",
      "fc layer 3 self.abs_max_out: 1842.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.263301/  0.952484, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3206%\n",
      "layer   2  Sparsity: 72.5614%\n",
      "layer   3  Sparsity: 65.7726%\n",
      "total_backward_count 930050 real_backward_count 66617   7.163%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.257869/  0.942339, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2899%\n",
      "layer   2  Sparsity: 72.1708%\n",
      "layer   3  Sparsity: 66.8022%\n",
      "total_backward_count 939840 real_backward_count 66938   7.122%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.258550/  1.029050, val:  67.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3063%\n",
      "layer   2  Sparsity: 71.8899%\n",
      "layer   3  Sparsity: 66.4486%\n",
      "total_backward_count 949630 real_backward_count 67243   7.081%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.260839/  0.949191, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 71.7112%\n",
      "layer   3  Sparsity: 66.2244%\n",
      "total_backward_count 959420 real_backward_count 67588   7.045%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.262276/  0.973035, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2930%\n",
      "layer   2  Sparsity: 71.4695%\n",
      "layer   3  Sparsity: 67.1618%\n",
      "total_backward_count 969210 real_backward_count 67934   7.009%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.259384/  0.924866, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 71.4340%\n",
      "layer   3  Sparsity: 67.0247%\n",
      "total_backward_count 979000 real_backward_count 68260   6.972%\n",
      "fc layer 1 self.abs_max_out: 12339.0\n",
      "fc layer 3 self.abs_max_out: 1924.0\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.251609/  0.911616, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3598%\n",
      "layer   2  Sparsity: 71.6598%\n",
      "layer   3  Sparsity: 65.9333%\n",
      "total_backward_count 988790 real_backward_count 68609   6.939%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.240143/  0.967585, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   2  Sparsity: 71.9302%\n",
      "layer   3  Sparsity: 66.4653%\n",
      "total_backward_count 998580 real_backward_count 68949   6.905%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.225758/  1.045735, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 72.0204%\n",
      "layer   3  Sparsity: 66.6368%\n",
      "total_backward_count 1008370 real_backward_count 69234   6.866%\n",
      "fc layer 1 self.abs_max_out: 12420.0\n",
      "lif layer 1 self.abs_max_v: 23334.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.221411/  0.953261, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3183%\n",
      "layer   2  Sparsity: 71.9492%\n",
      "layer   3  Sparsity: 67.3720%\n",
      "total_backward_count 1018160 real_backward_count 69502   6.826%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.222422/  0.934131, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3369%\n",
      "layer   2  Sparsity: 72.0624%\n",
      "layer   3  Sparsity: 65.9655%\n",
      "total_backward_count 1027950 real_backward_count 69790   6.789%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.221815/  0.934879, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3501%\n",
      "layer   2  Sparsity: 72.2201%\n",
      "layer   3  Sparsity: 67.1017%\n",
      "total_backward_count 1037740 real_backward_count 70067   6.752%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.244024/  0.907578, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3182%\n",
      "layer   2  Sparsity: 71.7861%\n",
      "layer   3  Sparsity: 67.1525%\n",
      "total_backward_count 1047530 real_backward_count 70359   6.717%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.251300/  0.918889, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3324%\n",
      "layer   2  Sparsity: 71.9031%\n",
      "layer   3  Sparsity: 65.9975%\n",
      "total_backward_count 1057320 real_backward_count 70688   6.686%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.238595/  0.937708, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3035%\n",
      "layer   2  Sparsity: 71.9996%\n",
      "layer   3  Sparsity: 66.5350%\n",
      "total_backward_count 1067110 real_backward_count 70981   6.652%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.236910/  0.938487, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3076%\n",
      "layer   2  Sparsity: 71.8782%\n",
      "layer   3  Sparsity: 67.2529%\n",
      "total_backward_count 1076900 real_backward_count 71253   6.616%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.227055/  0.971411, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3637%\n",
      "layer   2  Sparsity: 71.6743%\n",
      "layer   3  Sparsity: 67.2639%\n",
      "total_backward_count 1086690 real_backward_count 71538   6.583%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.235228/  0.948973, val:  75.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3031%\n",
      "layer   2  Sparsity: 71.4014%\n",
      "layer   3  Sparsity: 66.1421%\n",
      "total_backward_count 1096480 real_backward_count 71852   6.553%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.224483/  0.925502, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3565%\n",
      "layer   2  Sparsity: 71.6124%\n",
      "layer   3  Sparsity: 66.9938%\n",
      "total_backward_count 1106270 real_backward_count 72137   6.521%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.215318/  0.879603, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3365%\n",
      "layer   2  Sparsity: 71.6318%\n",
      "layer   3  Sparsity: 66.6733%\n",
      "total_backward_count 1116060 real_backward_count 72375   6.485%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.230121/  0.942210, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3261%\n",
      "layer   2  Sparsity: 71.3654%\n",
      "layer   3  Sparsity: 67.0141%\n",
      "total_backward_count 1125850 real_backward_count 72673   6.455%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.219643/  0.856520, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3047%\n",
      "layer   2  Sparsity: 71.8276%\n",
      "layer   3  Sparsity: 66.3864%\n",
      "total_backward_count 1135640 real_backward_count 72927   6.422%\n",
      "fc layer 3 self.abs_max_out: 1929.0\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.202293/  0.921822, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3151%\n",
      "layer   2  Sparsity: 71.7997%\n",
      "layer   3  Sparsity: 67.9211%\n",
      "total_backward_count 1145430 real_backward_count 73141   6.385%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.217485/  0.909540, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 71.6506%\n",
      "layer   3  Sparsity: 68.3122%\n",
      "total_backward_count 1155220 real_backward_count 73375   6.352%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.204866/  1.022326, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3311%\n",
      "layer   2  Sparsity: 71.9715%\n",
      "layer   3  Sparsity: 68.4828%\n",
      "total_backward_count 1165010 real_backward_count 73582   6.316%\n",
      "fc layer 3 self.abs_max_out: 1938.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.209691/  0.940921, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3779%\n",
      "layer   2  Sparsity: 71.9311%\n",
      "layer   3  Sparsity: 66.9927%\n",
      "total_backward_count 1174800 real_backward_count 73853   6.286%\n",
      "fc layer 3 self.abs_max_out: 1956.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.210262/  0.918750, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3366%\n",
      "layer   2  Sparsity: 71.6569%\n",
      "layer   3  Sparsity: 67.2662%\n",
      "total_backward_count 1184590 real_backward_count 74119   6.257%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.203175/  0.948397, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3407%\n",
      "layer   2  Sparsity: 71.6351%\n",
      "layer   3  Sparsity: 68.4564%\n",
      "total_backward_count 1194380 real_backward_count 74356   6.225%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.213263/  0.899050, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3551%\n",
      "layer   2  Sparsity: 71.6189%\n",
      "layer   3  Sparsity: 68.0712%\n",
      "total_backward_count 1204170 real_backward_count 74607   6.196%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.211906/  0.935117, val:  76.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3231%\n",
      "layer   2  Sparsity: 71.5345%\n",
      "layer   3  Sparsity: 68.1725%\n",
      "total_backward_count 1213960 real_backward_count 74842   6.165%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.216366/  0.857755, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3334%\n",
      "layer   2  Sparsity: 71.5883%\n",
      "layer   3  Sparsity: 68.6840%\n",
      "total_backward_count 1223750 real_backward_count 75100   6.137%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.203871/  0.957923, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3626%\n",
      "layer   2  Sparsity: 72.1475%\n",
      "layer   3  Sparsity: 68.3391%\n",
      "total_backward_count 1233540 real_backward_count 75331   6.107%\n",
      "fc layer 3 self.abs_max_out: 1977.0\n",
      "fc layer 3 self.abs_max_out: 2011.0\n",
      "fc layer 3 self.abs_max_out: 2014.0\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.199399/  0.963296, val:  76.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3858%\n",
      "layer   2  Sparsity: 72.1193%\n",
      "layer   3  Sparsity: 67.9549%\n",
      "total_backward_count 1243330 real_backward_count 75558   6.077%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.197303/  0.856692, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3263%\n",
      "layer   2  Sparsity: 72.0180%\n",
      "layer   3  Sparsity: 66.8091%\n",
      "total_backward_count 1253120 real_backward_count 75759   6.046%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.198751/  0.905400, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3586%\n",
      "layer   2  Sparsity: 71.9591%\n",
      "layer   3  Sparsity: 67.0485%\n",
      "total_backward_count 1262910 real_backward_count 75962   6.015%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.195237/  0.938625, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3090%\n",
      "layer   2  Sparsity: 71.9392%\n",
      "layer   3  Sparsity: 67.1039%\n",
      "total_backward_count 1272700 real_backward_count 76181   5.986%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.189283/  0.963546, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3631%\n",
      "layer   2  Sparsity: 71.5568%\n",
      "layer   3  Sparsity: 67.3606%\n",
      "total_backward_count 1282490 real_backward_count 76382   5.956%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.193220/  0.932952, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3278%\n",
      "layer   2  Sparsity: 71.7651%\n",
      "layer   3  Sparsity: 67.3645%\n",
      "total_backward_count 1292280 real_backward_count 76590   5.927%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.181411/  0.908468, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3214%\n",
      "layer   2  Sparsity: 71.7612%\n",
      "layer   3  Sparsity: 67.5753%\n",
      "total_backward_count 1302070 real_backward_count 76790   5.898%\n",
      "fc layer 3 self.abs_max_out: 2099.0\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.184888/  0.920990, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3374%\n",
      "layer   2  Sparsity: 71.4400%\n",
      "layer   3  Sparsity: 66.9202%\n",
      "total_backward_count 1311860 real_backward_count 76993   5.869%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.182324/  0.897263, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 71.3808%\n",
      "layer   3  Sparsity: 67.3290%\n",
      "total_backward_count 1321650 real_backward_count 77165   5.839%\n",
      "fc layer 3 self.abs_max_out: 2119.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.184410/  0.918138, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 71.3539%\n",
      "layer   3  Sparsity: 67.0137%\n",
      "total_backward_count 1331440 real_backward_count 77348   5.809%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.172702/  0.870516, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3424%\n",
      "layer   2  Sparsity: 71.6893%\n",
      "layer   3  Sparsity: 66.5147%\n",
      "total_backward_count 1341230 real_backward_count 77542   5.781%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.175793/  0.912775, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3496%\n",
      "layer   2  Sparsity: 71.7655%\n",
      "layer   3  Sparsity: 66.8977%\n",
      "total_backward_count 1351020 real_backward_count 77752   5.755%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.183695/  0.943906, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3279%\n",
      "layer   2  Sparsity: 71.5422%\n",
      "layer   3  Sparsity: 66.3148%\n",
      "total_backward_count 1360810 real_backward_count 77960   5.729%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.172853/  0.958146, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 71.4013%\n",
      "layer   3  Sparsity: 67.5634%\n",
      "total_backward_count 1370600 real_backward_count 78152   5.702%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.164240/  0.979895, val:  75.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3190%\n",
      "layer   2  Sparsity: 71.7337%\n",
      "layer   3  Sparsity: 66.8401%\n",
      "total_backward_count 1380390 real_backward_count 78310   5.673%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.165275/  1.002558, val:  75.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3559%\n",
      "layer   2  Sparsity: 71.6628%\n",
      "layer   3  Sparsity: 66.2624%\n",
      "total_backward_count 1390180 real_backward_count 78502   5.647%\n",
      "fc layer 3 self.abs_max_out: 2127.0\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.177515/  0.949234, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3515%\n",
      "layer   2  Sparsity: 71.6188%\n",
      "layer   3  Sparsity: 65.7977%\n",
      "total_backward_count 1399970 real_backward_count 78722   5.623%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.169519/  0.916762, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2864%\n",
      "layer   2  Sparsity: 71.6326%\n",
      "layer   3  Sparsity: 66.0541%\n",
      "total_backward_count 1409760 real_backward_count 78905   5.597%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.157367/  0.906920, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3405%\n",
      "layer   2  Sparsity: 72.0736%\n",
      "layer   3  Sparsity: 65.9459%\n",
      "total_backward_count 1419550 real_backward_count 79072   5.570%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.158002/  0.904861, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3323%\n",
      "layer   2  Sparsity: 72.0781%\n",
      "layer   3  Sparsity: 66.4734%\n",
      "total_backward_count 1429340 real_backward_count 79241   5.544%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.167279/  0.851299, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3318%\n",
      "layer   2  Sparsity: 71.6631%\n",
      "layer   3  Sparsity: 66.2218%\n",
      "total_backward_count 1439130 real_backward_count 79393   5.517%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.168636/  0.854959, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3289%\n",
      "layer   2  Sparsity: 71.6982%\n",
      "layer   3  Sparsity: 66.8594%\n",
      "total_backward_count 1448920 real_backward_count 79585   5.493%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.166780/  0.927106, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3204%\n",
      "layer   2  Sparsity: 71.7014%\n",
      "layer   3  Sparsity: 68.1773%\n",
      "total_backward_count 1458710 real_backward_count 79769   5.468%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.164787/  0.930501, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3307%\n",
      "layer   2  Sparsity: 71.7865%\n",
      "layer   3  Sparsity: 67.7696%\n",
      "total_backward_count 1468500 real_backward_count 79937   5.443%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.156513/  0.895851, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 71.6975%\n",
      "layer   3  Sparsity: 67.9906%\n",
      "total_backward_count 1478290 real_backward_count 80086   5.417%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.154734/  1.008308, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3448%\n",
      "layer   2  Sparsity: 72.1107%\n",
      "layer   3  Sparsity: 67.0950%\n",
      "total_backward_count 1488080 real_backward_count 80241   5.392%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.155948/  0.985228, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3282%\n",
      "layer   2  Sparsity: 72.4686%\n",
      "layer   3  Sparsity: 66.5890%\n",
      "total_backward_count 1497870 real_backward_count 80410   5.368%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.147788/  0.878004, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 72.0858%\n",
      "layer   3  Sparsity: 67.6286%\n",
      "total_backward_count 1507660 real_backward_count 80568   5.344%\n",
      "fc layer 3 self.abs_max_out: 2133.0\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.154533/  0.989229, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3642%\n",
      "layer   2  Sparsity: 72.0077%\n",
      "layer   3  Sparsity: 67.2678%\n",
      "total_backward_count 1517450 real_backward_count 80747   5.321%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.158981/  0.928804, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.2999%\n",
      "layer   2  Sparsity: 72.0890%\n",
      "layer   3  Sparsity: 66.5464%\n",
      "total_backward_count 1527240 real_backward_count 80918   5.298%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.152943/  0.942851, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3447%\n",
      "layer   2  Sparsity: 72.0316%\n",
      "layer   3  Sparsity: 66.3927%\n",
      "total_backward_count 1537030 real_backward_count 81075   5.275%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.166806/  0.904976, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3001%\n",
      "layer   2  Sparsity: 71.4490%\n",
      "layer   3  Sparsity: 65.5749%\n",
      "total_backward_count 1546820 real_backward_count 81264   5.254%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.150726/  0.988669, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3402%\n",
      "layer   2  Sparsity: 71.6255%\n",
      "layer   3  Sparsity: 66.1088%\n",
      "total_backward_count 1556610 real_backward_count 81427   5.231%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.151445/  0.953068, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3253%\n",
      "layer   2  Sparsity: 71.8805%\n",
      "layer   3  Sparsity: 66.6123%\n",
      "total_backward_count 1566400 real_backward_count 81575   5.208%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.148370/  0.882329, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3443%\n",
      "layer   2  Sparsity: 72.1048%\n",
      "layer   3  Sparsity: 66.2968%\n",
      "total_backward_count 1576190 real_backward_count 81723   5.185%\n",
      "fc layer 3 self.abs_max_out: 2151.0\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.141654/  0.892109, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3023%\n",
      "layer   2  Sparsity: 72.4951%\n",
      "layer   3  Sparsity: 66.8349%\n",
      "total_backward_count 1585980 real_backward_count 81864   5.162%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.135900/  0.955875, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3355%\n",
      "layer   2  Sparsity: 72.5259%\n",
      "layer   3  Sparsity: 67.6384%\n",
      "total_backward_count 1595770 real_backward_count 82002   5.139%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.139447/  0.874693, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 72.1253%\n",
      "layer   3  Sparsity: 67.1804%\n",
      "total_backward_count 1605560 real_backward_count 82138   5.116%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.139508/  0.890106, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2903%\n",
      "layer   2  Sparsity: 71.8962%\n",
      "layer   3  Sparsity: 67.4278%\n",
      "total_backward_count 1615350 real_backward_count 82269   5.093%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.135345/  1.026425, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3084%\n",
      "layer   2  Sparsity: 71.9702%\n",
      "layer   3  Sparsity: 67.3314%\n",
      "total_backward_count 1625140 real_backward_count 82378   5.069%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.131713/  0.912651, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3134%\n",
      "layer   2  Sparsity: 72.3338%\n",
      "layer   3  Sparsity: 66.4483%\n",
      "total_backward_count 1634930 real_backward_count 82486   5.045%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.130985/  0.875302, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 72.2280%\n",
      "layer   3  Sparsity: 66.5341%\n",
      "total_backward_count 1644720 real_backward_count 82607   5.023%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.124705/  0.960985, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3690%\n",
      "layer   2  Sparsity: 72.2332%\n",
      "layer   3  Sparsity: 68.0669%\n",
      "total_backward_count 1654510 real_backward_count 82713   4.999%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.119512/  0.884136, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3504%\n",
      "layer   2  Sparsity: 72.5611%\n",
      "layer   3  Sparsity: 68.0566%\n",
      "total_backward_count 1664300 real_backward_count 82798   4.975%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.133404/  0.918014, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3495%\n",
      "layer   2  Sparsity: 72.4368%\n",
      "layer   3  Sparsity: 66.9249%\n",
      "total_backward_count 1674090 real_backward_count 82936   4.954%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.133344/  0.906573, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3696%\n",
      "layer   2  Sparsity: 72.2259%\n",
      "layer   3  Sparsity: 66.3953%\n",
      "total_backward_count 1683880 real_backward_count 83069   4.933%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.133584/  0.913711, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3602%\n",
      "layer   2  Sparsity: 72.1627%\n",
      "layer   3  Sparsity: 66.8482%\n",
      "total_backward_count 1693670 real_backward_count 83203   4.913%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.125746/  0.891619, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3412%\n",
      "layer   2  Sparsity: 72.3075%\n",
      "layer   3  Sparsity: 66.3388%\n",
      "total_backward_count 1703460 real_backward_count 83319   4.891%\n",
      "fc layer 3 self.abs_max_out: 2168.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.131561/  0.909926, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 72.2954%\n",
      "layer   3  Sparsity: 66.5233%\n",
      "total_backward_count 1713250 real_backward_count 83458   4.871%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.132972/  0.934296, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2967%\n",
      "layer   2  Sparsity: 72.3681%\n",
      "layer   3  Sparsity: 66.2888%\n",
      "total_backward_count 1723040 real_backward_count 83583   4.851%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.132164/  1.000425, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 71.9742%\n",
      "layer   3  Sparsity: 66.6183%\n",
      "total_backward_count 1732830 real_backward_count 83707   4.831%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.131723/  0.886999, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3123%\n",
      "layer   2  Sparsity: 72.0811%\n",
      "layer   3  Sparsity: 66.5222%\n",
      "total_backward_count 1742620 real_backward_count 83816   4.810%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.122672/  0.869734, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3472%\n",
      "layer   2  Sparsity: 71.9478%\n",
      "layer   3  Sparsity: 66.7001%\n",
      "total_backward_count 1752410 real_backward_count 83911   4.788%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.138821/  0.915206, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3422%\n",
      "layer   2  Sparsity: 71.6991%\n",
      "layer   3  Sparsity: 66.3329%\n",
      "total_backward_count 1762200 real_backward_count 84058   4.770%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.134384/  0.871240, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3571%\n",
      "layer   2  Sparsity: 72.0517%\n",
      "layer   3  Sparsity: 66.3202%\n",
      "total_backward_count 1771990 real_backward_count 84174   4.750%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.128159/  0.880462, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3210%\n",
      "layer   2  Sparsity: 72.4617%\n",
      "layer   3  Sparsity: 66.8339%\n",
      "total_backward_count 1781780 real_backward_count 84303   4.731%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.139155/  0.915448, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3381%\n",
      "layer   2  Sparsity: 72.0384%\n",
      "layer   3  Sparsity: 65.8882%\n",
      "total_backward_count 1791570 real_backward_count 84438   4.713%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.127856/  0.906213, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3218%\n",
      "layer   2  Sparsity: 72.3221%\n",
      "layer   3  Sparsity: 65.4547%\n",
      "total_backward_count 1801360 real_backward_count 84524   4.692%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.123913/  0.944321, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3481%\n",
      "layer   2  Sparsity: 72.1260%\n",
      "layer   3  Sparsity: 66.3738%\n",
      "total_backward_count 1811150 real_backward_count 84621   4.672%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.129179/  0.952815, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 72.1719%\n",
      "layer   3  Sparsity: 66.7905%\n",
      "total_backward_count 1820940 real_backward_count 84718   4.652%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.125171/  0.924635, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3375%\n",
      "layer   2  Sparsity: 72.0347%\n",
      "layer   3  Sparsity: 66.8199%\n",
      "total_backward_count 1830730 real_backward_count 84816   4.633%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.117674/  0.952611, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3426%\n",
      "layer   2  Sparsity: 71.9552%\n",
      "layer   3  Sparsity: 67.3788%\n",
      "total_backward_count 1840520 real_backward_count 84914   4.614%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.125712/  0.955984, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 72.2421%\n",
      "layer   3  Sparsity: 67.6217%\n",
      "total_backward_count 1850310 real_backward_count 85038   4.596%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.126071/  0.934526, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 72.3642%\n",
      "layer   3  Sparsity: 67.0904%\n",
      "total_backward_count 1860100 real_backward_count 85131   4.577%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.130817/  0.894062, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 71.8846%\n",
      "layer   3  Sparsity: 66.7580%\n",
      "total_backward_count 1869890 real_backward_count 85249   4.559%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.124057/  0.959014, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3118%\n",
      "layer   2  Sparsity: 71.9821%\n",
      "layer   3  Sparsity: 66.5266%\n",
      "total_backward_count 1879680 real_backward_count 85341   4.540%\n",
      "fc layer 3 self.abs_max_out: 2207.0\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.115086/  0.932484, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 72.0872%\n",
      "layer   3  Sparsity: 67.3847%\n",
      "total_backward_count 1889470 real_backward_count 85423   4.521%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.107331/  0.935632, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3283%\n",
      "layer   2  Sparsity: 72.0706%\n",
      "layer   3  Sparsity: 67.6584%\n",
      "total_backward_count 1899260 real_backward_count 85483   4.501%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.108719/  0.959601, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3299%\n",
      "layer   2  Sparsity: 72.2215%\n",
      "layer   3  Sparsity: 68.0158%\n",
      "total_backward_count 1909050 real_backward_count 85563   4.482%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.113453/  0.899309, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3164%\n",
      "layer   2  Sparsity: 72.0224%\n",
      "layer   3  Sparsity: 67.6647%\n",
      "total_backward_count 1918840 real_backward_count 85616   4.462%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.127547/  0.934424, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3302%\n",
      "layer   2  Sparsity: 71.9650%\n",
      "layer   3  Sparsity: 67.7879%\n",
      "total_backward_count 1928630 real_backward_count 85744   4.446%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.121557/  0.920420, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3580%\n",
      "layer   2  Sparsity: 72.0584%\n",
      "layer   3  Sparsity: 67.2525%\n",
      "total_backward_count 1938420 real_backward_count 85827   4.428%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.120093/  1.008716, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3238%\n",
      "layer   2  Sparsity: 72.1501%\n",
      "layer   3  Sparsity: 66.7767%\n",
      "total_backward_count 1948210 real_backward_count 85913   4.410%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.126020/  0.904184, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3284%\n",
      "layer   2  Sparsity: 71.6708%\n",
      "layer   3  Sparsity: 67.0969%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32682c3531294d449d70d4cb2df0d229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.12602</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>0.90418</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-sweep-79</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9fzru9dg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9fzru9dg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_175616-9fzru9dg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6ibz11zh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_221159-6ibz11zh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ibz11zh' target=\"_blank\">graceful-sweep-85</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ibz11zh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ibz11zh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251115_221209_409', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 552.0\n",
      "lif layer 2 self.abs_max_v: 552.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "lif layer 2 self.abs_max_v: 643.5\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "lif layer 1 self.abs_max_v: 339.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 3 self.abs_max_out: 219.0\n",
      "fc layer 1 self.abs_max_out: 328.0\n",
      "lif layer 1 self.abs_max_v: 353.5\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 469.0\n",
      "lif layer 1 self.abs_max_v: 527.0\n",
      "fc layer 1 self.abs_max_out: 519.0\n",
      "lif layer 1 self.abs_max_v: 527.5\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "lif layer 2 self.abs_max_v: 903.0\n",
      "fc layer 1 self.abs_max_out: 530.0\n",
      "lif layer 1 self.abs_max_v: 530.0\n",
      "fc layer 2 self.abs_max_out: 589.0\n",
      "lif layer 1 self.abs_max_v: 620.0\n",
      "fc layer 2 self.abs_max_out: 628.0\n",
      "lif layer 1 self.abs_max_v: 664.0\n",
      "lif layer 1 self.abs_max_v: 674.0\n",
      "lif layer 1 self.abs_max_v: 708.0\n",
      "fc layer 1 self.abs_max_out: 570.0\n",
      "lif layer 1 self.abs_max_v: 719.0\n",
      "lif layer 2 self.abs_max_v: 930.0\n",
      "lif layer 2 self.abs_max_v: 965.0\n",
      "lif layer 2 self.abs_max_v: 1000.5\n",
      "lif layer 1 self.abs_max_v: 741.0\n",
      "fc layer 1 self.abs_max_out: 630.0\n",
      "fc layer 1 self.abs_max_out: 667.0\n",
      "fc layer 2 self.abs_max_out: 675.0\n",
      "fc layer 1 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 998.0\n",
      "lif layer 2 self.abs_max_v: 1016.5\n",
      "fc layer 1 self.abs_max_out: 1053.0\n",
      "lif layer 1 self.abs_max_v: 1053.0\n",
      "lif layer 2 self.abs_max_v: 1081.5\n",
      "lif layer 1 self.abs_max_v: 1144.5\n",
      "lif layer 1 self.abs_max_v: 1169.5\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "lif layer 1 self.abs_max_v: 1271.5\n",
      "fc layer 1 self.abs_max_out: 1352.0\n",
      "lif layer 1 self.abs_max_v: 1352.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "lif layer 2 self.abs_max_v: 1114.0\n",
      "lif layer 2 self.abs_max_v: 1127.0\n",
      "lif layer 2 self.abs_max_v: 1129.5\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "lif layer 2 self.abs_max_v: 1141.5\n",
      "lif layer 1 self.abs_max_v: 1386.5\n",
      "lif layer 1 self.abs_max_v: 1393.5\n",
      "lif layer 2 self.abs_max_v: 1166.5\n",
      "lif layer 1 self.abs_max_v: 1642.0\n",
      "lif layer 2 self.abs_max_v: 1296.5\n",
      "lif layer 1 self.abs_max_v: 1822.0\n",
      "fc layer 2 self.abs_max_out: 846.0\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "lif layer 1 self.abs_max_v: 1841.5\n",
      "lif layer 2 self.abs_max_v: 1329.5\n",
      "lif layer 2 self.abs_max_v: 1343.5\n",
      "fc layer 1 self.abs_max_out: 1451.0\n",
      "lif layer 1 self.abs_max_v: 2364.5\n",
      "lif layer 2 self.abs_max_v: 1352.0\n",
      "lif layer 2 self.abs_max_v: 1519.0\n",
      "lif layer 2 self.abs_max_v: 1558.5\n",
      "fc layer 2 self.abs_max_out: 869.0\n",
      "fc layer 2 self.abs_max_out: 883.0\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "fc layer 2 self.abs_max_out: 920.0\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "fc layer 2 self.abs_max_out: 933.0\n",
      "fc layer 1 self.abs_max_out: 1725.0\n",
      "fc layer 1 self.abs_max_out: 1841.0\n",
      "fc layer 1 self.abs_max_out: 1942.0\n",
      "fc layer 1 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "fc layer 2 self.abs_max_out: 1019.0\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "lif layer 2 self.abs_max_v: 1577.5\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "lif layer 2 self.abs_max_v: 1584.5\n",
      "lif layer 2 self.abs_max_v: 1621.5\n",
      "lif layer 2 self.abs_max_v: 1644.5\n",
      "fc layer 2 self.abs_max_out: 1087.0\n",
      "lif layer 2 self.abs_max_v: 1654.0\n",
      "fc layer 2 self.abs_max_out: 1092.0\n",
      "lif layer 1 self.abs_max_v: 2379.0\n",
      "lif layer 1 self.abs_max_v: 2399.5\n",
      "lif layer 1 self.abs_max_v: 2597.5\n",
      "lif layer 1 self.abs_max_v: 2755.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "lif layer 2 self.abs_max_v: 1713.0\n",
      "lif layer 2 self.abs_max_v: 1735.0\n",
      "lif layer 2 self.abs_max_v: 1742.5\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "lif layer 2 self.abs_max_v: 1768.0\n",
      "lif layer 2 self.abs_max_v: 1782.5\n",
      "lif layer 1 self.abs_max_v: 2762.0\n",
      "lif layer 1 self.abs_max_v: 2841.0\n",
      "lif layer 1 self.abs_max_v: 2881.5\n",
      "lif layer 1 self.abs_max_v: 2966.5\n",
      "fc layer 3 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 1793.0\n",
      "lif layer 2 self.abs_max_v: 1852.0\n",
      "lif layer 1 self.abs_max_v: 3000.5\n",
      "lif layer 1 self.abs_max_v: 3030.5\n",
      "lif layer 2 self.abs_max_v: 1890.5\n",
      "lif layer 1 self.abs_max_v: 3039.5\n",
      "lif layer 1 self.abs_max_v: 3153.5\n",
      "lif layer 1 self.abs_max_v: 3315.0\n",
      "lif layer 2 self.abs_max_v: 1921.0\n",
      "lif layer 2 self.abs_max_v: 1942.0\n",
      "lif layer 2 self.abs_max_v: 1962.0\n",
      "lif layer 2 self.abs_max_v: 2009.0\n",
      "lif layer 2 self.abs_max_v: 2015.5\n",
      "lif layer 1 self.abs_max_v: 3322.0\n",
      "lif layer 1 self.abs_max_v: 3596.0\n",
      "lif layer 1 self.abs_max_v: 3835.0\n",
      "lif layer 1 self.abs_max_v: 4099.5\n",
      "lif layer 2 self.abs_max_v: 2066.5\n",
      "lif layer 2 self.abs_max_v: 2077.5\n",
      "fc layer 2 self.abs_max_out: 1152.0\n",
      "fc layer 2 self.abs_max_out: 1191.0\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "fc layer 1 self.abs_max_out: 2205.0\n",
      "fc layer 1 self.abs_max_out: 2269.0\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "fc layer 1 self.abs_max_out: 2272.0\n",
      "lif layer 1 self.abs_max_v: 4187.0\n",
      "fc layer 1 self.abs_max_out: 2276.0\n",
      "lif layer 1 self.abs_max_v: 4368.0\n",
      "fc layer 1 self.abs_max_out: 2569.0\n",
      "lif layer 1 self.abs_max_v: 4499.5\n",
      "lif layer 1 self.abs_max_v: 4611.5\n",
      "lif layer 1 self.abs_max_v: 4752.0\n",
      "lif layer 2 self.abs_max_v: 2085.0\n",
      "lif layer 2 self.abs_max_v: 2101.5\n",
      "fc layer 1 self.abs_max_out: 2636.0\n",
      "fc layer 2 self.abs_max_out: 1310.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.437368/  1.907761, val:  24.58%, val_best:  24.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9369%\n",
      "layer   2  Sparsity: 69.5035%\n",
      "layer   3  Sparsity: 60.1516%\n",
      "total_backward_count 9790 real_backward_count 1210  12.360%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "fc layer 2 self.abs_max_out: 1390.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "fc layer 1 self.abs_max_out: 2674.0\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "fc layer 2 self.abs_max_out: 1412.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 3 self.abs_max_out: 753.0\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "fc layer 3 self.abs_max_out: 758.0\n",
      "fc layer 3 self.abs_max_out: 759.0\n",
      "fc layer 1 self.abs_max_out: 2920.0\n",
      "lif layer 1 self.abs_max_v: 4782.5\n",
      "lif layer 1 self.abs_max_v: 5177.5\n",
      "lif layer 2 self.abs_max_v: 2103.0\n",
      "lif layer 2 self.abs_max_v: 2209.5\n",
      "lif layer 2 self.abs_max_v: 2229.5\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "lif layer 2 self.abs_max_v: 2246.0\n",
      "lif layer 2 self.abs_max_v: 2259.5\n",
      "fc layer 1 self.abs_max_out: 3031.0\n",
      "lif layer 1 self.abs_max_v: 5382.5\n",
      "fc layer 1 self.abs_max_out: 3116.0\n",
      "lif layer 1 self.abs_max_v: 5710.5\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "lif layer 2 self.abs_max_v: 2333.5\n",
      "lif layer 2 self.abs_max_v: 2420.5\n",
      "lif layer 2 self.abs_max_v: 2484.5\n",
      "lif layer 2 self.abs_max_v: 2519.5\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.275365/  1.794874, val:  33.75%, val_best:  33.75%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9249%\n",
      "layer   2  Sparsity: 75.3316%\n",
      "layer   3  Sparsity: 62.4747%\n",
      "total_backward_count 19580 real_backward_count 2376  12.135%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "lif layer 2 self.abs_max_v: 2640.0\n",
      "lif layer 2 self.abs_max_v: 2683.0\n",
      "fc layer 1 self.abs_max_out: 3249.0\n",
      "lif layer 1 self.abs_max_v: 6029.5\n",
      "fc layer 3 self.abs_max_out: 1013.0\n",
      "fc layer 3 self.abs_max_out: 1080.0\n",
      "fc layer 1 self.abs_max_out: 3270.0\n",
      "fc layer 2 self.abs_max_out: 1439.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "lif layer 2 self.abs_max_v: 2729.5\n",
      "lif layer 2 self.abs_max_v: 2799.0\n",
      "lif layer 2 self.abs_max_v: 2805.0\n",
      "lif layer 2 self.abs_max_v: 2835.5\n",
      "fc layer 2 self.abs_max_out: 1569.0\n",
      "lif layer 2 self.abs_max_v: 2846.0\n",
      "lif layer 2 self.abs_max_v: 2862.0\n",
      "lif layer 2 self.abs_max_v: 2868.5\n",
      "fc layer 1 self.abs_max_out: 3455.0\n",
      "lif layer 1 self.abs_max_v: 6039.5\n",
      "lif layer 1 self.abs_max_v: 6447.0\n",
      "lif layer 1 self.abs_max_v: 6658.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.199556/  1.698989, val:  36.25%, val_best:  36.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 76.2959%\n",
      "layer   3  Sparsity: 62.7532%\n",
      "total_backward_count 29370 real_backward_count 3511  11.954%\n",
      "fc layer 1 self.abs_max_out: 3479.0\n",
      "fc layer 3 self.abs_max_out: 1083.0\n",
      "fc layer 1 self.abs_max_out: 3569.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.166509/  1.696723, val:  44.58%, val_best:  44.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 75.6832%\n",
      "layer   3  Sparsity: 63.4009%\n",
      "total_backward_count 39160 real_backward_count 4668  11.920%\n",
      "lif layer 2 self.abs_max_v: 2885.5\n",
      "lif layer 2 self.abs_max_v: 2898.0\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "lif layer 2 self.abs_max_v: 3020.0\n",
      "lif layer 2 self.abs_max_v: 3021.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "fc layer 2 self.abs_max_out: 1591.0\n",
      "fc layer 2 self.abs_max_out: 1592.0\n",
      "lif layer 2 self.abs_max_v: 3060.5\n",
      "lif layer 2 self.abs_max_v: 3068.5\n",
      "lif layer 2 self.abs_max_v: 3118.5\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 2 self.abs_max_out: 1731.0\n",
      "fc layer 1 self.abs_max_out: 3791.0\n",
      "fc layer 3 self.abs_max_out: 1086.0\n",
      "fc layer 3 self.abs_max_out: 1087.0\n",
      "fc layer 3 self.abs_max_out: 1107.0\n",
      "fc layer 3 self.abs_max_out: 1156.0\n",
      "fc layer 1 self.abs_max_out: 3886.0\n",
      "fc layer 3 self.abs_max_out: 1177.0\n",
      "fc layer 1 self.abs_max_out: 3888.0\n",
      "fc layer 3 self.abs_max_out: 1203.0\n",
      "lif layer 1 self.abs_max_v: 6714.5\n",
      "lif layer 1 self.abs_max_v: 6951.5\n",
      "lif layer 1 self.abs_max_v: 7064.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.093054/  1.666866, val:  45.42%, val_best:  45.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 76.1818%\n",
      "layer   3  Sparsity: 64.0582%\n",
      "total_backward_count 48950 real_backward_count 5845  11.941%\n",
      "lif layer 2 self.abs_max_v: 3123.5\n",
      "lif layer 2 self.abs_max_v: 3144.0\n",
      "lif layer 2 self.abs_max_v: 3162.0\n",
      "fc layer 1 self.abs_max_out: 3912.0\n",
      "fc layer 1 self.abs_max_out: 3941.0\n",
      "lif layer 2 self.abs_max_v: 3226.0\n",
      "fc layer 1 self.abs_max_out: 4163.0\n",
      "lif layer 1 self.abs_max_v: 7232.0\n",
      "fc layer 1 self.abs_max_out: 4197.0\n",
      "lif layer 1 self.abs_max_v: 7754.5\n",
      "fc layer 1 self.abs_max_out: 4454.0\n",
      "lif layer 1 self.abs_max_v: 7839.5\n",
      "fc layer 1 self.abs_max_out: 4827.0\n",
      "lif layer 1 self.abs_max_v: 8319.5\n",
      "lif layer 1 self.abs_max_v: 8853.0\n",
      "lif layer 1 self.abs_max_v: 9097.5\n",
      "lif layer 1 self.abs_max_v: 9333.0\n",
      "lif layer 1 self.abs_max_v: 9404.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.088931/  1.632339, val:  42.50%, val_best:  45.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9070%\n",
      "layer   2  Sparsity: 76.2648%\n",
      "layer   3  Sparsity: 64.4828%\n",
      "total_backward_count 58740 real_backward_count 6986  11.893%\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.061068/  1.592912, val:  49.17%, val_best:  49.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 76.6445%\n",
      "layer   3  Sparsity: 64.3067%\n",
      "total_backward_count 68530 real_backward_count 8102  11.823%\n",
      "lif layer 2 self.abs_max_v: 3328.0\n",
      "lif layer 2 self.abs_max_v: 3357.0\n",
      "lif layer 2 self.abs_max_v: 3361.5\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "lif layer 2 self.abs_max_v: 3488.0\n",
      "lif layer 2 self.abs_max_v: 3501.0\n",
      "lif layer 2 self.abs_max_v: 3579.5\n",
      "lif layer 2 self.abs_max_v: 3641.0\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "fc layer 2 self.abs_max_out: 1954.0\n",
      "fc layer 2 self.abs_max_out: 2021.0\n",
      "lif layer 2 self.abs_max_v: 3698.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.042636/  1.511220, val:  49.17%, val_best:  49.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9136%\n",
      "layer   2  Sparsity: 75.6900%\n",
      "layer   3  Sparsity: 66.1721%\n",
      "total_backward_count 78320 real_backward_count 9207  11.756%\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "lif layer 2 self.abs_max_v: 3737.5\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "lif layer 2 self.abs_max_v: 3908.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.051646/  1.481333, val:  52.50%, val_best:  52.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 76.2484%\n",
      "layer   3  Sparsity: 66.2679%\n",
      "total_backward_count 88110 real_backward_count 10376  11.776%\n",
      "fc layer 1 self.abs_max_out: 4983.0\n",
      "fc layer 2 self.abs_max_out: 2040.0\n",
      "fc layer 1 self.abs_max_out: 4986.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.058034/  1.488324, val:  51.67%, val_best:  52.50%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 75.9046%\n",
      "layer   3  Sparsity: 66.7845%\n",
      "total_backward_count 97900 real_backward_count 11473  11.719%\n",
      "fc layer 3 self.abs_max_out: 1222.0\n",
      "fc layer 3 self.abs_max_out: 1235.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.008209/  1.473153, val:  54.58%, val_best:  54.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 75.8269%\n",
      "layer   3  Sparsity: 66.6232%\n",
      "total_backward_count 107690 real_backward_count 12547  11.651%\n",
      "fc layer 2 self.abs_max_out: 2082.0\n",
      "lif layer 2 self.abs_max_v: 3912.0\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.020461/  1.446002, val:  57.08%, val_best:  57.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9446%\n",
      "layer   2  Sparsity: 75.8436%\n",
      "layer   3  Sparsity: 66.6518%\n",
      "total_backward_count 117480 real_backward_count 13612  11.587%\n",
      "fc layer 3 self.abs_max_out: 1264.0\n",
      "fc layer 1 self.abs_max_out: 4991.0\n",
      "fc layer 1 self.abs_max_out: 5086.0\n",
      "lif layer 2 self.abs_max_v: 4002.5\n",
      "lif layer 1 self.abs_max_v: 9602.0\n",
      "fc layer 1 self.abs_max_out: 5130.0\n",
      "fc layer 1 self.abs_max_out: 5238.0\n",
      "lif layer 1 self.abs_max_v: 9925.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.963813/  1.455655, val:  47.08%, val_best:  57.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 76.1695%\n",
      "layer   3  Sparsity: 66.5223%\n",
      "total_backward_count 127270 real_backward_count 14656  11.516%\n",
      "fc layer 3 self.abs_max_out: 1294.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.969455/  1.577357, val:  41.67%, val_best:  57.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9257%\n",
      "layer   2  Sparsity: 76.3208%\n",
      "layer   3  Sparsity: 65.8874%\n",
      "total_backward_count 137060 real_backward_count 15725  11.473%\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "lif layer 2 self.abs_max_v: 4016.0\n",
      "fc layer 2 self.abs_max_out: 2218.0\n",
      "fc layer 3 self.abs_max_out: 1347.0\n",
      "fc layer 2 self.abs_max_out: 2220.0\n",
      "fc layer 1 self.abs_max_out: 5248.0\n",
      "lif layer 1 self.abs_max_v: 9977.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.946264/  1.428732, val:  54.17%, val_best:  57.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9279%\n",
      "layer   2  Sparsity: 75.8439%\n",
      "layer   3  Sparsity: 65.6420%\n",
      "total_backward_count 146850 real_backward_count 16739  11.399%\n",
      "fc layer 3 self.abs_max_out: 1359.0\n",
      "fc layer 1 self.abs_max_out: 5255.0\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.930028/  1.449604, val:  57.50%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9142%\n",
      "layer   2  Sparsity: 75.4847%\n",
      "layer   3  Sparsity: 64.9968%\n",
      "total_backward_count 156640 real_backward_count 17773  11.346%\n",
      "fc layer 1 self.abs_max_out: 5369.0\n",
      "lif layer 1 self.abs_max_v: 10225.5\n",
      "lif layer 2 self.abs_max_v: 4169.5\n",
      "lif layer 1 self.abs_max_v: 10386.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.909993/  1.318498, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8762%\n",
      "layer   2  Sparsity: 74.9071%\n",
      "layer   3  Sparsity: 63.8935%\n",
      "total_backward_count 166430 real_backward_count 18760  11.272%\n",
      "fc layer 3 self.abs_max_out: 1371.0\n",
      "fc layer 1 self.abs_max_out: 5405.0\n",
      "fc layer 1 self.abs_max_out: 5455.0\n",
      "fc layer 1 self.abs_max_out: 5589.0\n",
      "lif layer 1 self.abs_max_v: 10633.5\n",
      "lif layer 1 self.abs_max_v: 10706.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.866296/  1.421978, val:  57.08%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9105%\n",
      "layer   2  Sparsity: 75.1871%\n",
      "layer   3  Sparsity: 63.5621%\n",
      "total_backward_count 176220 real_backward_count 19799  11.235%\n",
      "lif layer 1 self.abs_max_v: 10754.0\n",
      "lif layer 2 self.abs_max_v: 4236.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.883674/  1.330903, val:  56.25%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 74.1450%\n",
      "layer   3  Sparsity: 64.1705%\n",
      "total_backward_count 186010 real_backward_count 20814  11.190%\n",
      "fc layer 1 self.abs_max_out: 5823.0\n",
      "lif layer 1 self.abs_max_v: 10921.0\n",
      "fc layer 3 self.abs_max_out: 1373.0\n",
      "fc layer 3 self.abs_max_out: 1375.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.847308/  1.569032, val:  42.50%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 74.7642%\n",
      "layer   3  Sparsity: 64.7841%\n",
      "total_backward_count 195800 real_backward_count 21754  11.110%\n",
      "fc layer 3 self.abs_max_out: 1379.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.859319/  1.392891, val:  49.58%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8994%\n",
      "layer   2  Sparsity: 74.8662%\n",
      "layer   3  Sparsity: 65.7789%\n",
      "total_backward_count 205590 real_backward_count 22693  11.038%\n",
      "lif layer 2 self.abs_max_v: 4387.0\n",
      "fc layer 3 self.abs_max_out: 1442.0\n",
      "fc layer 2 self.abs_max_out: 2528.0\n",
      "lif layer 2 self.abs_max_v: 4513.0\n",
      "lif layer 2 self.abs_max_v: 4557.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.855551/  1.326129, val:  60.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9562%\n",
      "layer   2  Sparsity: 75.2055%\n",
      "layer   3  Sparsity: 66.9697%\n",
      "total_backward_count 215380 real_backward_count 23723  11.014%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.886989/  1.387220, val:  49.58%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9311%\n",
      "layer   2  Sparsity: 76.7852%\n",
      "layer   3  Sparsity: 67.2444%\n",
      "total_backward_count 225170 real_backward_count 24742  10.988%\n",
      "lif layer 2 self.abs_max_v: 4665.5\n",
      "lif layer 2 self.abs_max_v: 4739.0\n",
      "fc layer 2 self.abs_max_out: 2541.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.846439/  1.398644, val:  56.67%, val_best:  60.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 75.6619%\n",
      "layer   3  Sparsity: 66.5426%\n",
      "total_backward_count 234960 real_backward_count 25729  10.950%\n",
      "fc layer 2 self.abs_max_out: 2574.0\n",
      "lif layer 1 self.abs_max_v: 11118.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.844712/  1.285085, val:  59.58%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9511%\n",
      "layer   2  Sparsity: 75.0903%\n",
      "layer   3  Sparsity: 66.3660%\n",
      "total_backward_count 244750 real_backward_count 26690  10.905%\n",
      "fc layer 1 self.abs_max_out: 5840.0\n",
      "fc layer 1 self.abs_max_out: 6045.0\n",
      "lif layer 1 self.abs_max_v: 11144.0\n",
      "lif layer 1 self.abs_max_v: 11482.0\n",
      "fc layer 2 self.abs_max_out: 2632.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.835146/  1.332509, val:  56.25%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 75.7534%\n",
      "layer   3  Sparsity: 66.5817%\n",
      "total_backward_count 254540 real_backward_count 27764  10.908%\n",
      "lif layer 2 self.abs_max_v: 4767.0\n",
      "fc layer 2 self.abs_max_out: 2642.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.825739/  1.260263, val:  60.00%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 76.0231%\n",
      "layer   3  Sparsity: 66.8385%\n",
      "total_backward_count 264330 real_backward_count 28724  10.867%\n",
      "fc layer 3 self.abs_max_out: 1717.0\n",
      "fc layer 1 self.abs_max_out: 6064.0\n",
      "fc layer 1 self.abs_max_out: 6269.0\n",
      "fc layer 2 self.abs_max_out: 2649.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.793494/  1.286057, val:  59.58%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9239%\n",
      "layer   2  Sparsity: 75.4588%\n",
      "layer   3  Sparsity: 65.4419%\n",
      "total_backward_count 274120 real_backward_count 29704  10.836%\n",
      "fc layer 2 self.abs_max_out: 2661.0\n",
      "fc layer 2 self.abs_max_out: 2668.0\n",
      "lif layer 2 self.abs_max_v: 4994.0\n",
      "lif layer 2 self.abs_max_v: 5003.0\n",
      "fc layer 1 self.abs_max_out: 6439.0\n",
      "fc layer 2 self.abs_max_out: 2850.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.799318/  1.332503, val:  51.67%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9076%\n",
      "layer   2  Sparsity: 74.7871%\n",
      "layer   3  Sparsity: 65.9970%\n",
      "total_backward_count 283910 real_backward_count 30650  10.796%\n",
      "lif layer 2 self.abs_max_v: 5117.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.792771/  1.367971, val:  51.25%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 74.7623%\n",
      "layer   3  Sparsity: 65.3781%\n",
      "total_backward_count 293700 real_backward_count 31629  10.769%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.777219/  1.245633, val:  63.33%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 74.3340%\n",
      "layer   3  Sparsity: 64.5003%\n",
      "total_backward_count 303490 real_backward_count 32578  10.734%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.762313/  1.303977, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9590%\n",
      "layer   2  Sparsity: 73.7433%\n",
      "layer   3  Sparsity: 64.5431%\n",
      "total_backward_count 313280 real_backward_count 33563  10.713%\n",
      "fc layer 2 self.abs_max_out: 2911.0\n",
      "lif layer 2 self.abs_max_v: 5176.5\n",
      "lif layer 2 self.abs_max_v: 5384.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.747717/  1.354417, val:  59.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8996%\n",
      "layer   2  Sparsity: 73.9630%\n",
      "layer   3  Sparsity: 65.7934%\n",
      "total_backward_count 323070 real_backward_count 34502  10.679%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.744359/  1.207121, val:  66.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 74.6368%\n",
      "layer   3  Sparsity: 65.9416%\n",
      "total_backward_count 332860 real_backward_count 35429  10.644%\n",
      "fc layer 1 self.abs_max_out: 6478.0\n",
      "fc layer 1 self.abs_max_out: 6754.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.711191/  1.270307, val:  57.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9063%\n",
      "layer   2  Sparsity: 74.1047%\n",
      "layer   3  Sparsity: 66.2216%\n",
      "total_backward_count 342650 real_backward_count 36385  10.619%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.742493/  1.316759, val:  52.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9461%\n",
      "layer   2  Sparsity: 74.1932%\n",
      "layer   3  Sparsity: 66.0686%\n",
      "total_backward_count 352440 real_backward_count 37343  10.596%\n",
      "fc layer 2 self.abs_max_out: 3029.0\n",
      "lif layer 1 self.abs_max_v: 11583.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.718816/  1.269954, val:  59.58%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 75.2104%\n",
      "layer   3  Sparsity: 66.2611%\n",
      "total_backward_count 362230 real_backward_count 38268  10.565%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.728483/  1.319475, val:  54.58%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 75.4516%\n",
      "layer   3  Sparsity: 66.9985%\n",
      "total_backward_count 372020 real_backward_count 39119  10.515%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.735059/  1.254896, val:  62.92%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 75.5356%\n",
      "layer   3  Sparsity: 66.2470%\n",
      "total_backward_count 381810 real_backward_count 40069  10.494%\n",
      "fc layer 2 self.abs_max_out: 3032.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.683734/  1.299238, val:  64.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 75.5993%\n",
      "layer   3  Sparsity: 65.8904%\n",
      "total_backward_count 391600 real_backward_count 40980  10.465%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.708497/  1.197514, val:  61.67%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9477%\n",
      "layer   2  Sparsity: 75.1612%\n",
      "layer   3  Sparsity: 66.8208%\n",
      "total_backward_count 401390 real_backward_count 41908  10.441%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.698781/  1.169001, val:  68.33%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9231%\n",
      "layer   2  Sparsity: 75.1893%\n",
      "layer   3  Sparsity: 65.7358%\n",
      "total_backward_count 411180 real_backward_count 42814  10.412%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.673831/  1.267998, val:  58.33%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9254%\n",
      "layer   2  Sparsity: 74.4103%\n",
      "layer   3  Sparsity: 65.4373%\n",
      "total_backward_count 420970 real_backward_count 43679  10.376%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.690529/  1.198937, val:  60.00%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9274%\n",
      "layer   2  Sparsity: 74.6591%\n",
      "layer   3  Sparsity: 66.0712%\n",
      "total_backward_count 430760 real_backward_count 44529  10.337%\n",
      "fc layer 1 self.abs_max_out: 6957.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.674446/  1.169383, val:  67.92%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9248%\n",
      "layer   2  Sparsity: 75.1363%\n",
      "layer   3  Sparsity: 65.6321%\n",
      "total_backward_count 440550 real_backward_count 45433  10.313%\n",
      "lif layer 2 self.abs_max_v: 5453.5\n",
      "lif layer 2 self.abs_max_v: 5551.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.660222/  1.149893, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 74.9264%\n",
      "layer   3  Sparsity: 65.1758%\n",
      "total_backward_count 450340 real_backward_count 46274  10.275%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.654024/  1.254300, val:  61.67%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 75.0283%\n",
      "layer   3  Sparsity: 65.4543%\n",
      "total_backward_count 460130 real_backward_count 47148  10.247%\n",
      "lif layer 1 self.abs_max_v: 11636.5\n",
      "lif layer 1 self.abs_max_v: 11748.5\n",
      "lif layer 1 self.abs_max_v: 11868.0\n",
      "lif layer 1 self.abs_max_v: 11934.5\n",
      "fc layer 1 self.abs_max_out: 7172.0\n",
      "fc layer 1 self.abs_max_out: 7499.0\n",
      "lif layer 1 self.abs_max_v: 12303.5\n",
      "lif layer 1 self.abs_max_v: 12435.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.645728/  1.424306, val:  44.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 74.7262%\n",
      "layer   3  Sparsity: 65.8403%\n",
      "total_backward_count 469920 real_backward_count 48057  10.227%\n",
      "fc layer 2 self.abs_max_out: 3295.0\n",
      "fc layer 1 self.abs_max_out: 7624.0\n",
      "lif layer 1 self.abs_max_v: 12513.5\n",
      "lif layer 1 self.abs_max_v: 12671.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.646413/  1.236188, val:  57.92%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9425%\n",
      "layer   2  Sparsity: 74.4285%\n",
      "layer   3  Sparsity: 65.8957%\n",
      "total_backward_count 479710 real_backward_count 48932  10.200%\n",
      "lif layer 2 self.abs_max_v: 5818.0\n",
      "lif layer 2 self.abs_max_v: 5820.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.620769/  1.179511, val:  64.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9284%\n",
      "layer   2  Sparsity: 75.1334%\n",
      "layer   3  Sparsity: 65.7239%\n",
      "total_backward_count 489500 real_backward_count 49795  10.173%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.623346/  1.234532, val:  55.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 75.3403%\n",
      "layer   3  Sparsity: 65.7575%\n",
      "total_backward_count 499290 real_backward_count 50651  10.145%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.595065/  1.121074, val:  66.25%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 75.2664%\n",
      "layer   3  Sparsity: 66.9679%\n",
      "total_backward_count 509080 real_backward_count 51438  10.104%\n",
      "fc layer 3 self.abs_max_out: 1737.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.599957/  1.153777, val:  68.33%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9299%\n",
      "layer   2  Sparsity: 75.0167%\n",
      "layer   3  Sparsity: 65.8993%\n",
      "total_backward_count 518870 real_backward_count 52292  10.078%\n",
      "fc layer 1 self.abs_max_out: 7765.0\n",
      "lif layer 1 self.abs_max_v: 12836.5\n",
      "lif layer 1 self.abs_max_v: 13020.5\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.603605/  1.106456, val:  64.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9044%\n",
      "layer   2  Sparsity: 74.8189%\n",
      "layer   3  Sparsity: 65.5516%\n",
      "total_backward_count 528660 real_backward_count 53105  10.045%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.582265/  1.141922, val:  67.08%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9200%\n",
      "layer   2  Sparsity: 74.9628%\n",
      "layer   3  Sparsity: 65.3256%\n",
      "total_backward_count 538450 real_backward_count 53938  10.017%\n",
      "fc layer 3 self.abs_max_out: 1742.0\n",
      "fc layer 3 self.abs_max_out: 1770.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.597229/  1.111953, val:  68.75%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 87.9145%\n",
      "layer   2  Sparsity: 75.5657%\n",
      "layer   3  Sparsity: 65.5277%\n",
      "total_backward_count 548240 real_backward_count 54809   9.997%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.585341/  1.219039, val:  60.00%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 75.4631%\n",
      "layer   3  Sparsity: 66.0299%\n",
      "total_backward_count 558030 real_backward_count 55634   9.970%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.602495/  1.178876, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9417%\n",
      "layer   2  Sparsity: 74.7836%\n",
      "layer   3  Sparsity: 66.4190%\n",
      "total_backward_count 567820 real_backward_count 56398   9.932%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.551582/  1.108135, val:  67.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 75.3760%\n",
      "layer   3  Sparsity: 65.2463%\n",
      "total_backward_count 577610 real_backward_count 57142   9.893%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.546483/  1.164804, val:  61.67%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 75.4623%\n",
      "layer   3  Sparsity: 65.0466%\n",
      "total_backward_count 587400 real_backward_count 57898   9.857%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.567575/  1.176442, val:  54.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9277%\n",
      "layer   2  Sparsity: 75.6477%\n",
      "layer   3  Sparsity: 65.2211%\n",
      "total_backward_count 597190 real_backward_count 58681   9.826%\n",
      "lif layer 1 self.abs_max_v: 13199.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.558078/  1.113009, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9349%\n",
      "layer   2  Sparsity: 75.8749%\n",
      "layer   3  Sparsity: 65.2279%\n",
      "total_backward_count 606980 real_backward_count 59408   9.787%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.535156/  1.103731, val:  60.42%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8898%\n",
      "layer   2  Sparsity: 75.6182%\n",
      "layer   3  Sparsity: 65.1747%\n",
      "total_backward_count 616770 real_backward_count 60192   9.759%\n",
      "lif layer 1 self.abs_max_v: 13316.0\n",
      "fc layer 1 self.abs_max_out: 7931.0\n",
      "fc layer 1 self.abs_max_out: 8144.0\n",
      "lif layer 1 self.abs_max_v: 13670.0\n",
      "lif layer 1 self.abs_max_v: 14155.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.513833/  1.118803, val:  60.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 75.7747%\n",
      "layer   3  Sparsity: 64.5507%\n",
      "total_backward_count 626560 real_backward_count 60890   9.718%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.508015/  1.086835, val:  65.42%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9546%\n",
      "layer   2  Sparsity: 75.6836%\n",
      "layer   3  Sparsity: 65.2562%\n",
      "total_backward_count 636350 real_backward_count 61642   9.687%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.517757/  1.175030, val:  68.75%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9114%\n",
      "layer   2  Sparsity: 76.1359%\n",
      "layer   3  Sparsity: 65.6244%\n",
      "total_backward_count 646140 real_backward_count 62361   9.651%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.534236/  1.090375, val:  72.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 75.8000%\n",
      "layer   3  Sparsity: 65.2959%\n",
      "total_backward_count 655930 real_backward_count 63094   9.619%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.513371/  1.182848, val:  60.00%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8998%\n",
      "layer   2  Sparsity: 75.3905%\n",
      "layer   3  Sparsity: 64.3164%\n",
      "total_backward_count 665720 real_backward_count 63765   9.578%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.515928/  1.076745, val:  70.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9334%\n",
      "layer   2  Sparsity: 75.5853%\n",
      "layer   3  Sparsity: 63.4356%\n",
      "total_backward_count 675510 real_backward_count 64479   9.545%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.489442/  1.231262, val:  58.33%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9102%\n",
      "layer   2  Sparsity: 75.7768%\n",
      "layer   3  Sparsity: 64.0893%\n",
      "total_backward_count 685300 real_backward_count 65146   9.506%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.517362/  1.110403, val:  73.33%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 75.8064%\n",
      "layer   3  Sparsity: 65.3991%\n",
      "total_backward_count 695090 real_backward_count 65879   9.478%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.488358/  1.077760, val:  61.25%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9232%\n",
      "layer   2  Sparsity: 75.9604%\n",
      "layer   3  Sparsity: 64.4541%\n",
      "total_backward_count 704880 real_backward_count 66599   9.448%\n",
      "fc layer 3 self.abs_max_out: 1794.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.488347/  1.050075, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9288%\n",
      "layer   2  Sparsity: 75.6495%\n",
      "layer   3  Sparsity: 64.2155%\n",
      "total_backward_count 714670 real_backward_count 67303   9.417%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.480173/  1.251457, val:  63.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8941%\n",
      "layer   2  Sparsity: 75.5610%\n",
      "layer   3  Sparsity: 63.2273%\n",
      "total_backward_count 724460 real_backward_count 67962   9.381%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.477218/  1.134115, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8986%\n",
      "layer   2  Sparsity: 75.4000%\n",
      "layer   3  Sparsity: 64.6121%\n",
      "total_backward_count 734250 real_backward_count 68653   9.350%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.478805/  1.025715, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8861%\n",
      "layer   2  Sparsity: 75.8377%\n",
      "layer   3  Sparsity: 63.9645%\n",
      "total_backward_count 744040 real_backward_count 69305   9.315%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.471057/  1.139869, val:  63.33%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9370%\n",
      "layer   2  Sparsity: 75.6342%\n",
      "layer   3  Sparsity: 64.0504%\n",
      "total_backward_count 753830 real_backward_count 69997   9.286%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.469411/  1.136890, val:  61.25%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 75.8416%\n",
      "layer   3  Sparsity: 63.2763%\n",
      "total_backward_count 763620 real_backward_count 70660   9.253%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.451283/  1.113119, val:  66.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9424%\n",
      "layer   2  Sparsity: 76.3037%\n",
      "layer   3  Sparsity: 63.0462%\n",
      "total_backward_count 773410 real_backward_count 71327   9.222%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.437457/  1.077026, val:  70.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9021%\n",
      "layer   2  Sparsity: 76.1987%\n",
      "layer   3  Sparsity: 64.2328%\n",
      "total_backward_count 783200 real_backward_count 71958   9.188%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.467150/  1.012713, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9190%\n",
      "layer   2  Sparsity: 76.5749%\n",
      "layer   3  Sparsity: 64.7817%\n",
      "total_backward_count 792990 real_backward_count 72616   9.157%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.447225/  1.164085, val:  60.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 76.3560%\n",
      "layer   3  Sparsity: 65.2144%\n",
      "total_backward_count 802780 real_backward_count 73257   9.125%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.455099/  1.072050, val:  72.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9199%\n",
      "layer   2  Sparsity: 76.2025%\n",
      "layer   3  Sparsity: 64.9469%\n",
      "total_backward_count 812570 real_backward_count 73871   9.091%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.455171/  1.177982, val:  59.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9220%\n",
      "layer   2  Sparsity: 75.9794%\n",
      "layer   3  Sparsity: 64.3151%\n",
      "total_backward_count 822360 real_backward_count 74537   9.064%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.447830/  1.047071, val:  72.92%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 76.0863%\n",
      "layer   3  Sparsity: 64.4317%\n",
      "total_backward_count 832150 real_backward_count 75189   9.036%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.439940/  1.023888, val:  76.25%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9449%\n",
      "layer   2  Sparsity: 76.1977%\n",
      "layer   3  Sparsity: 64.4878%\n",
      "total_backward_count 841940 real_backward_count 75857   9.010%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.449820/  1.112473, val:  62.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8963%\n",
      "layer   2  Sparsity: 76.3630%\n",
      "layer   3  Sparsity: 64.2561%\n",
      "total_backward_count 851730 real_backward_count 76514   8.983%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.436245/  1.053147, val:  71.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9393%\n",
      "layer   2  Sparsity: 76.4240%\n",
      "layer   3  Sparsity: 64.6315%\n",
      "total_backward_count 861520 real_backward_count 77186   8.959%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.414058/  1.093290, val:  64.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 76.4356%\n",
      "layer   3  Sparsity: 64.2413%\n",
      "total_backward_count 871310 real_backward_count 77816   8.931%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.428040/  1.034479, val:  67.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9218%\n",
      "layer   2  Sparsity: 76.2588%\n",
      "layer   3  Sparsity: 63.9438%\n",
      "total_backward_count 881100 real_backward_count 78454   8.904%\n",
      "fc layer 1 self.abs_max_out: 8274.0\n",
      "lif layer 1 self.abs_max_v: 14270.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.427939/  1.094108, val:  72.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9225%\n",
      "layer   2  Sparsity: 75.9168%\n",
      "layer   3  Sparsity: 64.1190%\n",
      "total_backward_count 890890 real_backward_count 79066   8.875%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.414351/  1.034128, val:  76.67%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8826%\n",
      "layer   2  Sparsity: 75.7671%\n",
      "layer   3  Sparsity: 64.4956%\n",
      "total_backward_count 900680 real_backward_count 79666   8.845%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.405024/  0.951074, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 76.2644%\n",
      "layer   3  Sparsity: 64.2684%\n",
      "total_backward_count 910470 real_backward_count 80274   8.817%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.411767/  1.069852, val:  68.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 76.5448%\n",
      "layer   3  Sparsity: 64.9030%\n",
      "total_backward_count 920260 real_backward_count 80888   8.790%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.419879/  1.212407, val:  56.25%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 76.3619%\n",
      "layer   3  Sparsity: 64.9694%\n",
      "total_backward_count 930050 real_backward_count 81495   8.762%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.415939/  1.026421, val:  69.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8824%\n",
      "layer   2  Sparsity: 76.5038%\n",
      "layer   3  Sparsity: 65.5066%\n",
      "total_backward_count 939840 real_backward_count 82097   8.735%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.402775/  1.037617, val:  68.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8978%\n",
      "layer   2  Sparsity: 76.5957%\n",
      "layer   3  Sparsity: 65.5261%\n",
      "total_backward_count 949630 real_backward_count 82677   8.706%\n",
      "fc layer 3 self.abs_max_out: 1795.0\n",
      "fc layer 3 self.abs_max_out: 1818.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.403345/  1.013380, val:  72.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9068%\n",
      "layer   2  Sparsity: 76.2772%\n",
      "layer   3  Sparsity: 65.3061%\n",
      "total_backward_count 959420 real_backward_count 83223   8.674%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.398357/  1.047657, val:  72.08%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8816%\n",
      "layer   2  Sparsity: 76.1598%\n",
      "layer   3  Sparsity: 65.3782%\n",
      "total_backward_count 969210 real_backward_count 83775   8.644%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.403256/  1.063953, val:  67.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 76.3420%\n",
      "layer   3  Sparsity: 65.8991%\n",
      "total_backward_count 979000 real_backward_count 84342   8.615%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.416056/  1.000357, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9453%\n",
      "layer   2  Sparsity: 76.5944%\n",
      "layer   3  Sparsity: 65.3261%\n",
      "total_backward_count 988790 real_backward_count 84880   8.584%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.420131/  0.970484, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9125%\n",
      "layer   2  Sparsity: 76.1845%\n",
      "layer   3  Sparsity: 65.1137%\n",
      "total_backward_count 998580 real_backward_count 85489   8.561%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.414460/  0.943842, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 76.0578%\n",
      "layer   3  Sparsity: 63.9346%\n",
      "total_backward_count 1008370 real_backward_count 86071   8.536%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.393295/  1.018693, val:  70.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9054%\n",
      "layer   2  Sparsity: 76.1358%\n",
      "layer   3  Sparsity: 64.7339%\n",
      "total_backward_count 1018160 real_backward_count 86590   8.505%\n",
      "lif layer 1 self.abs_max_v: 14290.5\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.381973/  0.981187, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9300%\n",
      "layer   2  Sparsity: 76.0215%\n",
      "layer   3  Sparsity: 64.8256%\n",
      "total_backward_count 1027950 real_backward_count 87112   8.474%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.389036/  0.993723, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9371%\n",
      "layer   2  Sparsity: 76.2720%\n",
      "layer   3  Sparsity: 65.1493%\n",
      "total_backward_count 1037740 real_backward_count 87623   8.444%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.387008/  0.993595, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 76.2534%\n",
      "layer   3  Sparsity: 65.0550%\n",
      "total_backward_count 1047530 real_backward_count 88155   8.416%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.409373/  1.021270, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 75.8678%\n",
      "layer   3  Sparsity: 65.2767%\n",
      "total_backward_count 1057320 real_backward_count 88728   8.392%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.395587/  0.953219, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8958%\n",
      "layer   2  Sparsity: 76.2037%\n",
      "layer   3  Sparsity: 65.1765%\n",
      "total_backward_count 1067110 real_backward_count 89279   8.366%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.394434/  1.001485, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9002%\n",
      "layer   2  Sparsity: 76.3974%\n",
      "layer   3  Sparsity: 65.7774%\n",
      "total_backward_count 1076900 real_backward_count 89837   8.342%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.396532/  1.020363, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9488%\n",
      "layer   2  Sparsity: 76.4279%\n",
      "layer   3  Sparsity: 66.1644%\n",
      "total_backward_count 1086690 real_backward_count 90385   8.317%\n",
      "lif layer 1 self.abs_max_v: 14291.5\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.395793/  1.049227, val:  66.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8934%\n",
      "layer   2  Sparsity: 76.2460%\n",
      "layer   3  Sparsity: 65.4798%\n",
      "total_backward_count 1096480 real_backward_count 90878   8.288%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.370530/  0.960754, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9416%\n",
      "layer   2  Sparsity: 76.2564%\n",
      "layer   3  Sparsity: 65.4994%\n",
      "total_backward_count 1106270 real_backward_count 91340   8.257%\n",
      "lif layer 1 self.abs_max_v: 14404.5\n",
      "lif layer 1 self.abs_max_v: 14498.5\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.365203/  0.992823, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9226%\n",
      "layer   2  Sparsity: 76.5448%\n",
      "layer   3  Sparsity: 66.0505%\n",
      "total_backward_count 1116060 real_backward_count 91832   8.228%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.369264/  1.033161, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 76.3628%\n",
      "layer   3  Sparsity: 66.2324%\n",
      "total_backward_count 1125850 real_backward_count 92339   8.202%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.363185/  1.022520, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8946%\n",
      "layer   2  Sparsity: 76.1749%\n",
      "layer   3  Sparsity: 66.5987%\n",
      "total_backward_count 1135640 real_backward_count 92878   8.178%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.375478/  1.036625, val:  68.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9046%\n",
      "layer   2  Sparsity: 76.0041%\n",
      "layer   3  Sparsity: 66.6006%\n",
      "total_backward_count 1145430 real_backward_count 93387   8.153%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.370536/  0.956201, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 75.8210%\n",
      "layer   3  Sparsity: 65.8484%\n",
      "total_backward_count 1155220 real_backward_count 93924   8.130%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.351416/  1.046087, val:  66.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9164%\n",
      "layer   2  Sparsity: 76.1670%\n",
      "layer   3  Sparsity: 65.4703%\n",
      "total_backward_count 1165010 real_backward_count 94431   8.106%\n",
      "fc layer 1 self.abs_max_out: 8353.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.341496/  0.940761, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9637%\n",
      "layer   2  Sparsity: 76.4023%\n",
      "layer   3  Sparsity: 65.1977%\n",
      "total_backward_count 1174800 real_backward_count 94919   8.080%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.349816/  1.046621, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 76.2206%\n",
      "layer   3  Sparsity: 64.4439%\n",
      "total_backward_count 1184590 real_backward_count 95402   8.054%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.332260/  1.039379, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9270%\n",
      "layer   2  Sparsity: 76.1582%\n",
      "layer   3  Sparsity: 64.4192%\n",
      "total_backward_count 1194380 real_backward_count 95839   8.024%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.354211/  0.950965, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9389%\n",
      "layer   2  Sparsity: 75.8791%\n",
      "layer   3  Sparsity: 64.9191%\n",
      "total_backward_count 1204170 real_backward_count 96381   8.004%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.360346/  1.019417, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9110%\n",
      "layer   2  Sparsity: 75.5444%\n",
      "layer   3  Sparsity: 65.7471%\n",
      "total_backward_count 1213960 real_backward_count 96854   7.978%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.354059/  0.947068, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 75.8669%\n",
      "layer   3  Sparsity: 66.2389%\n",
      "total_backward_count 1223750 real_backward_count 97359   7.956%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.347619/  0.972779, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9451%\n",
      "layer   2  Sparsity: 76.0655%\n",
      "layer   3  Sparsity: 66.3747%\n",
      "total_backward_count 1233540 real_backward_count 97828   7.931%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.343064/  0.924215, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9661%\n",
      "layer   2  Sparsity: 75.7802%\n",
      "layer   3  Sparsity: 66.3790%\n",
      "total_backward_count 1243330 real_backward_count 98297   7.906%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.342837/  0.944943, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9169%\n",
      "layer   2  Sparsity: 75.5571%\n",
      "layer   3  Sparsity: 65.3616%\n",
      "total_backward_count 1253120 real_backward_count 98760   7.881%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.356533/  0.982244, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9431%\n",
      "layer   2  Sparsity: 75.6947%\n",
      "layer   3  Sparsity: 64.8376%\n",
      "total_backward_count 1262910 real_backward_count 99243   7.858%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.329435/  0.905887, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9023%\n",
      "layer   2  Sparsity: 75.7027%\n",
      "layer   3  Sparsity: 65.1234%\n",
      "total_backward_count 1272700 real_backward_count 99691   7.833%\n",
      "fc layer 3 self.abs_max_out: 1841.0\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.341173/  0.983639, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9455%\n",
      "layer   2  Sparsity: 75.6773%\n",
      "layer   3  Sparsity: 64.8635%\n",
      "total_backward_count 1282490 real_backward_count 100162   7.810%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.343175/  0.992687, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 76.2303%\n",
      "layer   3  Sparsity: 64.8751%\n",
      "total_backward_count 1292280 real_backward_count 100637   7.788%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.344910/  1.005610, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9112%\n",
      "layer   2  Sparsity: 76.5982%\n",
      "layer   3  Sparsity: 65.2241%\n",
      "total_backward_count 1302070 real_backward_count 101102   7.765%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.345846/  1.036030, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 76.9129%\n",
      "layer   3  Sparsity: 65.1797%\n",
      "total_backward_count 1311860 real_backward_count 101574   7.743%\n",
      "fc layer 1 self.abs_max_out: 8355.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.359497/  1.068194, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9441%\n",
      "layer   2  Sparsity: 77.1964%\n",
      "layer   3  Sparsity: 65.4733%\n",
      "total_backward_count 1321650 real_backward_count 102060   7.722%\n",
      "fc layer 1 self.abs_max_out: 8521.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.342232/  0.994800, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9201%\n",
      "layer   2  Sparsity: 77.0954%\n",
      "layer   3  Sparsity: 65.8433%\n",
      "total_backward_count 1331440 real_backward_count 102529   7.701%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.326466/  0.940712, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9308%\n",
      "layer   2  Sparsity: 77.0775%\n",
      "layer   3  Sparsity: 65.6888%\n",
      "total_backward_count 1341230 real_backward_count 102933   7.675%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.312381/  0.965862, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 76.9695%\n",
      "layer   3  Sparsity: 66.3148%\n",
      "total_backward_count 1351020 real_backward_count 103377   7.652%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.333331/  1.010257, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 77.0338%\n",
      "layer   3  Sparsity: 65.5508%\n",
      "total_backward_count 1360810 real_backward_count 103821   7.629%\n",
      "lif layer 1 self.abs_max_v: 14710.5\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.342502/  0.954703, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9294%\n",
      "layer   2  Sparsity: 76.9265%\n",
      "layer   3  Sparsity: 65.9785%\n",
      "total_backward_count 1370600 real_backward_count 104224   7.604%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.340321/  0.969719, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9078%\n",
      "layer   2  Sparsity: 76.5819%\n",
      "layer   3  Sparsity: 66.3158%\n",
      "total_backward_count 1380390 real_backward_count 104655   7.582%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.334052/  0.930022, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9458%\n",
      "layer   2  Sparsity: 76.5025%\n",
      "layer   3  Sparsity: 66.3664%\n",
      "total_backward_count 1390180 real_backward_count 105065   7.558%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.342591/  0.999205, val:  71.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9376%\n",
      "layer   2  Sparsity: 76.4683%\n",
      "layer   3  Sparsity: 66.1468%\n",
      "total_backward_count 1399970 real_backward_count 105488   7.535%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.330129/  0.959240, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8808%\n",
      "layer   2  Sparsity: 76.7146%\n",
      "layer   3  Sparsity: 65.8089%\n",
      "total_backward_count 1409760 real_backward_count 105876   7.510%\n",
      "fc layer 1 self.abs_max_out: 8695.0\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.323612/  0.992473, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9289%\n",
      "layer   2  Sparsity: 76.8305%\n",
      "layer   3  Sparsity: 65.9498%\n",
      "total_backward_count 1419550 real_backward_count 106306   7.489%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.312690/  1.051106, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 76.9536%\n",
      "layer   3  Sparsity: 65.7602%\n",
      "total_backward_count 1429340 real_backward_count 106700   7.465%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.314002/  1.026582, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 76.9135%\n",
      "layer   3  Sparsity: 65.7600%\n",
      "total_backward_count 1439130 real_backward_count 107135   7.444%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.299734/  0.935265, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9180%\n",
      "layer   2  Sparsity: 77.0884%\n",
      "layer   3  Sparsity: 65.8194%\n",
      "total_backward_count 1448920 real_backward_count 107538   7.422%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.329056/  0.958693, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9117%\n",
      "layer   2  Sparsity: 76.8974%\n",
      "layer   3  Sparsity: 66.4943%\n",
      "total_backward_count 1458710 real_backward_count 107973   7.402%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.313057/  0.979918, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 76.9437%\n",
      "layer   3  Sparsity: 66.6156%\n",
      "total_backward_count 1468500 real_backward_count 108373   7.380%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.329397/  1.020864, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9235%\n",
      "layer   2  Sparsity: 77.1884%\n",
      "layer   3  Sparsity: 67.1034%\n",
      "total_backward_count 1478290 real_backward_count 108813   7.361%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.322380/  0.970261, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 77.0667%\n",
      "layer   3  Sparsity: 66.8908%\n",
      "total_backward_count 1488080 real_backward_count 109290   7.344%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.299916/  0.939793, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 77.1667%\n",
      "layer   3  Sparsity: 67.1297%\n",
      "total_backward_count 1497870 real_backward_count 109690   7.323%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.317024/  0.971384, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9245%\n",
      "layer   2  Sparsity: 76.9537%\n",
      "layer   3  Sparsity: 67.5348%\n",
      "total_backward_count 1507660 real_backward_count 110142   7.305%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.326781/  1.031012, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9502%\n",
      "layer   2  Sparsity: 77.0055%\n",
      "layer   3  Sparsity: 67.9770%\n",
      "total_backward_count 1517450 real_backward_count 110537   7.284%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.313252/  0.973171, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8933%\n",
      "layer   2  Sparsity: 76.7477%\n",
      "layer   3  Sparsity: 67.1180%\n",
      "total_backward_count 1527240 real_backward_count 110916   7.263%\n",
      "fc layer 3 self.abs_max_out: 1847.0\n",
      "fc layer 3 self.abs_max_out: 1867.0\n",
      "fc layer 3 self.abs_max_out: 1893.0\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.306308/  1.045755, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 76.8159%\n",
      "layer   3  Sparsity: 66.5461%\n",
      "total_backward_count 1537030 real_backward_count 111324   7.243%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.303061/  0.952903, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8926%\n",
      "layer   2  Sparsity: 76.8902%\n",
      "layer   3  Sparsity: 65.8342%\n",
      "total_backward_count 1546820 real_backward_count 111710   7.222%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.300370/  0.960678, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 76.5445%\n",
      "layer   3  Sparsity: 65.2518%\n",
      "total_backward_count 1556610 real_backward_count 112101   7.202%\n",
      "lif layer 1 self.abs_max_v: 14789.5\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.302870/  0.933402, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9141%\n",
      "layer   2  Sparsity: 76.2970%\n",
      "layer   3  Sparsity: 65.2236%\n",
      "total_backward_count 1566400 real_backward_count 112474   7.180%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.304547/  0.936202, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 76.2330%\n",
      "layer   3  Sparsity: 65.7570%\n",
      "total_backward_count 1576190 real_backward_count 112827   7.158%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.300474/  0.957553, val:  74.17%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8972%\n",
      "layer   2  Sparsity: 76.6467%\n",
      "layer   3  Sparsity: 66.6235%\n",
      "total_backward_count 1585980 real_backward_count 113198   7.137%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.305146/  0.969944, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9236%\n",
      "layer   2  Sparsity: 76.8190%\n",
      "layer   3  Sparsity: 66.3787%\n",
      "total_backward_count 1595770 real_backward_count 113605   7.119%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.313739/  1.062955, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9259%\n",
      "layer   2  Sparsity: 76.8842%\n",
      "layer   3  Sparsity: 66.2825%\n",
      "total_backward_count 1605560 real_backward_count 114020   7.102%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.304167/  0.941718, val:  77.92%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8838%\n",
      "layer   2  Sparsity: 76.8275%\n",
      "layer   3  Sparsity: 66.3634%\n",
      "total_backward_count 1615350 real_backward_count 114386   7.081%\n",
      "lif layer 1 self.abs_max_v: 14888.5\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.296389/  0.931106, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8980%\n",
      "layer   2  Sparsity: 76.9195%\n",
      "layer   3  Sparsity: 66.4890%\n",
      "total_backward_count 1625140 real_backward_count 114751   7.061%\n",
      "lif layer 1 self.abs_max_v: 15086.0\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.273866/  0.950254, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 76.8783%\n",
      "layer   3  Sparsity: 66.9424%\n",
      "total_backward_count 1634930 real_backward_count 115097   7.040%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.301736/  1.000050, val:  74.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9183%\n",
      "layer   2  Sparsity: 76.7341%\n",
      "layer   3  Sparsity: 66.7428%\n",
      "total_backward_count 1644720 real_backward_count 115489   7.022%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.301711/  0.960006, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9592%\n",
      "layer   2  Sparsity: 76.8945%\n",
      "layer   3  Sparsity: 65.9479%\n",
      "total_backward_count 1654510 real_backward_count 115897   7.005%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.296367/  1.006509, val:  71.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9400%\n",
      "layer   2  Sparsity: 77.0056%\n",
      "layer   3  Sparsity: 66.3859%\n",
      "total_backward_count 1664300 real_backward_count 116278   6.987%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.283024/  0.931848, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9351%\n",
      "layer   2  Sparsity: 76.9871%\n",
      "layer   3  Sparsity: 66.9514%\n",
      "total_backward_count 1674090 real_backward_count 116573   6.963%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.291204/  1.123142, val:  65.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9583%\n",
      "layer   2  Sparsity: 76.8778%\n",
      "layer   3  Sparsity: 66.7138%\n",
      "total_backward_count 1683880 real_backward_count 116915   6.943%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.298132/  0.942319, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9464%\n",
      "layer   2  Sparsity: 76.9806%\n",
      "layer   3  Sparsity: 66.2678%\n",
      "total_backward_count 1693670 real_backward_count 117253   6.923%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.301271/  1.019710, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 76.9279%\n",
      "layer   3  Sparsity: 66.7139%\n",
      "total_backward_count 1703460 real_backward_count 117614   6.904%\n",
      "lif layer 1 self.abs_max_v: 15319.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.306415/  1.013469, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9328%\n",
      "layer   2  Sparsity: 77.0300%\n",
      "layer   3  Sparsity: 66.4416%\n",
      "total_backward_count 1713250 real_backward_count 117968   6.886%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.310684/  0.917644, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8885%\n",
      "layer   2  Sparsity: 76.9360%\n",
      "layer   3  Sparsity: 67.1009%\n",
      "total_backward_count 1723040 real_backward_count 118300   6.866%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.308032/  0.932243, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9290%\n",
      "layer   2  Sparsity: 76.8073%\n",
      "layer   3  Sparsity: 66.9624%\n",
      "total_backward_count 1732830 real_backward_count 118659   6.848%\n",
      "fc layer 3 self.abs_max_out: 1910.0\n",
      "fc layer 3 self.abs_max_out: 1954.0\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.300492/  0.962465, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9033%\n",
      "layer   2  Sparsity: 76.7492%\n",
      "layer   3  Sparsity: 67.0060%\n",
      "total_backward_count 1742620 real_backward_count 118998   6.829%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.309006/  0.939551, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 76.6089%\n",
      "layer   3  Sparsity: 66.7664%\n",
      "total_backward_count 1752410 real_backward_count 119348   6.811%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.302764/  0.924182, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9329%\n",
      "layer   2  Sparsity: 76.4853%\n",
      "layer   3  Sparsity: 66.5491%\n",
      "total_backward_count 1762200 real_backward_count 119696   6.792%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.290863/  0.912436, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9406%\n",
      "layer   2  Sparsity: 76.6238%\n",
      "layer   3  Sparsity: 67.1448%\n",
      "total_backward_count 1771990 real_backward_count 120031   6.774%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.285334/  0.935654, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9135%\n",
      "layer   2  Sparsity: 76.7407%\n",
      "layer   3  Sparsity: 66.7895%\n",
      "total_backward_count 1781780 real_backward_count 120340   6.754%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.291245/  0.945411, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9184%\n",
      "layer   2  Sparsity: 76.7573%\n",
      "layer   3  Sparsity: 66.4601%\n",
      "total_backward_count 1791570 real_backward_count 120697   6.737%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.295971/  1.126925, val:  68.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 76.8367%\n",
      "layer   3  Sparsity: 66.2893%\n",
      "total_backward_count 1801360 real_backward_count 121032   6.719%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.309070/  1.048856, val:  74.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 76.6585%\n",
      "layer   3  Sparsity: 66.7302%\n",
      "total_backward_count 1811150 real_backward_count 121367   6.701%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.300240/  0.986874, val:  76.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 76.6009%\n",
      "layer   3  Sparsity: 67.2689%\n",
      "total_backward_count 1820940 real_backward_count 121725   6.685%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.296317/  0.992137, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9224%\n",
      "layer   2  Sparsity: 76.7477%\n",
      "layer   3  Sparsity: 67.5668%\n",
      "total_backward_count 1830730 real_backward_count 122057   6.667%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.280667/  0.973028, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 76.8234%\n",
      "layer   3  Sparsity: 67.0655%\n",
      "total_backward_count 1840520 real_backward_count 122363   6.648%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.276519/  0.955141, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9251%\n",
      "layer   2  Sparsity: 76.8372%\n",
      "layer   3  Sparsity: 66.9841%\n",
      "total_backward_count 1850310 real_backward_count 122654   6.629%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.280238/  0.935261, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9298%\n",
      "layer   2  Sparsity: 76.8570%\n",
      "layer   3  Sparsity: 66.9343%\n",
      "total_backward_count 1860100 real_backward_count 122964   6.611%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.282947/  0.958413, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9260%\n",
      "layer   2  Sparsity: 76.7950%\n",
      "layer   3  Sparsity: 66.6879%\n",
      "total_backward_count 1869890 real_backward_count 123277   6.593%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.291589/  0.943901, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 76.4098%\n",
      "layer   3  Sparsity: 66.9480%\n",
      "total_backward_count 1879680 real_backward_count 123609   6.576%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.269280/  0.909240, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 76.6282%\n",
      "layer   3  Sparsity: 68.0596%\n",
      "total_backward_count 1889470 real_backward_count 123895   6.557%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.270070/  0.915346, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9115%\n",
      "layer   2  Sparsity: 76.7665%\n",
      "layer   3  Sparsity: 67.3781%\n",
      "total_backward_count 1899260 real_backward_count 124203   6.540%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.276119/  0.952022, val:  74.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9172%\n",
      "layer   2  Sparsity: 76.6848%\n",
      "layer   3  Sparsity: 67.3842%\n",
      "total_backward_count 1909050 real_backward_count 124476   6.520%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.275228/  1.009483, val:  72.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9042%\n",
      "layer   2  Sparsity: 76.6587%\n",
      "layer   3  Sparsity: 67.6254%\n",
      "total_backward_count 1918840 real_backward_count 124766   6.502%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.277692/  0.975093, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9185%\n",
      "layer   2  Sparsity: 76.7002%\n",
      "layer   3  Sparsity: 67.4164%\n",
      "total_backward_count 1928630 real_backward_count 125068   6.485%\n",
      "lif layer 1 self.abs_max_v: 15378.5\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.274584/  1.001904, val:  77.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9420%\n",
      "layer   2  Sparsity: 76.4671%\n",
      "layer   3  Sparsity: 66.7918%\n",
      "total_backward_count 1938420 real_backward_count 125372   6.468%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.265513/  0.930624, val:  75.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9119%\n",
      "layer   2  Sparsity: 76.5283%\n",
      "layer   3  Sparsity: 67.1603%\n",
      "total_backward_count 1948210 real_backward_count 125658   6.450%\n",
      "fc layer 1 self.abs_max_out: 9003.0\n",
      "lif layer 1 self.abs_max_v: 15890.0\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.272666/  0.940963, val:  77.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9202%\n",
      "layer   2  Sparsity: 76.4096%\n",
      "layer   3  Sparsity: 67.4425%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d01315abd934760adc8121e7de40dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.27267</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>0.94096</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-85</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ibz11zh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ibz11zh</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_221159-6ibz11zh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 186b83va with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_022847-186b83va</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/186b83va' target=\"_blank\">icy-sweep-91</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/186b83va' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/186b83va</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251116_022856_249', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 945.0\n",
      "lif layer 1 self.abs_max_v: 945.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1349.0\n",
      "lif layer 2 self.abs_max_v: 1349.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "lif layer 1 self.abs_max_v: 1284.5\n",
      "fc layer 2 self.abs_max_out: 1767.0\n",
      "lif layer 2 self.abs_max_v: 2068.5\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 1433.0\n",
      "lif layer 2 self.abs_max_v: 2751.5\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 1 self.abs_max_out: 1280.0\n",
      "lif layer 1 self.abs_max_v: 1544.5\n",
      "lif layer 2 self.abs_max_v: 2929.0\n",
      "fc layer 1 self.abs_max_out: 1977.0\n",
      "lif layer 1 self.abs_max_v: 1977.0\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "lif layer 2 self.abs_max_v: 3185.5\n",
      "fc layer 1 self.abs_max_out: 2001.0\n",
      "lif layer 1 self.abs_max_v: 2001.0\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "fc layer 1 self.abs_max_out: 2324.0\n",
      "lif layer 1 self.abs_max_v: 2324.0\n",
      "fc layer 2 self.abs_max_out: 1949.0\n",
      "lif layer 2 self.abs_max_v: 3280.0\n",
      "fc layer 3 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 1991.0\n",
      "lif layer 2 self.abs_max_v: 3538.5\n",
      "fc layer 1 self.abs_max_out: 2376.0\n",
      "lif layer 1 self.abs_max_v: 2376.0\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "lif layer 1 self.abs_max_v: 2741.5\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 3024.0\n",
      "fc layer 1 self.abs_max_out: 2442.0\n",
      "lif layer 1 self.abs_max_v: 3045.5\n",
      "fc layer 1 self.abs_max_out: 3015.0\n",
      "fc layer 1 self.abs_max_out: 3352.0\n",
      "lif layer 1 self.abs_max_v: 3352.0\n",
      "lif layer 1 self.abs_max_v: 3746.0\n",
      "lif layer 1 self.abs_max_v: 3772.5\n",
      "fc layer 2 self.abs_max_out: 2415.0\n",
      "fc layer 1 self.abs_max_out: 4837.0\n",
      "lif layer 1 self.abs_max_v: 5861.5\n",
      "fc layer 1 self.abs_max_out: 4954.0\n",
      "lif layer 1 self.abs_max_v: 7122.0\n",
      "fc layer 3 self.abs_max_out: 1402.0\n",
      "fc layer 3 self.abs_max_out: 1494.0\n",
      "lif layer 2 self.abs_max_v: 3688.5\n",
      "fc layer 1 self.abs_max_out: 5839.0\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "lif layer 1 self.abs_max_v: 7625.0\n",
      "fc layer 2 self.abs_max_out: 2653.0\n",
      "lif layer 2 self.abs_max_v: 4202.0\n",
      "fc layer 2 self.abs_max_out: 2700.0\n",
      "lif layer 2 self.abs_max_v: 4569.5\n",
      "fc layer 2 self.abs_max_out: 2878.0\n",
      "fc layer 1 self.abs_max_out: 6193.0\n",
      "lif layer 1 self.abs_max_v: 7868.5\n",
      "lif layer 1 self.abs_max_v: 8604.5\n",
      "fc layer 1 self.abs_max_out: 6503.0\n",
      "fc layer 1 self.abs_max_out: 6993.0\n",
      "lif layer 1 self.abs_max_v: 8833.5\n",
      "fc layer 3 self.abs_max_out: 1646.0\n",
      "fc layer 2 self.abs_max_out: 2923.0\n",
      "fc layer 2 self.abs_max_out: 3434.0\n",
      "fc layer 2 self.abs_max_out: 3566.0\n",
      "lif layer 2 self.abs_max_v: 4615.0\n",
      "lif layer 1 self.abs_max_v: 8869.5\n",
      "lif layer 2 self.abs_max_v: 5189.5\n",
      "lif layer 1 self.abs_max_v: 9614.0\n",
      "lif layer 1 self.abs_max_v: 9713.5\n",
      "fc layer 2 self.abs_max_out: 3575.0\n",
      "lif layer 2 self.abs_max_v: 5280.5\n",
      "fc layer 2 self.abs_max_out: 3588.0\n",
      "fc layer 2 self.abs_max_out: 3663.0\n",
      "fc layer 2 self.abs_max_out: 3789.0\n",
      "fc layer 2 self.abs_max_out: 3823.0\n",
      "lif layer 2 self.abs_max_v: 5508.0\n",
      "lif layer 2 self.abs_max_v: 5835.0\n",
      "lif layer 2 self.abs_max_v: 5925.5\n",
      "lif layer 2 self.abs_max_v: 5941.5\n",
      "fc layer 2 self.abs_max_out: 3860.0\n",
      "fc layer 2 self.abs_max_out: 3966.0\n",
      "lif layer 2 self.abs_max_v: 6246.5\n",
      "lif layer 2 self.abs_max_v: 6373.5\n",
      "lif layer 2 self.abs_max_v: 6711.0\n",
      "fc layer 2 self.abs_max_out: 4003.0\n",
      "lif layer 2 self.abs_max_v: 7012.5\n",
      "lif layer 1 self.abs_max_v: 10007.0\n",
      "lif layer 1 self.abs_max_v: 10316.0\n",
      "fc layer 1 self.abs_max_out: 7068.0\n",
      "fc layer 3 self.abs_max_out: 1842.0\n",
      "lif layer 1 self.abs_max_v: 10362.5\n",
      "lif layer 1 self.abs_max_v: 10790.5\n",
      "lif layer 1 self.abs_max_v: 11132.0\n",
      "lif layer 1 self.abs_max_v: 11200.0\n",
      "lif layer 1 self.abs_max_v: 11699.0\n",
      "lif layer 1 self.abs_max_v: 12265.0\n",
      "fc layer 1 self.abs_max_out: 7535.0\n",
      "lif layer 1 self.abs_max_v: 12391.5\n",
      "lif layer 1 self.abs_max_v: 12585.0\n",
      "lif layer 1 self.abs_max_v: 13047.0\n",
      "fc layer 1 self.abs_max_out: 7805.0\n",
      "lif layer 1 self.abs_max_v: 13398.5\n",
      "fc layer 3 self.abs_max_out: 1902.0\n",
      "lif layer 1 self.abs_max_v: 13809.5\n",
      "fc layer 1 self.abs_max_out: 8875.0\n",
      "lif layer 1 self.abs_max_v: 15613.5\n",
      "lif layer 1 self.abs_max_v: 16160.0\n",
      "lif layer 1 self.abs_max_v: 16450.5\n",
      "lif layer 1 self.abs_max_v: 16644.5\n",
      "fc layer 3 self.abs_max_out: 1907.0\n",
      "fc layer 3 self.abs_max_out: 1967.0\n",
      "fc layer 3 self.abs_max_out: 1990.0\n",
      "fc layer 2 self.abs_max_out: 4023.0\n",
      "fc layer 2 self.abs_max_out: 4077.0\n",
      "fc layer 2 self.abs_max_out: 4162.0\n",
      "fc layer 3 self.abs_max_out: 2026.0\n",
      "fc layer 1 self.abs_max_out: 8911.0\n",
      "fc layer 3 self.abs_max_out: 2124.0\n",
      "fc layer 1 self.abs_max_out: 9056.0\n",
      "fc layer 3 self.abs_max_out: 2157.0\n",
      "fc layer 3 self.abs_max_out: 2335.0\n",
      "fc layer 3 self.abs_max_out: 2507.0\n",
      "fc layer 1 self.abs_max_out: 9450.0\n",
      "lif layer 1 self.abs_max_v: 17082.5\n",
      "lif layer 1 self.abs_max_v: 17372.5\n",
      "fc layer 1 self.abs_max_out: 9857.0\n",
      "lif layer 1 self.abs_max_v: 18261.5\n",
      "fc layer 1 self.abs_max_out: 9916.0\n",
      "lif layer 1 self.abs_max_v: 18321.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.542339/  1.966643, val:  26.67%, val_best:  26.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3599%\n",
      "layer   2  Sparsity: 68.3281%\n",
      "layer   3  Sparsity: 64.6008%\n",
      "total_backward_count 9790 real_backward_count 1234  12.605%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 9987.0\n",
      "fc layer 3 self.abs_max_out: 2706.0\n",
      "fc layer 3 self.abs_max_out: 2925.0\n",
      "fc layer 1 self.abs_max_out: 10011.0\n",
      "lif layer 2 self.abs_max_v: 7050.5\n",
      "fc layer 3 self.abs_max_out: 2959.0\n",
      "fc layer 1 self.abs_max_out: 10931.0\n",
      "lif layer 2 self.abs_max_v: 7426.5\n",
      "lif layer 2 self.abs_max_v: 7605.5\n",
      "fc layer 3 self.abs_max_out: 3124.0\n",
      "fc layer 3 self.abs_max_out: 3176.0\n",
      "fc layer 1 self.abs_max_out: 10963.0\n",
      "lif layer 1 self.abs_max_v: 19517.0\n",
      "fc layer 1 self.abs_max_out: 12095.0\n",
      "lif layer 1 self.abs_max_v: 21614.0\n",
      "fc layer 1 self.abs_max_out: 12607.0\n",
      "lif layer 1 self.abs_max_v: 22969.0\n",
      "fc layer 2 self.abs_max_out: 4245.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.358566/  1.821499, val:  41.25%, val_best:  41.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3392%\n",
      "layer   2  Sparsity: 71.5660%\n",
      "layer   3  Sparsity: 65.9429%\n",
      "total_backward_count 19580 real_backward_count 2434  12.431%\n",
      "lif layer 2 self.abs_max_v: 7627.0\n",
      "lif layer 2 self.abs_max_v: 7696.5\n",
      "fc layer 2 self.abs_max_out: 4293.0\n",
      "lif layer 2 self.abs_max_v: 8013.0\n",
      "lif layer 2 self.abs_max_v: 8163.5\n",
      "lif layer 2 self.abs_max_v: 8292.0\n",
      "fc layer 2 self.abs_max_out: 4348.0\n",
      "fc layer 2 self.abs_max_out: 4474.0\n",
      "fc layer 2 self.abs_max_out: 4719.0\n",
      "lif layer 2 self.abs_max_v: 8528.5\n",
      "fc layer 2 self.abs_max_out: 4960.0\n",
      "lif layer 2 self.abs_max_v: 8780.0\n",
      "fc layer 1 self.abs_max_out: 12796.0\n",
      "fc layer 1 self.abs_max_out: 12807.0\n",
      "lif layer 1 self.abs_max_v: 23273.5\n",
      "lif layer 1 self.abs_max_v: 24083.0\n",
      "lif layer 1 self.abs_max_v: 24455.5\n",
      "fc layer 1 self.abs_max_out: 13119.0\n",
      "lif layer 1 self.abs_max_v: 25347.0\n",
      "lif layer 1 self.abs_max_v: 25366.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.361285/  1.790996, val:  37.08%, val_best:  41.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 73.2751%\n",
      "layer   3  Sparsity: 67.9914%\n",
      "total_backward_count 29370 real_backward_count 3654  12.441%\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.373314/  1.828734, val:  43.33%, val_best:  43.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3457%\n",
      "layer   2  Sparsity: 73.9978%\n",
      "layer   3  Sparsity: 69.5311%\n",
      "total_backward_count 39160 real_backward_count 4874  12.446%\n",
      "fc layer 1 self.abs_max_out: 13601.0\n",
      "fc layer 3 self.abs_max_out: 3306.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.384007/  1.804488, val:  46.25%, val_best:  46.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 74.9012%\n",
      "layer   3  Sparsity: 72.2567%\n",
      "total_backward_count 48950 real_backward_count 6065  12.390%\n",
      "fc layer 1 self.abs_max_out: 13810.0\n",
      "lif layer 2 self.abs_max_v: 8862.5\n",
      "lif layer 2 self.abs_max_v: 8994.0\n",
      "lif layer 2 self.abs_max_v: 9028.5\n",
      "lif layer 2 self.abs_max_v: 9341.5\n",
      "lif layer 1 self.abs_max_v: 25955.0\n",
      "lif layer 1 self.abs_max_v: 26399.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.383877/  1.838733, val:  36.25%, val_best:  46.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 75.9481%\n",
      "layer   3  Sparsity: 73.2251%\n",
      "total_backward_count 58740 real_backward_count 7292  12.414%\n",
      "fc layer 1 self.abs_max_out: 13908.0\n",
      "fc layer 1 self.abs_max_out: 14811.0\n",
      "fc layer 1 self.abs_max_out: 14971.0\n",
      "lif layer 1 self.abs_max_v: 27293.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.410402/  1.798726, val:  45.42%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3125%\n",
      "layer   2  Sparsity: 76.8651%\n",
      "layer   3  Sparsity: 74.5873%\n",
      "total_backward_count 68530 real_backward_count 8515  12.425%\n",
      "fc layer 1 self.abs_max_out: 15487.0\n",
      "lif layer 1 self.abs_max_v: 27380.0\n",
      "lif layer 1 self.abs_max_v: 27512.0\n",
      "fc layer 1 self.abs_max_out: 15494.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.417698/  1.800568, val:  44.17%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3242%\n",
      "layer   2  Sparsity: 77.2901%\n",
      "layer   3  Sparsity: 76.1593%\n",
      "total_backward_count 78320 real_backward_count 9721  12.412%\n",
      "fc layer 1 self.abs_max_out: 15588.0\n",
      "lif layer 1 self.abs_max_v: 27782.5\n",
      "fc layer 1 self.abs_max_out: 15633.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.436511/  1.739632, val:  48.33%, val_best:  48.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 77.9018%\n",
      "layer   3  Sparsity: 77.2609%\n",
      "total_backward_count 88110 real_backward_count 10971  12.451%\n",
      "fc layer 1 self.abs_max_out: 15696.0\n",
      "lif layer 1 self.abs_max_v: 28568.5\n",
      "fc layer 1 self.abs_max_out: 15734.0\n",
      "lif layer 1 self.abs_max_v: 29785.5\n",
      "lif layer 1 self.abs_max_v: 29790.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.402845/  1.858307, val:  42.92%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 77.4991%\n",
      "layer   3  Sparsity: 75.9523%\n",
      "total_backward_count 97900 real_backward_count 12187  12.448%\n",
      "fc layer 2 self.abs_max_out: 4977.0\n",
      "fc layer 1 self.abs_max_out: 15823.0\n",
      "lif layer 1 self.abs_max_v: 29819.0\n",
      "fc layer 1 self.abs_max_out: 15900.0\n",
      "lif layer 1 self.abs_max_v: 30809.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.426387/  1.904177, val:  37.92%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3585%\n",
      "layer   2  Sparsity: 76.0971%\n",
      "layer   3  Sparsity: 76.4918%\n",
      "total_backward_count 107690 real_backward_count 13371  12.416%\n",
      "fc layer 2 self.abs_max_out: 5106.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.477676/  1.829177, val:  36.67%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 76.6968%\n",
      "layer   3  Sparsity: 78.1123%\n",
      "total_backward_count 117480 real_backward_count 14580  12.411%\n",
      "fc layer 1 self.abs_max_out: 16125.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.464819/  1.793653, val:  36.67%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3343%\n",
      "layer   2  Sparsity: 76.7666%\n",
      "layer   3  Sparsity: 78.7948%\n",
      "total_backward_count 127270 real_backward_count 15729  12.359%\n",
      "fc layer 1 self.abs_max_out: 16331.0\n",
      "lif layer 1 self.abs_max_v: 31001.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.479658/  1.910334, val:  32.08%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3433%\n",
      "layer   2  Sparsity: 76.7763%\n",
      "layer   3  Sparsity: 78.8831%\n",
      "total_backward_count 137060 real_backward_count 16916  12.342%\n",
      "lif layer 1 self.abs_max_v: 31057.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.469294/  1.848372, val:  41.25%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3454%\n",
      "layer   2  Sparsity: 77.6857%\n",
      "layer   3  Sparsity: 79.1910%\n",
      "total_backward_count 146850 real_backward_count 18119  12.338%\n",
      "fc layer 2 self.abs_max_out: 5111.0\n",
      "lif layer 1 self.abs_max_v: 31360.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.486274/  1.781066, val:  44.17%, val_best:  48.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3256%\n",
      "layer   2  Sparsity: 76.8687%\n",
      "layer   3  Sparsity: 78.9646%\n",
      "total_backward_count 156640 real_backward_count 19322  12.335%\n",
      "fc layer 2 self.abs_max_out: 5229.0\n",
      "fc layer 2 self.abs_max_out: 5287.0\n",
      "fc layer 2 self.abs_max_out: 5351.0\n",
      "lif layer 2 self.abs_max_v: 9570.5\n",
      "lif layer 2 self.abs_max_v: 9717.5\n",
      "fc layer 2 self.abs_max_out: 5462.0\n",
      "lif layer 2 self.abs_max_v: 10233.0\n",
      "lif layer 1 self.abs_max_v: 31506.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.427929/  1.793062, val:  50.42%, val_best:  50.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2795%\n",
      "layer   2  Sparsity: 77.0212%\n",
      "layer   3  Sparsity: 79.0847%\n",
      "total_backward_count 166430 real_backward_count 20505  12.320%\n",
      "lif layer 1 self.abs_max_v: 31551.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.516058/  1.863492, val:  43.33%, val_best:  50.42%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 77.2436%\n",
      "layer   3  Sparsity: 80.2768%\n",
      "total_backward_count 176220 real_backward_count 21734  12.333%\n",
      "lif layer 1 self.abs_max_v: 31620.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.521104/  1.797568, val:  41.67%, val_best:  50.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3354%\n",
      "layer   2  Sparsity: 76.8944%\n",
      "layer   3  Sparsity: 81.4252%\n",
      "total_backward_count 186010 real_backward_count 22993  12.361%\n",
      "fc layer 1 self.abs_max_out: 16561.0\n",
      "lif layer 1 self.abs_max_v: 31639.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.506094/  1.936835, val:  35.83%, val_best:  50.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3349%\n",
      "layer   2  Sparsity: 77.6104%\n",
      "layer   3  Sparsity: 82.0894%\n",
      "total_backward_count 195800 real_backward_count 24212  12.366%\n",
      "fc layer 1 self.abs_max_out: 16922.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.524225/  1.861990, val:  36.25%, val_best:  50.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3081%\n",
      "layer   2  Sparsity: 78.2107%\n",
      "layer   3  Sparsity: 81.8244%\n",
      "total_backward_count 205590 real_backward_count 25430  12.369%\n",
      "fc layer 1 self.abs_max_out: 17012.0\n",
      "lif layer 1 self.abs_max_v: 32544.5\n",
      "fc layer 1 self.abs_max_out: 17218.0\n",
      "fc layer 1 self.abs_max_out: 18882.0\n",
      "lif layer 1 self.abs_max_v: 33502.0\n",
      "lif layer 1 self.abs_max_v: 34305.0\n",
      "lif layer 1 self.abs_max_v: 34745.5\n",
      "lif layer 1 self.abs_max_v: 34828.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.525265/  1.859495, val:  31.25%, val_best:  50.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3738%\n",
      "layer   2  Sparsity: 79.2085%\n",
      "layer   3  Sparsity: 80.4422%\n",
      "total_backward_count 215380 real_backward_count 26711  12.402%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.494407/  1.782584, val:  51.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3379%\n",
      "layer   2  Sparsity: 79.3377%\n",
      "layer   3  Sparsity: 79.9876%\n",
      "total_backward_count 225170 real_backward_count 27943  12.410%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.495127/  1.803450, val:  43.75%, val_best:  51.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 79.1186%\n",
      "layer   3  Sparsity: 80.0718%\n",
      "total_backward_count 234960 real_backward_count 29197  12.426%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.486494/  1.761603, val:  56.67%, val_best:  56.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3679%\n",
      "layer   2  Sparsity: 79.2230%\n",
      "layer   3  Sparsity: 80.0828%\n",
      "total_backward_count 244750 real_backward_count 30458  12.445%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.490433/  1.739623, val:  59.17%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3476%\n",
      "layer   2  Sparsity: 78.2812%\n",
      "layer   3  Sparsity: 79.6038%\n",
      "total_backward_count 254540 real_backward_count 31776  12.484%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.460641/  1.793889, val:  55.42%, val_best:  59.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3467%\n",
      "layer   2  Sparsity: 78.0676%\n",
      "layer   3  Sparsity: 79.4144%\n",
      "total_backward_count 264330 real_backward_count 33010  12.488%\n",
      "fc layer 2 self.abs_max_out: 5577.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.484267/  1.823636, val:  48.33%, val_best:  59.17%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3408%\n",
      "layer   2  Sparsity: 77.3087%\n",
      "layer   3  Sparsity: 79.5548%\n",
      "total_backward_count 274120 real_backward_count 34314  12.518%\n",
      "lif layer 2 self.abs_max_v: 10437.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.476216/  1.860673, val:  36.67%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3173%\n",
      "layer   2  Sparsity: 78.4504%\n",
      "layer   3  Sparsity: 79.3672%\n",
      "total_backward_count 283910 real_backward_count 35562  12.526%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.513938/  1.779611, val:  49.58%, val_best:  59.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3423%\n",
      "layer   2  Sparsity: 80.3581%\n",
      "layer   3  Sparsity: 80.3212%\n",
      "total_backward_count 293700 real_backward_count 36866  12.552%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.520471/  1.825369, val:  44.17%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 80.2204%\n",
      "layer   3  Sparsity: 81.3393%\n",
      "total_backward_count 303490 real_backward_count 38147  12.569%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.578827/  1.875729, val:  39.58%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3777%\n",
      "layer   2  Sparsity: 78.9162%\n",
      "layer   3  Sparsity: 82.2824%\n",
      "total_backward_count 313280 real_backward_count 39488  12.605%\n",
      "fc layer 1 self.abs_max_out: 19527.0\n",
      "lif layer 1 self.abs_max_v: 35088.5\n",
      "fc layer 1 self.abs_max_out: 19541.0\n",
      "lif layer 1 self.abs_max_v: 35350.5\n",
      "lif layer 1 self.abs_max_v: 35515.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.504788/  1.844700, val:  44.58%, val_best:  59.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3057%\n",
      "layer   2  Sparsity: 79.6048%\n",
      "layer   3  Sparsity: 80.0777%\n",
      "total_backward_count 323070 real_backward_count 40827  12.637%\n",
      "fc layer 1 self.abs_max_out: 20306.0\n",
      "lif layer 1 self.abs_max_v: 36279.5\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.465001/  1.793805, val:  38.33%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3417%\n",
      "layer   2  Sparsity: 79.3429%\n",
      "layer   3  Sparsity: 79.8490%\n",
      "total_backward_count 332860 real_backward_count 42101  12.648%\n",
      "fc layer 1 self.abs_max_out: 20350.0\n",
      "lif layer 1 self.abs_max_v: 36373.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.492835/  1.780487, val:  45.42%, val_best:  59.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3143%\n",
      "layer   2  Sparsity: 80.0220%\n",
      "layer   3  Sparsity: 80.3302%\n",
      "total_backward_count 342650 real_backward_count 43335  12.647%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.466893/  1.829330, val:  30.00%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3625%\n",
      "layer   2  Sparsity: 80.1054%\n",
      "layer   3  Sparsity: 78.9922%\n",
      "total_backward_count 352440 real_backward_count 44603  12.655%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.445366/  1.770337, val:  52.92%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3480%\n",
      "layer   2  Sparsity: 79.7096%\n",
      "layer   3  Sparsity: 77.4880%\n",
      "total_backward_count 362230 real_backward_count 45874  12.664%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.370076/  1.762254, val:  45.83%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 79.8328%\n",
      "layer   3  Sparsity: 77.4244%\n",
      "total_backward_count 372020 real_backward_count 47097  12.660%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.413771/  1.775640, val:  47.50%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3158%\n",
      "layer   2  Sparsity: 80.5151%\n",
      "layer   3  Sparsity: 78.6339%\n",
      "total_backward_count 381810 real_backward_count 48390  12.674%\n",
      "fc layer 1 self.abs_max_out: 21240.0\n",
      "lif layer 1 self.abs_max_v: 36925.0\n",
      "lif layer 1 self.abs_max_v: 38399.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.425930/  1.818841, val:  42.50%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3330%\n",
      "layer   2  Sparsity: 81.0355%\n",
      "layer   3  Sparsity: 79.6937%\n",
      "total_backward_count 391600 real_backward_count 49650  12.679%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.454451/  1.780304, val:  50.00%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3621%\n",
      "layer   2  Sparsity: 79.8952%\n",
      "layer   3  Sparsity: 79.4866%\n",
      "total_backward_count 401390 real_backward_count 50956  12.695%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.450580/  1.758655, val:  50.00%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3357%\n",
      "layer   2  Sparsity: 79.3777%\n",
      "layer   3  Sparsity: 79.6916%\n",
      "total_backward_count 411180 real_backward_count 52251  12.708%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.489685/  1.824392, val:  48.33%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 79.4457%\n",
      "layer   3  Sparsity: 80.9242%\n",
      "total_backward_count 420970 real_backward_count 53443  12.695%\n",
      "fc layer 2 self.abs_max_out: 5699.0\n",
      "lif layer 2 self.abs_max_v: 10828.0\n",
      "fc layer 2 self.abs_max_out: 5769.0\n",
      "lif layer 2 self.abs_max_v: 11082.0\n",
      "lif layer 2 self.abs_max_v: 11084.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.495209/  1.764793, val:  48.75%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3415%\n",
      "layer   2  Sparsity: 79.8931%\n",
      "layer   3  Sparsity: 80.3612%\n",
      "total_backward_count 430760 real_backward_count 54685  12.695%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.441878/  1.781505, val:  47.08%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3327%\n",
      "layer   2  Sparsity: 79.6089%\n",
      "layer   3  Sparsity: 78.8910%\n",
      "total_backward_count 440550 real_backward_count 55936  12.697%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.420529/  1.747288, val:  55.83%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 79.4471%\n",
      "layer   3  Sparsity: 79.1994%\n",
      "total_backward_count 450340 real_backward_count 57191  12.700%\n",
      "fc layer 1 self.abs_max_out: 21486.0\n",
      "lif layer 1 self.abs_max_v: 39023.5\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.462575/  1.807657, val:  42.08%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3312%\n",
      "layer   2  Sparsity: 78.7960%\n",
      "layer   3  Sparsity: 80.4230%\n",
      "total_backward_count 460130 real_backward_count 58482  12.710%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.444310/  1.880341, val:  25.83%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 79.3215%\n",
      "layer   3  Sparsity: 78.7866%\n",
      "total_backward_count 469920 real_backward_count 59716  12.708%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.438608/  1.862292, val:  38.33%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3594%\n",
      "layer   2  Sparsity: 78.2996%\n",
      "layer   3  Sparsity: 80.3055%\n",
      "total_backward_count 479710 real_backward_count 60955  12.707%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.416424/  1.731737, val:  45.83%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3317%\n",
      "layer   2  Sparsity: 78.1917%\n",
      "layer   3  Sparsity: 78.1872%\n",
      "total_backward_count 489500 real_backward_count 62254  12.718%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.455555/  1.798339, val:  47.92%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 79.5534%\n",
      "layer   3  Sparsity: 79.7379%\n",
      "total_backward_count 499290 real_backward_count 63564  12.731%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.452476/  1.812902, val:  39.58%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3353%\n",
      "layer   2  Sparsity: 80.6607%\n",
      "layer   3  Sparsity: 80.4418%\n",
      "total_backward_count 509080 real_backward_count 64872  12.743%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.510486/  1.811039, val:  43.33%, val_best:  59.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 81.1035%\n",
      "layer   3  Sparsity: 81.7689%\n",
      "total_backward_count 518870 real_backward_count 66290  12.776%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.435549/  1.712303, val:  47.50%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3163%\n",
      "layer   2  Sparsity: 80.5943%\n",
      "layer   3  Sparsity: 80.3424%\n",
      "total_backward_count 528660 real_backward_count 67566  12.781%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.446467/  1.768119, val:  57.50%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3340%\n",
      "layer   2  Sparsity: 79.3820%\n",
      "layer   3  Sparsity: 80.3849%\n",
      "total_backward_count 538450 real_backward_count 68881  12.792%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.465451/  1.781018, val:  40.83%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3274%\n",
      "layer   2  Sparsity: 78.4190%\n",
      "layer   3  Sparsity: 80.5223%\n",
      "total_backward_count 548240 real_backward_count 70243  12.812%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.469165/  1.819384, val:  31.67%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3386%\n",
      "layer   2  Sparsity: 79.0192%\n",
      "layer   3  Sparsity: 81.2931%\n",
      "total_backward_count 558030 real_backward_count 71559  12.824%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.448341/  1.762576, val:  39.17%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3535%\n",
      "layer   2  Sparsity: 79.3425%\n",
      "layer   3  Sparsity: 81.1559%\n",
      "total_backward_count 567820 real_backward_count 72938  12.845%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.495616/  1.776282, val:  47.08%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3322%\n",
      "layer   2  Sparsity: 80.1774%\n",
      "layer   3  Sparsity: 80.6859%\n",
      "total_backward_count 577610 real_backward_count 74199  12.846%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.457140/  1.754987, val:  47.50%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 79.2198%\n",
      "layer   3  Sparsity: 81.0503%\n",
      "total_backward_count 587400 real_backward_count 75518  12.856%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.427171/  1.745044, val:  50.42%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3436%\n",
      "layer   2  Sparsity: 78.0356%\n",
      "layer   3  Sparsity: 79.5200%\n",
      "total_backward_count 597190 real_backward_count 76807  12.861%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.427296/  1.772940, val:  47.50%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3487%\n",
      "layer   2  Sparsity: 77.8201%\n",
      "layer   3  Sparsity: 79.0021%\n",
      "total_backward_count 606980 real_backward_count 78085  12.865%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.407195/  1.730792, val:  40.83%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 86.2906%\n",
      "layer   2  Sparsity: 77.1185%\n",
      "layer   3  Sparsity: 77.7898%\n",
      "total_backward_count 616770 real_backward_count 79304  12.858%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.382667/  1.794164, val:  37.08%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 76.7943%\n",
      "layer   3  Sparsity: 78.9064%\n",
      "total_backward_count 626560 real_backward_count 80540  12.854%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.448831/  1.836062, val:  47.08%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3683%\n",
      "layer   2  Sparsity: 77.3906%\n",
      "layer   3  Sparsity: 79.4372%\n",
      "total_backward_count 636350 real_backward_count 81850  12.862%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.499337/  1.808703, val:  46.67%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3233%\n",
      "layer   2  Sparsity: 78.0820%\n",
      "layer   3  Sparsity: 82.2324%\n",
      "total_backward_count 646140 real_backward_count 83181  12.874%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.469200/  1.794254, val:  44.58%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3300%\n",
      "layer   2  Sparsity: 78.3646%\n",
      "layer   3  Sparsity: 80.6457%\n",
      "total_backward_count 655930 real_backward_count 84489  12.881%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.446965/  1.738825, val:  50.00%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3041%\n",
      "layer   2  Sparsity: 78.6710%\n",
      "layer   3  Sparsity: 80.8697%\n",
      "total_backward_count 665720 real_backward_count 85734  12.878%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.433677/  1.798784, val:  47.08%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3483%\n",
      "layer   2  Sparsity: 79.3857%\n",
      "layer   3  Sparsity: 80.1311%\n",
      "total_backward_count 675510 real_backward_count 87011  12.881%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.461405/  1.784498, val:  45.83%, val_best:  59.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3217%\n",
      "layer   2  Sparsity: 79.5418%\n",
      "layer   3  Sparsity: 80.5821%\n",
      "total_backward_count 685300 real_backward_count 88294  12.884%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.463470/  1.766181, val:  48.75%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 78.4509%\n",
      "layer   3  Sparsity: 79.5261%\n",
      "total_backward_count 695090 real_backward_count 89649  12.897%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.439265/  1.863847, val:  30.83%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3388%\n",
      "layer   2  Sparsity: 79.1374%\n",
      "layer   3  Sparsity: 79.4368%\n",
      "total_backward_count 704880 real_backward_count 90962  12.905%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.438639/  1.733691, val:  53.33%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3409%\n",
      "layer   2  Sparsity: 79.3182%\n",
      "layer   3  Sparsity: 78.2530%\n",
      "total_backward_count 714670 real_backward_count 92225  12.905%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.409716/  1.761970, val:  40.42%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3058%\n",
      "layer   2  Sparsity: 80.3893%\n",
      "layer   3  Sparsity: 79.6452%\n",
      "total_backward_count 724460 real_backward_count 93500  12.906%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.409635/  1.758652, val:  49.17%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3068%\n",
      "layer   2  Sparsity: 79.9339%\n",
      "layer   3  Sparsity: 78.0981%\n",
      "total_backward_count 734250 real_backward_count 94826  12.915%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.409130/  1.747271, val:  42.92%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2863%\n",
      "layer   2  Sparsity: 78.8306%\n",
      "layer   3  Sparsity: 78.8845%\n",
      "total_backward_count 744040 real_backward_count 96132  12.920%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.515879/  1.832041, val:  54.17%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 78.3518%\n",
      "layer   3  Sparsity: 81.3873%\n",
      "total_backward_count 753830 real_backward_count 97445  12.927%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.473016/  1.896072, val:  33.33%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3268%\n",
      "layer   2  Sparsity: 79.3223%\n",
      "layer   3  Sparsity: 80.5439%\n",
      "total_backward_count 763620 real_backward_count 98735  12.930%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.492985/  1.815088, val:  40.83%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3537%\n",
      "layer   2  Sparsity: 79.5767%\n",
      "layer   3  Sparsity: 80.7852%\n",
      "total_backward_count 773410 real_backward_count 100031  12.934%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.452346/  1.748940, val:  53.75%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3138%\n",
      "layer   2  Sparsity: 78.8468%\n",
      "layer   3  Sparsity: 79.7920%\n",
      "total_backward_count 783200 real_backward_count 101281  12.932%\n",
      "fc layer 1 self.abs_max_out: 21636.0\n",
      "lif layer 1 self.abs_max_v: 39387.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.422172/  1.710733, val:  54.17%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 77.8611%\n",
      "layer   3  Sparsity: 80.4439%\n",
      "total_backward_count 792990 real_backward_count 102588  12.937%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.440806/  1.874251, val:  21.67%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 78.1795%\n",
      "layer   3  Sparsity: 80.3594%\n",
      "total_backward_count 802780 real_backward_count 103847  12.936%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.515276/  1.805347, val:  50.00%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3336%\n",
      "layer   2  Sparsity: 78.4873%\n",
      "layer   3  Sparsity: 81.3440%\n",
      "total_backward_count 812570 real_backward_count 105114  12.936%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.512921/  1.871407, val:  44.17%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3339%\n",
      "layer   2  Sparsity: 77.3821%\n",
      "layer   3  Sparsity: 81.7660%\n",
      "total_backward_count 822360 real_backward_count 106424  12.941%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.501544/  1.880409, val:  36.25%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3401%\n",
      "layer   2  Sparsity: 79.1582%\n",
      "layer   3  Sparsity: 81.6643%\n",
      "total_backward_count 832150 real_backward_count 107816  12.956%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.431057/  1.827021, val:  41.67%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3605%\n",
      "layer   2  Sparsity: 80.5421%\n",
      "layer   3  Sparsity: 78.3523%\n",
      "total_backward_count 841940 real_backward_count 109143  12.963%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.428725/  1.747504, val:  45.00%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3079%\n",
      "layer   2  Sparsity: 80.0351%\n",
      "layer   3  Sparsity: 79.7035%\n",
      "total_backward_count 851730 real_backward_count 110420  12.964%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.415148/  1.751573, val:  45.00%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3506%\n",
      "layer   2  Sparsity: 80.0916%\n",
      "layer   3  Sparsity: 79.2959%\n",
      "total_backward_count 861520 real_backward_count 111751  12.971%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.422680/  1.708589, val:  53.75%, val_best:  59.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3556%\n",
      "layer   2  Sparsity: 78.9426%\n",
      "layer   3  Sparsity: 79.4172%\n",
      "total_backward_count 871310 real_backward_count 112997  12.969%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.373391/  1.778130, val:  36.25%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3377%\n",
      "layer   2  Sparsity: 78.7881%\n",
      "layer   3  Sparsity: 77.1562%\n",
      "total_backward_count 881100 real_backward_count 114271  12.969%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.370006/  1.826485, val:  39.58%, val_best:  59.17%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 79.1227%\n",
      "layer   3  Sparsity: 77.2810%\n",
      "total_backward_count 890890 real_backward_count 115582  12.974%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.348044/  1.694918, val:  53.33%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2931%\n",
      "layer   2  Sparsity: 80.4236%\n",
      "layer   3  Sparsity: 76.7078%\n",
      "total_backward_count 900680 real_backward_count 116845  12.973%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.356254/  1.700384, val:  45.83%, val_best:  59.17%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3298%\n",
      "layer   2  Sparsity: 80.8288%\n",
      "layer   3  Sparsity: 77.1522%\n",
      "total_backward_count 910470 real_backward_count 118227  12.985%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.291080/  1.736344, val:  53.75%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 79.8197%\n",
      "layer   3  Sparsity: 75.5745%\n",
      "total_backward_count 920260 real_backward_count 119477  12.983%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.368490/  1.744024, val:  46.67%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3206%\n",
      "layer   2  Sparsity: 79.0226%\n",
      "layer   3  Sparsity: 78.8830%\n",
      "total_backward_count 930050 real_backward_count 120802  12.989%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.423769/  1.761058, val:  52.50%, val_best:  59.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2899%\n",
      "layer   2  Sparsity: 78.2370%\n",
      "layer   3  Sparsity: 80.2832%\n",
      "total_backward_count 939840 real_backward_count 122076  12.989%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.398234/  1.814512, val:  42.50%, val_best:  59.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3063%\n",
      "layer   2  Sparsity: 80.2684%\n",
      "layer   3  Sparsity: 78.5537%\n",
      "total_backward_count 949630 real_backward_count 123344  12.989%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.336760/  1.714925, val:  39.58%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 80.2745%\n",
      "layer   3  Sparsity: 77.3758%\n",
      "total_backward_count 959420 real_backward_count 124645  12.992%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.323675/  1.766580, val:  33.75%, val_best:  59.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2930%\n",
      "layer   2  Sparsity: 79.8308%\n",
      "layer   3  Sparsity: 76.3185%\n",
      "total_backward_count 969210 real_backward_count 125849  12.985%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.300554/  1.707828, val:  50.42%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 80.4514%\n",
      "layer   3  Sparsity: 76.5997%\n",
      "total_backward_count 979000 real_backward_count 127058  12.978%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.392501/  1.829614, val:  43.33%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3598%\n",
      "layer   2  Sparsity: 80.1593%\n",
      "layer   3  Sparsity: 80.7047%\n",
      "total_backward_count 988790 real_backward_count 128345  12.980%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.419452/  1.741375, val:  47.50%, val_best:  59.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   2  Sparsity: 79.3840%\n",
      "layer   3  Sparsity: 80.2620%\n",
      "total_backward_count 998580 real_backward_count 129630  12.981%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.412174/  1.759242, val:  45.00%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 79.2214%\n",
      "layer   3  Sparsity: 78.8089%\n",
      "total_backward_count 1008370 real_backward_count 130893  12.981%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.384650/  1.725749, val:  52.92%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3183%\n",
      "layer   2  Sparsity: 79.4654%\n",
      "layer   3  Sparsity: 79.0563%\n",
      "total_backward_count 1018160 real_backward_count 132148  12.979%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.429739/  1.775438, val:  49.17%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3369%\n",
      "layer   2  Sparsity: 79.3588%\n",
      "layer   3  Sparsity: 80.6082%\n",
      "total_backward_count 1027950 real_backward_count 133418  12.979%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.435502/  1.698097, val:  52.50%, val_best:  59.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3501%\n",
      "layer   2  Sparsity: 79.8959%\n",
      "layer   3  Sparsity: 80.4333%\n",
      "total_backward_count 1037740 real_backward_count 134732  12.983%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.359679/  1.799582, val:  32.92%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3182%\n",
      "layer   2  Sparsity: 79.9064%\n",
      "layer   3  Sparsity: 77.1950%\n",
      "total_backward_count 1047530 real_backward_count 135965  12.980%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.385871/  1.747261, val:  42.50%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3324%\n",
      "layer   2  Sparsity: 80.2434%\n",
      "layer   3  Sparsity: 77.1158%\n",
      "total_backward_count 1057320 real_backward_count 137286  12.984%\n",
      "fc layer 3 self.abs_max_out: 3338.0\n",
      "fc layer 3 self.abs_max_out: 3357.0\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.365549/  1.740098, val:  48.33%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3035%\n",
      "layer   2  Sparsity: 80.6940%\n",
      "layer   3  Sparsity: 76.6531%\n",
      "total_backward_count 1067110 real_backward_count 138486  12.978%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.347843/  1.749096, val:  38.33%, val_best:  59.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3076%\n",
      "layer   2  Sparsity: 79.6856%\n",
      "layer   3  Sparsity: 77.5979%\n",
      "total_backward_count 1076900 real_backward_count 139770  12.979%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.447787/  1.737485, val:  49.17%, val_best:  59.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3637%\n",
      "layer   2  Sparsity: 79.7351%\n",
      "layer   3  Sparsity: 81.3819%\n",
      "total_backward_count 1086690 real_backward_count 141116  12.986%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.441069/  1.750029, val:  47.92%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3031%\n",
      "layer   2  Sparsity: 79.8838%\n",
      "layer   3  Sparsity: 80.2514%\n",
      "total_backward_count 1096480 real_backward_count 142489  12.995%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.377694/  1.646440, val:  64.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3565%\n",
      "layer   2  Sparsity: 80.1540%\n",
      "layer   3  Sparsity: 78.5432%\n",
      "total_backward_count 1106270 real_backward_count 143740  12.993%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.374563/  1.757438, val:  36.25%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3365%\n",
      "layer   2  Sparsity: 80.5354%\n",
      "layer   3  Sparsity: 78.7692%\n",
      "total_backward_count 1116060 real_backward_count 145038  12.996%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.360802/  1.774767, val:  40.42%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3261%\n",
      "layer   2  Sparsity: 79.2193%\n",
      "layer   3  Sparsity: 76.4712%\n",
      "total_backward_count 1125850 real_backward_count 146259  12.991%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.314570/  1.659203, val:  48.33%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3047%\n",
      "layer   2  Sparsity: 79.3725%\n",
      "layer   3  Sparsity: 75.0430%\n",
      "total_backward_count 1135640 real_backward_count 147506  12.989%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.334108/  1.661629, val:  53.75%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3151%\n",
      "layer   2  Sparsity: 80.0960%\n",
      "layer   3  Sparsity: 78.1784%\n",
      "total_backward_count 1145430 real_backward_count 148826  12.993%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.332422/  1.667288, val:  48.33%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 78.8534%\n",
      "layer   3  Sparsity: 75.9446%\n",
      "total_backward_count 1155220 real_backward_count 150076  12.991%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.284577/  1.673767, val:  43.75%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3311%\n",
      "layer   2  Sparsity: 79.3379%\n",
      "layer   3  Sparsity: 75.5025%\n",
      "total_backward_count 1165010 real_backward_count 151304  12.987%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.246181/  1.678349, val:  52.08%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3779%\n",
      "layer   2  Sparsity: 80.2251%\n",
      "layer   3  Sparsity: 74.3920%\n",
      "total_backward_count 1174800 real_backward_count 152518  12.982%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.291689/  1.712254, val:  52.08%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3366%\n",
      "layer   2  Sparsity: 79.9110%\n",
      "layer   3  Sparsity: 76.2117%\n",
      "total_backward_count 1184590 real_backward_count 153716  12.976%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.280051/  1.722340, val:  40.42%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3407%\n",
      "layer   2  Sparsity: 78.6525%\n",
      "layer   3  Sparsity: 76.1819%\n",
      "total_backward_count 1194380 real_backward_count 154946  12.973%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.297439/  1.750195, val:  37.92%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3551%\n",
      "layer   2  Sparsity: 78.7553%\n",
      "layer   3  Sparsity: 76.8198%\n",
      "total_backward_count 1204170 real_backward_count 156167  12.969%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.343869/  1.738851, val:  52.08%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3231%\n",
      "layer   2  Sparsity: 78.3733%\n",
      "layer   3  Sparsity: 78.4501%\n",
      "total_backward_count 1213960 real_backward_count 157422  12.968%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.361086/  1.740563, val:  46.25%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3334%\n",
      "layer   2  Sparsity: 79.5238%\n",
      "layer   3  Sparsity: 78.0056%\n",
      "total_backward_count 1223750 real_backward_count 158713  12.969%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.317155/  1.712858, val:  47.50%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3626%\n",
      "layer   2  Sparsity: 78.4473%\n",
      "layer   3  Sparsity: 76.6936%\n",
      "total_backward_count 1233540 real_backward_count 159931  12.965%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.320663/  1.764939, val:  39.17%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3858%\n",
      "layer   2  Sparsity: 78.0686%\n",
      "layer   3  Sparsity: 77.2344%\n",
      "total_backward_count 1243330 real_backward_count 161168  12.963%\n",
      "fc layer 3 self.abs_max_out: 3393.0\n",
      "fc layer 3 self.abs_max_out: 3562.0\n",
      "fc layer 3 self.abs_max_out: 3618.0\n",
      "fc layer 3 self.abs_max_out: 3625.0\n",
      "fc layer 3 self.abs_max_out: 3687.0\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.337687/  1.684961, val:  56.25%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3263%\n",
      "layer   2  Sparsity: 77.6117%\n",
      "layer   3  Sparsity: 77.3493%\n",
      "total_backward_count 1253120 real_backward_count 162381  12.958%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.315767/  1.715518, val:  43.33%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3586%\n",
      "layer   2  Sparsity: 78.3220%\n",
      "layer   3  Sparsity: 76.2533%\n",
      "total_backward_count 1262910 real_backward_count 163586  12.953%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.275744/  1.676947, val:  45.83%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3090%\n",
      "layer   2  Sparsity: 79.5207%\n",
      "layer   3  Sparsity: 75.3636%\n",
      "total_backward_count 1272700 real_backward_count 164779  12.947%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.283543/  1.766171, val:  40.42%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3631%\n",
      "layer   2  Sparsity: 79.2856%\n",
      "layer   3  Sparsity: 75.6610%\n",
      "total_backward_count 1282490 real_backward_count 166072  12.949%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.310072/  1.697283, val:  42.92%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3278%\n",
      "layer   2  Sparsity: 79.8397%\n",
      "layer   3  Sparsity: 76.9307%\n",
      "total_backward_count 1292280 real_backward_count 167404  12.954%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.361970/  1.707734, val:  50.83%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3214%\n",
      "layer   2  Sparsity: 80.9202%\n",
      "layer   3  Sparsity: 79.3520%\n",
      "total_backward_count 1302070 real_backward_count 168637  12.951%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.372017/  1.695232, val:  50.83%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3374%\n",
      "layer   2  Sparsity: 81.4696%\n",
      "layer   3  Sparsity: 77.5459%\n",
      "total_backward_count 1311860 real_backward_count 169996  12.958%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.365842/  1.777494, val:  41.25%, val_best:  64.17%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 80.6959%\n",
      "layer   3  Sparsity: 77.6903%\n",
      "total_backward_count 1321650 real_backward_count 171298  12.961%\n",
      "fc layer 3 self.abs_max_out: 3818.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.324094/  1.669720, val:  49.58%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 79.6244%\n",
      "layer   3  Sparsity: 76.4337%\n",
      "total_backward_count 1331440 real_backward_count 172582  12.962%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.333870/  1.689453, val:  45.83%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3424%\n",
      "layer   2  Sparsity: 80.6943%\n",
      "layer   3  Sparsity: 78.6777%\n",
      "total_backward_count 1341230 real_backward_count 173898  12.966%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.351581/  1.786677, val:  38.33%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3496%\n",
      "layer   2  Sparsity: 79.3539%\n",
      "layer   3  Sparsity: 77.3127%\n",
      "total_backward_count 1351020 real_backward_count 175160  12.965%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.349567/  1.730882, val:  50.42%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3279%\n",
      "layer   2  Sparsity: 79.3434%\n",
      "layer   3  Sparsity: 77.0990%\n",
      "total_backward_count 1360810 real_backward_count 176508  12.971%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.356939/  1.612374, val:  55.42%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 79.8447%\n",
      "layer   3  Sparsity: 77.0555%\n",
      "total_backward_count 1370600 real_backward_count 177770  12.970%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.300954/  1.790821, val:  36.67%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3190%\n",
      "layer   2  Sparsity: 80.1921%\n",
      "layer   3  Sparsity: 76.9457%\n",
      "total_backward_count 1380390 real_backward_count 179052  12.971%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.303872/  1.684214, val:  54.58%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3559%\n",
      "layer   2  Sparsity: 80.5675%\n",
      "layer   3  Sparsity: 75.3390%\n",
      "total_backward_count 1390180 real_backward_count 180368  12.974%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.287757/  1.696933, val:  41.67%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3515%\n",
      "layer   2  Sparsity: 81.2827%\n",
      "layer   3  Sparsity: 75.5233%\n",
      "total_backward_count 1399970 real_backward_count 181685  12.978%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.252047/  1.646009, val:  50.83%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2864%\n",
      "layer   2  Sparsity: 80.7162%\n",
      "layer   3  Sparsity: 74.2258%\n",
      "total_backward_count 1409760 real_backward_count 182958  12.978%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.247157/  1.605236, val:  52.92%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3405%\n",
      "layer   2  Sparsity: 80.5168%\n",
      "layer   3  Sparsity: 73.2081%\n",
      "total_backward_count 1419550 real_backward_count 184206  12.976%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.266880/  1.756767, val:  39.17%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3323%\n",
      "layer   2  Sparsity: 81.9826%\n",
      "layer   3  Sparsity: 74.2192%\n",
      "total_backward_count 1429340 real_backward_count 185477  12.976%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.370831/  1.830848, val:  46.67%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3318%\n",
      "layer   2  Sparsity: 81.9221%\n",
      "layer   3  Sparsity: 79.0978%\n",
      "total_backward_count 1439130 real_backward_count 186842  12.983%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.414888/  1.837199, val:  37.50%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3289%\n",
      "layer   2  Sparsity: 81.6559%\n",
      "layer   3  Sparsity: 78.3121%\n",
      "total_backward_count 1448920 real_backward_count 188127  12.984%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.326985/  1.656061, val:  52.50%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3204%\n",
      "layer   2  Sparsity: 81.6724%\n",
      "layer   3  Sparsity: 76.0893%\n",
      "total_backward_count 1458710 real_backward_count 189415  12.985%\n",
      "fc layer 3 self.abs_max_out: 3836.0\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.318339/  1.697816, val:  47.50%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3307%\n",
      "layer   2  Sparsity: 82.7681%\n",
      "layer   3  Sparsity: 76.0956%\n",
      "total_backward_count 1468500 real_backward_count 190733  12.988%\n",
      "fc layer 3 self.abs_max_out: 3838.0\n",
      "fc layer 3 self.abs_max_out: 3865.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.284151/  1.728127, val:  46.25%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 83.0860%\n",
      "layer   3  Sparsity: 75.2767%\n",
      "total_backward_count 1478290 real_backward_count 191973  12.986%\n",
      "fc layer 3 self.abs_max_out: 4064.0\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.252787/  1.658836, val:  49.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3448%\n",
      "layer   2  Sparsity: 82.0525%\n",
      "layer   3  Sparsity: 74.0057%\n",
      "total_backward_count 1488080 real_backward_count 193266  12.988%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.293489/  1.667179, val:  55.42%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3282%\n",
      "layer   2  Sparsity: 81.1961%\n",
      "layer   3  Sparsity: 74.9813%\n",
      "total_backward_count 1497870 real_backward_count 194530  12.987%\n",
      "fc layer 1 self.abs_max_out: 21873.0\n",
      "lif layer 1 self.abs_max_v: 40007.5\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.212637/  1.589707, val:  49.58%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 81.4767%\n",
      "layer   3  Sparsity: 71.5317%\n",
      "total_backward_count 1507660 real_backward_count 195806  12.987%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.221879/  1.613153, val:  46.25%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3642%\n",
      "layer   2  Sparsity: 81.0002%\n",
      "layer   3  Sparsity: 71.9817%\n",
      "total_backward_count 1517450 real_backward_count 197052  12.986%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.219961/  1.640001, val:  54.58%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2999%\n",
      "layer   2  Sparsity: 80.5152%\n",
      "layer   3  Sparsity: 73.2345%\n",
      "total_backward_count 1527240 real_backward_count 198324  12.986%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.339298/  1.714887, val:  50.42%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3447%\n",
      "layer   2  Sparsity: 80.5550%\n",
      "layer   3  Sparsity: 76.9085%\n",
      "total_backward_count 1537030 real_backward_count 199594  12.986%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.342209/  1.685938, val:  52.08%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3001%\n",
      "layer   2  Sparsity: 81.0579%\n",
      "layer   3  Sparsity: 77.1976%\n",
      "total_backward_count 1546820 real_backward_count 200890  12.987%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.358868/  1.761442, val:  45.83%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3402%\n",
      "layer   2  Sparsity: 82.2107%\n",
      "layer   3  Sparsity: 77.4615%\n",
      "total_backward_count 1556610 real_backward_count 202232  12.992%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.393155/  1.770788, val:  45.00%, val_best:  64.17%, tr:  99.18%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3253%\n",
      "layer   2  Sparsity: 81.9766%\n",
      "layer   3  Sparsity: 78.8948%\n",
      "total_backward_count 1566400 real_backward_count 203597  12.998%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.463305/  1.755450, val:  46.67%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3443%\n",
      "layer   2  Sparsity: 82.7272%\n",
      "layer   3  Sparsity: 79.5516%\n",
      "total_backward_count 1576190 real_backward_count 204932  13.002%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.403479/  1.730107, val:  55.00%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3023%\n",
      "layer   2  Sparsity: 81.9031%\n",
      "layer   3  Sparsity: 78.2750%\n",
      "total_backward_count 1585980 real_backward_count 206304  13.008%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.403342/  1.759766, val:  53.75%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3355%\n",
      "layer   2  Sparsity: 81.6746%\n",
      "layer   3  Sparsity: 79.0027%\n",
      "total_backward_count 1595770 real_backward_count 207653  13.013%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.387091/  1.754955, val:  50.00%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 81.2212%\n",
      "layer   3  Sparsity: 77.4708%\n",
      "total_backward_count 1605560 real_backward_count 208912  13.012%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.338475/  1.730999, val:  41.67%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2903%\n",
      "layer   2  Sparsity: 80.7255%\n",
      "layer   3  Sparsity: 76.2176%\n",
      "total_backward_count 1615350 real_backward_count 210225  13.014%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.283874/  1.662447, val:  47.08%, val_best:  64.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3084%\n",
      "layer   2  Sparsity: 82.4011%\n",
      "layer   3  Sparsity: 75.3236%\n",
      "total_backward_count 1625140 real_backward_count 211625  13.022%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.245868/  1.683697, val:  48.33%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3134%\n",
      "layer   2  Sparsity: 81.8578%\n",
      "layer   3  Sparsity: 74.8781%\n",
      "total_backward_count 1634930 real_backward_count 212883  13.021%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.262097/  1.716339, val:  44.58%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 81.5767%\n",
      "layer   3  Sparsity: 74.5482%\n",
      "total_backward_count 1644720 real_backward_count 214157  13.021%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.197336/  1.633086, val:  53.33%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3690%\n",
      "layer   2  Sparsity: 82.2423%\n",
      "layer   3  Sparsity: 72.5172%\n",
      "total_backward_count 1654510 real_backward_count 215460  13.023%\n",
      "fc layer 3 self.abs_max_out: 4097.0\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.217447/  1.630822, val:  50.00%, val_best:  64.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3504%\n",
      "layer   2  Sparsity: 81.7225%\n",
      "layer   3  Sparsity: 73.6732%\n",
      "total_backward_count 1664300 real_backward_count 216707  13.021%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.228624/  1.725706, val:  40.83%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3495%\n",
      "layer   2  Sparsity: 81.9102%\n",
      "layer   3  Sparsity: 73.0204%\n",
      "total_backward_count 1674090 real_backward_count 217950  13.019%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.289775/  1.747235, val:  39.58%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3696%\n",
      "layer   2  Sparsity: 80.8373%\n",
      "layer   3  Sparsity: 74.5315%\n",
      "total_backward_count 1683880 real_backward_count 219256  13.021%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.213669/  1.596851, val:  56.67%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3602%\n",
      "layer   2  Sparsity: 80.5069%\n",
      "layer   3  Sparsity: 71.8060%\n",
      "total_backward_count 1693670 real_backward_count 220540  13.021%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.187282/  1.695309, val:  52.08%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3412%\n",
      "layer   2  Sparsity: 80.6198%\n",
      "layer   3  Sparsity: 71.3222%\n",
      "total_backward_count 1703460 real_backward_count 221679  13.013%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.151266/  1.621716, val:  49.58%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 81.3866%\n",
      "layer   3  Sparsity: 70.0005%\n",
      "total_backward_count 1713250 real_backward_count 222866  13.008%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.173455/  1.623324, val:  50.00%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2967%\n",
      "layer   2  Sparsity: 80.4415%\n",
      "layer   3  Sparsity: 71.1548%\n",
      "total_backward_count 1723040 real_backward_count 224121  13.007%\n",
      "fc layer 3 self.abs_max_out: 4121.0\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.145004/  1.563262, val:  50.83%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 80.3408%\n",
      "layer   3  Sparsity: 71.7475%\n",
      "total_backward_count 1732830 real_backward_count 225332  13.004%\n",
      "fc layer 3 self.abs_max_out: 4185.0\n",
      "fc layer 3 self.abs_max_out: 4253.0\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.183468/  1.624703, val:  47.92%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3123%\n",
      "layer   2  Sparsity: 79.6513%\n",
      "layer   3  Sparsity: 72.9042%\n",
      "total_backward_count 1742620 real_backward_count 226553  13.001%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.196121/  1.704081, val:  37.92%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3472%\n",
      "layer   2  Sparsity: 79.3642%\n",
      "layer   3  Sparsity: 71.9481%\n",
      "total_backward_count 1752410 real_backward_count 227755  12.997%\n",
      "fc layer 3 self.abs_max_out: 4314.0\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.158226/  1.788491, val:  34.17%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3422%\n",
      "layer   2  Sparsity: 80.5501%\n",
      "layer   3  Sparsity: 72.1839%\n",
      "total_backward_count 1762200 real_backward_count 229001  12.995%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.142214/  1.592092, val:  53.75%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3571%\n",
      "layer   2  Sparsity: 79.5410%\n",
      "layer   3  Sparsity: 70.7591%\n",
      "total_backward_count 1771990 real_backward_count 230244  12.994%\n",
      "fc layer 3 self.abs_max_out: 4495.0\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.136436/  1.690099, val:  45.00%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3210%\n",
      "layer   2  Sparsity: 79.9142%\n",
      "layer   3  Sparsity: 71.0532%\n",
      "total_backward_count 1781780 real_backward_count 231454  12.990%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.181538/  1.702333, val:  42.92%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3381%\n",
      "layer   2  Sparsity: 79.0459%\n",
      "layer   3  Sparsity: 72.9741%\n",
      "total_backward_count 1791570 real_backward_count 232692  12.988%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.204648/  1.632215, val:  50.83%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3218%\n",
      "layer   2  Sparsity: 80.6184%\n",
      "layer   3  Sparsity: 72.9496%\n",
      "total_backward_count 1801360 real_backward_count 233926  12.986%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.176928/  1.621939, val:  43.75%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3481%\n",
      "layer   2  Sparsity: 81.3395%\n",
      "layer   3  Sparsity: 72.7696%\n",
      "total_backward_count 1811150 real_backward_count 235171  12.985%\n",
      "fc layer 3 self.abs_max_out: 4593.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.179497/  1.700458, val:  47.08%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 80.8941%\n",
      "layer   3  Sparsity: 72.9523%\n",
      "total_backward_count 1820940 real_backward_count 236490  12.987%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.264482/  1.614172, val:  55.83%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3375%\n",
      "layer   2  Sparsity: 81.0316%\n",
      "layer   3  Sparsity: 74.4055%\n",
      "total_backward_count 1830730 real_backward_count 237816  12.990%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.224516/  1.722290, val:  33.75%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3426%\n",
      "layer   2  Sparsity: 82.0146%\n",
      "layer   3  Sparsity: 73.4090%\n",
      "total_backward_count 1840520 real_backward_count 239136  12.993%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.244059/  1.685692, val:  58.33%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 82.5240%\n",
      "layer   3  Sparsity: 74.4762%\n",
      "total_backward_count 1850310 real_backward_count 240392  12.992%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.289300/  1.659098, val:  47.92%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 81.1823%\n",
      "layer   3  Sparsity: 74.7925%\n",
      "total_backward_count 1860100 real_backward_count 241673  12.992%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.277833/  1.672345, val:  61.25%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 81.7924%\n",
      "layer   3  Sparsity: 74.9003%\n",
      "total_backward_count 1869890 real_backward_count 242947  12.993%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.294401/  1.720451, val:  40.00%, val_best:  64.17%, tr:  99.18%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3118%\n",
      "layer   2  Sparsity: 83.2194%\n",
      "layer   3  Sparsity: 77.6805%\n",
      "total_backward_count 1879680 real_backward_count 244310  12.997%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.335157/  1.712894, val:  45.00%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 82.2858%\n",
      "layer   3  Sparsity: 76.8934%\n",
      "total_backward_count 1889470 real_backward_count 245661  13.002%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.359942/  1.803330, val:  41.25%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3283%\n",
      "layer   2  Sparsity: 81.7749%\n",
      "layer   3  Sparsity: 77.0615%\n",
      "total_backward_count 1899260 real_backward_count 246992  13.005%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.407867/  1.676699, val:  48.33%, val_best:  64.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3299%\n",
      "layer   2  Sparsity: 81.1329%\n",
      "layer   3  Sparsity: 77.9291%\n",
      "total_backward_count 1909050 real_backward_count 248328  13.008%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.315666/  1.700810, val:  49.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3164%\n",
      "layer   2  Sparsity: 82.5887%\n",
      "layer   3  Sparsity: 75.1944%\n",
      "total_backward_count 1918840 real_backward_count 249600  13.008%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.288728/  1.737273, val:  48.33%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3302%\n",
      "layer   2  Sparsity: 83.3457%\n",
      "layer   3  Sparsity: 75.2801%\n",
      "total_backward_count 1928630 real_backward_count 250902  13.009%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.353158/  1.669471, val:  53.33%, val_best:  64.17%, tr:  99.18%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3580%\n",
      "layer   2  Sparsity: 83.2916%\n",
      "layer   3  Sparsity: 77.5360%\n",
      "total_backward_count 1938420 real_backward_count 252324  13.017%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.237396/  1.710271, val:  40.42%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3238%\n",
      "layer   2  Sparsity: 83.0286%\n",
      "layer   3  Sparsity: 73.7020%\n",
      "total_backward_count 1948210 real_backward_count 253598  13.017%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.280702/  1.674551, val:  56.67%, val_best:  64.17%, tr:  98.88%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3284%\n",
      "layer   2  Sparsity: 82.8251%\n",
      "layer   3  Sparsity: 76.3338%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f09157a0a9492b8cf25ee13c1310a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñà‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98876</td></tr><tr><td>tr_epoch_loss</td><td>1.2807</td></tr><tr><td>val_acc_best</td><td>0.64167</td></tr><tr><td>val_acc_now</td><td>0.56667</td></tr><tr><td>val_loss</td><td>1.67455</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-91</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/186b83va' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/186b83va</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_022847-186b83va/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6v3zl7ik with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_064603-6v3zl7ik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v3zl7ik' target=\"_blank\">winter-sweep-97</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v3zl7ik' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v3zl7ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251116_064612_293', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 687.0\n",
      "lif layer 1 self.abs_max_v: 687.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "lif layer 2 self.abs_max_v: 816.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "lif layer 1 self.abs_max_v: 1107.5\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "fc layer 3 self.abs_max_out: 350.0\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 1126.0\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "lif layer 2 self.abs_max_v: 1637.5\n",
      "fc layer 1 self.abs_max_out: 1289.0\n",
      "lif layer 1 self.abs_max_v: 1289.0\n",
      "lif layer 2 self.abs_max_v: 1641.0\n",
      "fc layer 1 self.abs_max_out: 1419.0\n",
      "lif layer 1 self.abs_max_v: 1419.0\n",
      "lif layer 2 self.abs_max_v: 1743.5\n",
      "fc layer 1 self.abs_max_out: 1576.0\n",
      "lif layer 1 self.abs_max_v: 1583.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "lif layer 1 self.abs_max_v: 1708.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "lif layer 2 self.abs_max_v: 1785.5\n",
      "lif layer 2 self.abs_max_v: 2097.0\n",
      "lif layer 1 self.abs_max_v: 1714.5\n",
      "fc layer 1 self.abs_max_out: 1604.0\n",
      "lif layer 1 self.abs_max_v: 1918.0\n",
      "fc layer 1 self.abs_max_out: 1882.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "lif layer 1 self.abs_max_v: 2117.0\n",
      "fc layer 1 self.abs_max_out: 2032.0\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 2280.0\n",
      "fc layer 1 self.abs_max_out: 2781.0\n",
      "lif layer 1 self.abs_max_v: 2877.0\n",
      "fc layer 1 self.abs_max_out: 3208.0\n",
      "lif layer 1 self.abs_max_v: 3274.5\n",
      "lif layer 1 self.abs_max_v: 3309.0\n",
      "fc layer 3 self.abs_max_out: 535.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "lif layer 1 self.abs_max_v: 3328.0\n",
      "lif layer 1 self.abs_max_v: 3418.0\n",
      "lif layer 1 self.abs_max_v: 3637.0\n",
      "lif layer 2 self.abs_max_v: 2326.0\n",
      "lif layer 1 self.abs_max_v: 3986.0\n",
      "lif layer 1 self.abs_max_v: 4634.0\n",
      "fc layer 1 self.abs_max_out: 3480.0\n",
      "lif layer 1 self.abs_max_v: 5433.0\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "fc layer 1 self.abs_max_out: 3893.0\n",
      "lif layer 1 self.abs_max_v: 6465.5\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 1 self.abs_max_v: 6753.0\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "fc layer 2 self.abs_max_out: 1640.0\n",
      "lif layer 2 self.abs_max_v: 2393.5\n",
      "lif layer 2 self.abs_max_v: 2438.0\n",
      "fc layer 2 self.abs_max_out: 1768.0\n",
      "lif layer 2 self.abs_max_v: 2505.0\n",
      "lif layer 2 self.abs_max_v: 2593.0\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 2 self.abs_max_out: 1841.0\n",
      "lif layer 2 self.abs_max_v: 2627.5\n",
      "lif layer 2 self.abs_max_v: 2636.0\n",
      "lif layer 2 self.abs_max_v: 2711.0\n",
      "lif layer 2 self.abs_max_v: 2717.5\n",
      "lif layer 2 self.abs_max_v: 2720.0\n",
      "lif layer 2 self.abs_max_v: 2793.0\n",
      "lif layer 2 self.abs_max_v: 2870.5\n",
      "lif layer 2 self.abs_max_v: 2992.5\n",
      "fc layer 2 self.abs_max_out: 1900.0\n",
      "lif layer 2 self.abs_max_v: 3135.0\n",
      "fc layer 2 self.abs_max_out: 1941.0\n",
      "lif layer 2 self.abs_max_v: 3508.5\n",
      "lif layer 2 self.abs_max_v: 3588.0\n",
      "fc layer 2 self.abs_max_out: 2023.0\n",
      "fc layer 2 self.abs_max_out: 2186.0\n",
      "lif layer 1 self.abs_max_v: 6790.0\n",
      "lif layer 1 self.abs_max_v: 6892.0\n",
      "lif layer 2 self.abs_max_v: 3609.0\n",
      "fc layer 3 self.abs_max_out: 698.0\n",
      "fc layer 1 self.abs_max_out: 3968.0\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "lif layer 2 self.abs_max_v: 3702.5\n",
      "fc layer 1 self.abs_max_out: 3973.0\n",
      "lif layer 1 self.abs_max_v: 6965.5\n",
      "lif layer 1 self.abs_max_v: 7279.0\n",
      "fc layer 1 self.abs_max_out: 4203.0\n",
      "lif layer 1 self.abs_max_v: 7279.5\n",
      "lif layer 1 self.abs_max_v: 7636.0\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "fc layer 3 self.abs_max_out: 770.0\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "lif layer 2 self.abs_max_v: 3789.5\n",
      "fc layer 1 self.abs_max_out: 4226.0\n",
      "lif layer 2 self.abs_max_v: 3882.5\n",
      "lif layer 2 self.abs_max_v: 4003.5\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "lif layer 2 self.abs_max_v: 4069.0\n",
      "lif layer 2 self.abs_max_v: 4161.5\n",
      "fc layer 2 self.abs_max_out: 2197.0\n",
      "fc layer 2 self.abs_max_out: 2247.0\n",
      "fc layer 2 self.abs_max_out: 2528.0\n",
      "lif layer 2 self.abs_max_v: 4394.0\n",
      "lif layer 2 self.abs_max_v: 4556.0\n",
      "fc layer 1 self.abs_max_out: 4511.0\n",
      "lif layer 1 self.abs_max_v: 7652.5\n",
      "lif layer 2 self.abs_max_v: 4614.5\n",
      "fc layer 3 self.abs_max_out: 829.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 849.0\n",
      "lif layer 1 self.abs_max_v: 7683.0\n",
      "lif layer 1 self.abs_max_v: 7761.5\n",
      "lif layer 1 self.abs_max_v: 8055.5\n",
      "lif layer 1 self.abs_max_v: 8268.0\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "fc layer 1 self.abs_max_out: 4597.0\n",
      "fc layer 1 self.abs_max_out: 4673.0\n",
      "lif layer 1 self.abs_max_v: 8401.5\n",
      "lif layer 1 self.abs_max_v: 8798.0\n",
      "lif layer 1 self.abs_max_v: 8882.0\n",
      "fc layer 1 self.abs_max_out: 4807.0\n",
      "fc layer 1 self.abs_max_out: 5558.0\n",
      "lif layer 1 self.abs_max_v: 9106.0\n",
      "lif layer 1 self.abs_max_v: 9644.0\n",
      "lif layer 1 self.abs_max_v: 10036.5\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "fc layer 3 self.abs_max_out: 890.0\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "fc layer 3 self.abs_max_out: 896.0\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 3 self.abs_max_out: 944.0\n",
      "fc layer 3 self.abs_max_out: 953.0\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "fc layer 3 self.abs_max_out: 977.0\n",
      "fc layer 3 self.abs_max_out: 1007.0\n",
      "fc layer 3 self.abs_max_out: 1015.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.726076/  1.969208, val:  26.67%, val_best:  26.67%, tr:  99.08%, tr_best:  99.08%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7296%\n",
      "layer   2  Sparsity: 69.4706%\n",
      "layer   3  Sparsity: 63.2142%\n",
      "total_backward_count 9790 real_backward_count 1555  15.884%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 10073.0\n",
      "fc layer 1 self.abs_max_out: 5887.0\n",
      "lif layer 1 self.abs_max_v: 10923.5\n",
      "lif layer 1 self.abs_max_v: 10966.0\n",
      "lif layer 1 self.abs_max_v: 11059.5\n",
      "lif layer 1 self.abs_max_v: 11064.0\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "lif layer 2 self.abs_max_v: 4617.5\n",
      "fc layer 2 self.abs_max_out: 2645.0\n",
      "lif layer 2 self.abs_max_v: 4746.0\n",
      "lif layer 2 self.abs_max_v: 4978.0\n",
      "lif layer 2 self.abs_max_v: 5077.0\n",
      "fc layer 2 self.abs_max_out: 2726.0\n",
      "fc layer 2 self.abs_max_out: 2865.0\n",
      "fc layer 2 self.abs_max_out: 2892.0\n",
      "fc layer 2 self.abs_max_out: 2944.0\n",
      "fc layer 2 self.abs_max_out: 2947.0\n",
      "fc layer 1 self.abs_max_out: 5936.0\n",
      "fc layer 1 self.abs_max_out: 6128.0\n",
      "fc layer 1 self.abs_max_out: 6560.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.622283/  1.958834, val:  41.25%, val_best:  41.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7735%\n",
      "layer   2  Sparsity: 74.3954%\n",
      "layer   3  Sparsity: 66.9456%\n",
      "total_backward_count 19580 real_backward_count 2914  14.883%\n",
      "fc layer 3 self.abs_max_out: 1059.0\n",
      "lif layer 2 self.abs_max_v: 5178.0\n",
      "lif layer 1 self.abs_max_v: 11127.5\n",
      "lif layer 1 self.abs_max_v: 11211.5\n",
      "lif layer 1 self.abs_max_v: 11428.0\n",
      "fc layer 3 self.abs_max_out: 1063.0\n",
      "fc layer 2 self.abs_max_out: 2949.0\n",
      "fc layer 2 self.abs_max_out: 3105.0\n",
      "lif layer 2 self.abs_max_v: 5351.5\n",
      "lif layer 2 self.abs_max_v: 5774.0\n",
      "fc layer 2 self.abs_max_out: 3136.0\n",
      "lif layer 1 self.abs_max_v: 11600.0\n",
      "lif layer 1 self.abs_max_v: 12177.0\n",
      "fc layer 1 self.abs_max_out: 6808.0\n",
      "lif layer 1 self.abs_max_v: 12347.0\n",
      "lif layer 1 self.abs_max_v: 12574.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.609671/  1.889667, val:  45.42%, val_best:  45.42%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7784%\n",
      "layer   2  Sparsity: 76.9660%\n",
      "layer   3  Sparsity: 66.8833%\n",
      "total_backward_count 29370 real_backward_count 4208  14.328%\n",
      "fc layer 3 self.abs_max_out: 1106.0\n",
      "fc layer 3 self.abs_max_out: 1142.0\n",
      "fc layer 3 self.abs_max_out: 1171.0\n",
      "fc layer 1 self.abs_max_out: 6902.0\n",
      "fc layer 1 self.abs_max_out: 6973.0\n",
      "lif layer 1 self.abs_max_v: 12687.0\n",
      "lif layer 1 self.abs_max_v: 12896.0\n",
      "lif layer 1 self.abs_max_v: 12988.0\n",
      "fc layer 1 self.abs_max_out: 7478.0\n",
      "lif layer 1 self.abs_max_v: 13333.0\n",
      "lif layer 1 self.abs_max_v: 13721.5\n",
      "fc layer 2 self.abs_max_out: 3162.0\n",
      "fc layer 1 self.abs_max_out: 7589.0\n",
      "lif layer 1 self.abs_max_v: 13908.5\n",
      "lif layer 1 self.abs_max_v: 14286.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.582932/  1.883404, val:  33.33%, val_best:  45.42%, tr:  99.08%, tr_best:  99.49%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6942%\n",
      "layer   2  Sparsity: 78.4954%\n",
      "layer   3  Sparsity: 66.4887%\n",
      "total_backward_count 39160 real_backward_count 5523  14.104%\n",
      "fc layer 1 self.abs_max_out: 7629.0\n",
      "fc layer 1 self.abs_max_out: 7672.0\n",
      "fc layer 1 self.abs_max_out: 7676.0\n",
      "fc layer 1 self.abs_max_out: 7844.0\n",
      "fc layer 1 self.abs_max_out: 7937.0\n",
      "fc layer 1 self.abs_max_out: 8056.0\n",
      "fc layer 1 self.abs_max_out: 8522.0\n",
      "lif layer 1 self.abs_max_v: 15111.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.546051/  1.839645, val:  54.17%, val_best:  54.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7211%\n",
      "layer   2  Sparsity: 77.9638%\n",
      "layer   3  Sparsity: 65.3720%\n",
      "total_backward_count 48950 real_backward_count 6720  13.728%\n",
      "lif layer 1 self.abs_max_v: 15420.5\n",
      "fc layer 1 self.abs_max_out: 8552.0\n",
      "lif layer 2 self.abs_max_v: 5910.0\n",
      "fc layer 1 self.abs_max_out: 8579.0\n",
      "fc layer 1 self.abs_max_out: 8689.0\n",
      "lif layer 1 self.abs_max_v: 15579.0\n",
      "lif layer 1 self.abs_max_v: 16403.5\n",
      "fc layer 2 self.abs_max_out: 3192.0\n",
      "fc layer 1 self.abs_max_out: 8828.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.509374/  1.838614, val:  46.67%, val_best:  54.17%, tr:  99.39%, tr_best:  99.49%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7270%\n",
      "layer   2  Sparsity: 78.0072%\n",
      "layer   3  Sparsity: 64.8838%\n",
      "total_backward_count 58740 real_backward_count 7891  13.434%\n",
      "fc layer 1 self.abs_max_out: 8934.0\n",
      "lif layer 1 self.abs_max_v: 16898.5\n",
      "fc layer 2 self.abs_max_out: 3223.0\n",
      "lif layer 2 self.abs_max_v: 5955.0\n",
      "fc layer 1 self.abs_max_out: 9495.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.506652/  1.779598, val:  53.75%, val_best:  54.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7074%\n",
      "layer   2  Sparsity: 77.5921%\n",
      "layer   3  Sparsity: 65.3285%\n",
      "total_backward_count 68530 real_backward_count 9072  13.238%\n",
      "fc layer 3 self.abs_max_out: 1224.0\n",
      "fc layer 3 self.abs_max_out: 1300.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.461090/  1.749329, val:  50.00%, val_best:  54.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7254%\n",
      "layer   2  Sparsity: 77.2146%\n",
      "layer   3  Sparsity: 64.1116%\n",
      "total_backward_count 78320 real_backward_count 10200  13.023%\n",
      "fc layer 2 self.abs_max_out: 3399.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.461218/  1.764253, val:  47.92%, val_best:  54.17%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7159%\n",
      "layer   2  Sparsity: 75.9304%\n",
      "layer   3  Sparsity: 63.2192%\n",
      "total_backward_count 88110 real_backward_count 11382  12.918%\n",
      "fc layer 2 self.abs_max_out: 3525.0\n",
      "lif layer 1 self.abs_max_v: 17499.5\n",
      "fc layer 1 self.abs_max_out: 9681.0\n",
      "fc layer 1 self.abs_max_out: 10029.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.450257/  1.781847, val:  49.17%, val_best:  54.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7345%\n",
      "layer   2  Sparsity: 74.5926%\n",
      "layer   3  Sparsity: 64.0780%\n",
      "total_backward_count 97900 real_backward_count 12432  12.699%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.434000/  1.807198, val:  35.42%, val_best:  54.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7040%\n",
      "layer   2  Sparsity: 73.3783%\n",
      "layer   3  Sparsity: 63.2722%\n",
      "total_backward_count 107690 real_backward_count 13510  12.545%\n",
      "fc layer 2 self.abs_max_out: 3566.0\n",
      "fc layer 3 self.abs_max_out: 1305.0\n",
      "fc layer 3 self.abs_max_out: 1315.0\n",
      "fc layer 1 self.abs_max_out: 10188.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.418082/  1.758418, val:  41.67%, val_best:  54.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7022%\n",
      "layer   2  Sparsity: 73.9357%\n",
      "layer   3  Sparsity: 62.0895%\n",
      "total_backward_count 117480 real_backward_count 14567  12.400%\n",
      "lif layer 2 self.abs_max_v: 6223.5\n",
      "fc layer 2 self.abs_max_out: 3610.0\n",
      "lif layer 2 self.abs_max_v: 6722.0\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.399388/  1.757476, val:  44.17%, val_best:  54.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6933%\n",
      "layer   2  Sparsity: 75.7991%\n",
      "layer   3  Sparsity: 62.4287%\n",
      "total_backward_count 127270 real_backward_count 15598  12.256%\n",
      "fc layer 1 self.abs_max_out: 11285.0\n",
      "lif layer 1 self.abs_max_v: 19322.5\n",
      "lif layer 1 self.abs_max_v: 19446.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.419411/  1.760983, val:  44.17%, val_best:  54.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7564%\n",
      "layer   2  Sparsity: 74.1489%\n",
      "layer   3  Sparsity: 62.3394%\n",
      "total_backward_count 137060 real_backward_count 16653  12.150%\n",
      "lif layer 1 self.abs_max_v: 19520.5\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.387988/  1.704706, val:  54.17%, val_best:  54.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6725%\n",
      "layer   2  Sparsity: 72.9189%\n",
      "layer   3  Sparsity: 61.6359%\n",
      "total_backward_count 146850 real_backward_count 17714  12.063%\n",
      "fc layer 2 self.abs_max_out: 3724.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.404374/  1.714352, val:  53.75%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7040%\n",
      "layer   2  Sparsity: 72.8542%\n",
      "layer   3  Sparsity: 61.6319%\n",
      "total_backward_count 156640 real_backward_count 18765  11.980%\n",
      "lif layer 1 self.abs_max_v: 19716.0\n",
      "fc layer 2 self.abs_max_out: 3754.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.406112/  1.712597, val:  57.92%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7381%\n",
      "layer   2  Sparsity: 73.3139%\n",
      "layer   3  Sparsity: 63.4266%\n",
      "total_backward_count 166430 real_backward_count 19811  11.904%\n",
      "fc layer 1 self.abs_max_out: 11571.0\n",
      "fc layer 1 self.abs_max_out: 12025.0\n",
      "fc layer 2 self.abs_max_out: 3795.0\n",
      "lif layer 2 self.abs_max_v: 6936.5\n",
      "lif layer 2 self.abs_max_v: 6958.5\n",
      "fc layer 2 self.abs_max_out: 3860.0\n",
      "fc layer 2 self.abs_max_out: 4101.0\n",
      "lif layer 2 self.abs_max_v: 7178.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.422538/  1.710078, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6765%\n",
      "layer   2  Sparsity: 74.1559%\n",
      "layer   3  Sparsity: 65.3296%\n",
      "total_backward_count 176220 real_backward_count 20852  11.833%\n",
      "fc layer 3 self.abs_max_out: 1420.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.428095/  1.711512, val:  50.42%, val_best:  57.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6963%\n",
      "layer   2  Sparsity: 72.4012%\n",
      "layer   3  Sparsity: 64.9945%\n",
      "total_backward_count 186010 real_backward_count 21925  11.787%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.403266/  1.774554, val:  45.42%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7223%\n",
      "layer   2  Sparsity: 72.1671%\n",
      "layer   3  Sparsity: 64.3082%\n",
      "total_backward_count 195800 real_backward_count 22887  11.689%\n",
      "lif layer 1 self.abs_max_v: 20055.0\n",
      "lif layer 1 self.abs_max_v: 20278.5\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.398215/  1.775813, val:  42.08%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7018%\n",
      "layer   2  Sparsity: 72.5558%\n",
      "layer   3  Sparsity: 65.1264%\n",
      "total_backward_count 205590 real_backward_count 23909  11.629%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.399396/  1.699825, val:  54.17%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7236%\n",
      "layer   2  Sparsity: 72.9362%\n",
      "layer   3  Sparsity: 63.8434%\n",
      "total_backward_count 215380 real_backward_count 24952  11.585%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.380311/  1.685837, val:  50.42%, val_best:  57.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7586%\n",
      "layer   2  Sparsity: 72.9351%\n",
      "layer   3  Sparsity: 64.5098%\n",
      "total_backward_count 225170 real_backward_count 25962  11.530%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.378912/  1.702905, val:  58.33%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6727%\n",
      "layer   2  Sparsity: 72.3010%\n",
      "layer   3  Sparsity: 63.4243%\n",
      "total_backward_count 234960 real_backward_count 27000  11.491%\n",
      "lif layer 1 self.abs_max_v: 20386.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.387560/  1.683010, val:  59.58%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7443%\n",
      "layer   2  Sparsity: 73.3292%\n",
      "layer   3  Sparsity: 65.0164%\n",
      "total_backward_count 244750 real_backward_count 28017  11.447%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.385133/  1.619541, val:  62.92%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7009%\n",
      "layer   2  Sparsity: 72.2705%\n",
      "layer   3  Sparsity: 63.9755%\n",
      "total_backward_count 254540 real_backward_count 29028  11.404%\n",
      "lif layer 1 self.abs_max_v: 20522.5\n",
      "lif layer 1 self.abs_max_v: 20765.5\n",
      "lif layer 1 self.abs_max_v: 20860.0\n",
      "lif layer 1 self.abs_max_v: 20883.5\n",
      "lif layer 1 self.abs_max_v: 21294.0\n",
      "lif layer 1 self.abs_max_v: 21319.5\n",
      "lif layer 1 self.abs_max_v: 21954.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.362614/  1.644963, val:  57.92%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7243%\n",
      "layer   2  Sparsity: 73.2141%\n",
      "layer   3  Sparsity: 65.4353%\n",
      "total_backward_count 264330 real_backward_count 30027  11.360%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.349966/  1.644675, val:  62.08%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6580%\n",
      "layer   2  Sparsity: 72.0760%\n",
      "layer   3  Sparsity: 63.3429%\n",
      "total_backward_count 274120 real_backward_count 31021  11.317%\n",
      "lif layer 1 self.abs_max_v: 22060.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.348787/  1.694729, val:  46.25%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7411%\n",
      "layer   2  Sparsity: 73.0268%\n",
      "layer   3  Sparsity: 63.5661%\n",
      "total_backward_count 283910 real_backward_count 31999  11.271%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.369784/  1.707886, val:  48.75%, val_best:  62.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7118%\n",
      "layer   2  Sparsity: 72.6333%\n",
      "layer   3  Sparsity: 63.7617%\n",
      "total_backward_count 293700 real_backward_count 32996  11.235%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.371030/  1.658291, val:  55.83%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7580%\n",
      "layer   2  Sparsity: 72.3982%\n",
      "layer   3  Sparsity: 64.9084%\n",
      "total_backward_count 303490 real_backward_count 33977  11.195%\n",
      "fc layer 1 self.abs_max_out: 12063.0\n",
      "fc layer 2 self.abs_max_out: 4114.0\n",
      "fc layer 2 self.abs_max_out: 4206.0\n",
      "fc layer 2 self.abs_max_out: 4489.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.346864/  1.682218, val:  53.33%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7106%\n",
      "layer   2  Sparsity: 71.3056%\n",
      "layer   3  Sparsity: 64.7892%\n",
      "total_backward_count 313280 real_backward_count 34938  11.152%\n",
      "lif layer 1 self.abs_max_v: 22417.5\n",
      "fc layer 1 self.abs_max_out: 14548.0\n",
      "lif layer 1 self.abs_max_v: 24380.5\n",
      "lif layer 1 self.abs_max_v: 26331.5\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.331528/  1.688151, val:  59.17%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7036%\n",
      "layer   2  Sparsity: 71.5859%\n",
      "layer   3  Sparsity: 64.3395%\n",
      "total_backward_count 323070 real_backward_count 35859  11.099%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.349535/  1.635305, val:  57.08%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7470%\n",
      "layer   2  Sparsity: 71.1298%\n",
      "layer   3  Sparsity: 65.2422%\n",
      "total_backward_count 332860 real_backward_count 36846  11.070%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.332687/  1.652031, val:  49.58%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7026%\n",
      "layer   2  Sparsity: 71.0084%\n",
      "layer   3  Sparsity: 66.0870%\n",
      "total_backward_count 342650 real_backward_count 37828  11.040%\n",
      "lif layer 2 self.abs_max_v: 7199.0\n",
      "lif layer 2 self.abs_max_v: 7295.5\n",
      "fc layer 1 self.abs_max_out: 14658.0\n",
      "lif layer 1 self.abs_max_v: 26451.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.334042/  1.672636, val:  49.58%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6807%\n",
      "layer   2  Sparsity: 70.9114%\n",
      "layer   3  Sparsity: 66.6618%\n",
      "total_backward_count 352440 real_backward_count 38787  11.005%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.328789/  1.612429, val:  60.00%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.7813%\n",
      "layer   2  Sparsity: 70.4685%\n",
      "layer   3  Sparsity: 66.6316%\n",
      "total_backward_count 362230 real_backward_count 39723  10.966%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.338798/  1.646269, val:  56.25%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7484%\n",
      "layer   2  Sparsity: 69.8555%\n",
      "layer   3  Sparsity: 64.9199%\n",
      "total_backward_count 372020 real_backward_count 40623  10.920%\n",
      "lif layer 2 self.abs_max_v: 7304.0\n",
      "lif layer 2 self.abs_max_v: 7547.0\n",
      "lif layer 2 self.abs_max_v: 7733.5\n",
      "fc layer 1 self.abs_max_out: 14716.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.345752/  1.669370, val:  52.08%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   2  Sparsity: 70.4078%\n",
      "layer   3  Sparsity: 67.7712%\n",
      "total_backward_count 381810 real_backward_count 41607  10.897%\n",
      "lif layer 2 self.abs_max_v: 7880.0\n",
      "lif layer 2 self.abs_max_v: 8159.0\n",
      "fc layer 2 self.abs_max_out: 4661.0\n",
      "fc layer 2 self.abs_max_out: 4705.0\n",
      "lif layer 2 self.abs_max_v: 8162.5\n",
      "lif layer 2 self.abs_max_v: 8244.5\n",
      "lif layer 2 self.abs_max_v: 8467.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.356483/  1.656311, val:  55.42%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7466%\n",
      "layer   2  Sparsity: 69.4435%\n",
      "layer   3  Sparsity: 67.9489%\n",
      "total_backward_count 391600 real_backward_count 42533  10.861%\n",
      "fc layer 2 self.abs_max_out: 4937.0\n",
      "lif layer 2 self.abs_max_v: 8967.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.324587/  1.633459, val:  56.25%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6789%\n",
      "layer   2  Sparsity: 69.1170%\n",
      "layer   3  Sparsity: 66.8356%\n",
      "total_backward_count 401390 real_backward_count 43492  10.835%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.354246/  1.640022, val:  64.17%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7099%\n",
      "layer   2  Sparsity: 69.1802%\n",
      "layer   3  Sparsity: 66.1302%\n",
      "total_backward_count 411180 real_backward_count 44474  10.816%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.349708/  1.630284, val:  58.75%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7904%\n",
      "layer   2  Sparsity: 70.3902%\n",
      "layer   3  Sparsity: 66.6835%\n",
      "total_backward_count 420970 real_backward_count 45385  10.781%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.335163/  1.643167, val:  55.83%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7975%\n",
      "layer   2  Sparsity: 70.0656%\n",
      "layer   3  Sparsity: 65.8163%\n",
      "total_backward_count 430760 real_backward_count 46323  10.754%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.361338/  1.676124, val:  53.75%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7654%\n",
      "layer   2  Sparsity: 69.5678%\n",
      "layer   3  Sparsity: 66.5365%\n",
      "total_backward_count 440550 real_backward_count 47294  10.735%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.334860/  1.585710, val:  60.00%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6691%\n",
      "layer   2  Sparsity: 70.0027%\n",
      "layer   3  Sparsity: 65.6970%\n",
      "total_backward_count 450340 real_backward_count 48219  10.707%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.327016/  1.626386, val:  53.75%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6625%\n",
      "layer   2  Sparsity: 69.5898%\n",
      "layer   3  Sparsity: 65.1559%\n",
      "total_backward_count 460130 real_backward_count 49202  10.693%\n",
      "fc layer 2 self.abs_max_out: 4974.0\n",
      "fc layer 2 self.abs_max_out: 4977.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.310056/  1.597075, val:  61.25%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7842%\n",
      "layer   2  Sparsity: 69.4945%\n",
      "layer   3  Sparsity: 64.6543%\n",
      "total_backward_count 469920 real_backward_count 50100  10.661%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.310624/  1.665285, val:  54.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6550%\n",
      "layer   2  Sparsity: 69.4209%\n",
      "layer   3  Sparsity: 64.7766%\n",
      "total_backward_count 479710 real_backward_count 51053  10.642%\n",
      "fc layer 2 self.abs_max_out: 5107.0\n",
      "lif layer 2 self.abs_max_v: 9072.5\n",
      "lif layer 2 self.abs_max_v: 9135.5\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.321582/  1.620471, val:  52.92%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7171%\n",
      "layer   2  Sparsity: 69.0641%\n",
      "layer   3  Sparsity: 64.6749%\n",
      "total_backward_count 489500 real_backward_count 51977  10.618%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.282653/  1.604101, val:  55.42%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7460%\n",
      "layer   2  Sparsity: 68.5900%\n",
      "layer   3  Sparsity: 64.6822%\n",
      "total_backward_count 499290 real_backward_count 52937  10.602%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.280047/  1.610436, val:  59.17%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6920%\n",
      "layer   2  Sparsity: 68.9869%\n",
      "layer   3  Sparsity: 64.3559%\n",
      "total_backward_count 509080 real_backward_count 53869  10.582%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.278217/  1.572141, val:  67.50%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7432%\n",
      "layer   2  Sparsity: 68.8798%\n",
      "layer   3  Sparsity: 64.2012%\n",
      "total_backward_count 518870 real_backward_count 54801  10.562%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.295777/  1.575649, val:  61.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7473%\n",
      "layer   2  Sparsity: 67.8904%\n",
      "layer   3  Sparsity: 65.2151%\n",
      "total_backward_count 528660 real_backward_count 55723  10.540%\n",
      "fc layer 1 self.abs_max_out: 14831.0\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.297191/  1.590442, val:  60.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7588%\n",
      "layer   2  Sparsity: 68.6101%\n",
      "layer   3  Sparsity: 65.1145%\n",
      "total_backward_count 538450 real_backward_count 56602  10.512%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.265407/  1.595558, val:  59.17%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7332%\n",
      "layer   2  Sparsity: 69.3433%\n",
      "layer   3  Sparsity: 64.2967%\n",
      "total_backward_count 548240 real_backward_count 57540  10.495%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.259292/  1.623223, val:  50.83%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6889%\n",
      "layer   2  Sparsity: 69.9720%\n",
      "layer   3  Sparsity: 65.5262%\n",
      "total_backward_count 558030 real_backward_count 58482  10.480%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.284548/  1.592997, val:  60.42%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7634%\n",
      "layer   2  Sparsity: 70.4530%\n",
      "layer   3  Sparsity: 66.6982%\n",
      "total_backward_count 567820 real_backward_count 59404  10.462%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.268487/  1.564096, val:  62.92%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7406%\n",
      "layer   2  Sparsity: 70.1984%\n",
      "layer   3  Sparsity: 64.3817%\n",
      "total_backward_count 577610 real_backward_count 60286  10.437%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.254318/  1.579732, val:  59.58%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7295%\n",
      "layer   2  Sparsity: 70.0130%\n",
      "layer   3  Sparsity: 64.2461%\n",
      "total_backward_count 587400 real_backward_count 61191  10.417%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.252615/  1.599859, val:  52.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7508%\n",
      "layer   2  Sparsity: 69.9746%\n",
      "layer   3  Sparsity: 64.3745%\n",
      "total_backward_count 597190 real_backward_count 62130  10.404%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.258625/  1.594765, val:  55.83%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6867%\n",
      "layer   2  Sparsity: 69.9211%\n",
      "layer   3  Sparsity: 64.9559%\n",
      "total_backward_count 606980 real_backward_count 63031  10.384%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.278604/  1.616372, val:  52.92%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7109%\n",
      "layer   2  Sparsity: 70.5301%\n",
      "layer   3  Sparsity: 65.8241%\n",
      "total_backward_count 616770 real_backward_count 63920  10.364%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.264845/  1.578027, val:  55.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7456%\n",
      "layer   2  Sparsity: 69.3893%\n",
      "layer   3  Sparsity: 64.3731%\n",
      "total_backward_count 626560 real_backward_count 64772  10.338%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.247868/  1.594781, val:  55.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7001%\n",
      "layer   2  Sparsity: 68.5054%\n",
      "layer   3  Sparsity: 63.9723%\n",
      "total_backward_count 636350 real_backward_count 65658  10.318%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.231472/  1.560526, val:  54.17%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7142%\n",
      "layer   2  Sparsity: 69.2870%\n",
      "layer   3  Sparsity: 63.4370%\n",
      "total_backward_count 646140 real_backward_count 66559  10.301%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.238690/  1.653535, val:  45.83%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7051%\n",
      "layer   2  Sparsity: 69.1521%\n",
      "layer   3  Sparsity: 63.5626%\n",
      "total_backward_count 655930 real_backward_count 67430  10.280%\n",
      "fc layer 1 self.abs_max_out: 15254.0\n",
      "lif layer 1 self.abs_max_v: 26989.0\n",
      "fc layer 1 self.abs_max_out: 15377.0\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.271282/  1.589098, val:  60.42%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6989%\n",
      "layer   2  Sparsity: 69.2826%\n",
      "layer   3  Sparsity: 63.8221%\n",
      "total_backward_count 665720 real_backward_count 68318  10.262%\n",
      "lif layer 1 self.abs_max_v: 27421.5\n",
      "fc layer 1 self.abs_max_out: 16501.0\n",
      "lif layer 1 self.abs_max_v: 28682.0\n",
      "lif layer 1 self.abs_max_v: 28864.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.275177/  1.574194, val:  63.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7107%\n",
      "layer   2  Sparsity: 69.3727%\n",
      "layer   3  Sparsity: 64.7172%\n",
      "total_backward_count 675510 real_backward_count 69192  10.243%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.264855/  1.596172, val:  58.33%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6921%\n",
      "layer   2  Sparsity: 69.5349%\n",
      "layer   3  Sparsity: 63.9435%\n",
      "total_backward_count 685300 real_backward_count 70056  10.223%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.256531/  1.571497, val:  60.42%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7287%\n",
      "layer   2  Sparsity: 68.9154%\n",
      "layer   3  Sparsity: 64.1284%\n",
      "total_backward_count 695090 real_backward_count 70991  10.213%\n",
      "fc layer 1 self.abs_max_out: 16864.0\n",
      "lif layer 1 self.abs_max_v: 29476.0\n",
      "lif layer 1 self.abs_max_v: 29737.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.257669/  1.605525, val:  54.17%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7188%\n",
      "layer   2  Sparsity: 69.0474%\n",
      "layer   3  Sparsity: 64.9368%\n",
      "total_backward_count 704880 real_backward_count 71898  10.200%\n",
      "lif layer 1 self.abs_max_v: 29926.5\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.273771/  1.565478, val:  62.08%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7400%\n",
      "layer   2  Sparsity: 69.2053%\n",
      "layer   3  Sparsity: 66.9131%\n",
      "total_backward_count 714670 real_backward_count 72796  10.186%\n",
      "fc layer 3 self.abs_max_out: 1447.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.255431/  1.618361, val:  52.92%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7055%\n",
      "layer   2  Sparsity: 68.6693%\n",
      "layer   3  Sparsity: 66.0449%\n",
      "total_backward_count 724460 real_backward_count 73650  10.166%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.281400/  1.625575, val:  56.25%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7085%\n",
      "layer   2  Sparsity: 69.5136%\n",
      "layer   3  Sparsity: 66.7144%\n",
      "total_backward_count 734250 real_backward_count 74596  10.159%\n",
      "fc layer 3 self.abs_max_out: 1479.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.229830/  1.543893, val:  56.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7255%\n",
      "layer   2  Sparsity: 70.0940%\n",
      "layer   3  Sparsity: 65.1790%\n",
      "total_backward_count 744040 real_backward_count 75407  10.135%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.230353/  1.559098, val:  61.25%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7433%\n",
      "layer   2  Sparsity: 69.4809%\n",
      "layer   3  Sparsity: 64.1541%\n",
      "total_backward_count 753830 real_backward_count 76305  10.122%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.231173/  1.566365, val:  57.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7249%\n",
      "layer   2  Sparsity: 68.7115%\n",
      "layer   3  Sparsity: 65.1157%\n",
      "total_backward_count 763620 real_backward_count 77142  10.102%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.225376/  1.553783, val:  61.67%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7934%\n",
      "layer   2  Sparsity: 67.9426%\n",
      "layer   3  Sparsity: 65.4623%\n",
      "total_backward_count 773410 real_backward_count 77997  10.085%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.214466/  1.538196, val:  63.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7121%\n",
      "layer   2  Sparsity: 67.1164%\n",
      "layer   3  Sparsity: 63.8511%\n",
      "total_backward_count 783200 real_backward_count 78897  10.074%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.244574/  1.558190, val:  57.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7097%\n",
      "layer   2  Sparsity: 66.3195%\n",
      "layer   3  Sparsity: 65.5049%\n",
      "total_backward_count 792990 real_backward_count 79794  10.062%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.246395/  1.550713, val:  57.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7002%\n",
      "layer   2  Sparsity: 66.9588%\n",
      "layer   3  Sparsity: 65.4390%\n",
      "total_backward_count 802780 real_backward_count 80647  10.046%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.231602/  1.542886, val:  56.67%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7104%\n",
      "layer   2  Sparsity: 68.2569%\n",
      "layer   3  Sparsity: 66.0291%\n",
      "total_backward_count 812570 real_backward_count 81491  10.029%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.234827/  1.576507, val:  51.25%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6615%\n",
      "layer   2  Sparsity: 68.6236%\n",
      "layer   3  Sparsity: 66.4256%\n",
      "total_backward_count 822360 real_backward_count 82428  10.023%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.202277/  1.498999, val:  62.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6879%\n",
      "layer   2  Sparsity: 67.7199%\n",
      "layer   3  Sparsity: 65.4733%\n",
      "total_backward_count 832150 real_backward_count 83325  10.013%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.211650/  1.596150, val:  51.25%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7630%\n",
      "layer   2  Sparsity: 68.3914%\n",
      "layer   3  Sparsity: 64.6653%\n",
      "total_backward_count 841940 real_backward_count 84194  10.000%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.201395/  1.525847, val:  55.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7165%\n",
      "layer   2  Sparsity: 68.7968%\n",
      "layer   3  Sparsity: 64.0375%\n",
      "total_backward_count 851730 real_backward_count 85035   9.984%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.207730/  1.545644, val:  60.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6642%\n",
      "layer   2  Sparsity: 69.3204%\n",
      "layer   3  Sparsity: 64.2573%\n",
      "total_backward_count 861520 real_backward_count 85916   9.973%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.197136/  1.550496, val:  61.25%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6777%\n",
      "layer   2  Sparsity: 68.9711%\n",
      "layer   3  Sparsity: 65.1036%\n",
      "total_backward_count 871310 real_backward_count 86731   9.954%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.231983/  1.506359, val:  63.75%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7317%\n",
      "layer   2  Sparsity: 68.1949%\n",
      "layer   3  Sparsity: 65.6975%\n",
      "total_backward_count 881100 real_backward_count 87586   9.941%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.216037/  1.610542, val:  54.17%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7230%\n",
      "layer   2  Sparsity: 67.6233%\n",
      "layer   3  Sparsity: 65.6481%\n",
      "total_backward_count 890890 real_backward_count 88486   9.932%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.203542/  1.541621, val:  66.25%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6687%\n",
      "layer   2  Sparsity: 68.2929%\n",
      "layer   3  Sparsity: 63.5197%\n",
      "total_backward_count 900680 real_backward_count 89323   9.917%\n",
      "lif layer 1 self.abs_max_v: 30052.5\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.211834/  1.553554, val:  60.42%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7355%\n",
      "layer   2  Sparsity: 68.4311%\n",
      "layer   3  Sparsity: 65.0532%\n",
      "total_backward_count 910470 real_backward_count 90171   9.904%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.207504/  1.628314, val:  51.67%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7397%\n",
      "layer   2  Sparsity: 68.5278%\n",
      "layer   3  Sparsity: 63.2541%\n",
      "total_backward_count 920260 real_backward_count 91047   9.894%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.219104/  1.565826, val:  59.17%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7560%\n",
      "layer   2  Sparsity: 68.4878%\n",
      "layer   3  Sparsity: 65.4602%\n",
      "total_backward_count 930050 real_backward_count 91934   9.885%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.207202/  1.506477, val:  65.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7634%\n",
      "layer   2  Sparsity: 67.8130%\n",
      "layer   3  Sparsity: 65.2192%\n",
      "total_backward_count 939840 real_backward_count 92799   9.874%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.209085/  1.534151, val:  63.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6877%\n",
      "layer   2  Sparsity: 67.6683%\n",
      "layer   3  Sparsity: 64.7061%\n",
      "total_backward_count 949630 real_backward_count 93710   9.868%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.197861/  1.482135, val:  69.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7406%\n",
      "layer   2  Sparsity: 68.3918%\n",
      "layer   3  Sparsity: 64.2363%\n",
      "total_backward_count 959420 real_backward_count 94525   9.852%\n",
      "lif layer 1 self.abs_max_v: 30421.5\n",
      "lif layer 1 self.abs_max_v: 31410.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.173002/  1.613435, val:  49.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7239%\n",
      "layer   2  Sparsity: 68.1657%\n",
      "layer   3  Sparsity: 62.7244%\n",
      "total_backward_count 969210 real_backward_count 95347   9.838%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.186881/  1.516075, val:  63.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6710%\n",
      "layer   2  Sparsity: 67.8181%\n",
      "layer   3  Sparsity: 63.4938%\n",
      "total_backward_count 979000 real_backward_count 96173   9.824%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.185591/  1.522720, val:  67.50%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7082%\n",
      "layer   2  Sparsity: 67.0851%\n",
      "layer   3  Sparsity: 64.4490%\n",
      "total_backward_count 988790 real_backward_count 97009   9.811%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.214070/  1.530349, val:  65.00%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6969%\n",
      "layer   2  Sparsity: 67.3724%\n",
      "layer   3  Sparsity: 64.1464%\n",
      "total_backward_count 998580 real_backward_count 97887   9.803%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.178066/  1.508468, val:  57.92%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7617%\n",
      "layer   2  Sparsity: 67.6493%\n",
      "layer   3  Sparsity: 63.0975%\n",
      "total_backward_count 1008370 real_backward_count 98715   9.790%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.173957/  1.514545, val:  63.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.6872%\n",
      "layer   2  Sparsity: 67.5646%\n",
      "layer   3  Sparsity: 62.2387%\n",
      "total_backward_count 1018160 real_backward_count 99528   9.775%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.174206/  1.534172, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7479%\n",
      "layer   2  Sparsity: 68.8363%\n",
      "layer   3  Sparsity: 63.6068%\n",
      "total_backward_count 1027950 real_backward_count 100344   9.762%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.161810/  1.466914, val:  65.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7343%\n",
      "layer   2  Sparsity: 68.4172%\n",
      "layer   3  Sparsity: 63.0288%\n",
      "total_backward_count 1037740 real_backward_count 101193   9.751%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.144087/  1.602419, val:  46.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6914%\n",
      "layer   2  Sparsity: 68.4320%\n",
      "layer   3  Sparsity: 63.1210%\n",
      "total_backward_count 1047530 real_backward_count 101988   9.736%\n",
      "fc layer 3 self.abs_max_out: 1575.0\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.139884/  1.541471, val:  55.42%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7209%\n",
      "layer   2  Sparsity: 68.6293%\n",
      "layer   3  Sparsity: 63.6856%\n",
      "total_backward_count 1057320 real_backward_count 102802   9.723%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.142868/  1.466618, val:  68.75%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7304%\n",
      "layer   2  Sparsity: 68.7172%\n",
      "layer   3  Sparsity: 63.2715%\n",
      "total_backward_count 1067110 real_backward_count 103651   9.713%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.145970/  1.513107, val:  63.33%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6607%\n",
      "layer   2  Sparsity: 69.0033%\n",
      "layer   3  Sparsity: 65.5179%\n",
      "total_backward_count 1076900 real_backward_count 104497   9.704%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.173074/  1.511270, val:  64.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7963%\n",
      "layer   2  Sparsity: 69.2414%\n",
      "layer   3  Sparsity: 65.4579%\n",
      "total_backward_count 1086690 real_backward_count 105321   9.692%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.194753/  1.549075, val:  61.67%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7389%\n",
      "layer   2  Sparsity: 69.8658%\n",
      "layer   3  Sparsity: 66.2649%\n",
      "total_backward_count 1096480 real_backward_count 106117   9.678%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.167652/  1.518756, val:  60.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7443%\n",
      "layer   2  Sparsity: 70.4298%\n",
      "layer   3  Sparsity: 65.0931%\n",
      "total_backward_count 1106270 real_backward_count 106931   9.666%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.175499/  1.567845, val:  59.58%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6194%\n",
      "layer   2  Sparsity: 70.6436%\n",
      "layer   3  Sparsity: 65.2972%\n",
      "total_backward_count 1116060 real_backward_count 107745   9.654%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.176342/  1.568755, val:  52.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6910%\n",
      "layer   2  Sparsity: 70.1669%\n",
      "layer   3  Sparsity: 64.3474%\n",
      "total_backward_count 1125850 real_backward_count 108555   9.642%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.168806/  1.550801, val:  62.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7416%\n",
      "layer   2  Sparsity: 69.2363%\n",
      "layer   3  Sparsity: 63.2297%\n",
      "total_backward_count 1135640 real_backward_count 109337   9.628%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.195148/  1.513826, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7068%\n",
      "layer   2  Sparsity: 69.2710%\n",
      "layer   3  Sparsity: 61.9397%\n",
      "total_backward_count 1145430 real_backward_count 110137   9.615%\n",
      "fc layer 3 self.abs_max_out: 1591.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.143249/  1.502183, val:  59.58%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7853%\n",
      "layer   2  Sparsity: 69.5462%\n",
      "layer   3  Sparsity: 62.3660%\n",
      "total_backward_count 1155220 real_backward_count 110926   9.602%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.128542/  1.478117, val:  58.33%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6972%\n",
      "layer   2  Sparsity: 69.0836%\n",
      "layer   3  Sparsity: 62.2239%\n",
      "total_backward_count 1165010 real_backward_count 111716   9.589%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.129700/  1.467696, val:  65.42%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6957%\n",
      "layer   2  Sparsity: 68.6151%\n",
      "layer   3  Sparsity: 62.2505%\n",
      "total_backward_count 1174800 real_backward_count 112492   9.575%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.164924/  1.485593, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7091%\n",
      "layer   2  Sparsity: 68.4881%\n",
      "layer   3  Sparsity: 64.1882%\n",
      "total_backward_count 1184590 real_backward_count 113281   9.563%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.147331/  1.533047, val:  62.92%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7361%\n",
      "layer   2  Sparsity: 68.5580%\n",
      "layer   3  Sparsity: 64.0487%\n",
      "total_backward_count 1194380 real_backward_count 114107   9.554%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.128332/  1.521734, val:  57.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6887%\n",
      "layer   2  Sparsity: 69.0877%\n",
      "layer   3  Sparsity: 64.1986%\n",
      "total_backward_count 1204170 real_backward_count 114924   9.544%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.139487/  1.461771, val:  64.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6871%\n",
      "layer   2  Sparsity: 68.0045%\n",
      "layer   3  Sparsity: 62.6463%\n",
      "total_backward_count 1213960 real_backward_count 115774   9.537%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.119983/  1.443900, val:  62.08%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6949%\n",
      "layer   2  Sparsity: 67.8467%\n",
      "layer   3  Sparsity: 62.0819%\n",
      "total_backward_count 1223750 real_backward_count 116589   9.527%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.107831/  1.490511, val:  60.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6713%\n",
      "layer   2  Sparsity: 68.2280%\n",
      "layer   3  Sparsity: 62.5010%\n",
      "total_backward_count 1233540 real_backward_count 117411   9.518%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.150066/  1.529920, val:  63.33%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7258%\n",
      "layer   2  Sparsity: 68.3665%\n",
      "layer   3  Sparsity: 63.4659%\n",
      "total_backward_count 1243330 real_backward_count 118243   9.510%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.148363/  1.503028, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7652%\n",
      "layer   2  Sparsity: 67.9919%\n",
      "layer   3  Sparsity: 62.2523%\n",
      "total_backward_count 1253120 real_backward_count 119000   9.496%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.136320/  1.507680, val:  65.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7046%\n",
      "layer   2  Sparsity: 68.0341%\n",
      "layer   3  Sparsity: 60.8879%\n",
      "total_backward_count 1262910 real_backward_count 119737   9.481%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.146546/  1.456765, val:  67.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7369%\n",
      "layer   2  Sparsity: 67.0521%\n",
      "layer   3  Sparsity: 63.5587%\n",
      "total_backward_count 1272700 real_backward_count 120499   9.468%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.131739/  1.478122, val:  60.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7051%\n",
      "layer   2  Sparsity: 67.7354%\n",
      "layer   3  Sparsity: 62.7537%\n",
      "total_backward_count 1282490 real_backward_count 121308   9.459%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.122652/  1.461231, val:  62.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6765%\n",
      "layer   2  Sparsity: 68.1947%\n",
      "layer   3  Sparsity: 62.9671%\n",
      "total_backward_count 1292280 real_backward_count 122168   9.454%\n",
      "fc layer 3 self.abs_max_out: 1626.0\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.137394/  1.527596, val:  58.33%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7261%\n",
      "layer   2  Sparsity: 68.6293%\n",
      "layer   3  Sparsity: 63.4297%\n",
      "total_backward_count 1302070 real_backward_count 122983   9.445%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.148939/  1.470069, val:  57.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7069%\n",
      "layer   2  Sparsity: 67.8603%\n",
      "layer   3  Sparsity: 64.3125%\n",
      "total_backward_count 1311860 real_backward_count 123822   9.439%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.115584/  1.518246, val:  56.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7100%\n",
      "layer   2  Sparsity: 67.5817%\n",
      "layer   3  Sparsity: 65.0294%\n",
      "total_backward_count 1321650 real_backward_count 124602   9.428%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.129526/  1.462415, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6647%\n",
      "layer   2  Sparsity: 67.9238%\n",
      "layer   3  Sparsity: 64.3281%\n",
      "total_backward_count 1331440 real_backward_count 125435   9.421%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.135351/  1.443715, val:  67.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7370%\n",
      "layer   2  Sparsity: 68.0628%\n",
      "layer   3  Sparsity: 63.8529%\n",
      "total_backward_count 1341230 real_backward_count 126236   9.412%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.163510/  1.525846, val:  54.58%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7584%\n",
      "layer   2  Sparsity: 67.8341%\n",
      "layer   3  Sparsity: 65.8447%\n",
      "total_backward_count 1351020 real_backward_count 127038   9.403%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.160857/  1.497993, val:  57.50%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7183%\n",
      "layer   2  Sparsity: 68.6529%\n",
      "layer   3  Sparsity: 63.3608%\n",
      "total_backward_count 1360810 real_backward_count 127899   9.399%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.141571/  1.481493, val:  58.75%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7470%\n",
      "layer   2  Sparsity: 67.6450%\n",
      "layer   3  Sparsity: 64.0730%\n",
      "total_backward_count 1370600 real_backward_count 128697   9.390%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.173674/  1.476509, val:  55.83%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6453%\n",
      "layer   2  Sparsity: 68.1427%\n",
      "layer   3  Sparsity: 65.5647%\n",
      "total_backward_count 1380390 real_backward_count 129475   9.380%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.124534/  1.454870, val:  65.42%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6762%\n",
      "layer   2  Sparsity: 68.3854%\n",
      "layer   3  Sparsity: 63.6228%\n",
      "total_backward_count 1390180 real_backward_count 130268   9.371%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.115666/  1.493957, val:  57.08%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7284%\n",
      "layer   2  Sparsity: 68.2308%\n",
      "layer   3  Sparsity: 63.7712%\n",
      "total_backward_count 1399970 real_backward_count 131050   9.361%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.088843/  1.488643, val:  55.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7133%\n",
      "layer   2  Sparsity: 68.3684%\n",
      "layer   3  Sparsity: 62.7626%\n",
      "total_backward_count 1409760 real_backward_count 131828   9.351%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.098966/  1.459639, val:  58.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7225%\n",
      "layer   2  Sparsity: 67.9800%\n",
      "layer   3  Sparsity: 63.2566%\n",
      "total_backward_count 1419550 real_backward_count 132619   9.342%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.107653/  1.494359, val:  60.83%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7784%\n",
      "layer   2  Sparsity: 68.5444%\n",
      "layer   3  Sparsity: 64.2243%\n",
      "total_backward_count 1429340 real_backward_count 133411   9.334%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.125840/  1.485850, val:  64.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7561%\n",
      "layer   2  Sparsity: 68.1092%\n",
      "layer   3  Sparsity: 62.9202%\n",
      "total_backward_count 1439130 real_backward_count 134216   9.326%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.086105/  1.487575, val:  61.67%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7126%\n",
      "layer   2  Sparsity: 68.2826%\n",
      "layer   3  Sparsity: 63.8594%\n",
      "total_backward_count 1448920 real_backward_count 134957   9.314%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.102755/  1.498564, val:  56.67%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6945%\n",
      "layer   2  Sparsity: 69.1621%\n",
      "layer   3  Sparsity: 63.2847%\n",
      "total_backward_count 1458710 real_backward_count 135762   9.307%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.098716/  1.491436, val:  60.83%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7229%\n",
      "layer   2  Sparsity: 69.0051%\n",
      "layer   3  Sparsity: 62.9008%\n",
      "total_backward_count 1468500 real_backward_count 136558   9.299%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.127424/  1.520085, val:  58.75%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7750%\n",
      "layer   2  Sparsity: 67.8651%\n",
      "layer   3  Sparsity: 64.8272%\n",
      "total_backward_count 1478290 real_backward_count 137305   9.288%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.170703/  1.525731, val:  58.75%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7252%\n",
      "layer   2  Sparsity: 67.9357%\n",
      "layer   3  Sparsity: 65.7418%\n",
      "total_backward_count 1488080 real_backward_count 138142   9.283%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.188652/  1.589044, val:  50.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7278%\n",
      "layer   2  Sparsity: 68.3663%\n",
      "layer   3  Sparsity: 64.8953%\n",
      "total_backward_count 1497870 real_backward_count 138890   9.273%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.155738/  1.470911, val:  64.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6658%\n",
      "layer   2  Sparsity: 67.9708%\n",
      "layer   3  Sparsity: 65.6689%\n",
      "total_backward_count 1507660 real_backward_count 139767   9.270%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.134527/  1.495411, val:  60.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7302%\n",
      "layer   2  Sparsity: 67.5568%\n",
      "layer   3  Sparsity: 63.3648%\n",
      "total_backward_count 1517450 real_backward_count 140583   9.264%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.098927/  1.493751, val:  61.67%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7816%\n",
      "layer   2  Sparsity: 67.4395%\n",
      "layer   3  Sparsity: 62.7753%\n",
      "total_backward_count 1527240 real_backward_count 141342   9.255%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.097161/  1.509512, val:  57.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6830%\n",
      "layer   2  Sparsity: 67.3124%\n",
      "layer   3  Sparsity: 63.0273%\n",
      "total_backward_count 1537030 real_backward_count 142130   9.247%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.099911/  1.480267, val:  60.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7357%\n",
      "layer   2  Sparsity: 67.6740%\n",
      "layer   3  Sparsity: 64.2509%\n",
      "total_backward_count 1546820 real_backward_count 142888   9.238%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.117109/  1.535188, val:  52.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7620%\n",
      "layer   2  Sparsity: 68.5901%\n",
      "layer   3  Sparsity: 66.6711%\n",
      "total_backward_count 1556610 real_backward_count 143667   9.229%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.126955/  1.447979, val:  62.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7632%\n",
      "layer   2  Sparsity: 69.0407%\n",
      "layer   3  Sparsity: 65.8677%\n",
      "total_backward_count 1566400 real_backward_count 144444   9.221%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.080376/  1.479365, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7685%\n",
      "layer   2  Sparsity: 68.6757%\n",
      "layer   3  Sparsity: 63.7354%\n",
      "total_backward_count 1576190 real_backward_count 145257   9.216%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.092883/  1.441068, val:  66.67%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7225%\n",
      "layer   2  Sparsity: 68.1826%\n",
      "layer   3  Sparsity: 64.1989%\n",
      "total_backward_count 1585980 real_backward_count 146034   9.208%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.073336/  1.484384, val:  51.25%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6859%\n",
      "layer   2  Sparsity: 68.1425%\n",
      "layer   3  Sparsity: 63.5969%\n",
      "total_backward_count 1595770 real_backward_count 146838   9.202%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.068638/  1.497253, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7053%\n",
      "layer   2  Sparsity: 68.0537%\n",
      "layer   3  Sparsity: 63.9873%\n",
      "total_backward_count 1605560 real_backward_count 147594   9.193%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.108370/  1.497861, val:  53.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7005%\n",
      "layer   2  Sparsity: 68.2309%\n",
      "layer   3  Sparsity: 64.2111%\n",
      "total_backward_count 1615350 real_backward_count 148368   9.185%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.086316/  1.417967, val:  66.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7551%\n",
      "layer   2  Sparsity: 68.5075%\n",
      "layer   3  Sparsity: 64.7743%\n",
      "total_backward_count 1625140 real_backward_count 149151   9.178%\n",
      "fc layer 3 self.abs_max_out: 1718.0\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.070556/  1.417925, val:  62.08%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7799%\n",
      "layer   2  Sparsity: 68.3469%\n",
      "layer   3  Sparsity: 64.2099%\n",
      "total_backward_count 1634930 real_backward_count 149927   9.170%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.095578/  1.498019, val:  55.00%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.6387%\n",
      "layer   2  Sparsity: 68.3055%\n",
      "layer   3  Sparsity: 65.1288%\n",
      "total_backward_count 1644720 real_backward_count 150703   9.163%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.092823/  1.481296, val:  60.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7494%\n",
      "layer   2  Sparsity: 67.7673%\n",
      "layer   3  Sparsity: 64.1916%\n",
      "total_backward_count 1654510 real_backward_count 151504   9.157%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.089917/  1.430647, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7354%\n",
      "layer   2  Sparsity: 68.3194%\n",
      "layer   3  Sparsity: 63.3694%\n",
      "total_backward_count 1664300 real_backward_count 152261   9.149%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.112532/  1.556113, val:  51.67%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7311%\n",
      "layer   2  Sparsity: 68.1916%\n",
      "layer   3  Sparsity: 64.8037%\n",
      "total_backward_count 1674090 real_backward_count 153016   9.140%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.098783/  1.504336, val:  55.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6790%\n",
      "layer   2  Sparsity: 68.2986%\n",
      "layer   3  Sparsity: 63.5340%\n",
      "total_backward_count 1683880 real_backward_count 153796   9.133%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.089086/  1.478391, val:  56.25%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7242%\n",
      "layer   2  Sparsity: 67.4643%\n",
      "layer   3  Sparsity: 64.6976%\n",
      "total_backward_count 1693670 real_backward_count 154573   9.127%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.098637/  1.500606, val:  59.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.6811%\n",
      "layer   2  Sparsity: 67.5813%\n",
      "layer   3  Sparsity: 63.7249%\n",
      "total_backward_count 1703460 real_backward_count 155331   9.119%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.099144/  1.445886, val:  65.42%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7131%\n",
      "layer   2  Sparsity: 67.5932%\n",
      "layer   3  Sparsity: 64.3802%\n",
      "total_backward_count 1713250 real_backward_count 156109   9.112%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.071465/  1.415593, val:  66.67%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6857%\n",
      "layer   2  Sparsity: 68.1763%\n",
      "layer   3  Sparsity: 63.2803%\n",
      "total_backward_count 1723040 real_backward_count 156847   9.103%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.090343/  1.447914, val:  60.00%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.7265%\n",
      "layer   2  Sparsity: 67.5492%\n",
      "layer   3  Sparsity: 64.4571%\n",
      "total_backward_count 1732830 real_backward_count 157630   9.097%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.098440/  1.468715, val:  62.50%, val_best:  69.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7182%\n",
      "layer   2  Sparsity: 67.3673%\n",
      "layer   3  Sparsity: 66.0654%\n",
      "total_backward_count 1742620 real_backward_count 158413   9.091%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.135699/  1.563249, val:  55.83%, val_best:  69.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7112%\n",
      "layer   2  Sparsity: 67.8555%\n",
      "layer   3  Sparsity: 65.5272%\n",
      "total_backward_count 1752410 real_backward_count 159147   9.082%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.102220/  1.479860, val:  60.83%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7310%\n",
      "layer   2  Sparsity: 68.2847%\n",
      "layer   3  Sparsity: 65.1877%\n",
      "total_backward_count 1762200 real_backward_count 159939   9.076%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.086436/  1.437159, val:  64.17%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6853%\n",
      "layer   2  Sparsity: 68.3684%\n",
      "layer   3  Sparsity: 64.0089%\n",
      "total_backward_count 1771990 real_backward_count 160685   9.068%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.078029/  1.438116, val:  62.08%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7019%\n",
      "layer   2  Sparsity: 67.9808%\n",
      "layer   3  Sparsity: 64.5241%\n",
      "total_backward_count 1781780 real_backward_count 161408   9.059%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.096769/  1.511836, val:  62.92%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.6660%\n",
      "layer   2  Sparsity: 67.5119%\n",
      "layer   3  Sparsity: 64.8789%\n",
      "total_backward_count 1791570 real_backward_count 162174   9.052%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.097201/  1.512881, val:  54.58%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7164%\n",
      "layer   2  Sparsity: 67.3774%\n",
      "layer   3  Sparsity: 65.0386%\n",
      "total_backward_count 1801360 real_backward_count 162935   9.045%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.098892/  1.523417, val:  53.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7157%\n",
      "layer   2  Sparsity: 67.1398%\n",
      "layer   3  Sparsity: 63.9859%\n",
      "total_backward_count 1811150 real_backward_count 163667   9.037%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.093307/  1.461705, val:  57.92%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7139%\n",
      "layer   2  Sparsity: 67.3757%\n",
      "layer   3  Sparsity: 65.7131%\n",
      "total_backward_count 1820940 real_backward_count 164461   9.032%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.088486/  1.448155, val:  61.25%, val_best:  69.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6821%\n",
      "layer   2  Sparsity: 68.2889%\n",
      "layer   3  Sparsity: 64.9519%\n",
      "total_backward_count 1830730 real_backward_count 165214   9.024%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.064845/  1.512662, val:  53.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7112%\n",
      "layer   2  Sparsity: 68.9581%\n",
      "layer   3  Sparsity: 62.9883%\n",
      "total_backward_count 1840520 real_backward_count 166010   9.020%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.074359/  1.506381, val:  54.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6762%\n",
      "layer   2  Sparsity: 68.8456%\n",
      "layer   3  Sparsity: 62.2536%\n",
      "total_backward_count 1850310 real_backward_count 166782   9.014%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.073620/  1.450133, val:  61.67%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6679%\n",
      "layer   2  Sparsity: 68.4781%\n",
      "layer   3  Sparsity: 63.7578%\n",
      "total_backward_count 1860100 real_backward_count 167529   9.006%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.076507/  1.448101, val:  62.08%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.6342%\n",
      "layer   2  Sparsity: 68.7893%\n",
      "layer   3  Sparsity: 63.7737%\n",
      "total_backward_count 1869890 real_backward_count 168310   9.001%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.083255/  1.511393, val:  50.00%, val_best:  69.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7151%\n",
      "layer   2  Sparsity: 69.0922%\n",
      "layer   3  Sparsity: 65.8676%\n",
      "total_backward_count 1879680 real_backward_count 169087   8.996%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.089780/  1.442313, val:  66.25%, val_best:  69.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.7116%\n",
      "layer   2  Sparsity: 68.8352%\n",
      "layer   3  Sparsity: 64.8660%\n",
      "total_backward_count 1889470 real_backward_count 169869   8.990%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.093009/  1.552533, val:  53.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7379%\n",
      "layer   2  Sparsity: 69.1491%\n",
      "layer   3  Sparsity: 64.5018%\n",
      "total_backward_count 1899260 real_backward_count 170579   8.981%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.075447/  1.434653, val:  58.33%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.7233%\n",
      "layer   2  Sparsity: 68.3718%\n",
      "layer   3  Sparsity: 64.0034%\n",
      "total_backward_count 1909050 real_backward_count 171291   8.973%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.057646/  1.471861, val:  54.17%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.6653%\n",
      "layer   2  Sparsity: 69.5484%\n",
      "layer   3  Sparsity: 62.6926%\n",
      "total_backward_count 1918840 real_backward_count 172039   8.966%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.071749/  1.444396, val:  60.42%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7429%\n",
      "layer   2  Sparsity: 69.2022%\n",
      "layer   3  Sparsity: 61.5522%\n",
      "total_backward_count 1928630 real_backward_count 172754   8.957%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.047361/  1.417172, val:  59.58%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.8388%\n",
      "layer   2  Sparsity: 69.1687%\n",
      "layer   3  Sparsity: 64.3569%\n",
      "total_backward_count 1938420 real_backward_count 173493   8.950%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.028778/  1.456206, val:  62.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.7161%\n",
      "layer   2  Sparsity: 69.5938%\n",
      "layer   3  Sparsity: 63.5456%\n",
      "total_backward_count 1948210 real_backward_count 174229   8.943%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.024300/  1.438497, val:  65.00%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.7129%\n",
      "layer   2  Sparsity: 69.0920%\n",
      "layer   3  Sparsity: 61.8383%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f89b1642f444127bcdd370b1f5c41de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñá‚ñà‚ñá‚ñá‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñÉ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.0243</td></tr><tr><td>val_acc_best</td><td>0.69583</td></tr><tr><td>val_acc_now</td><td>0.65</td></tr><tr><td>val_loss</td><td>1.4385</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-97</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v3zl7ik' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6v3zl7ik</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_064603-6v3zl7ik/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iypukvcm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_110619-iypukvcm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iypukvcm' target=\"_blank\">glad-sweep-103</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iypukvcm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iypukvcm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251116_110629_000', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 778.0\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1059.5\n",
      "fc layer 1 self.abs_max_out: 1028.0\n",
      "lif layer 1 self.abs_max_v: 1393.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 596.0\n",
      "fc layer 1 self.abs_max_out: 1325.0\n",
      "lif layer 1 self.abs_max_v: 1706.0\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "lif layer 2 self.abs_max_v: 946.5\n",
      "fc layer 1 self.abs_max_out: 2054.0\n",
      "lif layer 1 self.abs_max_v: 2191.5\n",
      "fc layer 2 self.abs_max_out: 770.0\n",
      "lif layer 2 self.abs_max_v: 1009.0\n",
      "fc layer 1 self.abs_max_out: 2291.0\n",
      "lif layer 1 self.abs_max_v: 2291.0\n",
      "fc layer 2 self.abs_max_out: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1270.0\n",
      "fc layer 3 self.abs_max_out: 41.0\n",
      "lif layer 2 self.abs_max_v: 1415.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 1 self.abs_max_out: 2668.0\n",
      "lif layer 1 self.abs_max_v: 2668.0\n",
      "lif layer 2 self.abs_max_v: 1589.5\n",
      "fc layer 3 self.abs_max_out: 155.0\n",
      "lif layer 2 self.abs_max_v: 1619.0\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "lif layer 1 self.abs_max_v: 3303.0\n",
      "fc layer 2 self.abs_max_out: 1177.0\n",
      "fc layer 3 self.abs_max_out: 197.0\n",
      "fc layer 2 self.abs_max_out: 1207.0\n",
      "lif layer 2 self.abs_max_v: 1750.0\n",
      "fc layer 1 self.abs_max_out: 3880.0\n",
      "lif layer 1 self.abs_max_v: 3880.0\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "lif layer 2 self.abs_max_v: 1818.5\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 1873.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "lif layer 2 self.abs_max_v: 2152.5\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "fc layer 1 self.abs_max_out: 4143.0\n",
      "lif layer 1 self.abs_max_v: 4143.0\n",
      "fc layer 1 self.abs_max_out: 4673.0\n",
      "lif layer 1 self.abs_max_v: 4673.0\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "lif layer 2 self.abs_max_v: 2205.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 1716.0\n",
      "lif layer 2 self.abs_max_v: 2295.0\n",
      "lif layer 2 self.abs_max_v: 2332.5\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "lif layer 2 self.abs_max_v: 2377.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "lif layer 2 self.abs_max_v: 2470.5\n",
      "lif layer 2 self.abs_max_v: 2503.5\n",
      "lif layer 2 self.abs_max_v: 2692.0\n",
      "fc layer 2 self.abs_max_out: 1753.0\n",
      "lif layer 2 self.abs_max_v: 2961.5\n",
      "lif layer 2 self.abs_max_v: 2980.0\n",
      "lif layer 2 self.abs_max_v: 3159.5\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "lif layer 2 self.abs_max_v: 3458.0\n",
      "fc layer 1 self.abs_max_out: 5066.0\n",
      "lif layer 1 self.abs_max_v: 5066.0\n",
      "fc layer 2 self.abs_max_out: 2093.0\n",
      "fc layer 2 self.abs_max_out: 2231.0\n",
      "fc layer 2 self.abs_max_out: 2359.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 1 self.abs_max_out: 5404.0\n",
      "lif layer 1 self.abs_max_v: 5404.0\n",
      "fc layer 2 self.abs_max_out: 2635.0\n",
      "fc layer 2 self.abs_max_out: 2716.0\n",
      "fc layer 1 self.abs_max_out: 5482.0\n",
      "lif layer 1 self.abs_max_v: 5482.0\n",
      "fc layer 1 self.abs_max_out: 5550.0\n",
      "lif layer 1 self.abs_max_v: 5550.0\n",
      "fc layer 1 self.abs_max_out: 5586.0\n",
      "lif layer 1 self.abs_max_v: 5586.0\n",
      "fc layer 2 self.abs_max_out: 2813.0\n",
      "fc layer 2 self.abs_max_out: 2903.0\n",
      "lif layer 2 self.abs_max_v: 3512.5\n",
      "lif layer 2 self.abs_max_v: 3603.0\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "lif layer 2 self.abs_max_v: 3650.5\n",
      "fc layer 3 self.abs_max_out: 605.0\n",
      "lif layer 2 self.abs_max_v: 3727.5\n",
      "lif layer 2 self.abs_max_v: 4158.0\n",
      "lif layer 2 self.abs_max_v: 4449.0\n",
      "lif layer 2 self.abs_max_v: 4517.5\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "fc layer 1 self.abs_max_out: 6440.0\n",
      "lif layer 1 self.abs_max_v: 6440.0\n",
      "lif layer 1 self.abs_max_v: 6518.5\n",
      "fc layer 1 self.abs_max_out: 6478.0\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "fc layer 2 self.abs_max_out: 3031.0\n",
      "lif layer 1 self.abs_max_v: 6569.0\n",
      "fc layer 1 self.abs_max_out: 6739.0\n",
      "lif layer 1 self.abs_max_v: 7717.5\n",
      "lif layer 1 self.abs_max_v: 8251.0\n",
      "fc layer 2 self.abs_max_out: 3095.0\n",
      "fc layer 2 self.abs_max_out: 3234.0\n",
      "lif layer 2 self.abs_max_v: 4725.5\n",
      "lif layer 2 self.abs_max_v: 4924.0\n",
      "fc layer 2 self.abs_max_out: 3331.0\n",
      "lif layer 2 self.abs_max_v: 5293.0\n",
      "lif layer 1 self.abs_max_v: 8686.5\n",
      "lif layer 1 self.abs_max_v: 8699.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 1 self.abs_max_out: 7660.0\n",
      "fc layer 2 self.abs_max_out: 3335.0\n",
      "fc layer 2 self.abs_max_out: 3423.0\n",
      "lif layer 1 self.abs_max_v: 8727.5\n",
      "lif layer 1 self.abs_max_v: 8986.0\n",
      "lif layer 1 self.abs_max_v: 9453.0\n",
      "fc layer 2 self.abs_max_out: 3483.0\n",
      "fc layer 1 self.abs_max_out: 7687.0\n",
      "fc layer 1 self.abs_max_out: 8119.0\n",
      "fc layer 1 self.abs_max_out: 8124.0\n",
      "fc layer 1 self.abs_max_out: 8590.0\n",
      "lif layer 1 self.abs_max_v: 10774.0\n",
      "lif layer 1 self.abs_max_v: 11360.0\n",
      "fc layer 3 self.abs_max_out: 859.0\n",
      "fc layer 2 self.abs_max_out: 3543.0\n",
      "fc layer 2 self.abs_max_out: 3756.0\n",
      "fc layer 2 self.abs_max_out: 3884.0\n",
      "fc layer 2 self.abs_max_out: 3964.0\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 2 self.abs_max_out: 4169.0\n",
      "fc layer 2 self.abs_max_out: 4410.0\n",
      "lif layer 1 self.abs_max_v: 12011.5\n",
      "fc layer 1 self.abs_max_out: 8853.0\n",
      "lif layer 2 self.abs_max_v: 5415.0\n",
      "lif layer 2 self.abs_max_v: 5452.0\n",
      "lif layer 2 self.abs_max_v: 5540.0\n",
      "lif layer 1 self.abs_max_v: 13265.5\n",
      "lif layer 1 self.abs_max_v: 13307.0\n",
      "lif layer 1 self.abs_max_v: 13822.0\n",
      "fc layer 2 self.abs_max_out: 4412.0\n",
      "lif layer 1 self.abs_max_v: 14446.0\n",
      "fc layer 2 self.abs_max_out: 4507.0\n",
      "fc layer 2 self.abs_max_out: 4681.0\n",
      "fc layer 1 self.abs_max_out: 9185.0\n",
      "fc layer 1 self.abs_max_out: 9781.0\n",
      "lif layer 2 self.abs_max_v: 5867.0\n",
      "lif layer 1 self.abs_max_v: 15189.5\n",
      "fc layer 1 self.abs_max_out: 9965.0\n",
      "lif layer 1 self.abs_max_v: 17560.0\n",
      "fc layer 2 self.abs_max_out: 4707.0\n",
      "fc layer 1 self.abs_max_out: 10211.0\n",
      "fc layer 1 self.abs_max_out: 10557.0\n",
      "lif layer 2 self.abs_max_v: 5878.0\n",
      "lif layer 2 self.abs_max_v: 5986.0\n",
      "fc layer 2 self.abs_max_out: 4832.0\n",
      "fc layer 2 self.abs_max_out: 5080.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.996249/  2.109547, val:  36.67%, val_best:  36.67%, tr:  93.87%, tr_best:  93.87%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9369%\n",
      "layer   2  Sparsity: 78.6620%\n",
      "layer   3  Sparsity: 85.3689%\n",
      "total_backward_count 9790 real_backward_count 2667  27.242%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "lif layer 1 self.abs_max_v: 17673.5\n",
      "fc layer 1 self.abs_max_out: 11131.0\n",
      "lif layer 1 self.abs_max_v: 19405.5\n",
      "fc layer 1 self.abs_max_out: 12773.0\n",
      "lif layer 1 self.abs_max_v: 20452.0\n",
      "lif layer 1 self.abs_max_v: 20642.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.003081/  2.110753, val:  43.75%, val_best:  43.75%, tr:  98.26%, tr_best:  98.26%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9249%\n",
      "layer   2  Sparsity: 79.0840%\n",
      "layer   3  Sparsity: 85.6235%\n",
      "total_backward_count 19580 real_backward_count 4708  24.045%\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "lif layer 2 self.abs_max_v: 6015.0\n",
      "lif layer 2 self.abs_max_v: 6016.5\n",
      "lif layer 2 self.abs_max_v: 6073.5\n",
      "lif layer 2 self.abs_max_v: 6227.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.999641/  2.094229, val:  50.42%, val_best:  50.42%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 78.8658%\n",
      "layer   3  Sparsity: 85.9183%\n",
      "total_backward_count 29370 real_backward_count 6650  22.642%\n",
      "lif layer 1 self.abs_max_v: 21245.5\n",
      "lif layer 1 self.abs_max_v: 21327.0\n",
      "lif layer 2 self.abs_max_v: 6445.5\n",
      "fc layer 1 self.abs_max_out: 13840.0\n",
      "lif layer 1 self.abs_max_v: 22384.0\n",
      "lif layer 1 self.abs_max_v: 22536.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.994674/  2.136218, val:  37.92%, val_best:  50.42%, tr:  98.57%, tr_best:  98.77%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 80.3835%\n",
      "layer   3  Sparsity: 85.9751%\n",
      "total_backward_count 39160 real_backward_count 8476  21.645%\n",
      "lif layer 2 self.abs_max_v: 6459.0\n",
      "lif layer 2 self.abs_max_v: 6482.0\n",
      "lif layer 1 self.abs_max_v: 23690.5\n",
      "fc layer 3 self.abs_max_out: 986.0\n",
      "fc layer 1 self.abs_max_out: 14127.0\n",
      "lif layer 1 self.abs_max_v: 24129.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  2.014954/  2.108958, val:  45.00%, val_best:  50.42%, tr:  98.88%, tr_best:  98.88%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 79.7655%\n",
      "layer   3  Sparsity: 86.1963%\n",
      "total_backward_count 48950 real_backward_count 10233  20.905%\n",
      "lif layer 1 self.abs_max_v: 24170.0\n",
      "fc layer 1 self.abs_max_out: 14964.0\n",
      "lif layer 1 self.abs_max_v: 24654.5\n",
      "lif layer 1 self.abs_max_v: 25063.5\n",
      "lif layer 2 self.abs_max_v: 6550.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  2.013188/  2.126377, val:  45.00%, val_best:  50.42%, tr:  98.26%, tr_best:  98.88%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9070%\n",
      "layer   2  Sparsity: 79.8656%\n",
      "layer   3  Sparsity: 86.7168%\n",
      "total_backward_count 58740 real_backward_count 11997  20.424%\n",
      "lif layer 2 self.abs_max_v: 6594.5\n",
      "lif layer 2 self.abs_max_v: 6710.5\n",
      "lif layer 2 self.abs_max_v: 6714.5\n",
      "lif layer 2 self.abs_max_v: 6866.0\n",
      "lif layer 2 self.abs_max_v: 7104.0\n",
      "fc layer 2 self.abs_max_out: 5093.0\n",
      "fc layer 2 self.abs_max_out: 5133.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  2.014854/  2.132010, val:  52.92%, val_best:  52.92%, tr:  98.16%, tr_best:  98.88%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 79.7083%\n",
      "layer   3  Sparsity: 86.6039%\n",
      "total_backward_count 68530 real_backward_count 13799  20.136%\n",
      "lif layer 2 self.abs_max_v: 7193.5\n",
      "lif layer 2 self.abs_max_v: 7268.0\n",
      "lif layer 2 self.abs_max_v: 7299.5\n",
      "lif layer 2 self.abs_max_v: 7374.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  2.014853/  2.107467, val:  45.42%, val_best:  52.92%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9136%\n",
      "layer   2  Sparsity: 79.8372%\n",
      "layer   3  Sparsity: 86.0360%\n",
      "total_backward_count 78320 real_backward_count 15520  19.816%\n",
      "lif layer 2 self.abs_max_v: 7400.5\n",
      "lif layer 2 self.abs_max_v: 7420.5\n",
      "lif layer 2 self.abs_max_v: 7525.5\n",
      "lif layer 2 self.abs_max_v: 7576.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  2.001124/  2.103701, val:  42.50%, val_best:  52.92%, tr:  99.08%, tr_best:  99.28%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 79.6836%\n",
      "layer   3  Sparsity: 85.5497%\n",
      "total_backward_count 88110 real_backward_count 17269  19.599%\n",
      "fc layer 3 self.abs_max_out: 1015.0\n",
      "lif layer 2 self.abs_max_v: 7726.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.966491/  2.100475, val:  50.42%, val_best:  52.92%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 79.4507%\n",
      "layer   3  Sparsity: 84.5046%\n",
      "total_backward_count 97900 real_backward_count 18924  19.330%\n",
      "fc layer 3 self.abs_max_out: 1040.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.984969/  2.126643, val:  43.75%, val_best:  52.92%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 79.3554%\n",
      "layer   3  Sparsity: 84.9203%\n",
      "total_backward_count 107690 real_backward_count 20605  19.134%\n",
      "lif layer 2 self.abs_max_v: 7920.0\n",
      "lif layer 2 self.abs_max_v: 7922.0\n",
      "lif layer 2 self.abs_max_v: 8396.0\n",
      "lif layer 2 self.abs_max_v: 8578.5\n",
      "lif layer 2 self.abs_max_v: 8865.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  2.000043/  2.114724, val:  50.83%, val_best:  52.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9446%\n",
      "layer   2  Sparsity: 79.6335%\n",
      "layer   3  Sparsity: 85.5306%\n",
      "total_backward_count 117480 real_backward_count 22316  18.996%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.995129/  2.131019, val:  41.25%, val_best:  52.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 79.0697%\n",
      "layer   3  Sparsity: 84.8700%\n",
      "total_backward_count 127270 real_backward_count 23882  18.765%\n",
      "fc layer 3 self.abs_max_out: 1076.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.995924/  2.114867, val:  40.83%, val_best:  52.92%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9257%\n",
      "layer   2  Sparsity: 79.4715%\n",
      "layer   3  Sparsity: 85.3655%\n",
      "total_backward_count 137060 real_backward_count 25600  18.678%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.978590/  2.121487, val:  42.08%, val_best:  52.92%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9279%\n",
      "layer   2  Sparsity: 78.5758%\n",
      "layer   3  Sparsity: 84.3162%\n",
      "total_backward_count 146850 real_backward_count 27233  18.545%\n",
      "fc layer 2 self.abs_max_out: 5170.0\n",
      "fc layer 2 self.abs_max_out: 5259.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.991317/  2.119573, val:  43.75%, val_best:  52.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9142%\n",
      "layer   2  Sparsity: 79.0403%\n",
      "layer   3  Sparsity: 84.9879%\n",
      "total_backward_count 156640 real_backward_count 28905  18.453%\n",
      "fc layer 2 self.abs_max_out: 5316.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  2.028306/  2.114066, val:  52.08%, val_best:  52.92%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8762%\n",
      "layer   2  Sparsity: 78.7311%\n",
      "layer   3  Sparsity: 85.9952%\n",
      "total_backward_count 166430 real_backward_count 30584  18.376%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  2.006826/  2.139432, val:  40.42%, val_best:  52.92%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9105%\n",
      "layer   2  Sparsity: 78.7237%\n",
      "layer   3  Sparsity: 85.1093%\n",
      "total_backward_count 176220 real_backward_count 32209  18.278%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  2.004581/  2.113162, val:  43.75%, val_best:  52.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 78.7149%\n",
      "layer   3  Sparsity: 85.1993%\n",
      "total_backward_count 186010 real_backward_count 33880  18.214%\n",
      "fc layer 2 self.abs_max_out: 5426.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  2.001325/  2.134260, val:  43.33%, val_best:  52.92%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 78.9011%\n",
      "layer   3  Sparsity: 84.8614%\n",
      "total_backward_count 195800 real_backward_count 35408  18.084%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  2.004403/  2.121857, val:  38.33%, val_best:  52.92%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8994%\n",
      "layer   2  Sparsity: 78.8137%\n",
      "layer   3  Sparsity: 84.8374%\n",
      "total_backward_count 205590 real_backward_count 36969  17.982%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  2.022130/  2.135595, val:  54.58%, val_best:  54.58%, tr:  98.67%, tr_best:  99.59%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9562%\n",
      "layer   2  Sparsity: 79.4911%\n",
      "layer   3  Sparsity: 86.5123%\n",
      "total_backward_count 215380 real_backward_count 38715  17.975%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  2.016273/  2.090379, val:  54.17%, val_best:  54.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9311%\n",
      "layer   2  Sparsity: 78.8002%\n",
      "layer   3  Sparsity: 86.3320%\n",
      "total_backward_count 225170 real_backward_count 40368  17.928%\n",
      "lif layer 2 self.abs_max_v: 8947.5\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  2.011037/  2.117782, val:  46.25%, val_best:  54.58%, tr:  99.28%, tr_best:  99.59%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 78.7726%\n",
      "layer   3  Sparsity: 85.4142%\n",
      "total_backward_count 234960 real_backward_count 42028  17.887%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  2.010464/  2.114673, val:  53.33%, val_best:  54.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9511%\n",
      "layer   2  Sparsity: 79.3675%\n",
      "layer   3  Sparsity: 85.3063%\n",
      "total_backward_count 244750 real_backward_count 43686  17.849%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  2.012291/  2.106772, val:  50.42%, val_best:  54.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 79.4267%\n",
      "layer   3  Sparsity: 84.8585%\n",
      "total_backward_count 254540 real_backward_count 45404  17.838%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.990799/  2.104093, val:  55.42%, val_best:  55.42%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 79.6169%\n",
      "layer   3  Sparsity: 85.2343%\n",
      "total_backward_count 264330 real_backward_count 47082  17.812%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.987363/  2.121753, val:  53.75%, val_best:  55.42%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9239%\n",
      "layer   2  Sparsity: 79.2654%\n",
      "layer   3  Sparsity: 85.4018%\n",
      "total_backward_count 274120 real_backward_count 48786  17.797%\n",
      "lif layer 2 self.abs_max_v: 8988.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.997067/  2.107759, val:  45.42%, val_best:  55.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9076%\n",
      "layer   2  Sparsity: 79.0204%\n",
      "layer   3  Sparsity: 84.8390%\n",
      "total_backward_count 283910 real_backward_count 50446  17.768%\n",
      "lif layer 2 self.abs_max_v: 8993.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.976144/  2.102473, val:  47.92%, val_best:  55.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 79.4123%\n",
      "layer   3  Sparsity: 84.9019%\n",
      "total_backward_count 293700 real_backward_count 52128  17.749%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.976065/  2.086811, val:  51.67%, val_best:  55.42%, tr:  99.18%, tr_best:  99.59%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 79.5720%\n",
      "layer   3  Sparsity: 84.7159%\n",
      "total_backward_count 303490 real_backward_count 53757  17.713%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.989205/  2.116481, val:  45.42%, val_best:  55.42%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9590%\n",
      "layer   2  Sparsity: 79.5377%\n",
      "layer   3  Sparsity: 84.8405%\n",
      "total_backward_count 313280 real_backward_count 55389  17.680%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.991369/  2.113034, val:  45.42%, val_best:  55.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8996%\n",
      "layer   2  Sparsity: 79.1641%\n",
      "layer   3  Sparsity: 84.8035%\n",
      "total_backward_count 323070 real_backward_count 57041  17.656%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  2.001132/  2.108266, val:  40.00%, val_best:  55.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 79.4883%\n",
      "layer   3  Sparsity: 85.3603%\n",
      "total_backward_count 332860 real_backward_count 58716  17.640%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.987203/  2.119398, val:  42.08%, val_best:  55.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9063%\n",
      "layer   2  Sparsity: 79.2367%\n",
      "layer   3  Sparsity: 85.3409%\n",
      "total_backward_count 342650 real_backward_count 60394  17.626%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.993147/  2.094282, val:  47.08%, val_best:  55.42%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9461%\n",
      "layer   2  Sparsity: 78.9283%\n",
      "layer   3  Sparsity: 85.1710%\n",
      "total_backward_count 352440 real_backward_count 62048  17.605%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  2.003952/  2.121357, val:  54.58%, val_best:  55.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 79.1696%\n",
      "layer   3  Sparsity: 85.5237%\n",
      "total_backward_count 362230 real_backward_count 63716  17.590%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.995993/  2.121275, val:  47.08%, val_best:  55.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 80.2835%\n",
      "layer   3  Sparsity: 85.8120%\n",
      "total_backward_count 372020 real_backward_count 65360  17.569%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  2.015500/  2.120789, val:  51.25%, val_best:  55.42%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 79.5325%\n",
      "layer   3  Sparsity: 86.0005%\n",
      "total_backward_count 381810 real_backward_count 67103  17.575%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.999204/  2.101143, val:  60.83%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 79.2795%\n",
      "layer   3  Sparsity: 85.3637%\n",
      "total_backward_count 391600 real_backward_count 68666  17.535%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  2.024678/  2.117171, val:  50.83%, val_best:  60.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9477%\n",
      "layer   2  Sparsity: 79.6635%\n",
      "layer   3  Sparsity: 86.2957%\n",
      "total_backward_count 401390 real_backward_count 70303  17.515%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  2.018054/  2.130177, val:  47.50%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9231%\n",
      "layer   2  Sparsity: 79.8395%\n",
      "layer   3  Sparsity: 86.1454%\n",
      "total_backward_count 411180 real_backward_count 71934  17.495%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  2.010377/  2.120614, val:  45.00%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9254%\n",
      "layer   2  Sparsity: 79.2726%\n",
      "layer   3  Sparsity: 85.6568%\n",
      "total_backward_count 420970 real_backward_count 73569  17.476%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  2.016197/  2.146415, val:  54.58%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9274%\n",
      "layer   2  Sparsity: 79.6632%\n",
      "layer   3  Sparsity: 86.2213%\n",
      "total_backward_count 430760 real_backward_count 75248  17.469%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  2.013953/  2.114007, val:  53.33%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9248%\n",
      "layer   2  Sparsity: 79.2109%\n",
      "layer   3  Sparsity: 85.9931%\n",
      "total_backward_count 440550 real_backward_count 76841  17.442%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  2.017233/  2.149781, val:  50.83%, val_best:  60.83%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 78.7845%\n",
      "layer   3  Sparsity: 85.5872%\n",
      "total_backward_count 450340 real_backward_count 78501  17.431%\n",
      "lif layer 2 self.abs_max_v: 9152.5\n",
      "lif layer 2 self.abs_max_v: 9579.5\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  2.024285/  2.115490, val:  46.67%, val_best:  60.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 79.3647%\n",
      "layer   3  Sparsity: 85.8218%\n",
      "total_backward_count 460130 real_backward_count 80161  17.421%\n",
      "fc layer 3 self.abs_max_out: 1106.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  2.014356/  2.133064, val:  35.83%, val_best:  60.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 79.2147%\n",
      "layer   3  Sparsity: 85.0801%\n",
      "total_backward_count 469920 real_backward_count 81812  17.410%\n",
      "fc layer 1 self.abs_max_out: 15093.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.998716/  2.125810, val:  45.83%, val_best:  60.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9425%\n",
      "layer   2  Sparsity: 79.1712%\n",
      "layer   3  Sparsity: 85.9572%\n",
      "total_backward_count 479710 real_backward_count 83424  17.391%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  2.013426/  2.087124, val:  56.25%, val_best:  60.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9284%\n",
      "layer   2  Sparsity: 79.4481%\n",
      "layer   3  Sparsity: 86.7032%\n",
      "total_backward_count 489500 real_backward_count 85101  17.385%\n",
      "fc layer 3 self.abs_max_out: 1117.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  2.016808/  2.106930, val:  52.50%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 78.9628%\n",
      "layer   3  Sparsity: 86.2992%\n",
      "total_backward_count 499290 real_backward_count 86706  17.366%\n",
      "fc layer 1 self.abs_max_out: 15146.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  2.015401/  2.118139, val:  43.33%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 79.2094%\n",
      "layer   3  Sparsity: 85.6669%\n",
      "total_backward_count 509080 real_backward_count 88253  17.336%\n",
      "fc layer 1 self.abs_max_out: 15487.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  2.013113/  2.127763, val:  55.42%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9299%\n",
      "layer   2  Sparsity: 79.3167%\n",
      "layer   3  Sparsity: 85.6474%\n",
      "total_backward_count 518870 real_backward_count 89901  17.326%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  2.021986/  2.117966, val:  52.50%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9044%\n",
      "layer   2  Sparsity: 78.9521%\n",
      "layer   3  Sparsity: 85.5195%\n",
      "total_backward_count 528660 real_backward_count 91520  17.312%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  2.023574/  2.122209, val:  55.83%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9200%\n",
      "layer   2  Sparsity: 79.4214%\n",
      "layer   3  Sparsity: 86.4094%\n",
      "total_backward_count 538450 real_backward_count 93157  17.301%\n",
      "lif layer 2 self.abs_max_v: 9696.5\n",
      "lif layer 2 self.abs_max_v: 9710.5\n",
      "lif layer 2 self.abs_max_v: 9975.5\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  2.025173/  2.119411, val:  52.08%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9145%\n",
      "layer   2  Sparsity: 79.0095%\n",
      "layer   3  Sparsity: 86.0880%\n",
      "total_backward_count 548240 real_backward_count 94837  17.298%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  2.028681/  2.129196, val:  50.83%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 79.1246%\n",
      "layer   3  Sparsity: 86.7367%\n",
      "total_backward_count 558030 real_backward_count 96532  17.299%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.999423/  2.090420, val:  45.83%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9417%\n",
      "layer   2  Sparsity: 79.0865%\n",
      "layer   3  Sparsity: 85.8883%\n",
      "total_backward_count 567820 real_backward_count 98062  17.270%\n",
      "lif layer 1 self.abs_max_v: 25173.5\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  2.009718/  2.126296, val:  48.75%, val_best:  60.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 79.6986%\n",
      "layer   3  Sparsity: 86.8309%\n",
      "total_backward_count 577610 real_backward_count 99645  17.251%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  2.011130/  2.117773, val:  40.42%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 79.4006%\n",
      "layer   3  Sparsity: 86.5562%\n",
      "total_backward_count 587400 real_backward_count 101232  17.234%\n",
      "fc layer 3 self.abs_max_out: 1119.0\n",
      "fc layer 3 self.abs_max_out: 1127.0\n",
      "fc layer 3 self.abs_max_out: 1379.0\n",
      "lif layer 1 self.abs_max_v: 26751.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.999208/  2.118758, val:  47.50%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9277%\n",
      "layer   2  Sparsity: 79.0561%\n",
      "layer   3  Sparsity: 86.0566%\n",
      "total_backward_count 597190 real_backward_count 102868  17.225%\n",
      "lif layer 1 self.abs_max_v: 26915.5\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.992502/  2.126876, val:  52.50%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9349%\n",
      "layer   2  Sparsity: 79.5351%\n",
      "layer   3  Sparsity: 86.0044%\n",
      "total_backward_count 606980 real_backward_count 104494  17.215%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.998931/  2.093565, val:  46.25%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8898%\n",
      "layer   2  Sparsity: 78.8639%\n",
      "layer   3  Sparsity: 85.1242%\n",
      "total_backward_count 616770 real_backward_count 106128  17.207%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  2.004648/  2.126220, val:  47.08%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 79.3621%\n",
      "layer   3  Sparsity: 86.1400%\n",
      "total_backward_count 626560 real_backward_count 107611  17.175%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  2.011309/  2.107222, val:  40.00%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9546%\n",
      "layer   2  Sparsity: 79.0142%\n",
      "layer   3  Sparsity: 85.7504%\n",
      "total_backward_count 636350 real_backward_count 109146  17.152%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.994930/  2.096483, val:  53.33%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9114%\n",
      "layer   2  Sparsity: 78.8202%\n",
      "layer   3  Sparsity: 85.7238%\n",
      "total_backward_count 646140 real_backward_count 110712  17.134%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  2.029213/  2.133381, val:  53.75%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 79.1096%\n",
      "layer   3  Sparsity: 87.1198%\n",
      "total_backward_count 655930 real_backward_count 112374  17.132%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  2.020416/  2.122161, val:  49.17%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8998%\n",
      "layer   2  Sparsity: 79.4256%\n",
      "layer   3  Sparsity: 86.7137%\n",
      "total_backward_count 665720 real_backward_count 113954  17.117%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  2.010283/  2.115483, val:  58.33%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9334%\n",
      "layer   2  Sparsity: 78.8553%\n",
      "layer   3  Sparsity: 86.5355%\n",
      "total_backward_count 675510 real_backward_count 115469  17.094%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  2.018181/  2.141021, val:  43.33%, val_best:  60.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 87.9102%\n",
      "layer   2  Sparsity: 79.5769%\n",
      "layer   3  Sparsity: 86.6653%\n",
      "total_backward_count 685300 real_backward_count 116988  17.071%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  2.024411/  2.121494, val:  48.33%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 79.5630%\n",
      "layer   3  Sparsity: 86.5646%\n",
      "total_backward_count 695090 real_backward_count 118570  17.058%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  2.033204/  2.125098, val:  43.33%, val_best:  60.83%, tr:  97.85%, tr_best:  99.59%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9232%\n",
      "layer   2  Sparsity: 79.4272%\n",
      "layer   3  Sparsity: 86.7261%\n",
      "total_backward_count 704880 real_backward_count 120205  17.053%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  2.035858/  2.151376, val:  54.58%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9288%\n",
      "layer   2  Sparsity: 79.2288%\n",
      "layer   3  Sparsity: 87.4026%\n",
      "total_backward_count 714670 real_backward_count 121842  17.049%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  2.026140/  2.159360, val:  49.17%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8941%\n",
      "layer   2  Sparsity: 79.5973%\n",
      "layer   3  Sparsity: 86.8416%\n",
      "total_backward_count 724460 real_backward_count 123481  17.045%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  2.014992/  2.140438, val:  50.00%, val_best:  60.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8986%\n",
      "layer   2  Sparsity: 79.3033%\n",
      "layer   3  Sparsity: 86.2459%\n",
      "total_backward_count 734250 real_backward_count 125145  17.044%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  2.027517/  2.126521, val:  51.67%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8861%\n",
      "layer   2  Sparsity: 79.4667%\n",
      "layer   3  Sparsity: 86.4368%\n",
      "total_backward_count 744040 real_backward_count 126623  17.018%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  2.032771/  2.116188, val:  52.92%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9370%\n",
      "layer   2  Sparsity: 79.2891%\n",
      "layer   3  Sparsity: 86.5456%\n",
      "total_backward_count 753830 real_backward_count 128145  16.999%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  2.032388/  2.154624, val:  35.00%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 78.9756%\n",
      "layer   3  Sparsity: 86.9105%\n",
      "total_backward_count 763620 real_backward_count 129674  16.981%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  2.028371/  2.116886, val:  45.42%, val_best:  60.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9424%\n",
      "layer   2  Sparsity: 79.0892%\n",
      "layer   3  Sparsity: 86.4646%\n",
      "total_backward_count 773410 real_backward_count 131170  16.960%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  2.024856/  2.141192, val:  55.00%, val_best:  60.83%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9021%\n",
      "layer   2  Sparsity: 79.1848%\n",
      "layer   3  Sparsity: 86.7030%\n",
      "total_backward_count 783200 real_backward_count 132601  16.931%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  2.008385/  2.114695, val:  48.33%, val_best:  60.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9190%\n",
      "layer   2  Sparsity: 78.9836%\n",
      "layer   3  Sparsity: 86.7830%\n",
      "total_backward_count 792990 real_backward_count 134076  16.908%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  2.035648/  2.167035, val:  29.58%, val_best:  60.83%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 78.8233%\n",
      "layer   3  Sparsity: 87.8127%\n",
      "total_backward_count 802780 real_backward_count 135630  16.895%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  2.023175/  2.118633, val:  54.17%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9199%\n",
      "layer   2  Sparsity: 79.2373%\n",
      "layer   3  Sparsity: 85.9174%\n",
      "total_backward_count 812570 real_backward_count 137226  16.888%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  2.006463/  2.139327, val:  41.67%, val_best:  60.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9220%\n",
      "layer   2  Sparsity: 79.0674%\n",
      "layer   3  Sparsity: 86.2686%\n",
      "total_backward_count 822360 real_backward_count 138728  16.869%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  2.015802/  2.131971, val:  53.33%, val_best:  60.83%, tr:  98.67%, tr_best:  99.80%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 79.5233%\n",
      "layer   3  Sparsity: 86.6767%\n",
      "total_backward_count 832150 real_backward_count 140291  16.859%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  2.018300/  2.139478, val:  47.08%, val_best:  60.83%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9449%\n",
      "layer   2  Sparsity: 79.5502%\n",
      "layer   3  Sparsity: 86.6512%\n",
      "total_backward_count 841940 real_backward_count 141910  16.855%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  2.032916/  2.140168, val:  48.75%, val_best:  60.83%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8963%\n",
      "layer   2  Sparsity: 79.1405%\n",
      "layer   3  Sparsity: 87.1221%\n",
      "total_backward_count 851730 real_backward_count 143443  16.841%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  2.030047/  2.142006, val:  45.42%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9393%\n",
      "layer   2  Sparsity: 79.3221%\n",
      "layer   3  Sparsity: 86.9262%\n",
      "total_backward_count 861520 real_backward_count 144962  16.826%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  2.029431/  2.117648, val:  54.17%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 79.7045%\n",
      "layer   3  Sparsity: 86.9258%\n",
      "total_backward_count 871310 real_backward_count 146495  16.813%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.999558/  2.130594, val:  50.42%, val_best:  60.83%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9218%\n",
      "layer   2  Sparsity: 79.2983%\n",
      "layer   3  Sparsity: 85.7512%\n",
      "total_backward_count 881100 real_backward_count 148038  16.801%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  2.022579/  2.120820, val:  53.75%, val_best:  60.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9225%\n",
      "layer   2  Sparsity: 79.1348%\n",
      "layer   3  Sparsity: 85.9896%\n",
      "total_backward_count 890890 real_backward_count 149615  16.794%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  2.013697/  2.119397, val:  55.83%, val_best:  60.83%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8826%\n",
      "layer   2  Sparsity: 79.3334%\n",
      "layer   3  Sparsity: 85.8543%\n",
      "total_backward_count 900680 real_backward_count 151153  16.782%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  2.001299/  2.127049, val:  44.58%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 79.5016%\n",
      "layer   3  Sparsity: 85.9908%\n",
      "total_backward_count 910470 real_backward_count 152670  16.768%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  2.004711/  2.117647, val:  53.33%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 79.2808%\n",
      "layer   3  Sparsity: 85.8893%\n",
      "total_backward_count 920260 real_backward_count 154153  16.751%\n",
      "fc layer 2 self.abs_max_out: 5684.0\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  2.016716/  2.131915, val:  48.33%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 78.9555%\n",
      "layer   3  Sparsity: 86.7350%\n",
      "total_backward_count 930050 real_backward_count 155653  16.736%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  2.030414/  2.126692, val:  61.67%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8824%\n",
      "layer   2  Sparsity: 79.3801%\n",
      "layer   3  Sparsity: 87.1908%\n",
      "total_backward_count 939840 real_backward_count 157234  16.730%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  2.027459/  2.119976, val:  43.75%, val_best:  61.67%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8978%\n",
      "layer   2  Sparsity: 78.8066%\n",
      "layer   3  Sparsity: 85.7721%\n",
      "total_backward_count 949630 real_backward_count 158752  16.717%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  2.012551/  2.090635, val:  50.83%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9068%\n",
      "layer   2  Sparsity: 78.7146%\n",
      "layer   3  Sparsity: 85.9934%\n",
      "total_backward_count 959420 real_backward_count 160254  16.703%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  2.008766/  2.119598, val:  42.50%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8816%\n",
      "layer   2  Sparsity: 78.9937%\n",
      "layer   3  Sparsity: 86.0365%\n",
      "total_backward_count 969210 real_backward_count 161694  16.683%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  2.013609/  2.110738, val:  59.58%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 79.1954%\n",
      "layer   3  Sparsity: 86.5757%\n",
      "total_backward_count 979000 real_backward_count 163160  16.666%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  2.039283/  2.140952, val:  45.83%, val_best:  61.67%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9453%\n",
      "layer   2  Sparsity: 79.1918%\n",
      "layer   3  Sparsity: 87.3525%\n",
      "total_backward_count 988790 real_backward_count 164686  16.655%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  2.024621/  2.113537, val:  59.58%, val_best:  61.67%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9125%\n",
      "layer   2  Sparsity: 79.3183%\n",
      "layer   3  Sparsity: 86.7324%\n",
      "total_backward_count 998580 real_backward_count 166252  16.649%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  2.034195/  2.121143, val:  52.50%, val_best:  61.67%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 79.2838%\n",
      "layer   3  Sparsity: 87.2926%\n",
      "total_backward_count 1008370 real_backward_count 167732  16.634%\n",
      "lif layer 2 self.abs_max_v: 10006.5\n",
      "fc layer 2 self.abs_max_out: 5774.0\n",
      "lif layer 2 self.abs_max_v: 10193.5\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  2.030525/  2.144581, val:  50.00%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9054%\n",
      "layer   2  Sparsity: 79.1584%\n",
      "layer   3  Sparsity: 87.0516%\n",
      "total_backward_count 1018160 real_backward_count 169192  16.617%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  2.040514/  2.130287, val:  55.00%, val_best:  61.67%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9300%\n",
      "layer   2  Sparsity: 79.5429%\n",
      "layer   3  Sparsity: 87.2840%\n",
      "total_backward_count 1027950 real_backward_count 170743  16.610%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  2.038286/  2.143099, val:  58.33%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9371%\n",
      "layer   2  Sparsity: 79.6562%\n",
      "layer   3  Sparsity: 87.1299%\n",
      "total_backward_count 1037740 real_backward_count 172229  16.597%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  2.038259/  2.166158, val:  41.67%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 79.6258%\n",
      "layer   3  Sparsity: 87.3189%\n",
      "total_backward_count 1047530 real_backward_count 173639  16.576%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  2.040753/  2.126274, val:  50.42%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 79.2360%\n",
      "layer   3  Sparsity: 87.1298%\n",
      "total_backward_count 1057320 real_backward_count 175128  16.563%\n",
      "lif layer 1 self.abs_max_v: 27031.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  2.025205/  2.128261, val:  45.83%, val_best:  61.67%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8958%\n",
      "layer   2  Sparsity: 79.1626%\n",
      "layer   3  Sparsity: 86.2103%\n",
      "total_backward_count 1067110 real_backward_count 176565  16.546%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  2.012002/  2.117702, val:  46.67%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9002%\n",
      "layer   2  Sparsity: 79.1484%\n",
      "layer   3  Sparsity: 86.4045%\n",
      "total_backward_count 1076900 real_backward_count 177991  16.528%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  2.031031/  2.093025, val:  57.50%, val_best:  61.67%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9488%\n",
      "layer   2  Sparsity: 79.3141%\n",
      "layer   3  Sparsity: 87.0459%\n",
      "total_backward_count 1086690 real_backward_count 179498  16.518%\n",
      "lif layer 2 self.abs_max_v: 10768.0\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  2.024539/  2.145780, val:  51.67%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8934%\n",
      "layer   2  Sparsity: 79.0970%\n",
      "layer   3  Sparsity: 87.3717%\n",
      "total_backward_count 1096480 real_backward_count 180977  16.505%\n",
      "fc layer 2 self.abs_max_out: 5847.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  2.033478/  2.126382, val:  60.00%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9416%\n",
      "layer   2  Sparsity: 79.4435%\n",
      "layer   3  Sparsity: 87.6806%\n",
      "total_backward_count 1106270 real_backward_count 182498  16.497%\n",
      "fc layer 2 self.abs_max_out: 6002.0\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  2.028187/  2.139672, val:  42.92%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9226%\n",
      "layer   2  Sparsity: 79.4128%\n",
      "layer   3  Sparsity: 87.8253%\n",
      "total_backward_count 1116060 real_backward_count 183896  16.477%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  2.024706/  2.130318, val:  49.58%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 79.0168%\n",
      "layer   3  Sparsity: 86.9639%\n",
      "total_backward_count 1125850 real_backward_count 185390  16.467%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  2.035534/  2.144083, val:  52.50%, val_best:  61.67%, tr:  98.26%, tr_best:  99.80%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8946%\n",
      "layer   2  Sparsity: 79.3943%\n",
      "layer   3  Sparsity: 87.9260%\n",
      "total_backward_count 1135640 real_backward_count 186866  16.455%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  2.049580/  2.119190, val:  57.50%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9046%\n",
      "layer   2  Sparsity: 79.4889%\n",
      "layer   3  Sparsity: 87.4890%\n",
      "total_backward_count 1145430 real_backward_count 188349  16.444%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  2.035989/  2.131473, val:  60.00%, val_best:  61.67%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 79.0843%\n",
      "layer   3  Sparsity: 87.1572%\n",
      "total_backward_count 1155220 real_backward_count 189819  16.431%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  2.025331/  2.128816, val:  48.75%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9164%\n",
      "layer   2  Sparsity: 78.7421%\n",
      "layer   3  Sparsity: 86.5838%\n",
      "total_backward_count 1165010 real_backward_count 191247  16.416%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  2.021524/  2.129214, val:  58.33%, val_best:  61.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9637%\n",
      "layer   2  Sparsity: 78.4197%\n",
      "layer   3  Sparsity: 86.2458%\n",
      "total_backward_count 1174800 real_backward_count 192705  16.403%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  2.003166/  2.092982, val:  59.58%, val_best:  61.67%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 78.7818%\n",
      "layer   3  Sparsity: 86.2468%\n",
      "total_backward_count 1184590 real_backward_count 194091  16.385%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  2.017902/  2.141770, val:  43.33%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9270%\n",
      "layer   2  Sparsity: 78.6644%\n",
      "layer   3  Sparsity: 86.2317%\n",
      "total_backward_count 1194380 real_backward_count 195469  16.366%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  2.014095/  2.110796, val:  52.50%, val_best:  61.67%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9389%\n",
      "layer   2  Sparsity: 78.9737%\n",
      "layer   3  Sparsity: 86.6082%\n",
      "total_backward_count 1204170 real_backward_count 196905  16.352%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  2.025912/  2.130265, val:  62.50%, val_best:  62.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9110%\n",
      "layer   2  Sparsity: 78.7357%\n",
      "layer   3  Sparsity: 87.1039%\n",
      "total_backward_count 1213960 real_backward_count 198411  16.344%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  2.044367/  2.129236, val:  57.08%, val_best:  62.50%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 78.8179%\n",
      "layer   3  Sparsity: 87.9220%\n",
      "total_backward_count 1223750 real_backward_count 199968  16.341%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  2.024219/  2.123484, val:  48.75%, val_best:  62.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9451%\n",
      "layer   2  Sparsity: 78.5371%\n",
      "layer   3  Sparsity: 86.5202%\n",
      "total_backward_count 1233540 real_backward_count 201430  16.329%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  2.027800/  2.153779, val:  45.83%, val_best:  62.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9661%\n",
      "layer   2  Sparsity: 78.7469%\n",
      "layer   3  Sparsity: 86.9279%\n",
      "total_backward_count 1243330 real_backward_count 202945  16.323%\n",
      "fc layer 2 self.abs_max_out: 6064.0\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  2.020816/  2.123815, val:  58.75%, val_best:  62.50%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9169%\n",
      "layer   2  Sparsity: 78.9527%\n",
      "layer   3  Sparsity: 87.3675%\n",
      "total_backward_count 1253120 real_backward_count 204404  16.312%\n",
      "fc layer 2 self.abs_max_out: 6074.0\n",
      "lif layer 2 self.abs_max_v: 11202.0\n",
      "lif layer 2 self.abs_max_v: 11513.0\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  2.008818/  2.123921, val:  48.75%, val_best:  62.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9431%\n",
      "layer   2  Sparsity: 78.3357%\n",
      "layer   3  Sparsity: 86.6416%\n",
      "total_backward_count 1262910 real_backward_count 205817  16.297%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  2.022676/  2.150776, val:  53.33%, val_best:  62.50%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9023%\n",
      "layer   2  Sparsity: 79.0434%\n",
      "layer   3  Sparsity: 87.8172%\n",
      "total_backward_count 1272700 real_backward_count 207296  16.288%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  2.018868/  2.152193, val:  44.17%, val_best:  62.50%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9455%\n",
      "layer   2  Sparsity: 78.8978%\n",
      "layer   3  Sparsity: 87.4439%\n",
      "total_backward_count 1282490 real_backward_count 208764  16.278%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  2.014321/  2.123290, val:  55.00%, val_best:  62.50%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 78.9075%\n",
      "layer   3  Sparsity: 86.8796%\n",
      "total_backward_count 1292280 real_backward_count 210252  16.270%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  2.012481/  2.139949, val:  44.17%, val_best:  62.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9112%\n",
      "layer   2  Sparsity: 78.7111%\n",
      "layer   3  Sparsity: 86.9755%\n",
      "total_backward_count 1302070 real_backward_count 211668  16.256%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  2.017573/  2.153491, val:  43.33%, val_best:  62.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 78.4792%\n",
      "layer   3  Sparsity: 87.1527%\n",
      "total_backward_count 1311860 real_backward_count 213119  16.246%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  2.012479/  2.132175, val:  47.50%, val_best:  62.50%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9441%\n",
      "layer   2  Sparsity: 78.5611%\n",
      "layer   3  Sparsity: 86.7877%\n",
      "total_backward_count 1321650 real_backward_count 214613  16.238%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  2.010738/  2.113326, val:  52.92%, val_best:  62.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9201%\n",
      "layer   2  Sparsity: 78.5551%\n",
      "layer   3  Sparsity: 86.7278%\n",
      "total_backward_count 1331440 real_backward_count 216151  16.234%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  2.020663/  2.116743, val:  54.17%, val_best:  62.50%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9308%\n",
      "layer   2  Sparsity: 78.7019%\n",
      "layer   3  Sparsity: 86.9082%\n",
      "total_backward_count 1341230 real_backward_count 217626  16.226%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  2.027683/  2.110558, val:  42.50%, val_best:  62.50%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 78.6329%\n",
      "layer   3  Sparsity: 87.0052%\n",
      "total_backward_count 1351020 real_backward_count 219134  16.220%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  2.031228/  2.096318, val:  50.42%, val_best:  62.50%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 79.0985%\n",
      "layer   3  Sparsity: 86.7905%\n",
      "total_backward_count 1360810 real_backward_count 220636  16.214%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  2.019463/  2.103981, val:  55.00%, val_best:  62.50%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9294%\n",
      "layer   2  Sparsity: 78.3921%\n",
      "layer   3  Sparsity: 86.9965%\n",
      "total_backward_count 1370600 real_backward_count 222166  16.209%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  2.030163/  2.127277, val:  44.58%, val_best:  62.50%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9078%\n",
      "layer   2  Sparsity: 78.6220%\n",
      "layer   3  Sparsity: 87.4350%\n",
      "total_backward_count 1380390 real_backward_count 223642  16.201%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  2.022558/  2.130712, val:  46.67%, val_best:  62.50%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9458%\n",
      "layer   2  Sparsity: 78.6982%\n",
      "layer   3  Sparsity: 86.5280%\n",
      "total_backward_count 1390180 real_backward_count 225144  16.195%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  2.028736/  2.114603, val:  40.00%, val_best:  62.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9376%\n",
      "layer   2  Sparsity: 78.7897%\n",
      "layer   3  Sparsity: 87.3812%\n",
      "total_backward_count 1399970 real_backward_count 226710  16.194%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  2.013938/  2.128654, val:  54.58%, val_best:  62.50%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8808%\n",
      "layer   2  Sparsity: 78.6916%\n",
      "layer   3  Sparsity: 86.8393%\n",
      "total_backward_count 1409760 real_backward_count 228281  16.193%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  2.010676/  2.117125, val:  52.50%, val_best:  62.50%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9289%\n",
      "layer   2  Sparsity: 78.4467%\n",
      "layer   3  Sparsity: 86.7345%\n",
      "total_backward_count 1419550 real_backward_count 229739  16.184%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  2.018829/  2.130197, val:  37.92%, val_best:  62.50%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 78.7518%\n",
      "layer   3  Sparsity: 87.1000%\n",
      "total_backward_count 1429340 real_backward_count 231199  16.175%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  2.021387/  2.136433, val:  56.25%, val_best:  62.50%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 78.3771%\n",
      "layer   3  Sparsity: 87.3632%\n",
      "total_backward_count 1439130 real_backward_count 232761  16.174%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  2.035794/  2.117951, val:  48.75%, val_best:  62.50%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9180%\n",
      "layer   2  Sparsity: 78.7614%\n",
      "layer   3  Sparsity: 87.2939%\n",
      "total_backward_count 1448920 real_backward_count 234247  16.167%\n",
      "fc layer 1 self.abs_max_out: 15741.0\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  2.028408/  2.132010, val:  50.83%, val_best:  62.50%, tr:  98.37%, tr_best:  99.80%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9117%\n",
      "layer   2  Sparsity: 78.7630%\n",
      "layer   3  Sparsity: 87.0834%\n",
      "total_backward_count 1458710 real_backward_count 235750  16.162%\n",
      "fc layer 2 self.abs_max_out: 6201.0\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  2.012997/  2.120672, val:  57.50%, val_best:  62.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 78.9400%\n",
      "layer   3  Sparsity: 86.8269%\n",
      "total_backward_count 1468500 real_backward_count 237132  16.148%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  2.024858/  2.136605, val:  52.50%, val_best:  62.50%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9235%\n",
      "layer   2  Sparsity: 78.7436%\n",
      "layer   3  Sparsity: 87.4438%\n",
      "total_backward_count 1478290 real_backward_count 238542  16.136%\n",
      "fc layer 1 self.abs_max_out: 16043.0\n",
      "lif layer 1 self.abs_max_v: 27302.0\n",
      "lif layer 1 self.abs_max_v: 27415.0\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  2.028255/  2.134514, val:  47.08%, val_best:  62.50%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 78.8348%\n",
      "layer   3  Sparsity: 87.0594%\n",
      "total_backward_count 1488080 real_backward_count 240053  16.132%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  2.015464/  2.112083, val:  51.67%, val_best:  62.50%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 78.6930%\n",
      "layer   3  Sparsity: 86.6103%\n",
      "total_backward_count 1497870 real_backward_count 241518  16.124%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  2.004466/  2.091908, val:  58.33%, val_best:  62.50%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9245%\n",
      "layer   2  Sparsity: 78.9773%\n",
      "layer   3  Sparsity: 86.3550%\n",
      "total_backward_count 1507660 real_backward_count 242995  16.117%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  2.016974/  2.090164, val:  59.58%, val_best:  62.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9502%\n",
      "layer   2  Sparsity: 78.7094%\n",
      "layer   3  Sparsity: 86.8881%\n",
      "total_backward_count 1517450 real_backward_count 244453  16.109%\n",
      "fc layer 1 self.abs_max_out: 16114.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  2.011817/  2.126698, val:  66.25%, val_best:  66.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8933%\n",
      "layer   2  Sparsity: 79.0102%\n",
      "layer   3  Sparsity: 86.8418%\n",
      "total_backward_count 1527240 real_backward_count 245971  16.106%\n",
      "fc layer 1 self.abs_max_out: 16263.0\n",
      "lif layer 1 self.abs_max_v: 27534.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  2.028300/  2.115909, val:  56.67%, val_best:  66.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 78.8070%\n",
      "layer   3  Sparsity: 86.7984%\n",
      "total_backward_count 1537030 real_backward_count 247480  16.101%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  2.029443/  2.141322, val:  58.33%, val_best:  66.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8926%\n",
      "layer   2  Sparsity: 78.1604%\n",
      "layer   3  Sparsity: 87.3221%\n",
      "total_backward_count 1546820 real_backward_count 248992  16.097%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  2.030950/  2.144522, val:  46.67%, val_best:  66.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 78.3621%\n",
      "layer   3  Sparsity: 87.5374%\n",
      "total_backward_count 1556610 real_backward_count 250549  16.096%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  2.017662/  2.127408, val:  50.00%, val_best:  66.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9141%\n",
      "layer   2  Sparsity: 78.9015%\n",
      "layer   3  Sparsity: 86.9933%\n",
      "total_backward_count 1566400 real_backward_count 252102  16.094%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  2.032403/  2.129457, val:  57.08%, val_best:  66.25%, tr:  98.67%, tr_best:  99.80%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 78.8175%\n",
      "layer   3  Sparsity: 87.5154%\n",
      "total_backward_count 1576190 real_backward_count 253633  16.092%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  2.029772/  2.143626, val:  53.33%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8972%\n",
      "layer   2  Sparsity: 78.4518%\n",
      "layer   3  Sparsity: 86.8146%\n",
      "total_backward_count 1585980 real_backward_count 255172  16.089%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  2.024154/  2.117409, val:  63.33%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9236%\n",
      "layer   2  Sparsity: 78.3356%\n",
      "layer   3  Sparsity: 86.6104%\n",
      "total_backward_count 1595770 real_backward_count 256639  16.082%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  2.020785/  2.145410, val:  41.25%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9259%\n",
      "layer   2  Sparsity: 78.4435%\n",
      "layer   3  Sparsity: 86.9582%\n",
      "total_backward_count 1605560 real_backward_count 258060  16.073%\n",
      "fc layer 3 self.abs_max_out: 1425.0\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  2.005951/  2.122750, val:  54.58%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8838%\n",
      "layer   2  Sparsity: 78.5687%\n",
      "layer   3  Sparsity: 86.6701%\n",
      "total_backward_count 1615350 real_backward_count 259499  16.065%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  2.010967/  2.134221, val:  54.58%, val_best:  66.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8980%\n",
      "layer   2  Sparsity: 78.7090%\n",
      "layer   3  Sparsity: 86.4622%\n",
      "total_backward_count 1625140 real_backward_count 260968  16.058%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  2.010252/  2.110905, val:  53.75%, val_best:  66.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 79.0317%\n",
      "layer   3  Sparsity: 86.9631%\n",
      "total_backward_count 1634930 real_backward_count 262429  16.051%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  2.014692/  2.127036, val:  45.00%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9183%\n",
      "layer   2  Sparsity: 78.5173%\n",
      "layer   3  Sparsity: 86.7781%\n",
      "total_backward_count 1644720 real_backward_count 263912  16.046%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.998030/  2.094206, val:  48.75%, val_best:  66.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9592%\n",
      "layer   2  Sparsity: 78.6824%\n",
      "layer   3  Sparsity: 85.8244%\n",
      "total_backward_count 1654510 real_backward_count 265311  16.036%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.990560/  2.104164, val:  58.33%, val_best:  66.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9400%\n",
      "layer   2  Sparsity: 79.0819%\n",
      "layer   3  Sparsity: 86.4397%\n",
      "total_backward_count 1664300 real_backward_count 266721  16.026%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  2.010393/  2.117483, val:  44.58%, val_best:  66.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9351%\n",
      "layer   2  Sparsity: 78.4701%\n",
      "layer   3  Sparsity: 86.3458%\n",
      "total_backward_count 1674090 real_backward_count 268177  16.019%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  2.008615/  2.121369, val:  45.00%, val_best:  66.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9583%\n",
      "layer   2  Sparsity: 78.5936%\n",
      "layer   3  Sparsity: 86.3002%\n",
      "total_backward_count 1683880 real_backward_count 269644  16.013%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  2.022541/  2.097484, val:  59.17%, val_best:  66.25%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9464%\n",
      "layer   2  Sparsity: 78.7457%\n",
      "layer   3  Sparsity: 86.8719%\n",
      "total_backward_count 1693670 real_backward_count 271171  16.011%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.998139/  2.090996, val:  52.92%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 78.6545%\n",
      "layer   3  Sparsity: 86.3024%\n",
      "total_backward_count 1703460 real_backward_count 272552  16.000%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  2.019068/  2.111113, val:  58.33%, val_best:  66.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9328%\n",
      "layer   2  Sparsity: 79.0195%\n",
      "layer   3  Sparsity: 87.3170%\n",
      "total_backward_count 1713250 real_backward_count 274013  15.994%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  2.014083/  2.112936, val:  63.33%, val_best:  66.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8885%\n",
      "layer   2  Sparsity: 78.6380%\n",
      "layer   3  Sparsity: 86.8361%\n",
      "total_backward_count 1723040 real_backward_count 275534  15.991%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  2.033628/  2.119128, val:  50.83%, val_best:  66.25%, tr:  98.06%, tr_best:  99.80%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9290%\n",
      "layer   2  Sparsity: 78.5523%\n",
      "layer   3  Sparsity: 87.4484%\n",
      "total_backward_count 1732830 real_backward_count 277037  15.988%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  2.013381/  2.133570, val:  45.42%, val_best:  66.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9033%\n",
      "layer   2  Sparsity: 78.5604%\n",
      "layer   3  Sparsity: 86.9675%\n",
      "total_backward_count 1742620 real_backward_count 278401  15.976%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  2.010962/  2.113196, val:  45.83%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 78.9687%\n",
      "layer   3  Sparsity: 86.4961%\n",
      "total_backward_count 1752410 real_backward_count 279852  15.970%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  2.005572/  2.115511, val:  45.00%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9329%\n",
      "layer   2  Sparsity: 79.0921%\n",
      "layer   3  Sparsity: 86.8244%\n",
      "total_backward_count 1762200 real_backward_count 281360  15.966%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  2.004378/  2.105808, val:  52.92%, val_best:  66.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9406%\n",
      "layer   2  Sparsity: 78.8836%\n",
      "layer   3  Sparsity: 86.3130%\n",
      "total_backward_count 1771990 real_backward_count 282815  15.960%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  2.011257/  2.132903, val:  55.42%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9135%\n",
      "layer   2  Sparsity: 79.0957%\n",
      "layer   3  Sparsity: 87.0619%\n",
      "total_backward_count 1781780 real_backward_count 284237  15.952%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  2.026407/  2.128741, val:  52.50%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9184%\n",
      "layer   2  Sparsity: 79.0178%\n",
      "layer   3  Sparsity: 87.1187%\n",
      "total_backward_count 1791570 real_backward_count 285704  15.947%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  2.023602/  2.151772, val:  44.58%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 78.8999%\n",
      "layer   3  Sparsity: 87.8683%\n",
      "total_backward_count 1801360 real_backward_count 287123  15.939%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  2.027294/  2.123695, val:  48.33%, val_best:  66.25%, tr:  98.67%, tr_best:  99.80%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 78.7648%\n",
      "layer   3  Sparsity: 87.1156%\n",
      "total_backward_count 1811150 real_backward_count 288553  15.932%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  2.019202/  2.137281, val:  51.25%, val_best:  66.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 79.0158%\n",
      "layer   3  Sparsity: 87.8164%\n",
      "total_backward_count 1820940 real_backward_count 290054  15.929%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  2.031349/  2.122719, val:  53.75%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9224%\n",
      "layer   2  Sparsity: 78.7320%\n",
      "layer   3  Sparsity: 87.5129%\n",
      "total_backward_count 1830730 real_backward_count 291523  15.924%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  2.032539/  2.150499, val:  47.92%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 78.9545%\n",
      "layer   3  Sparsity: 87.3089%\n",
      "total_backward_count 1840520 real_backward_count 292966  15.918%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  2.029190/  2.106999, val:  56.67%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9251%\n",
      "layer   2  Sparsity: 78.6055%\n",
      "layer   3  Sparsity: 86.4835%\n",
      "total_backward_count 1850310 real_backward_count 294376  15.910%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  2.006754/  2.101889, val:  55.42%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9298%\n",
      "layer   2  Sparsity: 78.3411%\n",
      "layer   3  Sparsity: 86.1670%\n",
      "total_backward_count 1860100 real_backward_count 295854  15.905%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  2.011036/  2.102776, val:  56.67%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9260%\n",
      "layer   2  Sparsity: 78.4863%\n",
      "layer   3  Sparsity: 86.7615%\n",
      "total_backward_count 1869890 real_backward_count 297320  15.900%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  2.026167/  2.133946, val:  52.92%, val_best:  66.25%, tr:  98.57%, tr_best:  99.80%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 78.5906%\n",
      "layer   3  Sparsity: 87.3663%\n",
      "total_backward_count 1879680 real_backward_count 298846  15.899%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  2.015951/  2.134802, val:  52.50%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 78.5277%\n",
      "layer   3  Sparsity: 86.8753%\n",
      "total_backward_count 1889470 real_backward_count 300273  15.892%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  2.023984/  2.139771, val:  47.92%, val_best:  66.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9115%\n",
      "layer   2  Sparsity: 78.7037%\n",
      "layer   3  Sparsity: 87.2644%\n",
      "total_backward_count 1899260 real_backward_count 301661  15.883%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  2.022810/  2.119654, val:  55.00%, val_best:  66.25%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9172%\n",
      "layer   2  Sparsity: 79.1084%\n",
      "layer   3  Sparsity: 87.1368%\n",
      "total_backward_count 1909050 real_backward_count 303079  15.876%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  2.017742/  2.104497, val:  54.58%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9042%\n",
      "layer   2  Sparsity: 78.4999%\n",
      "layer   3  Sparsity: 86.5500%\n",
      "total_backward_count 1918840 real_backward_count 304520  15.870%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  2.018297/  2.112789, val:  55.42%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9185%\n",
      "layer   2  Sparsity: 78.8090%\n",
      "layer   3  Sparsity: 86.4387%\n",
      "total_backward_count 1928630 real_backward_count 305948  15.863%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  2.000214/  2.104323, val:  62.08%, val_best:  66.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9420%\n",
      "layer   2  Sparsity: 78.4436%\n",
      "layer   3  Sparsity: 86.7340%\n",
      "total_backward_count 1938420 real_backward_count 307435  15.860%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  2.005170/  2.124440, val:  51.67%, val_best:  66.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9119%\n",
      "layer   2  Sparsity: 78.7157%\n",
      "layer   3  Sparsity: 87.1344%\n",
      "total_backward_count 1948210 real_backward_count 308880  15.855%\n",
      "fc layer 2 self.abs_max_out: 6215.0\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  2.021547/  2.119121, val:  60.83%, val_best:  66.25%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9202%\n",
      "layer   2  Sparsity: 78.9062%\n",
      "layer   3  Sparsity: 87.5156%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00450a31faf8495f8dc39877670e85aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>2.02155</td></tr><tr><td>val_acc_best</td><td>0.6625</td></tr><tr><td>val_acc_now</td><td>0.60833</td></tr><tr><td>val_loss</td><td>2.11912</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-sweep-103</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iypukvcm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iypukvcm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_110619-iypukvcm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ix07cven with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_152143-ix07cven</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ix07cven' target=\"_blank\">sandy-sweep-109</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ix07cven' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ix07cven</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251116_152152_357', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 385.0\n",
      "lif layer 1 self.abs_max_v: 385.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1032.0\n",
      "lif layer 2 self.abs_max_v: 1032.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 1 self.abs_max_out: 476.0\n",
      "lif layer 1 self.abs_max_v: 504.0\n",
      "fc layer 2 self.abs_max_out: 1066.0\n",
      "lif layer 2 self.abs_max_v: 1476.0\n",
      "fc layer 1 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 758.0\n",
      "lif layer 2 self.abs_max_v: 1661.0\n",
      "lif layer 2 self.abs_max_v: 1678.5\n",
      "lif layer 1 self.abs_max_v: 773.5\n",
      "fc layer 1 self.abs_max_out: 659.0\n",
      "lif layer 1 self.abs_max_v: 964.5\n",
      "lif layer 2 self.abs_max_v: 1716.5\n",
      "fc layer 2 self.abs_max_out: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1984.5\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "fc layer 2 self.abs_max_out: 1248.0\n",
      "fc layer 1 self.abs_max_out: 670.0\n",
      "fc layer 1 self.abs_max_out: 686.0\n",
      "lif layer 1 self.abs_max_v: 1003.5\n",
      "lif layer 1 self.abs_max_v: 1010.5\n",
      "lif layer 1 self.abs_max_v: 1108.5\n",
      "fc layer 1 self.abs_max_out: 705.0\n",
      "lif layer 1 self.abs_max_v: 1117.0\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "fc layer 1 self.abs_max_out: 815.0\n",
      "lif layer 1 self.abs_max_v: 1169.5\n",
      "lif layer 1 self.abs_max_v: 1177.0\n",
      "fc layer 1 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 1317.0\n",
      "lif layer 1 self.abs_max_v: 1196.5\n",
      "lif layer 2 self.abs_max_v: 2224.5\n",
      "lif layer 1 self.abs_max_v: 1306.5\n",
      "fc layer 1 self.abs_max_out: 1123.0\n",
      "lif layer 1 self.abs_max_v: 1776.5\n",
      "fc layer 1 self.abs_max_out: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "lif layer 2 self.abs_max_v: 2261.0\n",
      "lif layer 2 self.abs_max_v: 2367.5\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 2 self.abs_max_out: 1521.0\n",
      "lif layer 2 self.abs_max_v: 2630.0\n",
      "fc layer 1 self.abs_max_out: 1211.0\n",
      "lif layer 1 self.abs_max_v: 1784.5\n",
      "fc layer 1 self.abs_max_out: 1249.0\n",
      "fc layer 1 self.abs_max_out: 1250.0\n",
      "lif layer 1 self.abs_max_v: 1785.5\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "fc layer 1 self.abs_max_out: 1335.0\n",
      "fc layer 1 self.abs_max_out: 1374.0\n",
      "lif layer 1 self.abs_max_v: 1786.5\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "lif layer 1 self.abs_max_v: 1864.5\n",
      "lif layer 1 self.abs_max_v: 1942.5\n",
      "fc layer 1 self.abs_max_out: 1509.0\n",
      "lif layer 1 self.abs_max_v: 2296.0\n",
      "fc layer 1 self.abs_max_out: 1547.0\n",
      "lif layer 1 self.abs_max_v: 2379.0\n",
      "fc layer 1 self.abs_max_out: 1550.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "fc layer 1 self.abs_max_out: 1557.0\n",
      "fc layer 2 self.abs_max_out: 1641.0\n",
      "lif layer 1 self.abs_max_v: 2381.5\n",
      "fc layer 1 self.abs_max_out: 1597.0\n",
      "fc layer 1 self.abs_max_out: 1777.0\n",
      "lif layer 1 self.abs_max_v: 2568.5\n",
      "fc layer 2 self.abs_max_out: 1690.0\n",
      "lif layer 2 self.abs_max_v: 2903.5\n",
      "fc layer 3 self.abs_max_out: 691.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 3 self.abs_max_out: 758.0\n",
      "fc layer 3 self.abs_max_out: 760.0\n",
      "fc layer 3 self.abs_max_out: 821.0\n",
      "lif layer 1 self.abs_max_v: 2616.0\n",
      "lif layer 2 self.abs_max_v: 2914.0\n",
      "lif layer 1 self.abs_max_v: 2709.5\n",
      "fc layer 1 self.abs_max_out: 1800.0\n",
      "lif layer 1 self.abs_max_v: 2943.5\n",
      "lif layer 1 self.abs_max_v: 2946.5\n",
      "lif layer 1 self.abs_max_v: 3077.5\n",
      "lif layer 1 self.abs_max_v: 3096.5\n",
      "fc layer 1 self.abs_max_out: 1854.0\n",
      "lif layer 1 self.abs_max_v: 3142.5\n",
      "lif layer 1 self.abs_max_v: 3164.5\n",
      "lif layer 2 self.abs_max_v: 3053.0\n",
      "fc layer 2 self.abs_max_out: 1693.0\n",
      "fc layer 2 self.abs_max_out: 1703.0\n",
      "lif layer 2 self.abs_max_v: 3139.5\n",
      "fc layer 1 self.abs_max_out: 1904.0\n",
      "fc layer 2 self.abs_max_out: 1753.0\n",
      "fc layer 2 self.abs_max_out: 1783.0\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "lif layer 1 self.abs_max_v: 3242.0\n",
      "lif layer 1 self.abs_max_v: 3410.5\n",
      "fc layer 1 self.abs_max_out: 2043.0\n",
      "fc layer 1 self.abs_max_out: 2268.0\n",
      "lif layer 1 self.abs_max_v: 3931.0\n",
      "lif layer 2 self.abs_max_v: 3153.0\n",
      "fc layer 2 self.abs_max_out: 1960.0\n",
      "lif layer 2 self.abs_max_v: 3196.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.809217/  2.035583, val:  30.83%, val_best:  30.83%, tr:  96.32%, tr_best:  96.32%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0145%\n",
      "layer   2  Sparsity: 65.7683%\n",
      "layer   3  Sparsity: 60.1795%\n",
      "total_backward_count 9790 real_backward_count 2326  23.759%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2345.0\n",
      "fc layer 1 self.abs_max_out: 2407.0\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "lif layer 2 self.abs_max_v: 3249.0\n",
      "lif layer 2 self.abs_max_v: 3310.5\n",
      "lif layer 2 self.abs_max_v: 3354.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.740412/  1.946720, val:  45.83%, val_best:  45.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0381%\n",
      "layer   2  Sparsity: 67.3590%\n",
      "layer   3  Sparsity: 58.9724%\n",
      "total_backward_count 19580 real_backward_count 3877  19.801%\n",
      "fc layer 2 self.abs_max_out: 1965.0\n",
      "fc layer 3 self.abs_max_out: 827.0\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "fc layer 3 self.abs_max_out: 862.0\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "fc layer 1 self.abs_max_out: 2544.0\n",
      "fc layer 1 self.abs_max_out: 2570.0\n",
      "lif layer 1 self.abs_max_v: 4147.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.722608/  1.940261, val:  51.67%, val_best:  51.67%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0438%\n",
      "layer   2  Sparsity: 68.7562%\n",
      "layer   3  Sparsity: 57.6699%\n",
      "total_backward_count 29370 real_backward_count 5285  17.995%\n",
      "lif layer 1 self.abs_max_v: 4448.5\n",
      "lif layer 1 self.abs_max_v: 4563.5\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "fc layer 1 self.abs_max_out: 2580.0\n",
      "fc layer 1 self.abs_max_out: 2620.0\n",
      "fc layer 1 self.abs_max_out: 2641.0\n",
      "lif layer 1 self.abs_max_v: 4650.0\n",
      "lif layer 1 self.abs_max_v: 4740.0\n",
      "lif layer 1 self.abs_max_v: 4822.0\n",
      "lif layer 1 self.abs_max_v: 4846.0\n",
      "lif layer 1 self.abs_max_v: 5041.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.710105/  1.923310, val:  42.50%, val_best:  51.67%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0033%\n",
      "layer   2  Sparsity: 68.5019%\n",
      "layer   3  Sparsity: 56.5430%\n",
      "total_backward_count 39160 real_backward_count 6642  16.961%\n",
      "fc layer 1 self.abs_max_out: 2726.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.704152/  1.910577, val:  45.42%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0190%\n",
      "layer   2  Sparsity: 68.7301%\n",
      "layer   3  Sparsity: 57.7439%\n",
      "total_backward_count 48950 real_backward_count 7892  16.123%\n",
      "lif layer 2 self.abs_max_v: 3355.5\n",
      "lif layer 2 self.abs_max_v: 3451.5\n",
      "fc layer 3 self.abs_max_out: 907.0\n",
      "fc layer 3 self.abs_max_out: 918.0\n",
      "fc layer 2 self.abs_max_out: 2057.0\n",
      "fc layer 2 self.abs_max_out: 2116.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.692059/  1.894537, val:  50.42%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0077%\n",
      "layer   2  Sparsity: 68.6396%\n",
      "layer   3  Sparsity: 57.7504%\n",
      "total_backward_count 58740 real_backward_count 9176  15.621%\n",
      "lif layer 2 self.abs_max_v: 3512.5\n",
      "lif layer 2 self.abs_max_v: 3523.0\n",
      "lif layer 2 self.abs_max_v: 3535.0\n",
      "fc layer 2 self.abs_max_out: 2153.0\n",
      "fc layer 2 self.abs_max_out: 2200.0\n",
      "fc layer 2 self.abs_max_out: 2210.0\n",
      "fc layer 1 self.abs_max_out: 2766.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.714446/  1.879480, val:  56.67%, val_best:  56.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0077%\n",
      "layer   2  Sparsity: 67.9591%\n",
      "layer   3  Sparsity: 56.4266%\n",
      "total_backward_count 68530 real_backward_count 10386  15.155%\n",
      "lif layer 2 self.abs_max_v: 3561.5\n",
      "fc layer 1 self.abs_max_out: 2786.0\n",
      "fc layer 1 self.abs_max_out: 2918.0\n",
      "lif layer 1 self.abs_max_v: 5327.0\n",
      "lif layer 1 self.abs_max_v: 5401.5\n",
      "fc layer 1 self.abs_max_out: 2949.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.697852/  1.867483, val:  56.25%, val_best:  56.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0284%\n",
      "layer   2  Sparsity: 68.8664%\n",
      "layer   3  Sparsity: 56.1755%\n",
      "total_backward_count 78320 real_backward_count 11528  14.719%\n",
      "fc layer 1 self.abs_max_out: 2959.0\n",
      "fc layer 1 self.abs_max_out: 2963.0\n",
      "fc layer 1 self.abs_max_out: 3100.0\n",
      "fc layer 1 self.abs_max_out: 3135.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.685478/  1.849914, val:  55.42%, val_best:  56.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0396%\n",
      "layer   2  Sparsity: 69.5945%\n",
      "layer   3  Sparsity: 55.8331%\n",
      "total_backward_count 88110 real_backward_count 12651  14.358%\n",
      "lif layer 1 self.abs_max_v: 5406.5\n",
      "fc layer 2 self.abs_max_out: 2325.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.669310/  1.879819, val:  51.25%, val_best:  56.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0286%\n",
      "layer   2  Sparsity: 69.4337%\n",
      "layer   3  Sparsity: 55.1795%\n",
      "total_backward_count 97900 real_backward_count 13752  14.047%\n",
      "lif layer 1 self.abs_max_v: 5652.5\n",
      "lif layer 1 self.abs_max_v: 5793.0\n",
      "lif layer 2 self.abs_max_v: 3579.0\n",
      "fc layer 1 self.abs_max_out: 3173.0\n",
      "lif layer 1 self.abs_max_v: 5812.0\n",
      "fc layer 1 self.abs_max_out: 3189.0\n",
      "fc layer 1 self.abs_max_out: 3208.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.667375/  1.861716, val:  55.00%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0209%\n",
      "layer   2  Sparsity: 69.3827%\n",
      "layer   3  Sparsity: 54.8078%\n",
      "total_backward_count 107690 real_backward_count 14828  13.769%\n",
      "fc layer 1 self.abs_max_out: 3279.0\n",
      "lif layer 1 self.abs_max_v: 5903.0\n",
      "fc layer 3 self.abs_max_out: 946.0\n",
      "lif layer 1 self.abs_max_v: 5924.0\n",
      "fc layer 1 self.abs_max_out: 3292.0\n",
      "lif layer 1 self.abs_max_v: 6067.5\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.656635/  1.855915, val:  55.00%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9884%\n",
      "layer   2  Sparsity: 69.9317%\n",
      "layer   3  Sparsity: 54.8045%\n",
      "total_backward_count 117480 real_backward_count 15933  13.562%\n",
      "fc layer 2 self.abs_max_out: 2390.0\n",
      "fc layer 1 self.abs_max_out: 3321.0\n",
      "fc layer 2 self.abs_max_out: 2401.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.642626/  1.821293, val:  52.92%, val_best:  56.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0132%\n",
      "layer   2  Sparsity: 70.6936%\n",
      "layer   3  Sparsity: 55.4009%\n",
      "total_backward_count 127270 real_backward_count 16928  13.301%\n",
      "fc layer 3 self.abs_max_out: 981.0\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "fc layer 3 self.abs_max_out: 1061.0\n",
      "fc layer 1 self.abs_max_out: 3509.0\n",
      "lif layer 1 self.abs_max_v: 6233.5\n",
      "fc layer 1 self.abs_max_out: 3576.0\n",
      "lif layer 1 self.abs_max_v: 6344.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.635252/  1.853798, val:  48.75%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0338%\n",
      "layer   2  Sparsity: 70.7242%\n",
      "layer   3  Sparsity: 54.6077%\n",
      "total_backward_count 137060 real_backward_count 17992  13.127%\n",
      "lif layer 2 self.abs_max_v: 3602.0\n",
      "fc layer 2 self.abs_max_out: 2454.0\n",
      "fc layer 2 self.abs_max_out: 2472.0\n",
      "fc layer 1 self.abs_max_out: 3603.0\n",
      "fc layer 1 self.abs_max_out: 3732.0\n",
      "lif layer 1 self.abs_max_v: 6386.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.630410/  1.808286, val:  56.67%, val_best:  56.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9974%\n",
      "layer   2  Sparsity: 70.3956%\n",
      "layer   3  Sparsity: 54.8927%\n",
      "total_backward_count 146850 real_backward_count 19017  12.950%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.612866/  1.818130, val:  57.08%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0060%\n",
      "layer   2  Sparsity: 69.5525%\n",
      "layer   3  Sparsity: 55.2088%\n",
      "total_backward_count 156640 real_backward_count 20114  12.841%\n",
      "fc layer 1 self.abs_max_out: 3743.0\n",
      "lif layer 1 self.abs_max_v: 6490.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.613851/  1.833277, val:  54.17%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0221%\n",
      "layer   2  Sparsity: 68.8879%\n",
      "layer   3  Sparsity: 55.8622%\n",
      "total_backward_count 166430 real_backward_count 21115  12.687%\n",
      "lif layer 2 self.abs_max_v: 3648.0\n",
      "lif layer 2 self.abs_max_v: 3981.0\n",
      "lif layer 2 self.abs_max_v: 4024.5\n",
      "lif layer 1 self.abs_max_v: 6666.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.635031/  1.811515, val:  62.92%, val_best:  62.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9822%\n",
      "layer   2  Sparsity: 69.0235%\n",
      "layer   3  Sparsity: 55.3937%\n",
      "total_backward_count 176220 real_backward_count 22146  12.567%\n",
      "lif layer 2 self.abs_max_v: 4034.5\n",
      "lif layer 2 self.abs_max_v: 4037.0\n",
      "fc layer 1 self.abs_max_out: 3769.0\n",
      "lif layer 1 self.abs_max_v: 6814.5\n",
      "lif layer 2 self.abs_max_v: 4037.5\n",
      "fc layer 1 self.abs_max_out: 3786.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.622383/  1.810809, val:  50.00%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0088%\n",
      "layer   2  Sparsity: 68.7964%\n",
      "layer   3  Sparsity: 55.0497%\n",
      "total_backward_count 186010 real_backward_count 23126  12.433%\n",
      "fc layer 1 self.abs_max_out: 3797.0\n",
      "lif layer 1 self.abs_max_v: 6835.0\n",
      "fc layer 1 self.abs_max_out: 3978.0\n",
      "fc layer 1 self.abs_max_out: 4229.0\n",
      "lif layer 1 self.abs_max_v: 7172.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.600466/  1.835829, val:  50.83%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0029%\n",
      "layer   2  Sparsity: 69.2796%\n",
      "layer   3  Sparsity: 55.1454%\n",
      "total_backward_count 195800 real_backward_count 24085  12.301%\n",
      "lif layer 2 self.abs_max_v: 4041.5\n",
      "lif layer 2 self.abs_max_v: 4062.5\n",
      "fc layer 1 self.abs_max_out: 4367.0\n",
      "lif layer 1 self.abs_max_v: 7235.5\n",
      "lif layer 1 self.abs_max_v: 7451.0\n",
      "lif layer 2 self.abs_max_v: 4087.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.615029/  1.830101, val:  52.92%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0136%\n",
      "layer   2  Sparsity: 68.6704%\n",
      "layer   3  Sparsity: 55.8343%\n",
      "total_backward_count 205590 real_backward_count 25036  12.178%\n",
      "lif layer 1 self.abs_max_v: 7614.5\n",
      "lif layer 2 self.abs_max_v: 4129.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.604363/  1.804744, val:  54.58%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0237%\n",
      "layer   2  Sparsity: 68.0630%\n",
      "layer   3  Sparsity: 56.8096%\n",
      "total_backward_count 215380 real_backward_count 26025  12.083%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.604613/  1.785166, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0111%\n",
      "layer   2  Sparsity: 68.1350%\n",
      "layer   3  Sparsity: 56.5458%\n",
      "total_backward_count 225170 real_backward_count 27012  11.996%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.602889/  1.802993, val:  62.50%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9750%\n",
      "layer   2  Sparsity: 68.3364%\n",
      "layer   3  Sparsity: 57.2218%\n",
      "total_backward_count 234960 real_backward_count 28065  11.945%\n",
      "lif layer 2 self.abs_max_v: 4193.0\n",
      "lif layer 2 self.abs_max_v: 4403.5\n",
      "lif layer 2 self.abs_max_v: 4659.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.618235/  1.812714, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0288%\n",
      "layer   2  Sparsity: 68.1801%\n",
      "layer   3  Sparsity: 57.3233%\n",
      "total_backward_count 244750 real_backward_count 29020  11.857%\n",
      "fc layer 2 self.abs_max_out: 2479.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.613766/  1.797873, val:  62.08%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0081%\n",
      "layer   2  Sparsity: 68.1059%\n",
      "layer   3  Sparsity: 56.8719%\n",
      "total_backward_count 254540 real_backward_count 30021  11.794%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.600747/  1.781890, val:  67.92%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0115%\n",
      "layer   2  Sparsity: 68.7025%\n",
      "layer   3  Sparsity: 58.3765%\n",
      "total_backward_count 264330 real_backward_count 30958  11.712%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.606232/  1.801635, val:  65.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9707%\n",
      "layer   2  Sparsity: 67.8498%\n",
      "layer   3  Sparsity: 58.8434%\n",
      "total_backward_count 274120 real_backward_count 31905  11.639%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.608143/  1.825052, val:  51.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0085%\n",
      "layer   2  Sparsity: 68.2806%\n",
      "layer   3  Sparsity: 60.0770%\n",
      "total_backward_count 283910 real_backward_count 32812  11.557%\n",
      "fc layer 1 self.abs_max_out: 4466.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.623390/  1.799299, val:  56.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0029%\n",
      "layer   2  Sparsity: 68.3068%\n",
      "layer   3  Sparsity: 60.2865%\n",
      "total_backward_count 293700 real_backward_count 33739  11.488%\n",
      "lif layer 1 self.abs_max_v: 7722.5\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.620771/  1.792341, val:  57.50%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0417%\n",
      "layer   2  Sparsity: 67.7035%\n",
      "layer   3  Sparsity: 59.4444%\n",
      "total_backward_count 303490 real_backward_count 34663  11.421%\n",
      "lif layer 1 self.abs_max_v: 7780.5\n",
      "fc layer 1 self.abs_max_out: 4531.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.600447/  1.803579, val:  50.83%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9929%\n",
      "layer   2  Sparsity: 67.7947%\n",
      "layer   3  Sparsity: 58.5309%\n",
      "total_backward_count 313280 real_backward_count 35602  11.364%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.608698/  1.799122, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0041%\n",
      "layer   2  Sparsity: 67.4167%\n",
      "layer   3  Sparsity: 58.5741%\n",
      "total_backward_count 323070 real_backward_count 36478  11.291%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.604174/  1.774433, val:  61.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0387%\n",
      "layer   2  Sparsity: 67.4441%\n",
      "layer   3  Sparsity: 58.9873%\n",
      "total_backward_count 332860 real_backward_count 37433  11.246%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.580782/  1.752371, val:  58.75%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0102%\n",
      "layer   2  Sparsity: 67.2616%\n",
      "layer   3  Sparsity: 59.5376%\n",
      "total_backward_count 342650 real_backward_count 38331  11.187%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.580894/  1.768081, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9952%\n",
      "layer   2  Sparsity: 67.1027%\n",
      "layer   3  Sparsity: 60.1339%\n",
      "total_backward_count 352440 real_backward_count 39204  11.124%\n",
      "fc layer 1 self.abs_max_out: 4633.0\n",
      "lif layer 1 self.abs_max_v: 7785.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.562276/  1.725143, val:  65.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0459%\n",
      "layer   2  Sparsity: 67.2343%\n",
      "layer   3  Sparsity: 59.8166%\n",
      "total_backward_count 362230 real_backward_count 40104  11.071%\n",
      "fc layer 2 self.abs_max_out: 2514.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.554553/  1.754559, val:  65.42%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0374%\n",
      "layer   2  Sparsity: 67.4227%\n",
      "layer   3  Sparsity: 59.4684%\n",
      "total_backward_count 372020 real_backward_count 40955  11.009%\n",
      "lif layer 1 self.abs_max_v: 7881.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.564433/  1.748162, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9673%\n",
      "layer   2  Sparsity: 67.3957%\n",
      "layer   3  Sparsity: 59.9218%\n",
      "total_backward_count 381810 real_backward_count 41804  10.949%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.557764/  1.739000, val:  65.42%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0254%\n",
      "layer   2  Sparsity: 67.5938%\n",
      "layer   3  Sparsity: 60.8964%\n",
      "total_backward_count 391600 real_backward_count 42689  10.901%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.568990/  1.761661, val:  63.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9937%\n",
      "layer   2  Sparsity: 66.9651%\n",
      "layer   3  Sparsity: 60.7367%\n",
      "total_backward_count 401390 real_backward_count 43560  10.852%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.579448/  1.764220, val:  62.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0029%\n",
      "layer   2  Sparsity: 67.6578%\n",
      "layer   3  Sparsity: 60.2805%\n",
      "total_backward_count 411180 real_backward_count 44451  10.811%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.566844/  1.737192, val:  66.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0529%\n",
      "layer   2  Sparsity: 67.5330%\n",
      "layer   3  Sparsity: 60.0385%\n",
      "total_backward_count 420970 real_backward_count 45316  10.765%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.564718/  1.748700, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0575%\n",
      "layer   2  Sparsity: 66.8878%\n",
      "layer   3  Sparsity: 59.8694%\n",
      "total_backward_count 430760 real_backward_count 46148  10.713%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.567634/  1.763183, val:  60.42%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0669%\n",
      "layer   2  Sparsity: 67.3577%\n",
      "layer   3  Sparsity: 60.6532%\n",
      "total_backward_count 440550 real_backward_count 47012  10.671%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.559097/  1.743635, val:  63.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9880%\n",
      "layer   2  Sparsity: 67.3052%\n",
      "layer   3  Sparsity: 61.2860%\n",
      "total_backward_count 450340 real_backward_count 47872  10.630%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.574992/  1.755315, val:  68.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9878%\n",
      "layer   2  Sparsity: 67.3723%\n",
      "layer   3  Sparsity: 61.4332%\n",
      "total_backward_count 460130 real_backward_count 48704  10.585%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.549560/  1.746192, val:  64.17%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0527%\n",
      "layer   2  Sparsity: 67.1968%\n",
      "layer   3  Sparsity: 61.8103%\n",
      "total_backward_count 469920 real_backward_count 49475  10.528%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.552993/  1.766960, val:  65.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9779%\n",
      "layer   2  Sparsity: 66.9947%\n",
      "layer   3  Sparsity: 62.0774%\n",
      "total_backward_count 479710 real_backward_count 50273  10.480%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.562889/  1.718311, val:  62.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0009%\n",
      "layer   2  Sparsity: 67.2623%\n",
      "layer   3  Sparsity: 61.5595%\n",
      "total_backward_count 489500 real_backward_count 51117  10.443%\n",
      "fc layer 1 self.abs_max_out: 4639.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.551607/  1.732948, val:  67.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0254%\n",
      "layer   2  Sparsity: 66.9712%\n",
      "layer   3  Sparsity: 61.9886%\n",
      "total_backward_count 499290 real_backward_count 51939  10.403%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.544532/  1.737456, val:  60.83%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9862%\n",
      "layer   2  Sparsity: 66.6498%\n",
      "layer   3  Sparsity: 62.4207%\n",
      "total_backward_count 509080 real_backward_count 52747  10.361%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.542825/  1.715152, val:  70.00%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0177%\n",
      "layer   2  Sparsity: 66.6994%\n",
      "layer   3  Sparsity: 62.1730%\n",
      "total_backward_count 518870 real_backward_count 53584  10.327%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.534850/  1.711100, val:  68.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0027%\n",
      "layer   2  Sparsity: 66.4757%\n",
      "layer   3  Sparsity: 61.5984%\n",
      "total_backward_count 528660 real_backward_count 54407  10.291%\n",
      "fc layer 1 self.abs_max_out: 4642.0\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.529716/  1.725974, val:  69.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0215%\n",
      "layer   2  Sparsity: 66.7963%\n",
      "layer   3  Sparsity: 62.0082%\n",
      "total_backward_count 538450 real_backward_count 55174  10.247%\n",
      "fc layer 1 self.abs_max_out: 4782.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.534503/  1.723338, val:  66.25%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0348%\n",
      "layer   2  Sparsity: 66.5418%\n",
      "layer   3  Sparsity: 62.1351%\n",
      "total_backward_count 548240 real_backward_count 55992  10.213%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.532572/  1.733114, val:  71.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9910%\n",
      "layer   2  Sparsity: 66.5016%\n",
      "layer   3  Sparsity: 62.3591%\n",
      "total_backward_count 558030 real_backward_count 56807  10.180%\n",
      "fc layer 3 self.abs_max_out: 1093.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.531765/  1.709431, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0325%\n",
      "layer   2  Sparsity: 66.9028%\n",
      "layer   3  Sparsity: 61.9160%\n",
      "total_backward_count 567820 real_backward_count 57565  10.138%\n",
      "fc layer 1 self.abs_max_out: 4839.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.535323/  1.699156, val:  66.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0283%\n",
      "layer   2  Sparsity: 66.3171%\n",
      "layer   3  Sparsity: 61.6037%\n",
      "total_backward_count 577610 real_backward_count 58301  10.093%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.536798/  1.726792, val:  65.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0262%\n",
      "layer   2  Sparsity: 66.4370%\n",
      "layer   3  Sparsity: 62.1724%\n",
      "total_backward_count 587400 real_backward_count 59060  10.054%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.536374/  1.733721, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0288%\n",
      "layer   2  Sparsity: 66.3987%\n",
      "layer   3  Sparsity: 62.0093%\n",
      "total_backward_count 597190 real_backward_count 59857  10.023%\n",
      "fc layer 1 self.abs_max_out: 4861.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.528001/  1.710510, val:  75.00%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9852%\n",
      "layer   2  Sparsity: 66.4129%\n",
      "layer   3  Sparsity: 61.8314%\n",
      "total_backward_count 606980 real_backward_count 60626   9.988%\n",
      "fc layer 2 self.abs_max_out: 2634.0\n",
      "fc layer 1 self.abs_max_out: 4894.0\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.526487/  1.724502, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0201%\n",
      "layer   2  Sparsity: 65.9220%\n",
      "layer   3  Sparsity: 61.9151%\n",
      "total_backward_count 616770 real_backward_count 61344   9.946%\n",
      "lif layer 1 self.abs_max_v: 7989.0\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.523177/  1.712251, val:  67.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0415%\n",
      "layer   2  Sparsity: 66.3511%\n",
      "layer   3  Sparsity: 62.4773%\n",
      "total_backward_count 626560 real_backward_count 62071   9.907%\n",
      "fc layer 1 self.abs_max_out: 4942.0\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.531705/  1.735911, val:  67.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0288%\n",
      "layer   2  Sparsity: 65.9527%\n",
      "layer   3  Sparsity: 62.5909%\n",
      "total_backward_count 636350 real_backward_count 62837   9.875%\n",
      "lif layer 1 self.abs_max_v: 8131.5\n",
      "lif layer 1 self.abs_max_v: 8151.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.526077/  1.725689, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0199%\n",
      "layer   2  Sparsity: 66.1178%\n",
      "layer   3  Sparsity: 63.0566%\n",
      "total_backward_count 646140 real_backward_count 63567   9.838%\n",
      "lif layer 1 self.abs_max_v: 8196.0\n",
      "lif layer 1 self.abs_max_v: 8225.0\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.512986/  1.713523, val:  69.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0026%\n",
      "layer   2  Sparsity: 66.1710%\n",
      "layer   3  Sparsity: 62.4148%\n",
      "total_backward_count 655930 real_backward_count 64292   9.802%\n",
      "lif layer 1 self.abs_max_v: 8321.0\n",
      "lif layer 1 self.abs_max_v: 8348.5\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.508119/  1.680911, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9904%\n",
      "layer   2  Sparsity: 65.9436%\n",
      "layer   3  Sparsity: 62.6162%\n",
      "total_backward_count 665720 real_backward_count 65033   9.769%\n",
      "fc layer 1 self.abs_max_out: 4968.0\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.504059/  1.669049, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.9944%\n",
      "layer   2  Sparsity: 65.6955%\n",
      "layer   3  Sparsity: 62.3911%\n",
      "total_backward_count 675510 real_backward_count 65760   9.735%\n",
      "fc layer 1 self.abs_max_out: 4986.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.498711/  1.671211, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0093%\n",
      "layer   2  Sparsity: 66.0389%\n",
      "layer   3  Sparsity: 62.9977%\n",
      "total_backward_count 685300 real_backward_count 66507   9.705%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.498517/  1.691465, val:  61.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0299%\n",
      "layer   2  Sparsity: 66.1709%\n",
      "layer   3  Sparsity: 62.2651%\n",
      "total_backward_count 695090 real_backward_count 67221   9.671%\n",
      "fc layer 1 self.abs_max_out: 5089.0\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.509236/  1.678807, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0016%\n",
      "layer   2  Sparsity: 65.6040%\n",
      "layer   3  Sparsity: 62.5171%\n",
      "total_backward_count 704880 real_backward_count 67968   9.642%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.502322/  1.665828, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0101%\n",
      "layer   2  Sparsity: 66.4068%\n",
      "layer   3  Sparsity: 62.3369%\n",
      "total_backward_count 714670 real_backward_count 68717   9.615%\n",
      "fc layer 1 self.abs_max_out: 5092.0\n",
      "fc layer 1 self.abs_max_out: 5200.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.488503/  1.687277, val:  62.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0047%\n",
      "layer   2  Sparsity: 66.4902%\n",
      "layer   3  Sparsity: 62.5169%\n",
      "total_backward_count 724460 real_backward_count 69426   9.583%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.499117/  1.684385, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0183%\n",
      "layer   2  Sparsity: 65.9269%\n",
      "layer   3  Sparsity: 62.9505%\n",
      "total_backward_count 734250 real_backward_count 70162   9.556%\n",
      "fc layer 1 self.abs_max_out: 5256.0\n",
      "lif layer 1 self.abs_max_v: 8375.0\n",
      "lif layer 1 self.abs_max_v: 8503.5\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.496632/  1.663414, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0079%\n",
      "layer   2  Sparsity: 65.9040%\n",
      "layer   3  Sparsity: 62.5877%\n",
      "total_backward_count 744040 real_backward_count 70855   9.523%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.495930/  1.671234, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 90.0263%\n",
      "layer   2  Sparsity: 65.9544%\n",
      "layer   3  Sparsity: 63.0699%\n",
      "total_backward_count 753830 real_backward_count 71547   9.491%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.490265/  1.690545, val:  65.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0362%\n",
      "layer   2  Sparsity: 66.2570%\n",
      "layer   3  Sparsity: 63.4568%\n",
      "total_backward_count 763620 real_backward_count 72170   9.451%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.484183/  1.661521, val:  77.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0497%\n",
      "layer   2  Sparsity: 66.5405%\n",
      "layer   3  Sparsity: 63.3276%\n",
      "total_backward_count 773410 real_backward_count 72863   9.421%\n",
      "fc layer 1 self.abs_max_out: 5269.0\n",
      "lif layer 1 self.abs_max_v: 8569.0\n",
      "lif layer 1 self.abs_max_v: 8860.5\n",
      "lif layer 1 self.abs_max_v: 9298.5\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.477011/  1.682664, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0098%\n",
      "layer   2  Sparsity: 66.6313%\n",
      "layer   3  Sparsity: 63.2270%\n",
      "total_backward_count 783200 real_backward_count 73519   9.387%\n",
      "fc layer 1 self.abs_max_out: 5487.0\n",
      "lif layer 1 self.abs_max_v: 9511.0\n",
      "fc layer 1 self.abs_max_out: 5677.0\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.496962/  1.672724, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.9955%\n",
      "layer   2  Sparsity: 66.1724%\n",
      "layer   3  Sparsity: 63.4281%\n",
      "total_backward_count 792990 real_backward_count 74193   9.356%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.489906/  1.687280, val:  69.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0172%\n",
      "layer   2  Sparsity: 66.0356%\n",
      "layer   3  Sparsity: 63.3342%\n",
      "total_backward_count 802780 real_backward_count 74824   9.321%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.493640/  1.664744, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0016%\n",
      "layer   2  Sparsity: 65.6792%\n",
      "layer   3  Sparsity: 63.1966%\n",
      "total_backward_count 812570 real_backward_count 75492   9.291%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.498989/  1.694378, val:  66.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9855%\n",
      "layer   2  Sparsity: 65.8031%\n",
      "layer   3  Sparsity: 64.4892%\n",
      "total_backward_count 822360 real_backward_count 76137   9.258%\n",
      "lif layer 1 self.abs_max_v: 9526.0\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.502443/  1.679264, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0087%\n",
      "layer   2  Sparsity: 66.0689%\n",
      "layer   3  Sparsity: 64.6973%\n",
      "total_backward_count 832150 real_backward_count 76786   9.227%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.490103/  1.678500, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0287%\n",
      "layer   2  Sparsity: 66.1002%\n",
      "layer   3  Sparsity: 65.0467%\n",
      "total_backward_count 841940 real_backward_count 77428   9.196%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.490305/  1.655670, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0020%\n",
      "layer   2  Sparsity: 65.8901%\n",
      "layer   3  Sparsity: 64.8756%\n",
      "total_backward_count 851730 real_backward_count 78034   9.162%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.484613/  1.658854, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9617%\n",
      "layer   2  Sparsity: 66.0397%\n",
      "layer   3  Sparsity: 64.8340%\n",
      "total_backward_count 861520 real_backward_count 78670   9.132%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.485245/  1.682774, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9783%\n",
      "layer   2  Sparsity: 66.0974%\n",
      "layer   3  Sparsity: 65.2324%\n",
      "total_backward_count 871310 real_backward_count 79326   9.104%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.498171/  1.665411, val:  84.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0211%\n",
      "layer   2  Sparsity: 65.9500%\n",
      "layer   3  Sparsity: 65.4136%\n",
      "total_backward_count 881100 real_backward_count 79979   9.077%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.488986/  1.687140, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0180%\n",
      "layer   2  Sparsity: 66.0030%\n",
      "layer   3  Sparsity: 65.1297%\n",
      "total_backward_count 890890 real_backward_count 80606   9.048%\n",
      "lif layer 2 self.abs_max_v: 4825.0\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.500586/  1.667820, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9829%\n",
      "layer   2  Sparsity: 66.2333%\n",
      "layer   3  Sparsity: 64.8483%\n",
      "total_backward_count 900680 real_backward_count 81217   9.017%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.494545/  1.670614, val:  73.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0242%\n",
      "layer   2  Sparsity: 66.2084%\n",
      "layer   3  Sparsity: 65.1808%\n",
      "total_backward_count 910470 real_backward_count 81807   8.985%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.489399/  1.679289, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0210%\n",
      "layer   2  Sparsity: 66.0223%\n",
      "layer   3  Sparsity: 64.5858%\n",
      "total_backward_count 920260 real_backward_count 82470   8.962%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.478888/  1.666184, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0336%\n",
      "layer   2  Sparsity: 66.0571%\n",
      "layer   3  Sparsity: 64.9000%\n",
      "total_backward_count 930050 real_backward_count 83069   8.932%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.481582/  1.639013, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0326%\n",
      "layer   2  Sparsity: 66.1920%\n",
      "layer   3  Sparsity: 64.9607%\n",
      "total_backward_count 939840 real_backward_count 83693   8.905%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.472764/  1.650242, val:  61.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9819%\n",
      "layer   2  Sparsity: 66.2555%\n",
      "layer   3  Sparsity: 64.6381%\n",
      "total_backward_count 949630 real_backward_count 84319   8.879%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.471395/  1.655080, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0210%\n",
      "layer   2  Sparsity: 66.3620%\n",
      "layer   3  Sparsity: 63.9781%\n",
      "total_backward_count 959420 real_backward_count 84921   8.851%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.473130/  1.682635, val:  67.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0187%\n",
      "layer   2  Sparsity: 66.1684%\n",
      "layer   3  Sparsity: 64.8523%\n",
      "total_backward_count 969210 real_backward_count 85538   8.826%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.483535/  1.670783, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0078%\n",
      "layer   2  Sparsity: 66.0427%\n",
      "layer   3  Sparsity: 64.2354%\n",
      "total_backward_count 979000 real_backward_count 86140   8.799%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.477037/  1.663527, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0061%\n",
      "layer   2  Sparsity: 66.1115%\n",
      "layer   3  Sparsity: 64.0767%\n",
      "total_backward_count 988790 real_backward_count 86738   8.772%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.474545/  1.647717, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0004%\n",
      "layer   2  Sparsity: 66.2880%\n",
      "layer   3  Sparsity: 64.6573%\n",
      "total_backward_count 998580 real_backward_count 87308   8.743%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.471187/  1.655958, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0654%\n",
      "layer   2  Sparsity: 66.0371%\n",
      "layer   3  Sparsity: 65.2010%\n",
      "total_backward_count 1008370 real_backward_count 87907   8.718%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.479405/  1.658785, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0083%\n",
      "layer   2  Sparsity: 66.2719%\n",
      "layer   3  Sparsity: 64.6887%\n",
      "total_backward_count 1018160 real_backward_count 88453   8.688%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.481058/  1.654314, val:  76.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0414%\n",
      "layer   2  Sparsity: 66.0704%\n",
      "layer   3  Sparsity: 65.1295%\n",
      "total_backward_count 1027950 real_backward_count 89012   8.659%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.481712/  1.654107, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0132%\n",
      "layer   2  Sparsity: 66.2829%\n",
      "layer   3  Sparsity: 65.0475%\n",
      "total_backward_count 1037740 real_backward_count 89569   8.631%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.480512/  1.670569, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0159%\n",
      "layer   2  Sparsity: 66.1845%\n",
      "layer   3  Sparsity: 65.6959%\n",
      "total_backward_count 1047530 real_backward_count 90096   8.601%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.477459/  1.656941, val:  67.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0138%\n",
      "layer   2  Sparsity: 66.4404%\n",
      "layer   3  Sparsity: 65.3084%\n",
      "total_backward_count 1057320 real_backward_count 90675   8.576%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.484196/  1.645435, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0020%\n",
      "layer   2  Sparsity: 66.3367%\n",
      "layer   3  Sparsity: 65.6737%\n",
      "total_backward_count 1067110 real_backward_count 91234   8.550%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.467837/  1.643499, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.9729%\n",
      "layer   2  Sparsity: 66.4705%\n",
      "layer   3  Sparsity: 65.6005%\n",
      "total_backward_count 1076900 real_backward_count 91772   8.522%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.466142/  1.660607, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0408%\n",
      "layer   2  Sparsity: 66.4394%\n",
      "layer   3  Sparsity: 65.7561%\n",
      "total_backward_count 1086690 real_backward_count 92297   8.493%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.468195/  1.648854, val:  68.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0283%\n",
      "layer   2  Sparsity: 66.3958%\n",
      "layer   3  Sparsity: 65.3372%\n",
      "total_backward_count 1096480 real_backward_count 92801   8.464%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.454316/  1.638026, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0272%\n",
      "layer   2  Sparsity: 66.2478%\n",
      "layer   3  Sparsity: 65.0209%\n",
      "total_backward_count 1106270 real_backward_count 93324   8.436%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.458384/  1.614479, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9463%\n",
      "layer   2  Sparsity: 66.3897%\n",
      "layer   3  Sparsity: 65.2973%\n",
      "total_backward_count 1116060 real_backward_count 93844   8.409%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.447304/  1.659642, val:  68.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0153%\n",
      "layer   2  Sparsity: 65.8507%\n",
      "layer   3  Sparsity: 65.2715%\n",
      "total_backward_count 1125850 real_backward_count 94408   8.385%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.451968/  1.628005, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0135%\n",
      "layer   2  Sparsity: 65.8572%\n",
      "layer   3  Sparsity: 64.8585%\n",
      "total_backward_count 1135640 real_backward_count 94915   8.358%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.448577/  1.625916, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0076%\n",
      "layer   2  Sparsity: 66.2596%\n",
      "layer   3  Sparsity: 64.3188%\n",
      "total_backward_count 1145430 real_backward_count 95421   8.331%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.447443/  1.639576, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0363%\n",
      "layer   2  Sparsity: 66.5055%\n",
      "layer   3  Sparsity: 64.5515%\n",
      "total_backward_count 1155220 real_backward_count 95906   8.302%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.450752/  1.648845, val:  72.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9919%\n",
      "layer   2  Sparsity: 66.1935%\n",
      "layer   3  Sparsity: 64.3046%\n",
      "total_backward_count 1165010 real_backward_count 96472   8.281%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.438177/  1.627635, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9983%\n",
      "layer   2  Sparsity: 65.8498%\n",
      "layer   3  Sparsity: 64.7051%\n",
      "total_backward_count 1174800 real_backward_count 97009   8.257%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.437399/  1.609190, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9972%\n",
      "layer   2  Sparsity: 66.1836%\n",
      "layer   3  Sparsity: 64.8492%\n",
      "total_backward_count 1184590 real_backward_count 97506   8.231%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.434348/  1.638622, val:  68.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0291%\n",
      "layer   2  Sparsity: 66.0997%\n",
      "layer   3  Sparsity: 65.2409%\n",
      "total_backward_count 1194380 real_backward_count 97993   8.205%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.430133/  1.638053, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0077%\n",
      "layer   2  Sparsity: 65.8494%\n",
      "layer   3  Sparsity: 65.1766%\n",
      "total_backward_count 1204170 real_backward_count 98503   8.180%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.450429/  1.625220, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9990%\n",
      "layer   2  Sparsity: 65.5744%\n",
      "layer   3  Sparsity: 65.1248%\n",
      "total_backward_count 1213960 real_backward_count 99017   8.157%\n",
      "fc layer 3 self.abs_max_out: 1101.0\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.424552/  1.615388, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0288%\n",
      "layer   2  Sparsity: 65.8546%\n",
      "layer   3  Sparsity: 65.5607%\n",
      "total_backward_count 1223750 real_backward_count 99510   8.132%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.428247/  1.638720, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9783%\n",
      "layer   2  Sparsity: 66.1332%\n",
      "layer   3  Sparsity: 64.5539%\n",
      "total_backward_count 1233540 real_backward_count 100007   8.107%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.434499/  1.618371, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0363%\n",
      "layer   2  Sparsity: 66.1181%\n",
      "layer   3  Sparsity: 64.4552%\n",
      "total_backward_count 1243330 real_backward_count 100514   8.084%\n",
      "fc layer 3 self.abs_max_out: 1135.0\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.418532/  1.605923, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0393%\n",
      "layer   2  Sparsity: 66.2110%\n",
      "layer   3  Sparsity: 64.1448%\n",
      "total_backward_count 1253120 real_backward_count 101039   8.063%\n",
      "fc layer 3 self.abs_max_out: 1143.0\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.401116/  1.601439, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.9945%\n",
      "layer   2  Sparsity: 66.0009%\n",
      "layer   3  Sparsity: 65.0961%\n",
      "total_backward_count 1262910 real_backward_count 101514   8.038%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.410488/  1.606509, val:  83.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0053%\n",
      "layer   2  Sparsity: 65.8030%\n",
      "layer   3  Sparsity: 64.7187%\n",
      "total_backward_count 1272700 real_backward_count 102008   8.015%\n",
      "fc layer 1 self.abs_max_out: 5691.0\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.412329/  1.600132, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.9881%\n",
      "layer   2  Sparsity: 66.0358%\n",
      "layer   3  Sparsity: 65.1731%\n",
      "total_backward_count 1282490 real_backward_count 102480   7.991%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.413980/  1.612513, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0026%\n",
      "layer   2  Sparsity: 65.7404%\n",
      "layer   3  Sparsity: 65.3042%\n",
      "total_backward_count 1292280 real_backward_count 102945   7.966%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.413745/  1.613599, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0255%\n",
      "layer   2  Sparsity: 65.8746%\n",
      "layer   3  Sparsity: 65.3043%\n",
      "total_backward_count 1302070 real_backward_count 103409   7.942%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.417071/  1.611079, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0170%\n",
      "layer   2  Sparsity: 66.0381%\n",
      "layer   3  Sparsity: 65.4355%\n",
      "total_backward_count 1311860 real_backward_count 103884   7.919%\n",
      "fc layer 1 self.abs_max_out: 5694.0\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.403470/  1.588073, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0132%\n",
      "layer   2  Sparsity: 66.1840%\n",
      "layer   3  Sparsity: 64.8837%\n",
      "total_backward_count 1321650 real_backward_count 104353   7.896%\n",
      "fc layer 1 self.abs_max_out: 5708.0\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.399268/  1.586408, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9838%\n",
      "layer   2  Sparsity: 66.1615%\n",
      "layer   3  Sparsity: 64.9929%\n",
      "total_backward_count 1331440 real_backward_count 104863   7.876%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.408246/  1.599025, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0135%\n",
      "layer   2  Sparsity: 66.1804%\n",
      "layer   3  Sparsity: 65.4100%\n",
      "total_backward_count 1341230 real_backward_count 105300   7.851%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.412879/  1.596112, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0383%\n",
      "layer   2  Sparsity: 66.2592%\n",
      "layer   3  Sparsity: 65.3383%\n",
      "total_backward_count 1351020 real_backward_count 105790   7.830%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.413655/  1.618095, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9952%\n",
      "layer   2  Sparsity: 66.4557%\n",
      "layer   3  Sparsity: 64.9725%\n",
      "total_backward_count 1360810 real_backward_count 106274   7.810%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.421642/  1.607921, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0243%\n",
      "layer   2  Sparsity: 66.0813%\n",
      "layer   3  Sparsity: 65.1746%\n",
      "total_backward_count 1370600 real_backward_count 106737   7.788%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.420472/  1.617699, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9646%\n",
      "layer   2  Sparsity: 66.0082%\n",
      "layer   3  Sparsity: 66.0320%\n",
      "total_backward_count 1380390 real_backward_count 107185   7.765%\n",
      "fc layer 2 self.abs_max_out: 2667.0\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.411755/  1.603410, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0000%\n",
      "layer   2  Sparsity: 65.7601%\n",
      "layer   3  Sparsity: 65.5522%\n",
      "total_backward_count 1390180 real_backward_count 107616   7.741%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.415398/  1.599940, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0003%\n",
      "layer   2  Sparsity: 65.9701%\n",
      "layer   3  Sparsity: 65.1944%\n",
      "total_backward_count 1399970 real_backward_count 108067   7.719%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.403901/  1.588957, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9905%\n",
      "layer   2  Sparsity: 66.0195%\n",
      "layer   3  Sparsity: 64.9900%\n",
      "total_backward_count 1409760 real_backward_count 108541   7.699%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.397496/  1.597250, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0191%\n",
      "layer   2  Sparsity: 66.2506%\n",
      "layer   3  Sparsity: 65.1179%\n",
      "total_backward_count 1419550 real_backward_count 109006   7.679%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.399362/  1.589000, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0374%\n",
      "layer   2  Sparsity: 66.3008%\n",
      "layer   3  Sparsity: 64.5340%\n",
      "total_backward_count 1429340 real_backward_count 109440   7.657%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.406880/  1.608563, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0235%\n",
      "layer   2  Sparsity: 66.0024%\n",
      "layer   3  Sparsity: 64.5879%\n",
      "total_backward_count 1439130 real_backward_count 109879   7.635%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.397702/  1.599202, val:  75.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0089%\n",
      "layer   2  Sparsity: 65.7037%\n",
      "layer   3  Sparsity: 64.9693%\n",
      "total_backward_count 1448920 real_backward_count 110308   7.613%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.404996/  1.583465, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0143%\n",
      "layer   2  Sparsity: 65.7906%\n",
      "layer   3  Sparsity: 64.3326%\n",
      "total_backward_count 1458710 real_backward_count 110765   7.593%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.398921/  1.594045, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0035%\n",
      "layer   2  Sparsity: 65.7189%\n",
      "layer   3  Sparsity: 65.0084%\n",
      "total_backward_count 1468500 real_backward_count 111212   7.573%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.401211/  1.600612, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0658%\n",
      "layer   2  Sparsity: 65.6922%\n",
      "layer   3  Sparsity: 65.1519%\n",
      "total_backward_count 1478290 real_backward_count 111654   7.553%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.399382/  1.602669, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0300%\n",
      "layer   2  Sparsity: 65.8664%\n",
      "layer   3  Sparsity: 65.5619%\n",
      "total_backward_count 1488080 real_backward_count 112099   7.533%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.393474/  1.607229, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0238%\n",
      "layer   2  Sparsity: 65.9599%\n",
      "layer   3  Sparsity: 65.7030%\n",
      "total_backward_count 1497870 real_backward_count 112509   7.511%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.398791/  1.573642, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9908%\n",
      "layer   2  Sparsity: 65.8215%\n",
      "layer   3  Sparsity: 66.1023%\n",
      "total_backward_count 1507660 real_backward_count 112954   7.492%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.403410/  1.592774, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0040%\n",
      "layer   2  Sparsity: 65.6094%\n",
      "layer   3  Sparsity: 66.3698%\n",
      "total_backward_count 1517450 real_backward_count 113349   7.470%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.394667/  1.588196, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0367%\n",
      "layer   2  Sparsity: 65.8535%\n",
      "layer   3  Sparsity: 66.0404%\n",
      "total_backward_count 1527240 real_backward_count 113756   7.448%\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.394385/  1.605510, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0035%\n",
      "layer   2  Sparsity: 66.2416%\n",
      "layer   3  Sparsity: 65.7306%\n",
      "total_backward_count 1537030 real_backward_count 114138   7.426%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.393236/  1.604172, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0051%\n",
      "layer   2  Sparsity: 65.7793%\n",
      "layer   3  Sparsity: 66.0445%\n",
      "total_backward_count 1546820 real_backward_count 114546   7.405%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.399626/  1.585933, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0356%\n",
      "layer   2  Sparsity: 66.0747%\n",
      "layer   3  Sparsity: 66.0890%\n",
      "total_backward_count 1556610 real_backward_count 114949   7.385%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.393435/  1.577691, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.0401%\n",
      "layer   2  Sparsity: 66.0410%\n",
      "layer   3  Sparsity: 65.6642%\n",
      "total_backward_count 1566400 real_backward_count 115348   7.364%\n",
      "fc layer 3 self.abs_max_out: 1151.0\n",
      "fc layer 3 self.abs_max_out: 1154.0\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.397031/  1.585424, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0398%\n",
      "layer   2  Sparsity: 66.1889%\n",
      "layer   3  Sparsity: 65.4656%\n",
      "total_backward_count 1576190 real_backward_count 115769   7.345%\n",
      "fc layer 1 self.abs_max_out: 5820.0\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.401578/  1.589758, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0164%\n",
      "layer   2  Sparsity: 66.1277%\n",
      "layer   3  Sparsity: 65.7987%\n",
      "total_backward_count 1585980 real_backward_count 116174   7.325%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.389202/  1.579636, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0034%\n",
      "layer   2  Sparsity: 65.5614%\n",
      "layer   3  Sparsity: 65.7120%\n",
      "total_backward_count 1595770 real_backward_count 116543   7.303%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.372808/  1.570040, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0071%\n",
      "layer   2  Sparsity: 65.6957%\n",
      "layer   3  Sparsity: 65.9167%\n",
      "total_backward_count 1605560 real_backward_count 116930   7.283%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.366579/  1.562413, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0066%\n",
      "layer   2  Sparsity: 65.6226%\n",
      "layer   3  Sparsity: 65.9352%\n",
      "total_backward_count 1615350 real_backward_count 117360   7.265%\n",
      "lif layer 1 self.abs_max_v: 9624.0\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.376131/  1.576915, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0158%\n",
      "layer   2  Sparsity: 65.6052%\n",
      "layer   3  Sparsity: 66.0139%\n",
      "total_backward_count 1625140 real_backward_count 117760   7.246%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.377858/  1.580007, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0342%\n",
      "layer   2  Sparsity: 65.8870%\n",
      "layer   3  Sparsity: 65.5235%\n",
      "total_backward_count 1634930 real_backward_count 118163   7.227%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.381090/  1.556712, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9809%\n",
      "layer   2  Sparsity: 66.1376%\n",
      "layer   3  Sparsity: 65.3313%\n",
      "total_backward_count 1644720 real_backward_count 118526   7.206%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.374053/  1.580386, val:  72.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0478%\n",
      "layer   2  Sparsity: 66.0025%\n",
      "layer   3  Sparsity: 66.4875%\n",
      "total_backward_count 1654510 real_backward_count 118880   7.185%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.366818/  1.571380, val:  75.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0153%\n",
      "layer   2  Sparsity: 66.1594%\n",
      "layer   3  Sparsity: 66.2901%\n",
      "total_backward_count 1664300 real_backward_count 119271   7.166%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.371881/  1.572069, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0192%\n",
      "layer   2  Sparsity: 66.1482%\n",
      "layer   3  Sparsity: 65.8891%\n",
      "total_backward_count 1674090 real_backward_count 119647   7.147%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.381949/  1.602304, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9811%\n",
      "layer   2  Sparsity: 66.0400%\n",
      "layer   3  Sparsity: 65.8137%\n",
      "total_backward_count 1683880 real_backward_count 120035   7.128%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.381264/  1.592406, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0094%\n",
      "layer   2  Sparsity: 65.8273%\n",
      "layer   3  Sparsity: 66.2090%\n",
      "total_backward_count 1693670 real_backward_count 120419   7.110%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.374984/  1.544118, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9949%\n",
      "layer   2  Sparsity: 66.0268%\n",
      "layer   3  Sparsity: 66.3734%\n",
      "total_backward_count 1703460 real_backward_count 120798   7.091%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.367832/  1.567777, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9892%\n",
      "layer   2  Sparsity: 65.8548%\n",
      "layer   3  Sparsity: 66.4379%\n",
      "total_backward_count 1713250 real_backward_count 121142   7.071%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.364471/  1.541769, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9723%\n",
      "layer   2  Sparsity: 65.9432%\n",
      "layer   3  Sparsity: 66.2742%\n",
      "total_backward_count 1723040 real_backward_count 121470   7.050%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.364752/  1.553631, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0092%\n",
      "layer   2  Sparsity: 65.8675%\n",
      "layer   3  Sparsity: 65.6400%\n",
      "total_backward_count 1732830 real_backward_count 121846   7.032%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.372237/  1.567206, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0143%\n",
      "layer   2  Sparsity: 65.7862%\n",
      "layer   3  Sparsity: 65.6317%\n",
      "total_backward_count 1742620 real_backward_count 122209   7.013%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.375860/  1.588472, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9956%\n",
      "layer   2  Sparsity: 65.8041%\n",
      "layer   3  Sparsity: 66.0261%\n",
      "total_backward_count 1752410 real_backward_count 122606   6.996%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.374820/  1.553550, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0245%\n",
      "layer   2  Sparsity: 65.8374%\n",
      "layer   3  Sparsity: 66.0966%\n",
      "total_backward_count 1762200 real_backward_count 122979   6.979%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.369944/  1.547781, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9839%\n",
      "layer   2  Sparsity: 66.0827%\n",
      "layer   3  Sparsity: 66.2239%\n",
      "total_backward_count 1771990 real_backward_count 123320   6.959%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.364460/  1.555326, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0145%\n",
      "layer   2  Sparsity: 66.1585%\n",
      "layer   3  Sparsity: 66.1235%\n",
      "total_backward_count 1781780 real_backward_count 123668   6.941%\n",
      "fc layer 2 self.abs_max_out: 2794.0\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.366428/  1.573690, val:  75.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.9728%\n",
      "layer   2  Sparsity: 66.2337%\n",
      "layer   3  Sparsity: 66.2575%\n",
      "total_backward_count 1791570 real_backward_count 123992   6.921%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.362520/  1.561795, val:  75.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0038%\n",
      "layer   2  Sparsity: 66.0368%\n",
      "layer   3  Sparsity: 66.7583%\n",
      "total_backward_count 1801360 real_backward_count 124317   6.901%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.364572/  1.567338, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0175%\n",
      "layer   2  Sparsity: 65.9375%\n",
      "layer   3  Sparsity: 66.6881%\n",
      "total_backward_count 1811150 real_backward_count 124664   6.883%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.374497/  1.562406, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9871%\n",
      "layer   2  Sparsity: 65.8323%\n",
      "layer   3  Sparsity: 65.8066%\n",
      "total_backward_count 1820940 real_backward_count 125035   6.867%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.365115/  1.571936, val:  81.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9917%\n",
      "layer   2  Sparsity: 66.1884%\n",
      "layer   3  Sparsity: 65.7573%\n",
      "total_backward_count 1830730 real_backward_count 125375   6.848%\n",
      "lif layer 2 self.abs_max_v: 4930.5\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.362870/  1.562919, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0084%\n",
      "layer   2  Sparsity: 66.1955%\n",
      "layer   3  Sparsity: 65.7818%\n",
      "total_backward_count 1840520 real_backward_count 125726   6.831%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.356706/  1.559980, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0029%\n",
      "layer   2  Sparsity: 66.1565%\n",
      "layer   3  Sparsity: 66.2503%\n",
      "total_backward_count 1850310 real_backward_count 126054   6.813%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.358950/  1.568379, val:  78.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9774%\n",
      "layer   2  Sparsity: 66.1592%\n",
      "layer   3  Sparsity: 65.8595%\n",
      "total_backward_count 1860100 real_backward_count 126440   6.797%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.359205/  1.569567, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9778%\n",
      "layer   2  Sparsity: 66.2776%\n",
      "layer   3  Sparsity: 65.5845%\n",
      "total_backward_count 1869890 real_backward_count 126783   6.780%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.359651/  1.589010, val:  76.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0077%\n",
      "layer   2  Sparsity: 66.0513%\n",
      "layer   3  Sparsity: 65.5559%\n",
      "total_backward_count 1879680 real_backward_count 127108   6.762%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.357351/  1.559728, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0039%\n",
      "layer   2  Sparsity: 66.1059%\n",
      "layer   3  Sparsity: 65.7581%\n",
      "total_backward_count 1889470 real_backward_count 127455   6.746%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.356016/  1.569166, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0306%\n",
      "layer   2  Sparsity: 66.1315%\n",
      "layer   3  Sparsity: 65.9901%\n",
      "total_backward_count 1899260 real_backward_count 127791   6.728%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.353599/  1.543925, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0111%\n",
      "layer   2  Sparsity: 66.0815%\n",
      "layer   3  Sparsity: 66.3119%\n",
      "total_backward_count 1909050 real_backward_count 128092   6.710%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.348894/  1.564510, val:  69.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9750%\n",
      "layer   2  Sparsity: 65.9102%\n",
      "layer   3  Sparsity: 66.2718%\n",
      "total_backward_count 1918840 real_backward_count 128432   6.693%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.349087/  1.558470, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.0275%\n",
      "layer   2  Sparsity: 65.9726%\n",
      "layer   3  Sparsity: 65.8685%\n",
      "total_backward_count 1928630 real_backward_count 128738   6.675%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.357717/  1.542678, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.0794%\n",
      "layer   2  Sparsity: 65.9571%\n",
      "layer   3  Sparsity: 66.3968%\n",
      "total_backward_count 1938420 real_backward_count 129034   6.657%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.347257/  1.553604, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9950%\n",
      "layer   2  Sparsity: 66.0220%\n",
      "layer   3  Sparsity: 66.3420%\n",
      "total_backward_count 1948210 real_backward_count 129331   6.638%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.345357/  1.550979, val:  73.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.0183%\n",
      "layer   2  Sparsity: 65.6942%\n",
      "layer   3  Sparsity: 65.9993%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b874b45f9f4019b4d087db089bdbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.34536</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.7375</td></tr><tr><td>val_loss</td><td>1.55098</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-sweep-109</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ix07cven' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ix07cven</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_152143-ix07cven/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jecm2yr9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_193751-jecm2yr9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jecm2yr9' target=\"_blank\">earnest-sweep-115</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jecm2yr9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jecm2yr9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251116_193800_371', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 525.5\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "fc layer 2 self.abs_max_out: 348.0\n",
      "lif layer 2 self.abs_max_v: 348.0\n",
      "fc layer 2 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 414.0\n",
      "fc layer 1 self.abs_max_out: 814.0\n",
      "lif layer 1 self.abs_max_v: 814.0\n",
      "lif layer 2 self.abs_max_v: 434.5\n",
      "fc layer 1 self.abs_max_out: 816.0\n",
      "lif layer 1 self.abs_max_v: 821.5\n",
      "fc layer 2 self.abs_max_out: 480.0\n",
      "lif layer 2 self.abs_max_v: 627.0\n",
      "fc layer 1 self.abs_max_out: 936.0\n",
      "lif layer 1 self.abs_max_v: 936.0\n",
      "fc layer 1 self.abs_max_out: 1065.0\n",
      "lif layer 1 self.abs_max_v: 1065.0\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 1 self.abs_max_out: 1145.0\n",
      "lif layer 1 self.abs_max_v: 1145.0\n",
      "lif layer 2 self.abs_max_v: 771.5\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 1 self.abs_max_out: 1222.0\n",
      "lif layer 1 self.abs_max_v: 1222.0\n",
      "lif layer 2 self.abs_max_v: 819.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 1 self.abs_max_out: 1623.0\n",
      "lif layer 1 self.abs_max_v: 1623.0\n",
      "fc layer 1 self.abs_max_out: 1815.0\n",
      "lif layer 1 self.abs_max_v: 1815.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 605.0\n",
      "lif layer 2 self.abs_max_v: 831.5\n",
      "lif layer 2 self.abs_max_v: 870.0\n",
      "lif layer 2 self.abs_max_v: 873.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 2 self.abs_max_out: 633.0\n",
      "lif layer 2 self.abs_max_v: 957.0\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "lif layer 2 self.abs_max_v: 958.5\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "lif layer 2 self.abs_max_v: 983.0\n",
      "lif layer 2 self.abs_max_v: 1027.0\n",
      "lif layer 2 self.abs_max_v: 1033.5\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "lif layer 2 self.abs_max_v: 1099.0\n",
      "lif layer 2 self.abs_max_v: 1258.0\n",
      "fc layer 1 self.abs_max_out: 1922.0\n",
      "lif layer 1 self.abs_max_v: 1922.0\n",
      "fc layer 2 self.abs_max_out: 981.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 1 self.abs_max_out: 1928.0\n",
      "lif layer 1 self.abs_max_v: 1928.0\n",
      "lif layer 2 self.abs_max_v: 1288.0\n",
      "lif layer 2 self.abs_max_v: 1370.5\n",
      "fc layer 1 self.abs_max_out: 2070.0\n",
      "lif layer 1 self.abs_max_v: 2070.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "fc layer 2 self.abs_max_out: 1091.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "lif layer 2 self.abs_max_v: 1372.0\n",
      "lif layer 2 self.abs_max_v: 1693.0\n",
      "fc layer 1 self.abs_max_out: 2172.0\n",
      "lif layer 1 self.abs_max_v: 2172.0\n",
      "fc layer 1 self.abs_max_out: 2371.0\n",
      "lif layer 1 self.abs_max_v: 2371.0\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 2389.0\n",
      "lif layer 2 self.abs_max_v: 1718.5\n",
      "lif layer 2 self.abs_max_v: 1734.0\n",
      "lif layer 2 self.abs_max_v: 1879.0\n",
      "fc layer 1 self.abs_max_out: 2449.0\n",
      "lif layer 1 self.abs_max_v: 2449.0\n",
      "lif layer 2 self.abs_max_v: 1890.5\n",
      "fc layer 2 self.abs_max_out: 1227.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 1 self.abs_max_out: 2451.0\n",
      "lif layer 1 self.abs_max_v: 2451.0\n",
      "fc layer 2 self.abs_max_out: 1233.0\n",
      "fc layer 2 self.abs_max_out: 1354.0\n",
      "fc layer 2 self.abs_max_out: 1384.0\n",
      "fc layer 1 self.abs_max_out: 2778.0\n",
      "lif layer 1 self.abs_max_v: 2778.0\n",
      "fc layer 1 self.abs_max_out: 3134.0\n",
      "lif layer 1 self.abs_max_v: 3134.0\n",
      "fc layer 2 self.abs_max_out: 1412.0\n",
      "fc layer 2 self.abs_max_out: 1482.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 2 self.abs_max_out: 1547.0\n",
      "fc layer 2 self.abs_max_out: 1613.0\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "fc layer 2 self.abs_max_out: 1633.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 1678.0\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 2 self.abs_max_out: 1697.0\n",
      "fc layer 2 self.abs_max_out: 1746.0\n",
      "fc layer 1 self.abs_max_out: 3177.0\n",
      "lif layer 1 self.abs_max_v: 3177.0\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "lif layer 1 self.abs_max_v: 3248.0\n",
      "lif layer 1 self.abs_max_v: 3272.0\n",
      "lif layer 1 self.abs_max_v: 3373.0\n",
      "lif layer 1 self.abs_max_v: 3511.0\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 1 self.abs_max_out: 3357.0\n",
      "fc layer 2 self.abs_max_out: 1812.0\n",
      "fc layer 1 self.abs_max_out: 3455.0\n",
      "fc layer 1 self.abs_max_out: 3560.0\n",
      "lif layer 1 self.abs_max_v: 3560.0\n",
      "fc layer 2 self.abs_max_out: 1917.0\n",
      "lif layer 2 self.abs_max_v: 1917.0\n",
      "fc layer 1 self.abs_max_out: 3589.0\n",
      "lif layer 1 self.abs_max_v: 3589.0\n",
      "lif layer 1 self.abs_max_v: 3898.5\n",
      "lif layer 1 self.abs_max_v: 4409.5\n",
      "fc layer 1 self.abs_max_out: 3665.0\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 1 self.abs_max_out: 3801.0\n",
      "fc layer 2 self.abs_max_out: 1920.0\n",
      "lif layer 2 self.abs_max_v: 1920.0\n",
      "fc layer 2 self.abs_max_out: 1928.0\n",
      "lif layer 2 self.abs_max_v: 1928.0\n",
      "lif layer 2 self.abs_max_v: 2039.0\n",
      "lif layer 2 self.abs_max_v: 2043.5\n",
      "lif layer 1 self.abs_max_v: 4475.0\n",
      "fc layer 1 self.abs_max_out: 4026.0\n",
      "fc layer 1 self.abs_max_out: 4080.0\n",
      "lif layer 1 self.abs_max_v: 4598.5\n",
      "fc layer 2 self.abs_max_out: 2009.0\n",
      "fc layer 1 self.abs_max_out: 4092.0\n",
      "fc layer 1 self.abs_max_out: 4179.0\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "lif layer 2 self.abs_max_v: 2062.0\n",
      "lif layer 1 self.abs_max_v: 4644.5\n",
      "fc layer 1 self.abs_max_out: 4205.0\n",
      "fc layer 2 self.abs_max_out: 2064.0\n",
      "lif layer 2 self.abs_max_v: 2064.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.112866/  2.149428, val:  30.00%, val_best:  30.00%, tr:  79.16%, tr_best:  79.16%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9369%\n",
      "layer   2  Sparsity: 84.6854%\n",
      "layer   3  Sparsity: 89.3056%\n",
      "total_backward_count 9790 real_backward_count 4061  41.481%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 4980.5\n",
      "fc layer 1 self.abs_max_out: 4252.0\n",
      "lif layer 2 self.abs_max_v: 2107.0\n",
      "fc layer 1 self.abs_max_out: 4356.0\n",
      "lif layer 1 self.abs_max_v: 5049.0\n",
      "lif layer 1 self.abs_max_v: 5135.5\n",
      "fc layer 1 self.abs_max_out: 4422.0\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "lif layer 2 self.abs_max_v: 2149.5\n",
      "lif layer 2 self.abs_max_v: 2160.0\n",
      "lif layer 2 self.abs_max_v: 2222.5\n",
      "lif layer 2 self.abs_max_v: 2287.0\n",
      "lif layer 2 self.abs_max_v: 2386.5\n",
      "fc layer 2 self.abs_max_out: 2111.0\n",
      "lif layer 1 self.abs_max_v: 5816.5\n",
      "lif layer 1 self.abs_max_v: 6414.5\n",
      "fc layer 1 self.abs_max_out: 4455.0\n",
      "fc layer 1 self.abs_max_out: 4478.0\n",
      "fc layer 1 self.abs_max_out: 4610.0\n",
      "fc layer 1 self.abs_max_out: 4721.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.096606/  2.146482, val:  43.75%, val_best:  43.75%, tr:  97.14%, tr_best:  97.14%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9249%\n",
      "layer   2  Sparsity: 83.7047%\n",
      "layer   3  Sparsity: 87.7265%\n",
      "total_backward_count 19580 real_backward_count 6332  32.339%\n",
      "fc layer 2 self.abs_max_out: 2222.0\n",
      "fc layer 1 self.abs_max_out: 4931.0\n",
      "fc layer 1 self.abs_max_out: 5048.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 1 self.abs_max_out: 5133.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.106900/  2.167125, val:  47.50%, val_best:  47.50%, tr:  98.26%, tr_best:  98.26%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 82.9541%\n",
      "layer   3  Sparsity: 87.2459%\n",
      "total_backward_count 29370 real_backward_count 8489  28.904%\n",
      "fc layer 1 self.abs_max_out: 5150.0\n",
      "fc layer 1 self.abs_max_out: 5401.0\n",
      "lif layer 1 self.abs_max_v: 6876.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.121058/  2.182914, val:  38.75%, val_best:  47.50%, tr:  96.94%, tr_best:  98.26%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 83.6740%\n",
      "layer   3  Sparsity: 87.6988%\n",
      "total_backward_count 39160 real_backward_count 10571  26.994%\n",
      "fc layer 1 self.abs_max_out: 5472.0\n",
      "fc layer 2 self.abs_max_out: 2226.0\n",
      "fc layer 2 self.abs_max_out: 2233.0\n",
      "fc layer 1 self.abs_max_out: 5490.0\n",
      "fc layer 1 self.abs_max_out: 5557.0\n",
      "fc layer 2 self.abs_max_out: 2263.0\n",
      "lif layer 1 self.abs_max_v: 6950.5\n",
      "fc layer 2 self.abs_max_out: 2282.0\n",
      "lif layer 1 self.abs_max_v: 7270.5\n",
      "fc layer 2 self.abs_max_out: 2289.0\n",
      "fc layer 2 self.abs_max_out: 2299.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.116763/  2.169795, val:  50.83%, val_best:  50.83%, tr:  98.47%, tr_best:  98.47%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 83.3977%\n",
      "layer   3  Sparsity: 86.7815%\n",
      "total_backward_count 48950 real_backward_count 12537  25.612%\n",
      "fc layer 1 self.abs_max_out: 5654.0\n",
      "lif layer 1 self.abs_max_v: 7414.5\n",
      "lif layer 1 self.abs_max_v: 7737.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.114188/  2.172381, val:  47.08%, val_best:  50.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9070%\n",
      "layer   2  Sparsity: 83.6701%\n",
      "layer   3  Sparsity: 86.8184%\n",
      "total_backward_count 58740 real_backward_count 14411  24.534%\n",
      "lif layer 2 self.abs_max_v: 2515.5\n",
      "fc layer 1 self.abs_max_out: 5723.0\n",
      "fc layer 2 self.abs_max_out: 2434.0\n",
      "fc layer 2 self.abs_max_out: 2435.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.117087/  2.171296, val:  47.92%, val_best:  50.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 83.7176%\n",
      "layer   3  Sparsity: 86.8596%\n",
      "total_backward_count 68530 real_backward_count 16200  23.639%\n",
      "fc layer 2 self.abs_max_out: 2489.0\n",
      "fc layer 1 self.abs_max_out: 5842.0\n",
      "fc layer 2 self.abs_max_out: 2502.0\n",
      "fc layer 2 self.abs_max_out: 2504.0\n",
      "fc layer 2 self.abs_max_out: 2558.0\n",
      "lif layer 2 self.abs_max_v: 2558.0\n",
      "fc layer 2 self.abs_max_out: 2626.0\n",
      "lif layer 2 self.abs_max_v: 2626.0\n",
      "fc layer 2 self.abs_max_out: 2684.0\n",
      "lif layer 2 self.abs_max_v: 2684.0\n",
      "lif layer 1 self.abs_max_v: 8032.5\n",
      "lif layer 1 self.abs_max_v: 8489.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.109109/  2.155501, val:  51.25%, val_best:  51.25%, tr:  98.98%, tr_best:  98.98%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9136%\n",
      "layer   2  Sparsity: 83.2459%\n",
      "layer   3  Sparsity: 86.2741%\n",
      "total_backward_count 78320 real_backward_count 17844  22.783%\n",
      "fc layer 2 self.abs_max_out: 2697.0\n",
      "lif layer 2 self.abs_max_v: 2697.0\n",
      "fc layer 1 self.abs_max_out: 5944.0\n",
      "fc layer 2 self.abs_max_out: 2750.0\n",
      "lif layer 2 self.abs_max_v: 2750.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.111126/  2.152506, val:  55.42%, val_best:  55.42%, tr:  98.06%, tr_best:  98.98%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 82.8155%\n",
      "layer   3  Sparsity: 86.2103%\n",
      "total_backward_count 88110 real_backward_count 19594  22.238%\n",
      "fc layer 1 self.abs_max_out: 6048.0\n",
      "lif layer 1 self.abs_max_v: 9099.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.103346/  2.178372, val:  55.00%, val_best:  55.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 82.7744%\n",
      "layer   3  Sparsity: 86.5193%\n",
      "total_backward_count 97900 real_backward_count 21289  21.746%\n",
      "fc layer 1 self.abs_max_out: 6058.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.108766/  2.153263, val:  52.92%, val_best:  55.42%, tr:  99.18%, tr_best:  99.49%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 82.3710%\n",
      "layer   3  Sparsity: 86.2320%\n",
      "total_backward_count 107690 real_backward_count 22877  21.243%\n",
      "fc layer 2 self.abs_max_out: 2966.0\n",
      "lif layer 2 self.abs_max_v: 2966.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.105817/  2.143109, val:  57.92%, val_best:  57.92%, tr:  98.98%, tr_best:  99.49%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9446%\n",
      "layer   2  Sparsity: 81.9986%\n",
      "layer   3  Sparsity: 85.7380%\n",
      "total_backward_count 117480 real_backward_count 24488  20.844%\n",
      "fc layer 1 self.abs_max_out: 6064.0\n",
      "fc layer 1 self.abs_max_out: 6228.0\n",
      "lif layer 1 self.abs_max_v: 9586.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.096113/  2.157677, val:  59.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 81.8839%\n",
      "layer   3  Sparsity: 86.0521%\n",
      "total_backward_count 127270 real_backward_count 26026  20.449%\n",
      "fc layer 1 self.abs_max_out: 6254.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.100858/  2.163428, val:  50.00%, val_best:  59.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9257%\n",
      "layer   2  Sparsity: 82.3152%\n",
      "layer   3  Sparsity: 86.3190%\n",
      "total_backward_count 137060 real_backward_count 27522  20.080%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.099473/  2.146072, val:  58.33%, val_best:  59.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9279%\n",
      "layer   2  Sparsity: 82.2318%\n",
      "layer   3  Sparsity: 86.4297%\n",
      "total_backward_count 146850 real_backward_count 28966  19.725%\n",
      "lif layer 2 self.abs_max_v: 2983.5\n",
      "fc layer 1 self.abs_max_out: 6420.0\n",
      "lif layer 1 self.abs_max_v: 9661.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.097253/  2.152765, val:  49.58%, val_best:  59.58%, tr:  99.39%, tr_best:  99.49%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9142%\n",
      "layer   2  Sparsity: 82.0208%\n",
      "layer   3  Sparsity: 86.2912%\n",
      "total_backward_count 156640 real_backward_count 30461  19.447%\n",
      "fc layer 1 self.abs_max_out: 6529.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.099255/  2.144877, val:  53.33%, val_best:  59.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8762%\n",
      "layer   2  Sparsity: 81.8364%\n",
      "layer   3  Sparsity: 86.5709%\n",
      "total_backward_count 166430 real_backward_count 31850  19.137%\n",
      "lif layer 2 self.abs_max_v: 3006.5\n",
      "lif layer 2 self.abs_max_v: 3070.5\n",
      "lif layer 2 self.abs_max_v: 3134.0\n",
      "lif layer 2 self.abs_max_v: 3245.5\n",
      "lif layer 2 self.abs_max_v: 3293.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.097988/  2.145355, val:  70.83%, val_best:  70.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9105%\n",
      "layer   2  Sparsity: 81.6076%\n",
      "layer   3  Sparsity: 86.4161%\n",
      "total_backward_count 176220 real_backward_count 33348  18.924%\n",
      "lif layer 1 self.abs_max_v: 10004.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.093623/  2.149041, val:  63.75%, val_best:  70.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 81.7306%\n",
      "layer   3  Sparsity: 86.6384%\n",
      "total_backward_count 186010 real_backward_count 34824  18.722%\n",
      "fc layer 1 self.abs_max_out: 6546.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.096952/  2.149094, val:  48.33%, val_best:  70.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 81.7123%\n",
      "layer   3  Sparsity: 86.7303%\n",
      "total_backward_count 195800 real_backward_count 36209  18.493%\n",
      "fc layer 1 self.abs_max_out: 6625.0\n",
      "lif layer 1 self.abs_max_v: 10251.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.083497/  2.147416, val:  55.00%, val_best:  70.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8994%\n",
      "layer   2  Sparsity: 81.8815%\n",
      "layer   3  Sparsity: 86.5360%\n",
      "total_backward_count 205590 real_backward_count 37555  18.267%\n",
      "fc layer 1 self.abs_max_out: 6729.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.093924/  2.140282, val:  66.25%, val_best:  70.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9562%\n",
      "layer   2  Sparsity: 81.1177%\n",
      "layer   3  Sparsity: 86.6407%\n",
      "total_backward_count 215380 real_backward_count 38971  18.094%\n",
      "fc layer 1 self.abs_max_out: 6793.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.085247/  2.125694, val:  59.17%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9311%\n",
      "layer   2  Sparsity: 80.9623%\n",
      "layer   3  Sparsity: 86.3690%\n",
      "total_backward_count 225170 real_backward_count 40297  17.896%\n",
      "fc layer 1 self.abs_max_out: 6903.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.076157/  2.134073, val:  62.92%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 81.2711%\n",
      "layer   3  Sparsity: 86.4290%\n",
      "total_backward_count 234960 real_backward_count 41611  17.710%\n",
      "fc layer 1 self.abs_max_out: 6988.0\n",
      "lif layer 1 self.abs_max_v: 10276.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.077833/  2.132846, val:  65.42%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9511%\n",
      "layer   2  Sparsity: 81.2628%\n",
      "layer   3  Sparsity: 86.2577%\n",
      "total_backward_count 244750 real_backward_count 42904  17.530%\n",
      "fc layer 1 self.abs_max_out: 7037.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.081477/  2.138250, val:  63.75%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 81.3205%\n",
      "layer   3  Sparsity: 86.8854%\n",
      "total_backward_count 254540 real_backward_count 44325  17.414%\n",
      "fc layer 1 self.abs_max_out: 7071.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.071989/  2.130198, val:  67.08%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 80.8907%\n",
      "layer   3  Sparsity: 86.2660%\n",
      "total_backward_count 264330 real_backward_count 45618  17.258%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.079542/  2.134644, val:  62.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9239%\n",
      "layer   2  Sparsity: 81.1749%\n",
      "layer   3  Sparsity: 86.7264%\n",
      "total_backward_count 274120 real_backward_count 46863  17.096%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.081089/  2.135671, val:  63.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9076%\n",
      "layer   2  Sparsity: 81.5317%\n",
      "layer   3  Sparsity: 86.9243%\n",
      "total_backward_count 283910 real_backward_count 48118  16.948%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.074903/  2.111360, val:  62.92%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 81.2022%\n",
      "layer   3  Sparsity: 86.6447%\n",
      "total_backward_count 293700 real_backward_count 49335  16.798%\n",
      "fc layer 1 self.abs_max_out: 7108.0\n",
      "lif layer 2 self.abs_max_v: 3315.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.067497/  2.127108, val:  69.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 80.7829%\n",
      "layer   3  Sparsity: 86.4576%\n",
      "total_backward_count 303490 real_backward_count 50572  16.663%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.069962/  2.139172, val:  61.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9590%\n",
      "layer   2  Sparsity: 81.1150%\n",
      "layer   3  Sparsity: 86.8490%\n",
      "total_backward_count 313280 real_backward_count 51767  16.524%\n",
      "lif layer 2 self.abs_max_v: 3328.0\n",
      "fc layer 1 self.abs_max_out: 7151.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.072201/  2.137493, val:  59.58%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8996%\n",
      "layer   2  Sparsity: 80.9765%\n",
      "layer   3  Sparsity: 86.8824%\n",
      "total_backward_count 323070 real_backward_count 52928  16.383%\n",
      "fc layer 1 self.abs_max_out: 7179.0\n",
      "lif layer 2 self.abs_max_v: 3334.5\n",
      "lif layer 2 self.abs_max_v: 3531.0\n",
      "lif layer 2 self.abs_max_v: 3557.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  2.063572/  2.117678, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 81.1080%\n",
      "layer   3  Sparsity: 87.1669%\n",
      "total_backward_count 332860 real_backward_count 54043  16.236%\n",
      "lif layer 2 self.abs_max_v: 3604.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  2.066519/  2.124922, val:  66.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9063%\n",
      "layer   2  Sparsity: 81.0553%\n",
      "layer   3  Sparsity: 87.1459%\n",
      "total_backward_count 342650 real_backward_count 55204  16.111%\n",
      "lif layer 2 self.abs_max_v: 3840.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  2.075345/  2.131473, val:  68.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9461%\n",
      "layer   2  Sparsity: 80.9891%\n",
      "layer   3  Sparsity: 87.3442%\n",
      "total_backward_count 352440 real_backward_count 56315  15.979%\n",
      "fc layer 1 self.abs_max_out: 7234.0\n",
      "lif layer 2 self.abs_max_v: 3873.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  2.067672/  2.122075, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 80.9817%\n",
      "layer   3  Sparsity: 87.3161%\n",
      "total_backward_count 362230 real_backward_count 57396  15.845%\n",
      "fc layer 2 self.abs_max_out: 3033.0\n",
      "fc layer 1 self.abs_max_out: 7318.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  2.059043/  2.119626, val:  62.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 80.7991%\n",
      "layer   3  Sparsity: 87.2267%\n",
      "total_backward_count 372020 real_backward_count 58434  15.707%\n",
      "fc layer 1 self.abs_max_out: 7415.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  2.060380/  2.116956, val:  65.42%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 80.9949%\n",
      "layer   3  Sparsity: 87.3496%\n",
      "total_backward_count 381810 real_backward_count 59480  15.578%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  2.056132/  2.119691, val:  71.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 81.2007%\n",
      "layer   3  Sparsity: 87.5262%\n",
      "total_backward_count 391600 real_backward_count 60520  15.455%\n",
      "lif layer 1 self.abs_max_v: 10343.5\n",
      "lif layer 1 self.abs_max_v: 10377.0\n",
      "fc layer 2 self.abs_max_out: 3036.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  2.058347/  2.117999, val:  72.50%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9477%\n",
      "layer   2  Sparsity: 81.2330%\n",
      "layer   3  Sparsity: 87.1923%\n",
      "total_backward_count 401390 real_backward_count 61581  15.342%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  2.053051/  2.102276, val:  72.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9231%\n",
      "layer   2  Sparsity: 80.9908%\n",
      "layer   3  Sparsity: 87.2803%\n",
      "total_backward_count 411180 real_backward_count 62604  15.225%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  2.046248/  2.105608, val:  71.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9254%\n",
      "layer   2  Sparsity: 80.8382%\n",
      "layer   3  Sparsity: 87.4696%\n",
      "total_backward_count 420970 real_backward_count 63609  15.110%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  2.048083/  2.109542, val:  76.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9274%\n",
      "layer   2  Sparsity: 80.8688%\n",
      "layer   3  Sparsity: 87.2514%\n",
      "total_backward_count 430760 real_backward_count 64581  14.992%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  2.050949/  2.105489, val:  74.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9248%\n",
      "layer   2  Sparsity: 80.7432%\n",
      "layer   3  Sparsity: 87.5417%\n",
      "total_backward_count 440550 real_backward_count 65561  14.882%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  2.053023/  2.110267, val:  70.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 80.9400%\n",
      "layer   3  Sparsity: 87.6613%\n",
      "total_backward_count 450340 real_backward_count 66518  14.771%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  2.057009/  2.107106, val:  69.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 81.2290%\n",
      "layer   3  Sparsity: 87.6336%\n",
      "total_backward_count 460130 real_backward_count 67446  14.658%\n",
      "lif layer 1 self.abs_max_v: 10501.5\n",
      "lif layer 1 self.abs_max_v: 10549.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  2.060695/  2.122545, val:  69.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 80.9164%\n",
      "layer   3  Sparsity: 87.5468%\n",
      "total_backward_count 469920 real_backward_count 68367  14.549%\n",
      "lif layer 2 self.abs_max_v: 4041.5\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  2.056944/  2.114310, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9425%\n",
      "layer   2  Sparsity: 81.0009%\n",
      "layer   3  Sparsity: 87.7755%\n",
      "total_backward_count 479710 real_backward_count 69302  14.447%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  2.049880/  2.098635, val:  66.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9284%\n",
      "layer   2  Sparsity: 80.9174%\n",
      "layer   3  Sparsity: 87.8765%\n",
      "total_backward_count 489500 real_backward_count 70220  14.345%\n",
      "lif layer 1 self.abs_max_v: 10844.5\n",
      "lif layer 1 self.abs_max_v: 10887.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  2.044467/  2.103166, val:  71.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 80.7514%\n",
      "layer   3  Sparsity: 87.7926%\n",
      "total_backward_count 499290 real_backward_count 71129  14.246%\n",
      "lif layer 1 self.abs_max_v: 11252.5\n",
      "lif layer 1 self.abs_max_v: 11255.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  2.045878/  2.108398, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 80.6143%\n",
      "layer   3  Sparsity: 87.4130%\n",
      "total_backward_count 509080 real_backward_count 72018  14.147%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  2.040439/  2.099584, val:  67.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9299%\n",
      "layer   2  Sparsity: 80.8566%\n",
      "layer   3  Sparsity: 87.7058%\n",
      "total_backward_count 518870 real_backward_count 72937  14.057%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  2.033748/  2.090739, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9044%\n",
      "layer   2  Sparsity: 80.8701%\n",
      "layer   3  Sparsity: 87.8245%\n",
      "total_backward_count 528660 real_backward_count 73814  13.962%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  2.033707/  2.098745, val:  75.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9200%\n",
      "layer   2  Sparsity: 80.7242%\n",
      "layer   3  Sparsity: 87.8815%\n",
      "total_backward_count 538450 real_backward_count 74609  13.856%\n",
      "lif layer 2 self.abs_max_v: 4195.0\n",
      "lif layer 1 self.abs_max_v: 11508.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  2.035599/  2.105846, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9145%\n",
      "layer   2  Sparsity: 80.7415%\n",
      "layer   3  Sparsity: 87.6728%\n",
      "total_backward_count 548240 real_backward_count 75476  13.767%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  2.036957/  2.095980, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 80.6858%\n",
      "layer   3  Sparsity: 87.6990%\n",
      "total_backward_count 558030 real_backward_count 76277  13.669%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  2.030532/  2.102276, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9417%\n",
      "layer   2  Sparsity: 80.7500%\n",
      "layer   3  Sparsity: 87.6787%\n",
      "total_backward_count 567820 real_backward_count 76999  13.560%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  2.031801/  2.093951, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 80.8783%\n",
      "layer   3  Sparsity: 88.0685%\n",
      "total_backward_count 577610 real_backward_count 77741  13.459%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  2.035468/  2.097165, val:  68.75%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 81.0146%\n",
      "layer   3  Sparsity: 88.1230%\n",
      "total_backward_count 587400 real_backward_count 78490  13.362%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  2.030841/  2.089121, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9277%\n",
      "layer   2  Sparsity: 80.9530%\n",
      "layer   3  Sparsity: 87.6475%\n",
      "total_backward_count 597190 real_backward_count 79261  13.272%\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  2.031967/  2.088854, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9349%\n",
      "layer   2  Sparsity: 80.8306%\n",
      "layer   3  Sparsity: 87.6405%\n",
      "total_backward_count 606980 real_backward_count 80055  13.189%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  2.034288/  2.108099, val:  75.42%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8898%\n",
      "layer   2  Sparsity: 80.8564%\n",
      "layer   3  Sparsity: 87.8558%\n",
      "total_backward_count 616770 real_backward_count 80815  13.103%\n",
      "fc layer 1 self.abs_max_out: 7436.0\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  2.034038/  2.090935, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 80.9062%\n",
      "layer   3  Sparsity: 87.6435%\n",
      "total_backward_count 626560 real_backward_count 81533  13.013%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  2.029281/  2.096191, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9546%\n",
      "layer   2  Sparsity: 81.0067%\n",
      "layer   3  Sparsity: 87.7139%\n",
      "total_backward_count 636350 real_backward_count 82230  12.922%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  2.026848/  2.095673, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9114%\n",
      "layer   2  Sparsity: 81.0788%\n",
      "layer   3  Sparsity: 87.8583%\n",
      "total_backward_count 646140 real_backward_count 82906  12.831%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  2.026679/  2.091787, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 81.1203%\n",
      "layer   3  Sparsity: 87.7071%\n",
      "total_backward_count 655930 real_backward_count 83572  12.741%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  2.027584/  2.081823, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8998%\n",
      "layer   2  Sparsity: 80.8454%\n",
      "layer   3  Sparsity: 87.6559%\n",
      "total_backward_count 665720 real_backward_count 84280  12.660%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  2.025910/  2.086283, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9334%\n",
      "layer   2  Sparsity: 80.9541%\n",
      "layer   3  Sparsity: 87.8808%\n",
      "total_backward_count 675510 real_backward_count 84924  12.572%\n",
      "fc layer 1 self.abs_max_out: 7512.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  2.017925/  2.084781, val:  71.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9102%\n",
      "layer   2  Sparsity: 81.2044%\n",
      "layer   3  Sparsity: 87.6999%\n",
      "total_backward_count 685300 real_backward_count 85572  12.487%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  2.027271/  2.087653, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 81.0087%\n",
      "layer   3  Sparsity: 87.5242%\n",
      "total_backward_count 695090 real_backward_count 86249  12.408%\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "lif layer 2 self.abs_max_v: 4220.5\n",
      "fc layer 1 self.abs_max_out: 7543.0\n",
      "lif layer 2 self.abs_max_v: 4321.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  2.027527/  2.092546, val:  77.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9232%\n",
      "layer   2  Sparsity: 80.9317%\n",
      "layer   3  Sparsity: 87.8395%\n",
      "total_backward_count 704880 real_backward_count 86894  12.327%\n",
      "lif layer 2 self.abs_max_v: 4393.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  2.018808/  2.086350, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9288%\n",
      "layer   2  Sparsity: 81.0827%\n",
      "layer   3  Sparsity: 87.8009%\n",
      "total_backward_count 714670 real_backward_count 87561  12.252%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  2.016470/  2.078478, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8941%\n",
      "layer   2  Sparsity: 80.9890%\n",
      "layer   3  Sparsity: 87.8030%\n",
      "total_backward_count 724460 real_backward_count 88184  12.172%\n",
      "lif layer 2 self.abs_max_v: 4394.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  2.013060/  2.077708, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8986%\n",
      "layer   2  Sparsity: 80.7989%\n",
      "layer   3  Sparsity: 88.0191%\n",
      "total_backward_count 734250 real_backward_count 88823  12.097%\n",
      "fc layer 1 self.abs_max_out: 7545.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  2.011248/  2.082569, val:  83.75%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8861%\n",
      "layer   2  Sparsity: 81.0324%\n",
      "layer   3  Sparsity: 88.0629%\n",
      "total_backward_count 744040 real_backward_count 89414  12.017%\n",
      "fc layer 1 self.abs_max_out: 7555.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  2.016001/  2.084991, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9370%\n",
      "layer   2  Sparsity: 81.0642%\n",
      "layer   3  Sparsity: 88.0194%\n",
      "total_backward_count 753830 real_backward_count 89972  11.935%\n",
      "fc layer 1 self.abs_max_out: 7565.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  2.013293/  2.078802, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 81.0340%\n",
      "layer   3  Sparsity: 87.7448%\n",
      "total_backward_count 763620 real_backward_count 90593  11.864%\n",
      "fc layer 1 self.abs_max_out: 7573.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  2.013398/  2.079517, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9424%\n",
      "layer   2  Sparsity: 80.8629%\n",
      "layer   3  Sparsity: 87.8187%\n",
      "total_backward_count 773410 real_backward_count 91187  11.790%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  2.010498/  2.078437, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9021%\n",
      "layer   2  Sparsity: 81.0884%\n",
      "layer   3  Sparsity: 87.7231%\n",
      "total_backward_count 783200 real_backward_count 91774  11.718%\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  2.013104/  2.071463, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9190%\n",
      "layer   2  Sparsity: 80.8559%\n",
      "layer   3  Sparsity: 87.6352%\n",
      "total_backward_count 792990 real_backward_count 92336  11.644%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  2.006170/  2.080465, val:  71.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 81.0588%\n",
      "layer   3  Sparsity: 87.7271%\n",
      "total_backward_count 802780 real_backward_count 92903  11.573%\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  2.004102/  2.069201, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 87.9199%\n",
      "layer   2  Sparsity: 81.2624%\n",
      "layer   3  Sparsity: 87.8932%\n",
      "total_backward_count 812570 real_backward_count 93459  11.502%\n",
      "fc layer 1 self.abs_max_out: 7612.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  2.002390/  2.070047, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9220%\n",
      "layer   2  Sparsity: 81.0007%\n",
      "layer   3  Sparsity: 87.9696%\n",
      "total_backward_count 822360 real_backward_count 94012  11.432%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  2.007483/  2.074873, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 80.9986%\n",
      "layer   3  Sparsity: 87.6980%\n",
      "total_backward_count 832150 real_backward_count 94555  11.363%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  2.007004/  2.069403, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9449%\n",
      "layer   2  Sparsity: 80.9522%\n",
      "layer   3  Sparsity: 87.5713%\n",
      "total_backward_count 841940 real_backward_count 95150  11.301%\n",
      "fc layer 1 self.abs_max_out: 7632.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  2.005560/  2.072042, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8963%\n",
      "layer   2  Sparsity: 80.7959%\n",
      "layer   3  Sparsity: 87.7598%\n",
      "total_backward_count 851730 real_backward_count 95663  11.232%\n",
      "fc layer 1 self.abs_max_out: 7646.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.999837/  2.063750, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9393%\n",
      "layer   2  Sparsity: 80.7848%\n",
      "layer   3  Sparsity: 87.5346%\n",
      "total_backward_count 861520 real_backward_count 96218  11.168%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.995048/  2.058661, val:  84.58%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 80.7970%\n",
      "layer   3  Sparsity: 87.5464%\n",
      "total_backward_count 871310 real_backward_count 96713  11.100%\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.991651/  2.059515, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9218%\n",
      "layer   2  Sparsity: 81.1694%\n",
      "layer   3  Sparsity: 87.5483%\n",
      "total_backward_count 881100 real_backward_count 97241  11.036%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.990791/  2.058170, val:  83.33%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9225%\n",
      "layer   2  Sparsity: 81.1635%\n",
      "layer   3  Sparsity: 87.2877%\n",
      "total_backward_count 890890 real_backward_count 97752  10.972%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.980633/  2.046644, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8826%\n",
      "layer   2  Sparsity: 80.9420%\n",
      "layer   3  Sparsity: 87.3297%\n",
      "total_backward_count 900680 real_backward_count 98222  10.905%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.982791/  2.050786, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 80.9721%\n",
      "layer   3  Sparsity: 87.1862%\n",
      "total_backward_count 910470 real_backward_count 98707  10.841%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.987469/  2.054746, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 80.8363%\n",
      "layer   3  Sparsity: 87.4154%\n",
      "total_backward_count 920260 real_backward_count 99189  10.778%\n",
      "fc layer 1 self.abs_max_out: 7688.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.991211/  2.065989, val:  80.00%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 80.8628%\n",
      "layer   3  Sparsity: 87.4836%\n",
      "total_backward_count 930050 real_backward_count 99661  10.716%\n",
      "fc layer 1 self.abs_max_out: 7698.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.987176/  2.062258, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8824%\n",
      "layer   2  Sparsity: 80.9598%\n",
      "layer   3  Sparsity: 87.8856%\n",
      "total_backward_count 939840 real_backward_count 100097  10.650%\n",
      "lif layer 2 self.abs_max_v: 4432.0\n",
      "fc layer 1 self.abs_max_out: 7704.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.992618/  2.067913, val:  75.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8978%\n",
      "layer   2  Sparsity: 80.9515%\n",
      "layer   3  Sparsity: 88.2398%\n",
      "total_backward_count 949630 real_backward_count 100529  10.586%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.994416/  2.058712, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9068%\n",
      "layer   2  Sparsity: 81.0425%\n",
      "layer   3  Sparsity: 88.0308%\n",
      "total_backward_count 959420 real_backward_count 100967  10.524%\n",
      "fc layer 1 self.abs_max_out: 7713.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.989075/  2.066144, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8816%\n",
      "layer   2  Sparsity: 81.0853%\n",
      "layer   3  Sparsity: 87.9392%\n",
      "total_backward_count 969210 real_backward_count 101427  10.465%\n",
      "fc layer 3 self.abs_max_out: 459.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.992267/  2.058702, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 80.9586%\n",
      "layer   3  Sparsity: 87.9960%\n",
      "total_backward_count 979000 real_backward_count 101848  10.403%\n",
      "lif layer 2 self.abs_max_v: 4443.0\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.988776/  2.058637, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9453%\n",
      "layer   2  Sparsity: 80.8933%\n",
      "layer   3  Sparsity: 87.8058%\n",
      "total_backward_count 988790 real_backward_count 102290  10.345%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.985059/  2.054507, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9125%\n",
      "layer   2  Sparsity: 81.0127%\n",
      "layer   3  Sparsity: 87.8765%\n",
      "total_backward_count 998580 real_backward_count 102702  10.285%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.984365/  2.045864, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 80.9288%\n",
      "layer   3  Sparsity: 87.7481%\n",
      "total_backward_count 1008370 real_backward_count 103132  10.228%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.977685/  2.054848, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9054%\n",
      "layer   2  Sparsity: 81.0386%\n",
      "layer   3  Sparsity: 87.8957%\n",
      "total_backward_count 1018160 real_backward_count 103542  10.170%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.987616/  2.059801, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9300%\n",
      "layer   2  Sparsity: 81.0707%\n",
      "layer   3  Sparsity: 88.0807%\n",
      "total_backward_count 1027950 real_backward_count 103940  10.111%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.985892/  2.053649, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9371%\n",
      "layer   2  Sparsity: 81.2688%\n",
      "layer   3  Sparsity: 87.8380%\n",
      "total_backward_count 1037740 real_backward_count 104330  10.054%\n",
      "fc layer 1 self.abs_max_out: 7738.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.987328/  2.060082, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 81.1399%\n",
      "layer   3  Sparsity: 87.8026%\n",
      "total_backward_count 1047530 real_backward_count 104719   9.997%\n",
      "fc layer 1 self.abs_max_out: 7753.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.981933/  2.050555, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 81.1494%\n",
      "layer   3  Sparsity: 88.0037%\n",
      "total_backward_count 1057320 real_backward_count 105121   9.942%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.977605/  2.053354, val:  80.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8958%\n",
      "layer   2  Sparsity: 81.1582%\n",
      "layer   3  Sparsity: 88.1116%\n",
      "total_backward_count 1067110 real_backward_count 105538   9.890%\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.976370/  2.048034, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9002%\n",
      "layer   2  Sparsity: 81.0211%\n",
      "layer   3  Sparsity: 88.0826%\n",
      "total_backward_count 1076900 real_backward_count 105959   9.839%\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.975957/  2.046058, val:  85.42%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9488%\n",
      "layer   2  Sparsity: 81.0145%\n",
      "layer   3  Sparsity: 88.1529%\n",
      "total_backward_count 1086690 real_backward_count 106306   9.783%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.975115/  2.049568, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8934%\n",
      "layer   2  Sparsity: 80.9369%\n",
      "layer   3  Sparsity: 88.0128%\n",
      "total_backward_count 1096480 real_backward_count 106687   9.730%\n",
      "lif layer 1 self.abs_max_v: 11573.0\n",
      "fc layer 1 self.abs_max_out: 7760.0\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.973078/  2.053350, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9416%\n",
      "layer   2  Sparsity: 80.8094%\n",
      "layer   3  Sparsity: 88.1606%\n",
      "total_backward_count 1106270 real_backward_count 107080   9.679%\n",
      "fc layer 1 self.abs_max_out: 7765.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.974089/  2.051805, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9226%\n",
      "layer   2  Sparsity: 80.9141%\n",
      "layer   3  Sparsity: 88.0354%\n",
      "total_backward_count 1116060 real_backward_count 107464   9.629%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.979094/  2.054862, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 81.1258%\n",
      "layer   3  Sparsity: 87.9785%\n",
      "total_backward_count 1125850 real_backward_count 107823   9.577%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.984852/  2.056496, val:  79.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8946%\n",
      "layer   2  Sparsity: 80.9862%\n",
      "layer   3  Sparsity: 87.8961%\n",
      "total_backward_count 1135640 real_backward_count 108203   9.528%\n",
      "fc layer 1 self.abs_max_out: 7779.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.982388/  2.056272, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9046%\n",
      "layer   2  Sparsity: 80.9143%\n",
      "layer   3  Sparsity: 87.9918%\n",
      "total_backward_count 1145430 real_backward_count 108580   9.479%\n",
      "fc layer 1 self.abs_max_out: 7782.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.978543/  2.045340, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 80.9285%\n",
      "layer   3  Sparsity: 88.0593%\n",
      "total_backward_count 1155220 real_backward_count 108943   9.430%\n",
      "fc layer 1 self.abs_max_out: 7788.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.975560/  2.047063, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9164%\n",
      "layer   2  Sparsity: 80.9591%\n",
      "layer   3  Sparsity: 87.9472%\n",
      "total_backward_count 1165010 real_backward_count 109266   9.379%\n",
      "fc layer 1 self.abs_max_out: 7791.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.976097/  2.045055, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9637%\n",
      "layer   2  Sparsity: 80.9871%\n",
      "layer   3  Sparsity: 87.9475%\n",
      "total_backward_count 1174800 real_backward_count 109619   9.331%\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.975513/  2.053994, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 81.2292%\n",
      "layer   3  Sparsity: 88.1205%\n",
      "total_backward_count 1184590 real_backward_count 109969   9.283%\n",
      "fc layer 1 self.abs_max_out: 7807.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.975288/  2.051889, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9270%\n",
      "layer   2  Sparsity: 81.0883%\n",
      "layer   3  Sparsity: 88.1253%\n",
      "total_backward_count 1194380 real_backward_count 110353   9.239%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.973335/  2.040662, val:  81.67%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9389%\n",
      "layer   2  Sparsity: 81.0119%\n",
      "layer   3  Sparsity: 88.0063%\n",
      "total_backward_count 1204170 real_backward_count 110703   9.193%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.967017/  2.042850, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9110%\n",
      "layer   2  Sparsity: 80.9566%\n",
      "layer   3  Sparsity: 87.8985%\n",
      "total_backward_count 1213960 real_backward_count 111024   9.146%\n",
      "fc layer 1 self.abs_max_out: 7810.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.966598/  2.044483, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 81.1026%\n",
      "layer   3  Sparsity: 87.7311%\n",
      "total_backward_count 1223750 real_backward_count 111331   9.098%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.966514/  2.043738, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9451%\n",
      "layer   2  Sparsity: 81.1414%\n",
      "layer   3  Sparsity: 87.8721%\n",
      "total_backward_count 1233540 real_backward_count 111676   9.053%\n",
      "fc layer 1 self.abs_max_out: 7818.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.965779/  2.043002, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9661%\n",
      "layer   2  Sparsity: 81.1102%\n",
      "layer   3  Sparsity: 88.0132%\n",
      "total_backward_count 1243330 real_backward_count 111987   9.007%\n",
      "fc layer 1 self.abs_max_out: 7821.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.964942/  2.037740, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9169%\n",
      "layer   2  Sparsity: 81.1021%\n",
      "layer   3  Sparsity: 87.8542%\n",
      "total_backward_count 1253120 real_backward_count 112273   8.959%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.966966/  2.041558, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9431%\n",
      "layer   2  Sparsity: 81.0088%\n",
      "layer   3  Sparsity: 87.7856%\n",
      "total_backward_count 1262910 real_backward_count 112606   8.916%\n",
      "fc layer 1 self.abs_max_out: 7825.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.969833/  2.048680, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9023%\n",
      "layer   2  Sparsity: 81.1856%\n",
      "layer   3  Sparsity: 87.8362%\n",
      "total_backward_count 1272700 real_backward_count 112895   8.871%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.962364/  2.033800, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9455%\n",
      "layer   2  Sparsity: 80.9361%\n",
      "layer   3  Sparsity: 87.6750%\n",
      "total_backward_count 1282490 real_backward_count 113200   8.827%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.958949/  2.038522, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 80.8279%\n",
      "layer   3  Sparsity: 87.7548%\n",
      "total_backward_count 1292280 real_backward_count 113493   8.782%\n",
      "fc layer 1 self.abs_max_out: 7830.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.965441/  2.043460, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9112%\n",
      "layer   2  Sparsity: 81.0278%\n",
      "layer   3  Sparsity: 87.9084%\n",
      "total_backward_count 1302070 real_backward_count 113810   8.741%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.968940/  2.044023, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 80.8517%\n",
      "layer   3  Sparsity: 87.8101%\n",
      "total_backward_count 1311860 real_backward_count 114120   8.699%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.968231/  2.038914, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9441%\n",
      "layer   2  Sparsity: 81.0077%\n",
      "layer   3  Sparsity: 87.5425%\n",
      "total_backward_count 1321650 real_backward_count 114379   8.654%\n",
      "fc layer 1 self.abs_max_out: 7831.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.964786/  2.035667, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9201%\n",
      "layer   2  Sparsity: 80.9940%\n",
      "layer   3  Sparsity: 87.4571%\n",
      "total_backward_count 1331440 real_backward_count 114679   8.613%\n",
      "fc layer 1 self.abs_max_out: 7879.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.960803/  2.040342, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9308%\n",
      "layer   2  Sparsity: 81.0094%\n",
      "layer   3  Sparsity: 87.7764%\n",
      "total_backward_count 1341230 real_backward_count 114941   8.570%\n",
      "fc layer 1 self.abs_max_out: 7881.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.961205/  2.040089, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 80.7923%\n",
      "layer   3  Sparsity: 88.1005%\n",
      "total_backward_count 1351020 real_backward_count 115209   8.528%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.959896/  2.043650, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 80.9659%\n",
      "layer   3  Sparsity: 88.0832%\n",
      "total_backward_count 1360810 real_backward_count 115485   8.486%\n",
      "fc layer 1 self.abs_max_out: 7884.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.969729/  2.045178, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9294%\n",
      "layer   2  Sparsity: 80.9594%\n",
      "layer   3  Sparsity: 88.4277%\n",
      "total_backward_count 1370600 real_backward_count 115784   8.448%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.963591/  2.041117, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9078%\n",
      "layer   2  Sparsity: 80.8832%\n",
      "layer   3  Sparsity: 88.1940%\n",
      "total_backward_count 1380390 real_backward_count 116046   8.407%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.966183/  2.040342, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9458%\n",
      "layer   2  Sparsity: 80.8891%\n",
      "layer   3  Sparsity: 88.4455%\n",
      "total_backward_count 1390180 real_backward_count 116299   8.366%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.968061/  2.045148, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9376%\n",
      "layer   2  Sparsity: 80.9929%\n",
      "layer   3  Sparsity: 88.3042%\n",
      "total_backward_count 1399970 real_backward_count 116580   8.327%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.966305/  2.042831, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8808%\n",
      "layer   2  Sparsity: 81.0777%\n",
      "layer   3  Sparsity: 88.3074%\n",
      "total_backward_count 1409760 real_backward_count 116849   8.289%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.961206/  2.038428, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9289%\n",
      "layer   2  Sparsity: 81.0459%\n",
      "layer   3  Sparsity: 88.1548%\n",
      "total_backward_count 1419550 real_backward_count 117132   8.251%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.959663/  2.040320, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 80.9808%\n",
      "layer   3  Sparsity: 88.2009%\n",
      "total_backward_count 1429340 real_backward_count 117454   8.217%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.954983/  2.026997, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 80.9156%\n",
      "layer   3  Sparsity: 87.8470%\n",
      "total_backward_count 1439130 real_backward_count 117769   8.183%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.947825/  2.027691, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9180%\n",
      "layer   2  Sparsity: 81.1361%\n",
      "layer   3  Sparsity: 88.0066%\n",
      "total_backward_count 1448920 real_backward_count 118035   8.146%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.951077/  2.035912, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9117%\n",
      "layer   2  Sparsity: 81.1167%\n",
      "layer   3  Sparsity: 88.1594%\n",
      "total_backward_count 1458710 real_backward_count 118293   8.109%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.950265/  2.022545, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 81.1718%\n",
      "layer   3  Sparsity: 88.0244%\n",
      "total_backward_count 1468500 real_backward_count 118550   8.073%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.944533/  2.027950, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9235%\n",
      "layer   2  Sparsity: 81.0459%\n",
      "layer   3  Sparsity: 87.8350%\n",
      "total_backward_count 1478290 real_backward_count 118785   8.035%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.949829/  2.033130, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 81.1862%\n",
      "layer   3  Sparsity: 87.8153%\n",
      "total_backward_count 1488080 real_backward_count 119031   7.999%\n",
      "fc layer 1 self.abs_max_out: 7891.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.946400/  2.027035, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 81.0621%\n",
      "layer   3  Sparsity: 87.8270%\n",
      "total_backward_count 1497870 real_backward_count 119286   7.964%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.948782/  2.030002, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9245%\n",
      "layer   2  Sparsity: 80.9440%\n",
      "layer   3  Sparsity: 87.9375%\n",
      "total_backward_count 1507660 real_backward_count 119512   7.927%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.951998/  2.037251, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9502%\n",
      "layer   2  Sparsity: 80.8719%\n",
      "layer   3  Sparsity: 88.1393%\n",
      "total_backward_count 1517450 real_backward_count 119724   7.890%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.952811/  2.036732, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8933%\n",
      "layer   2  Sparsity: 81.0002%\n",
      "layer   3  Sparsity: 88.0671%\n",
      "total_backward_count 1527240 real_backward_count 119953   7.854%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.950120/  2.032453, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 80.9906%\n",
      "layer   3  Sparsity: 87.8648%\n",
      "total_backward_count 1537030 real_backward_count 120211   7.821%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.949703/  2.031513, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8926%\n",
      "layer   2  Sparsity: 80.9820%\n",
      "layer   3  Sparsity: 87.9838%\n",
      "total_backward_count 1546820 real_backward_count 120452   7.787%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.953702/  2.029832, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 80.9946%\n",
      "layer   3  Sparsity: 88.1332%\n",
      "total_backward_count 1556610 real_backward_count 120713   7.755%\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "lif layer 2 self.abs_max_v: 4555.5\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.950817/  2.028430, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9141%\n",
      "layer   2  Sparsity: 80.9999%\n",
      "layer   3  Sparsity: 88.1369%\n",
      "total_backward_count 1566400 real_backward_count 120928   7.720%\n",
      "lif layer 2 self.abs_max_v: 4608.5\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.944101/  2.031431, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 81.0459%\n",
      "layer   3  Sparsity: 88.0791%\n",
      "total_backward_count 1576190 real_backward_count 121122   7.684%\n",
      "fc layer 1 self.abs_max_out: 7895.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.941595/  2.021846, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8972%\n",
      "layer   2  Sparsity: 81.1572%\n",
      "layer   3  Sparsity: 87.9044%\n",
      "total_backward_count 1585980 real_backward_count 121326   7.650%\n",
      "fc layer 1 self.abs_max_out: 7899.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.942616/  2.025736, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9236%\n",
      "layer   2  Sparsity: 81.0984%\n",
      "layer   3  Sparsity: 88.0333%\n",
      "total_backward_count 1595770 real_backward_count 121524   7.615%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.944659/  2.030563, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9259%\n",
      "layer   2  Sparsity: 81.1272%\n",
      "layer   3  Sparsity: 87.9571%\n",
      "total_backward_count 1605560 real_backward_count 121761   7.584%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.948039/  2.030548, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8838%\n",
      "layer   2  Sparsity: 81.0493%\n",
      "layer   3  Sparsity: 88.1700%\n",
      "total_backward_count 1615350 real_backward_count 121981   7.551%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.944598/  2.032916, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8980%\n",
      "layer   2  Sparsity: 81.0228%\n",
      "layer   3  Sparsity: 88.0989%\n",
      "total_backward_count 1625140 real_backward_count 122203   7.520%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.944945/  2.034253, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 81.0220%\n",
      "layer   3  Sparsity: 87.9715%\n",
      "total_backward_count 1634930 real_backward_count 122437   7.489%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.947942/  2.029642, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9183%\n",
      "layer   2  Sparsity: 80.8858%\n",
      "layer   3  Sparsity: 87.9460%\n",
      "total_backward_count 1644720 real_backward_count 122678   7.459%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.946448/  2.028948, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9592%\n",
      "layer   2  Sparsity: 81.0076%\n",
      "layer   3  Sparsity: 87.9119%\n",
      "total_backward_count 1654510 real_backward_count 122918   7.429%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.949016/  2.034568, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9400%\n",
      "layer   2  Sparsity: 81.1808%\n",
      "layer   3  Sparsity: 88.0309%\n",
      "total_backward_count 1664300 real_backward_count 123139   7.399%\n",
      "lif layer 1 self.abs_max_v: 11681.5\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.946275/  2.033659, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9351%\n",
      "layer   2  Sparsity: 81.0769%\n",
      "layer   3  Sparsity: 88.1232%\n",
      "total_backward_count 1674090 real_backward_count 123341   7.368%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.939157/  2.024137, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9583%\n",
      "layer   2  Sparsity: 81.0703%\n",
      "layer   3  Sparsity: 88.0597%\n",
      "total_backward_count 1683880 real_backward_count 123554   7.337%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.937164/  2.024487, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9464%\n",
      "layer   2  Sparsity: 80.9784%\n",
      "layer   3  Sparsity: 88.1219%\n",
      "total_backward_count 1693670 real_backward_count 123765   7.308%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.935280/  2.022346, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 81.0162%\n",
      "layer   3  Sparsity: 88.2386%\n",
      "total_backward_count 1703460 real_backward_count 123966   7.277%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.931960/  2.017711, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9328%\n",
      "layer   2  Sparsity: 80.8416%\n",
      "layer   3  Sparsity: 88.1271%\n",
      "total_backward_count 1713250 real_backward_count 124148   7.246%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.933527/  2.023375, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8885%\n",
      "layer   2  Sparsity: 80.8439%\n",
      "layer   3  Sparsity: 88.1568%\n",
      "total_backward_count 1723040 real_backward_count 124323   7.215%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.931522/  2.017340, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9290%\n",
      "layer   2  Sparsity: 81.0767%\n",
      "layer   3  Sparsity: 88.2298%\n",
      "total_backward_count 1732830 real_backward_count 124517   7.186%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.929180/  2.017685, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9033%\n",
      "layer   2  Sparsity: 81.0952%\n",
      "layer   3  Sparsity: 88.2104%\n",
      "total_backward_count 1742620 real_backward_count 124723   7.157%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.930442/  2.024034, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 81.0857%\n",
      "layer   3  Sparsity: 88.3450%\n",
      "total_backward_count 1752410 real_backward_count 124901   7.127%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.938995/  2.026462, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9329%\n",
      "layer   2  Sparsity: 80.9853%\n",
      "layer   3  Sparsity: 88.3405%\n",
      "total_backward_count 1762200 real_backward_count 125100   7.099%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.942204/  2.027754, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9406%\n",
      "layer   2  Sparsity: 81.2254%\n",
      "layer   3  Sparsity: 88.1404%\n",
      "total_backward_count 1771990 real_backward_count 125307   7.072%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.943327/  2.030518, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9135%\n",
      "layer   2  Sparsity: 81.1429%\n",
      "layer   3  Sparsity: 88.1015%\n",
      "total_backward_count 1781780 real_backward_count 125545   7.046%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.943132/  2.026484, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9184%\n",
      "layer   2  Sparsity: 81.1098%\n",
      "layer   3  Sparsity: 88.1791%\n",
      "total_backward_count 1791570 real_backward_count 125728   7.018%\n",
      "fc layer 1 self.abs_max_out: 7921.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.941957/  2.030364, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 81.0693%\n",
      "layer   3  Sparsity: 87.9161%\n",
      "total_backward_count 1801360 real_backward_count 125897   6.989%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.934465/  2.025475, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 81.0338%\n",
      "layer   3  Sparsity: 88.0419%\n",
      "total_backward_count 1811150 real_backward_count 126063   6.960%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.931291/  2.017268, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 81.0693%\n",
      "layer   3  Sparsity: 87.8106%\n",
      "total_backward_count 1820940 real_backward_count 126240   6.933%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.925370/  2.012406, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9224%\n",
      "layer   2  Sparsity: 81.1197%\n",
      "layer   3  Sparsity: 87.5846%\n",
      "total_backward_count 1830730 real_backward_count 126449   6.907%\n",
      "fc layer 1 self.abs_max_out: 7927.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.924252/  2.015416, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 81.0638%\n",
      "layer   3  Sparsity: 87.5651%\n",
      "total_backward_count 1840520 real_backward_count 126650   6.881%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.924984/  2.019205, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9251%\n",
      "layer   2  Sparsity: 80.9630%\n",
      "layer   3  Sparsity: 87.7151%\n",
      "total_backward_count 1850310 real_backward_count 126812   6.854%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.927270/  2.018840, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9298%\n",
      "layer   2  Sparsity: 81.0221%\n",
      "layer   3  Sparsity: 87.7557%\n",
      "total_backward_count 1860100 real_backward_count 126976   6.826%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.930532/  2.024755, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9260%\n",
      "layer   2  Sparsity: 81.0019%\n",
      "layer   3  Sparsity: 87.8715%\n",
      "total_backward_count 1869890 real_backward_count 127138   6.799%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.932357/  2.021094, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 81.0064%\n",
      "layer   3  Sparsity: 87.8832%\n",
      "total_backward_count 1879680 real_backward_count 127313   6.773%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.928191/  2.022719, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 81.1277%\n",
      "layer   3  Sparsity: 87.9803%\n",
      "total_backward_count 1889470 real_backward_count 127469   6.746%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.928548/  2.022168, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9115%\n",
      "layer   2  Sparsity: 81.2046%\n",
      "layer   3  Sparsity: 87.9674%\n",
      "total_backward_count 1899260 real_backward_count 127639   6.720%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.926648/  2.019446, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9172%\n",
      "layer   2  Sparsity: 81.2386%\n",
      "layer   3  Sparsity: 87.9878%\n",
      "total_backward_count 1909050 real_backward_count 127800   6.694%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.926857/  2.019282, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9042%\n",
      "layer   2  Sparsity: 81.0263%\n",
      "layer   3  Sparsity: 87.7664%\n",
      "total_backward_count 1918840 real_backward_count 127965   6.669%\n",
      "fc layer 1 self.abs_max_out: 7935.0\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.923207/  2.018449, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9185%\n",
      "layer   2  Sparsity: 81.1023%\n",
      "layer   3  Sparsity: 87.9134%\n",
      "total_backward_count 1928630 real_backward_count 128136   6.644%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.927647/  2.017127, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9420%\n",
      "layer   2  Sparsity: 81.1252%\n",
      "layer   3  Sparsity: 87.8294%\n",
      "total_backward_count 1938420 real_backward_count 128302   6.619%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.921794/  2.015384, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9119%\n",
      "layer   2  Sparsity: 81.3275%\n",
      "layer   3  Sparsity: 87.6561%\n",
      "total_backward_count 1948210 real_backward_count 128470   6.594%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.921441/  2.008714, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9202%\n",
      "layer   2  Sparsity: 81.1652%\n",
      "layer   3  Sparsity: 87.7450%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dff9f3a8307482e91a7a2e4e105cd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.92144</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>2.00871</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-sweep-115</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jecm2yr9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jecm2yr9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_193751-jecm2yr9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ut70wscx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_235433-ut70wscx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ut70wscx' target=\"_blank\">fiery-sweep-120</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ut70wscx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ut70wscx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251116_235443_286', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 277.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "lif layer 2 self.abs_max_v: 739.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 1 self.abs_max_out: 363.0\n",
      "lif layer 1 self.abs_max_v: 396.5\n",
      "fc layer 2 self.abs_max_out: 1125.0\n",
      "lif layer 2 self.abs_max_v: 1315.0\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 1 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 547.0\n",
      "fc layer 1 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 1348.5\n",
      "fc layer 1 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 547.5\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "lif layer 1 self.abs_max_v: 591.5\n",
      "lif layer 2 self.abs_max_v: 1484.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 565.0\n",
      "lif layer 1 self.abs_max_v: 724.0\n",
      "lif layer 2 self.abs_max_v: 1503.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 1 self.abs_max_out: 596.0\n",
      "lif layer 1 self.abs_max_v: 780.0\n",
      "lif layer 2 self.abs_max_v: 1576.5\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "fc layer 1 self.abs_max_out: 675.0\n",
      "lif layer 1 self.abs_max_v: 795.0\n",
      "lif layer 1 self.abs_max_v: 886.5\n",
      "lif layer 1 self.abs_max_v: 1029.0\n",
      "fc layer 1 self.abs_max_out: 729.0\n",
      "lif layer 2 self.abs_max_v: 1877.0\n",
      "fc layer 1 self.abs_max_out: 744.0\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "fc layer 1 self.abs_max_out: 884.0\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1063.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "lif layer 1 self.abs_max_v: 1271.0\n",
      "fc layer 3 self.abs_max_out: 788.0\n",
      "fc layer 1 self.abs_max_out: 1430.0\n",
      "lif layer 1 self.abs_max_v: 1924.0\n",
      "fc layer 2 self.abs_max_out: 1251.0\n",
      "fc layer 2 self.abs_max_out: 1303.0\n",
      "lif layer 2 self.abs_max_v: 2092.0\n",
      "fc layer 2 self.abs_max_out: 1322.0\n",
      "lif layer 2 self.abs_max_v: 2095.0\n",
      "lif layer 2 self.abs_max_v: 2146.5\n",
      "fc layer 2 self.abs_max_out: 1337.0\n",
      "fc layer 2 self.abs_max_out: 1501.0\n",
      "fc layer 1 self.abs_max_out: 1436.0\n",
      "lif layer 2 self.abs_max_v: 2177.0\n",
      "fc layer 1 self.abs_max_out: 1789.0\n",
      "lif layer 2 self.abs_max_v: 2293.0\n",
      "lif layer 2 self.abs_max_v: 2347.5\n",
      "lif layer 2 self.abs_max_v: 2554.0\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "fc layer 3 self.abs_max_out: 838.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "lif layer 2 self.abs_max_v: 2602.5\n",
      "fc layer 2 self.abs_max_out: 1800.0\n",
      "lif layer 2 self.abs_max_v: 2652.5\n",
      "lif layer 2 self.abs_max_v: 2687.5\n",
      "lif layer 2 self.abs_max_v: 2823.0\n",
      "lif layer 2 self.abs_max_v: 2834.5\n",
      "lif layer 2 self.abs_max_v: 2989.5\n",
      "lif layer 1 self.abs_max_v: 2243.5\n",
      "lif layer 1 self.abs_max_v: 2605.0\n",
      "fc layer 1 self.abs_max_out: 2039.0\n",
      "lif layer 2 self.abs_max_v: 3103.0\n",
      "fc layer 2 self.abs_max_out: 1932.0\n",
      "lif layer 2 self.abs_max_v: 3140.0\n",
      "lif layer 2 self.abs_max_v: 3410.0\n",
      "fc layer 2 self.abs_max_out: 2107.0\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "lif layer 1 self.abs_max_v: 2732.0\n",
      "lif layer 1 self.abs_max_v: 3209.0\n",
      "lif layer 1 self.abs_max_v: 3354.5\n",
      "fc layer 1 self.abs_max_out: 2200.0\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "fc layer 3 self.abs_max_out: 942.0\n",
      "lif layer 2 self.abs_max_v: 3516.5\n",
      "fc layer 1 self.abs_max_out: 2249.0\n",
      "fc layer 1 self.abs_max_out: 2333.0\n",
      "lif layer 1 self.abs_max_v: 3891.0\n",
      "lif layer 2 self.abs_max_v: 3607.5\n",
      "fc layer 1 self.abs_max_out: 2351.0\n",
      "lif layer 1 self.abs_max_v: 3901.5\n",
      "lif layer 2 self.abs_max_v: 3650.0\n",
      "lif layer 2 self.abs_max_v: 3692.0\n",
      "fc layer 2 self.abs_max_out: 2214.0\n",
      "lif layer 2 self.abs_max_v: 3874.5\n",
      "lif layer 2 self.abs_max_v: 3906.5\n",
      "lif layer 2 self.abs_max_v: 4029.0\n",
      "lif layer 2 self.abs_max_v: 4113.5\n",
      "fc layer 3 self.abs_max_out: 987.0\n",
      "fc layer 3 self.abs_max_out: 991.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "fc layer 3 self.abs_max_out: 1006.0\n",
      "fc layer 3 self.abs_max_out: 1063.0\n",
      "fc layer 3 self.abs_max_out: 1065.0\n",
      "fc layer 3 self.abs_max_out: 1093.0\n",
      "fc layer 3 self.abs_max_out: 1111.0\n",
      "fc layer 2 self.abs_max_out: 2320.0\n",
      "fc layer 3 self.abs_max_out: 1132.0\n",
      "fc layer 2 self.abs_max_out: 2332.0\n",
      "fc layer 1 self.abs_max_out: 2399.0\n",
      "fc layer 3 self.abs_max_out: 1180.0\n",
      "fc layer 1 self.abs_max_out: 2518.0\n",
      "fc layer 1 self.abs_max_out: 2743.0\n",
      "fc layer 3 self.abs_max_out: 1285.0\n",
      "lif layer 2 self.abs_max_v: 4121.0\n",
      "lif layer 1 self.abs_max_v: 3991.5\n",
      "lif layer 2 self.abs_max_v: 4230.0\n",
      "lif layer 2 self.abs_max_v: 4289.0\n",
      "lif layer 1 self.abs_max_v: 4210.0\n",
      "lif layer 2 self.abs_max_v: 4318.5\n",
      "fc layer 1 self.abs_max_out: 2919.0\n",
      "lif layer 1 self.abs_max_v: 4745.0\n",
      "fc layer 1 self.abs_max_out: 2962.0\n",
      "lif layer 1 self.abs_max_v: 5334.5\n",
      "fc layer 1 self.abs_max_out: 3160.0\n",
      "fc layer 2 self.abs_max_out: 2409.0\n",
      "lif layer 2 self.abs_max_v: 4335.5\n",
      "lif layer 2 self.abs_max_v: 4440.0\n",
      "lif layer 2 self.abs_max_v: 4590.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2517.0\n",
      "fc layer 1 self.abs_max_out: 3235.0\n",
      "fc layer 1 self.abs_max_out: 3641.0\n",
      "lif layer 1 self.abs_max_v: 6039.0\n",
      "fc layer 2 self.abs_max_out: 2585.0\n",
      "fc layer 3 self.abs_max_out: 1294.0\n",
      "lif layer 1 self.abs_max_v: 6195.5\n",
      "lif layer 1 self.abs_max_v: 6548.0\n",
      "fc layer 3 self.abs_max_out: 1348.0\n",
      "fc layer 3 self.abs_max_out: 1350.0\n",
      "fc layer 3 self.abs_max_out: 1470.0\n",
      "fc layer 1 self.abs_max_out: 3645.0\n",
      "lif layer 1 self.abs_max_v: 6562.0\n",
      "fc layer 1 self.abs_max_out: 3937.0\n",
      "lif layer 1 self.abs_max_v: 6663.5\n",
      "fc layer 1 self.abs_max_out: 4138.0\n",
      "lif layer 1 self.abs_max_v: 6782.5\n",
      "lif layer 1 self.abs_max_v: 6996.0\n",
      "fc layer 2 self.abs_max_out: 2655.0\n",
      "fc layer 2 self.abs_max_out: 2667.0\n",
      "fc layer 3 self.abs_max_out: 1533.0\n",
      "fc layer 3 self.abs_max_out: 1541.0\n",
      "fc layer 3 self.abs_max_out: 1581.0\n",
      "fc layer 1 self.abs_max_out: 4471.0\n",
      "fc layer 3 self.abs_max_out: 1717.0\n",
      "fc layer 3 self.abs_max_out: 1722.0\n",
      "fc layer 3 self.abs_max_out: 1832.0\n",
      "lif layer 1 self.abs_max_v: 7118.5\n",
      "lif layer 1 self.abs_max_v: 7677.5\n",
      "fc layer 1 self.abs_max_out: 4822.0\n",
      "lif layer 1 self.abs_max_v: 8661.0\n",
      "lif layer 2 self.abs_max_v: 4598.0\n",
      "fc layer 1 self.abs_max_out: 4842.0\n",
      "fc layer 1 self.abs_max_out: 5102.0\n",
      "lif layer 1 self.abs_max_v: 8697.0\n",
      "lif layer 1 self.abs_max_v: 9328.5\n",
      "lif layer 1 self.abs_max_v: 9688.0\n",
      "fc layer 1 self.abs_max_out: 5360.0\n",
      "lif layer 1 self.abs_max_v: 10204.0\n",
      "lif layer 2 self.abs_max_v: 4659.0\n",
      "lif layer 2 self.abs_max_v: 4719.0\n",
      "lif layer 2 self.abs_max_v: 4729.5\n",
      "lif layer 2 self.abs_max_v: 4760.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.368881/  1.800679, val:  38.33%, val_best:  38.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7141%\n",
      "layer   2  Sparsity: 68.4832%\n",
      "layer   3  Sparsity: 59.3382%\n",
      "total_backward_count 9790 real_backward_count 1293  13.207%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2726.0\n",
      "lif layer 2 self.abs_max_v: 4851.0\n",
      "lif layer 2 self.abs_max_v: 4893.0\n",
      "lif layer 2 self.abs_max_v: 4982.0\n",
      "fc layer 2 self.abs_max_out: 2733.0\n",
      "lif layer 2 self.abs_max_v: 4985.0\n",
      "fc layer 2 self.abs_max_out: 2789.0\n",
      "fc layer 2 self.abs_max_out: 2796.0\n",
      "fc layer 2 self.abs_max_out: 2818.0\n",
      "fc layer 2 self.abs_max_out: 2819.0\n",
      "fc layer 2 self.abs_max_out: 2841.0\n",
      "fc layer 2 self.abs_max_out: 2923.0\n",
      "lif layer 2 self.abs_max_v: 5208.0\n",
      "fc layer 2 self.abs_max_out: 3057.0\n",
      "fc layer 1 self.abs_max_out: 5467.0\n",
      "lif layer 2 self.abs_max_v: 5350.5\n",
      "lif layer 2 self.abs_max_v: 5374.0\n",
      "lif layer 2 self.abs_max_v: 5451.5\n",
      "lif layer 2 self.abs_max_v: 5563.5\n",
      "lif layer 2 self.abs_max_v: 5597.5\n",
      "lif layer 2 self.abs_max_v: 5611.0\n",
      "lif layer 2 self.abs_max_v: 5692.0\n",
      "fc layer 2 self.abs_max_out: 3121.0\n",
      "lif layer 2 self.abs_max_v: 5865.5\n",
      "fc layer 1 self.abs_max_out: 5561.0\n",
      "fc layer 1 self.abs_max_out: 5624.0\n",
      "lif layer 1 self.abs_max_v: 10237.0\n",
      "lif layer 1 self.abs_max_v: 10671.5\n",
      "lif layer 2 self.abs_max_v: 5874.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.196876/  1.803591, val:  38.33%, val_best:  38.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7183%\n",
      "layer   2  Sparsity: 71.3386%\n",
      "layer   3  Sparsity: 60.5295%\n",
      "total_backward_count 19580 real_backward_count 2468  12.605%\n",
      "fc layer 2 self.abs_max_out: 3173.0\n",
      "fc layer 2 self.abs_max_out: 3228.0\n",
      "fc layer 2 self.abs_max_out: 3230.0\n",
      "fc layer 2 self.abs_max_out: 3332.0\n",
      "lif layer 2 self.abs_max_v: 6233.5\n",
      "lif layer 2 self.abs_max_v: 6335.0\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "lif layer 2 self.abs_max_v: 6617.5\n",
      "fc layer 3 self.abs_max_out: 1875.0\n",
      "fc layer 2 self.abs_max_out: 3625.0\n",
      "fc layer 3 self.abs_max_out: 1889.0\n",
      "fc layer 3 self.abs_max_out: 1955.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.171893/  1.770104, val:  36.67%, val_best:  38.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7199%\n",
      "layer   2  Sparsity: 73.0995%\n",
      "layer   3  Sparsity: 63.1253%\n",
      "total_backward_count 29370 real_backward_count 3691  12.567%\n",
      "fc layer 3 self.abs_max_out: 1986.0\n",
      "fc layer 3 self.abs_max_out: 2068.0\n",
      "fc layer 1 self.abs_max_out: 5877.0\n",
      "lif layer 2 self.abs_max_v: 6673.0\n",
      "fc layer 2 self.abs_max_out: 3774.0\n",
      "lif layer 2 self.abs_max_v: 6825.5\n",
      "fc layer 1 self.abs_max_out: 6497.0\n",
      "lif layer 1 self.abs_max_v: 10691.5\n",
      "fc layer 1 self.abs_max_out: 6607.0\n",
      "fc layer 1 self.abs_max_out: 6883.0\n",
      "lif layer 1 self.abs_max_v: 11591.5\n",
      "fc layer 1 self.abs_max_out: 6918.0\n",
      "lif layer 1 self.abs_max_v: 12714.0\n",
      "lif layer 1 self.abs_max_v: 12726.0\n",
      "lif layer 1 self.abs_max_v: 12861.0\n",
      "lif layer 2 self.abs_max_v: 6869.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.117567/  1.721898, val:  42.50%, val_best:  42.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7165%\n",
      "layer   2  Sparsity: 72.7160%\n",
      "layer   3  Sparsity: 62.2017%\n",
      "total_backward_count 39160 real_backward_count 4868  12.431%\n",
      "lif layer 2 self.abs_max_v: 6953.0\n",
      "fc layer 2 self.abs_max_out: 3812.0\n",
      "lif layer 2 self.abs_max_v: 7288.5\n",
      "fc layer 3 self.abs_max_out: 2207.0\n",
      "fc layer 1 self.abs_max_out: 7066.0\n",
      "lif layer 1 self.abs_max_v: 13100.5\n",
      "fc layer 1 self.abs_max_out: 7078.0\n",
      "lif layer 1 self.abs_max_v: 13628.5\n",
      "lif layer 1 self.abs_max_v: 13665.5\n",
      "lif layer 1 self.abs_max_v: 13697.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.087220/  1.574559, val:  51.25%, val_best:  51.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7248%\n",
      "layer   2  Sparsity: 73.3916%\n",
      "layer   3  Sparsity: 61.4262%\n",
      "total_backward_count 48950 real_backward_count 5984  12.225%\n",
      "fc layer 2 self.abs_max_out: 3835.0\n",
      "fc layer 2 self.abs_max_out: 3890.0\n",
      "fc layer 2 self.abs_max_out: 3944.0\n",
      "fc layer 2 self.abs_max_out: 3999.0\n",
      "fc layer 2 self.abs_max_out: 4015.0\n",
      "lif layer 2 self.abs_max_v: 7401.0\n",
      "lif layer 2 self.abs_max_v: 7524.0\n",
      "fc layer 1 self.abs_max_out: 7112.0\n",
      "fc layer 1 self.abs_max_out: 7143.0\n",
      "fc layer 2 self.abs_max_out: 4022.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.055234/  1.618235, val:  45.83%, val_best:  51.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7055%\n",
      "layer   2  Sparsity: 73.2763%\n",
      "layer   3  Sparsity: 62.0335%\n",
      "total_backward_count 58740 real_backward_count 7061  12.021%\n",
      "fc layer 2 self.abs_max_out: 4057.0\n",
      "fc layer 2 self.abs_max_out: 4060.0\n",
      "lif layer 2 self.abs_max_v: 7720.0\n",
      "lif layer 2 self.abs_max_v: 7768.0\n",
      "lif layer 2 self.abs_max_v: 7839.0\n",
      "fc layer 2 self.abs_max_out: 4163.0\n",
      "fc layer 2 self.abs_max_out: 4229.0\n",
      "fc layer 3 self.abs_max_out: 2324.0\n",
      "fc layer 1 self.abs_max_out: 7450.0\n",
      "lif layer 1 self.abs_max_v: 13794.0\n",
      "lif layer 1 self.abs_max_v: 14317.0\n",
      "lif layer 1 self.abs_max_v: 14319.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.041888/  1.548881, val:  52.92%, val_best:  52.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7398%\n",
      "layer   2  Sparsity: 73.3069%\n",
      "layer   3  Sparsity: 62.9675%\n",
      "total_backward_count 68530 real_backward_count 8185  11.944%\n",
      "fc layer 2 self.abs_max_out: 4330.0\n",
      "fc layer 1 self.abs_max_out: 7563.0\n",
      "lif layer 1 self.abs_max_v: 14397.0\n",
      "fc layer 1 self.abs_max_out: 8548.0\n",
      "lif layer 1 self.abs_max_v: 15746.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.033420/  1.603382, val:  40.83%, val_best:  52.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7148%\n",
      "layer   2  Sparsity: 74.3915%\n",
      "layer   3  Sparsity: 63.3169%\n",
      "total_backward_count 78320 real_backward_count 9243  11.802%\n",
      "lif layer 2 self.abs_max_v: 7937.0\n",
      "fc layer 2 self.abs_max_out: 4431.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.026484/  1.534917, val:  50.42%, val_best:  52.92%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7263%\n",
      "layer   2  Sparsity: 73.9336%\n",
      "layer   3  Sparsity: 63.4373%\n",
      "total_backward_count 88110 real_backward_count 10380  11.781%\n",
      "lif layer 2 self.abs_max_v: 7939.0\n",
      "lif layer 2 self.abs_max_v: 8021.5\n",
      "fc layer 3 self.abs_max_out: 2433.0\n",
      "fc layer 3 self.abs_max_out: 2555.0\n",
      "lif layer 1 self.abs_max_v: 16151.5\n",
      "fc layer 1 self.abs_max_out: 9542.0\n",
      "lif layer 1 self.abs_max_v: 17618.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.024184/  1.583709, val:  45.42%, val_best:  52.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7052%\n",
      "layer   2  Sparsity: 73.7593%\n",
      "layer   3  Sparsity: 64.3967%\n",
      "total_backward_count 97900 real_backward_count 11479  11.725%\n",
      "fc layer 2 self.abs_max_out: 4519.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.974319/  1.610589, val:  46.67%, val_best:  52.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7046%\n",
      "layer   2  Sparsity: 73.7626%\n",
      "layer   3  Sparsity: 63.3879%\n",
      "total_backward_count 107690 real_backward_count 12582  11.684%\n",
      "fc layer 2 self.abs_max_out: 4557.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.008045/  1.512777, val:  53.75%, val_best:  53.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7229%\n",
      "layer   2  Sparsity: 72.3395%\n",
      "layer   3  Sparsity: 63.3050%\n",
      "total_backward_count 117480 real_backward_count 13659  11.627%\n",
      "lif layer 2 self.abs_max_v: 8028.0\n",
      "lif layer 2 self.abs_max_v: 8183.5\n",
      "lif layer 2 self.abs_max_v: 8347.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.952407/  1.584473, val:  44.17%, val_best:  53.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7317%\n",
      "layer   2  Sparsity: 72.8260%\n",
      "layer   3  Sparsity: 63.7235%\n",
      "total_backward_count 127270 real_backward_count 14679  11.534%\n",
      "lif layer 2 self.abs_max_v: 8377.5\n",
      "fc layer 2 self.abs_max_out: 4888.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.967348/  1.598791, val:  44.17%, val_best:  53.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7391%\n",
      "layer   2  Sparsity: 73.2400%\n",
      "layer   3  Sparsity: 64.0258%\n",
      "total_backward_count 137060 real_backward_count 15745  11.488%\n",
      "lif layer 2 self.abs_max_v: 8660.0\n",
      "lif layer 2 self.abs_max_v: 8754.5\n",
      "fc layer 1 self.abs_max_out: 10079.0\n",
      "lif layer 1 self.abs_max_v: 18868.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.966655/  1.518616, val:  48.33%, val_best:  53.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7433%\n",
      "layer   2  Sparsity: 73.4324%\n",
      "layer   3  Sparsity: 64.8630%\n",
      "total_backward_count 146850 real_backward_count 16753  11.408%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.979256/  1.473075, val:  51.67%, val_best:  53.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7051%\n",
      "layer   2  Sparsity: 73.0818%\n",
      "layer   3  Sparsity: 67.0518%\n",
      "total_backward_count 156640 real_backward_count 17808  11.369%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.958899/  1.446090, val:  62.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7175%\n",
      "layer   2  Sparsity: 73.1954%\n",
      "layer   3  Sparsity: 67.1002%\n",
      "total_backward_count 166430 real_backward_count 18857  11.330%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.978848/  1.468624, val:  52.08%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7241%\n",
      "layer   2  Sparsity: 72.9904%\n",
      "layer   3  Sparsity: 67.2628%\n",
      "total_backward_count 176220 real_backward_count 19927  11.308%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.954135/  1.367663, val:  55.42%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7100%\n",
      "layer   2  Sparsity: 72.4312%\n",
      "layer   3  Sparsity: 66.5729%\n",
      "total_backward_count 186010 real_backward_count 20965  11.271%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.934506/  1.670651, val:  37.08%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7260%\n",
      "layer   2  Sparsity: 72.5121%\n",
      "layer   3  Sparsity: 68.0324%\n",
      "total_backward_count 195800 real_backward_count 22033  11.253%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.936351/  1.487988, val:  48.33%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7126%\n",
      "layer   2  Sparsity: 72.7559%\n",
      "layer   3  Sparsity: 68.6745%\n",
      "total_backward_count 205590 real_backward_count 23110  11.241%\n",
      "lif layer 2 self.abs_max_v: 8803.0\n",
      "lif layer 1 self.abs_max_v: 18964.5\n",
      "lif layer 1 self.abs_max_v: 19246.5\n",
      "lif layer 1 self.abs_max_v: 19540.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.961265/  1.468255, val:  61.67%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6909%\n",
      "layer   2  Sparsity: 72.7541%\n",
      "layer   3  Sparsity: 69.0034%\n",
      "total_backward_count 215380 real_backward_count 24162  11.218%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.978243/  1.405979, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7030%\n",
      "layer   2  Sparsity: 72.9182%\n",
      "layer   3  Sparsity: 69.3894%\n",
      "total_backward_count 225170 real_backward_count 25221  11.201%\n",
      "fc layer 1 self.abs_max_out: 10313.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.983431/  1.388988, val:  58.33%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7295%\n",
      "layer   2  Sparsity: 72.4711%\n",
      "layer   3  Sparsity: 70.2103%\n",
      "total_backward_count 234960 real_backward_count 26322  11.203%\n",
      "fc layer 1 self.abs_max_out: 10792.0\n",
      "lif layer 1 self.abs_max_v: 20308.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.973381/  1.445005, val:  60.00%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7060%\n",
      "layer   2  Sparsity: 72.0895%\n",
      "layer   3  Sparsity: 70.3627%\n",
      "total_backward_count 244750 real_backward_count 27346  11.173%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.981921/  1.313972, val:  67.08%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7115%\n",
      "layer   2  Sparsity: 72.2854%\n",
      "layer   3  Sparsity: 70.0140%\n",
      "total_backward_count 254540 real_backward_count 28427  11.168%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.973495/  1.460062, val:  53.75%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6997%\n",
      "layer   2  Sparsity: 72.9019%\n",
      "layer   3  Sparsity: 70.7232%\n",
      "total_backward_count 264330 real_backward_count 29426  11.132%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.975393/  1.378167, val:  65.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7273%\n",
      "layer   2  Sparsity: 72.8696%\n",
      "layer   3  Sparsity: 70.4523%\n",
      "total_backward_count 274120 real_backward_count 30444  11.106%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.961336/  1.426291, val:  55.00%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7317%\n",
      "layer   2  Sparsity: 73.1164%\n",
      "layer   3  Sparsity: 71.0210%\n",
      "total_backward_count 283910 real_backward_count 31433  11.071%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.965468/  1.444314, val:  55.42%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7305%\n",
      "layer   2  Sparsity: 72.2647%\n",
      "layer   3  Sparsity: 70.7279%\n",
      "total_backward_count 293700 real_backward_count 32445  11.047%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.913098/  1.355917, val:  54.58%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7307%\n",
      "layer   2  Sparsity: 72.8859%\n",
      "layer   3  Sparsity: 69.9840%\n",
      "total_backward_count 303490 real_backward_count 33423  11.013%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.905201/  1.352179, val:  59.58%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7364%\n",
      "layer   2  Sparsity: 72.8111%\n",
      "layer   3  Sparsity: 69.6280%\n",
      "total_backward_count 313280 real_backward_count 34416  10.986%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.889898/  1.384494, val:  61.25%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7047%\n",
      "layer   2  Sparsity: 72.5136%\n",
      "layer   3  Sparsity: 69.2932%\n",
      "total_backward_count 323070 real_backward_count 35368  10.947%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.851093/  1.319657, val:  63.75%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7311%\n",
      "layer   2  Sparsity: 71.8954%\n",
      "layer   3  Sparsity: 68.5421%\n",
      "total_backward_count 332860 real_backward_count 36338  10.917%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.815992/  1.414093, val:  57.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7362%\n",
      "layer   2  Sparsity: 72.1879%\n",
      "layer   3  Sparsity: 68.7747%\n",
      "total_backward_count 342650 real_backward_count 37297  10.885%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.859451/  1.296992, val:  66.25%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7374%\n",
      "layer   2  Sparsity: 72.1838%\n",
      "layer   3  Sparsity: 68.9165%\n",
      "total_backward_count 352440 real_backward_count 38278  10.861%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.836006/  1.278728, val:  65.42%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7172%\n",
      "layer   2  Sparsity: 72.4237%\n",
      "layer   3  Sparsity: 68.1705%\n",
      "total_backward_count 362230 real_backward_count 39255  10.837%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.797035/  1.332984, val:  57.92%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7221%\n",
      "layer   2  Sparsity: 72.0589%\n",
      "layer   3  Sparsity: 67.6049%\n",
      "total_backward_count 372020 real_backward_count 40107  10.781%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.818909/  1.352314, val:  53.33%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7365%\n",
      "layer   2  Sparsity: 71.9933%\n",
      "layer   3  Sparsity: 68.1254%\n",
      "total_backward_count 381810 real_backward_count 41075  10.758%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.813845/  1.368043, val:  61.25%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7399%\n",
      "layer   2  Sparsity: 72.4258%\n",
      "layer   3  Sparsity: 67.6648%\n",
      "total_backward_count 391600 real_backward_count 41954  10.713%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.826446/  1.318938, val:  58.33%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7055%\n",
      "layer   2  Sparsity: 72.8566%\n",
      "layer   3  Sparsity: 68.4532%\n",
      "total_backward_count 401390 real_backward_count 42866  10.679%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.802355/  1.284028, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7276%\n",
      "layer   2  Sparsity: 72.5308%\n",
      "layer   3  Sparsity: 67.5701%\n",
      "total_backward_count 411180 real_backward_count 43791  10.650%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.812642/  1.336033, val:  64.17%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7245%\n",
      "layer   2  Sparsity: 72.6121%\n",
      "layer   3  Sparsity: 68.5582%\n",
      "total_backward_count 420970 real_backward_count 44737  10.627%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.804284/  1.199452, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7312%\n",
      "layer   2  Sparsity: 72.4926%\n",
      "layer   3  Sparsity: 68.1518%\n",
      "total_backward_count 430760 real_backward_count 45627  10.592%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.831877/  1.262781, val:  65.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7426%\n",
      "layer   2  Sparsity: 72.2084%\n",
      "layer   3  Sparsity: 68.3583%\n",
      "total_backward_count 440550 real_backward_count 46518  10.559%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.778897/  1.217982, val:  63.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7309%\n",
      "layer   2  Sparsity: 72.8616%\n",
      "layer   3  Sparsity: 68.7760%\n",
      "total_backward_count 450340 real_backward_count 47417  10.529%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.780393/  1.294174, val:  63.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.6940%\n",
      "layer   2  Sparsity: 72.8879%\n",
      "layer   3  Sparsity: 68.8265%\n",
      "total_backward_count 460130 real_backward_count 48304  10.498%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.784002/  1.324601, val:  63.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7180%\n",
      "layer   2  Sparsity: 72.8038%\n",
      "layer   3  Sparsity: 68.1896%\n",
      "total_backward_count 469920 real_backward_count 49166  10.463%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.817110/  1.346953, val:  57.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6970%\n",
      "layer   2  Sparsity: 72.8184%\n",
      "layer   3  Sparsity: 68.3991%\n",
      "total_backward_count 479710 real_backward_count 50044  10.432%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.809583/  1.293990, val:  65.00%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7151%\n",
      "layer   2  Sparsity: 73.2177%\n",
      "layer   3  Sparsity: 69.4401%\n",
      "total_backward_count 489500 real_backward_count 50953  10.409%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.765793/  1.244128, val:  57.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7104%\n",
      "layer   2  Sparsity: 73.2870%\n",
      "layer   3  Sparsity: 69.0638%\n",
      "total_backward_count 499290 real_backward_count 51840  10.383%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.756915/  1.269375, val:  65.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7330%\n",
      "layer   2  Sparsity: 73.4562%\n",
      "layer   3  Sparsity: 70.2644%\n",
      "total_backward_count 509080 real_backward_count 52710  10.354%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.818476/  1.356443, val:  58.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7042%\n",
      "layer   2  Sparsity: 73.6204%\n",
      "layer   3  Sparsity: 69.9488%\n",
      "total_backward_count 518870 real_backward_count 53637  10.337%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.832750/  1.335032, val:  66.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7174%\n",
      "layer   2  Sparsity: 73.4507%\n",
      "layer   3  Sparsity: 69.5009%\n",
      "total_backward_count 528660 real_backward_count 54510  10.311%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.808191/  1.271618, val:  67.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7130%\n",
      "layer   2  Sparsity: 73.1241%\n",
      "layer   3  Sparsity: 70.8847%\n",
      "total_backward_count 538450 real_backward_count 55319  10.274%\n",
      "lif layer 1 self.abs_max_v: 20587.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.823295/  1.242886, val:  60.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7152%\n",
      "layer   2  Sparsity: 73.0522%\n",
      "layer   3  Sparsity: 70.6845%\n",
      "total_backward_count 548240 real_backward_count 56141  10.240%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.775233/  1.259808, val:  71.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7144%\n",
      "layer   2  Sparsity: 72.6209%\n",
      "layer   3  Sparsity: 70.7374%\n",
      "total_backward_count 558030 real_backward_count 57028  10.220%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.780395/  1.219748, val:  62.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7364%\n",
      "layer   2  Sparsity: 72.4176%\n",
      "layer   3  Sparsity: 68.9120%\n",
      "total_backward_count 567820 real_backward_count 57889  10.195%\n",
      "lif layer 2 self.abs_max_v: 8814.5\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.769514/  1.203330, val:  74.58%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7109%\n",
      "layer   2  Sparsity: 72.5927%\n",
      "layer   3  Sparsity: 69.2043%\n",
      "total_backward_count 577610 real_backward_count 58723  10.167%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.770427/  1.229083, val:  65.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7306%\n",
      "layer   2  Sparsity: 72.6053%\n",
      "layer   3  Sparsity: 69.0928%\n",
      "total_backward_count 587400 real_backward_count 59550  10.138%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.760587/  1.255180, val:  62.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7013%\n",
      "layer   2  Sparsity: 72.6988%\n",
      "layer   3  Sparsity: 67.8587%\n",
      "total_backward_count 597190 real_backward_count 60431  10.119%\n",
      "fc layer 1 self.abs_max_out: 10857.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.766478/  1.158112, val:  67.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7072%\n",
      "layer   2  Sparsity: 72.2514%\n",
      "layer   3  Sparsity: 68.3963%\n",
      "total_backward_count 606980 real_backward_count 61307  10.100%\n",
      "fc layer 1 self.abs_max_out: 10893.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.721351/  1.267882, val:  63.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7139%\n",
      "layer   2  Sparsity: 72.8043%\n",
      "layer   3  Sparsity: 66.9224%\n",
      "total_backward_count 616770 real_backward_count 62153  10.077%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.721918/  1.296433, val:  61.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7073%\n",
      "layer   2  Sparsity: 72.8084%\n",
      "layer   3  Sparsity: 68.1224%\n",
      "total_backward_count 626560 real_backward_count 62973  10.051%\n",
      "fc layer 3 self.abs_max_out: 2559.0\n",
      "fc layer 3 self.abs_max_out: 2635.0\n",
      "fc layer 3 self.abs_max_out: 2643.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.718720/  1.197020, val:  65.42%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7308%\n",
      "layer   2  Sparsity: 73.1695%\n",
      "layer   3  Sparsity: 68.1614%\n",
      "total_backward_count 636350 real_backward_count 63808  10.027%\n",
      "fc layer 1 self.abs_max_out: 10931.0\n",
      "lif layer 1 self.abs_max_v: 20661.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.699089/  1.157133, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7233%\n",
      "layer   2  Sparsity: 72.8235%\n",
      "layer   3  Sparsity: 67.9980%\n",
      "total_backward_count 646140 real_backward_count 64616  10.000%\n",
      "fc layer 1 self.abs_max_out: 10936.0\n",
      "lif layer 1 self.abs_max_v: 20671.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.718184/  1.136381, val:  74.17%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7183%\n",
      "layer   2  Sparsity: 73.1473%\n",
      "layer   3  Sparsity: 68.4439%\n",
      "total_backward_count 655930 real_backward_count 65412   9.972%\n",
      "fc layer 1 self.abs_max_out: 11013.0\n",
      "lif layer 1 self.abs_max_v: 20824.0\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.705727/  1.165890, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7102%\n",
      "layer   2  Sparsity: 72.8452%\n",
      "layer   3  Sparsity: 69.2131%\n",
      "total_backward_count 665720 real_backward_count 66235   9.949%\n",
      "fc layer 1 self.abs_max_out: 11073.0\n",
      "lif layer 1 self.abs_max_v: 20942.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.731912/  1.178167, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7047%\n",
      "layer   2  Sparsity: 72.6933%\n",
      "layer   3  Sparsity: 69.1552%\n",
      "total_backward_count 675510 real_backward_count 67021   9.922%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.702533/  1.339623, val:  54.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7329%\n",
      "layer   2  Sparsity: 72.7594%\n",
      "layer   3  Sparsity: 69.3195%\n",
      "total_backward_count 685300 real_backward_count 67768   9.889%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.724936/  1.198351, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7459%\n",
      "layer   2  Sparsity: 72.6326%\n",
      "layer   3  Sparsity: 69.7936%\n",
      "total_backward_count 695090 real_backward_count 68576   9.866%\n",
      "fc layer 1 self.abs_max_out: 11087.0\n",
      "lif layer 1 self.abs_max_v: 20970.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.726938/  1.239072, val:  64.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7170%\n",
      "layer   2  Sparsity: 73.0245%\n",
      "layer   3  Sparsity: 69.8154%\n",
      "total_backward_count 704880 real_backward_count 69401   9.846%\n",
      "fc layer 1 self.abs_max_out: 11120.0\n",
      "lif layer 1 self.abs_max_v: 21034.5\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.737554/  1.126767, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7218%\n",
      "layer   2  Sparsity: 72.8930%\n",
      "layer   3  Sparsity: 70.7867%\n",
      "total_backward_count 714670 real_backward_count 70203   9.823%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.717892/  1.241670, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7246%\n",
      "layer   2  Sparsity: 72.5123%\n",
      "layer   3  Sparsity: 70.4568%\n",
      "total_backward_count 724460 real_backward_count 70990   9.799%\n",
      "fc layer 1 self.abs_max_out: 11128.0\n",
      "lif layer 1 self.abs_max_v: 21050.5\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.744326/  1.211081, val:  65.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7206%\n",
      "layer   2  Sparsity: 72.7918%\n",
      "layer   3  Sparsity: 71.1384%\n",
      "total_backward_count 734250 real_backward_count 71819   9.781%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.748250/  1.106933, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.6970%\n",
      "layer   2  Sparsity: 72.9242%\n",
      "layer   3  Sparsity: 69.8445%\n",
      "total_backward_count 744040 real_backward_count 72594   9.757%\n",
      "fc layer 1 self.abs_max_out: 11317.0\n",
      "lif layer 1 self.abs_max_v: 21439.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.724416/  1.247184, val:  57.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7397%\n",
      "layer   2  Sparsity: 72.4939%\n",
      "layer   3  Sparsity: 70.3087%\n",
      "total_backward_count 753830 real_backward_count 73363   9.732%\n",
      "fc layer 1 self.abs_max_out: 11332.0\n",
      "lif layer 1 self.abs_max_v: 21469.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.706283/  1.338137, val:  51.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7034%\n",
      "layer   2  Sparsity: 72.2013%\n",
      "layer   3  Sparsity: 70.3270%\n",
      "total_backward_count 763620 real_backward_count 74140   9.709%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.731821/  1.184454, val:  69.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6958%\n",
      "layer   2  Sparsity: 72.1544%\n",
      "layer   3  Sparsity: 70.5544%\n",
      "total_backward_count 773410 real_backward_count 74928   9.688%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.713952/  1.198332, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7200%\n",
      "layer   2  Sparsity: 72.1756%\n",
      "layer   3  Sparsity: 69.4492%\n",
      "total_backward_count 783200 real_backward_count 75731   9.669%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.719902/  1.236219, val:  62.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7214%\n",
      "layer   2  Sparsity: 72.5416%\n",
      "layer   3  Sparsity: 69.6575%\n",
      "total_backward_count 792990 real_backward_count 76537   9.652%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.734667/  1.283415, val:  60.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7112%\n",
      "layer   2  Sparsity: 72.5219%\n",
      "layer   3  Sparsity: 71.0942%\n",
      "total_backward_count 802780 real_backward_count 77272   9.626%\n",
      "fc layer 1 self.abs_max_out: 11370.0\n",
      "lif layer 1 self.abs_max_v: 21545.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.749214/  1.175511, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7309%\n",
      "layer   2  Sparsity: 72.8956%\n",
      "layer   3  Sparsity: 71.0258%\n",
      "total_backward_count 812570 real_backward_count 78050   9.605%\n",
      "fc layer 1 self.abs_max_out: 11505.0\n",
      "lif layer 1 self.abs_max_v: 21823.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.745908/  1.246875, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7120%\n",
      "layer   2  Sparsity: 72.3917%\n",
      "layer   3  Sparsity: 70.6242%\n",
      "total_backward_count 822360 real_backward_count 78820   9.585%\n",
      "fc layer 1 self.abs_max_out: 11512.0\n",
      "lif layer 1 self.abs_max_v: 21837.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.726796/  1.099401, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7317%\n",
      "layer   2  Sparsity: 72.2066%\n",
      "layer   3  Sparsity: 70.8627%\n",
      "total_backward_count 832150 real_backward_count 79542   9.559%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.709590/  1.170110, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7367%\n",
      "layer   2  Sparsity: 72.4623%\n",
      "layer   3  Sparsity: 70.3017%\n",
      "total_backward_count 841940 real_backward_count 80268   9.534%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.708919/  1.263549, val:  63.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7265%\n",
      "layer   2  Sparsity: 72.0941%\n",
      "layer   3  Sparsity: 69.4384%\n",
      "total_backward_count 851730 real_backward_count 81043   9.515%\n",
      "fc layer 1 self.abs_max_out: 11524.0\n",
      "lif layer 1 self.abs_max_v: 21860.5\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.719652/  1.176864, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7133%\n",
      "layer   2  Sparsity: 72.6653%\n",
      "layer   3  Sparsity: 70.1203%\n",
      "total_backward_count 861520 real_backward_count 81791   9.494%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.717830/  1.130025, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7324%\n",
      "layer   2  Sparsity: 72.4911%\n",
      "layer   3  Sparsity: 68.9039%\n",
      "total_backward_count 871310 real_backward_count 82522   9.471%\n",
      "fc layer 1 self.abs_max_out: 11542.0\n",
      "lif layer 1 self.abs_max_v: 21896.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.733327/  1.148559, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7009%\n",
      "layer   2  Sparsity: 72.6968%\n",
      "layer   3  Sparsity: 68.8477%\n",
      "total_backward_count 881100 real_backward_count 83246   9.448%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.705831/  1.239135, val:  61.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.6973%\n",
      "layer   2  Sparsity: 72.9588%\n",
      "layer   3  Sparsity: 69.3334%\n",
      "total_backward_count 890890 real_backward_count 83980   9.427%\n",
      "fc layer 1 self.abs_max_out: 11557.0\n",
      "lif layer 1 self.abs_max_v: 21925.5\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.728245/  1.215380, val:  74.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7233%\n",
      "layer   2  Sparsity: 72.5776%\n",
      "layer   3  Sparsity: 68.9269%\n",
      "total_backward_count 900680 real_backward_count 84742   9.409%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.717969/  1.204738, val:  68.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6982%\n",
      "layer   2  Sparsity: 72.6171%\n",
      "layer   3  Sparsity: 70.2630%\n",
      "total_backward_count 910470 real_backward_count 85463   9.387%\n",
      "lif layer 2 self.abs_max_v: 8940.5\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.701477/  1.194063, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7393%\n",
      "layer   2  Sparsity: 73.0237%\n",
      "layer   3  Sparsity: 70.0320%\n",
      "total_backward_count 920260 real_backward_count 86192   9.366%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.700283/  1.128675, val:  76.67%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7248%\n",
      "layer   2  Sparsity: 73.2492%\n",
      "layer   3  Sparsity: 69.6685%\n",
      "total_backward_count 930050 real_backward_count 86945   9.348%\n",
      "fc layer 1 self.abs_max_out: 11558.0\n",
      "lif layer 1 self.abs_max_v: 21927.5\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.704652/  1.180105, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7080%\n",
      "layer   2  Sparsity: 72.9051%\n",
      "layer   3  Sparsity: 70.1438%\n",
      "total_backward_count 939840 real_backward_count 87675   9.329%\n",
      "fc layer 1 self.abs_max_out: 11580.0\n",
      "lif layer 1 self.abs_max_v: 21968.5\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.706564/  1.238788, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7258%\n",
      "layer   2  Sparsity: 72.9622%\n",
      "layer   3  Sparsity: 70.4182%\n",
      "total_backward_count 949630 real_backward_count 88409   9.310%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.721758/  1.105863, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7218%\n",
      "layer   2  Sparsity: 72.7828%\n",
      "layer   3  Sparsity: 70.4868%\n",
      "total_backward_count 959420 real_backward_count 89113   9.288%\n",
      "fc layer 1 self.abs_max_out: 11594.0\n",
      "lif layer 1 self.abs_max_v: 21996.5\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.706178/  1.211625, val:  63.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7097%\n",
      "layer   2  Sparsity: 72.8685%\n",
      "layer   3  Sparsity: 70.5209%\n",
      "total_backward_count 969210 real_backward_count 89816   9.267%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.695235/  1.134135, val:  70.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7219%\n",
      "layer   2  Sparsity: 72.7470%\n",
      "layer   3  Sparsity: 70.7514%\n",
      "total_backward_count 979000 real_backward_count 90533   9.247%\n",
      "fc layer 1 self.abs_max_out: 11601.0\n",
      "lif layer 1 self.abs_max_v: 22010.0\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.682549/  1.110293, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7416%\n",
      "layer   2  Sparsity: 72.8967%\n",
      "layer   3  Sparsity: 69.2103%\n",
      "total_backward_count 988790 real_backward_count 91229   9.226%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.691824/  1.152583, val:  70.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7198%\n",
      "layer   2  Sparsity: 73.1669%\n",
      "layer   3  Sparsity: 70.4763%\n",
      "total_backward_count 998580 real_backward_count 91956   9.209%\n",
      "fc layer 1 self.abs_max_out: 11623.0\n",
      "lif layer 1 self.abs_max_v: 22053.5\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.684064/  1.094846, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7188%\n",
      "layer   2  Sparsity: 73.0999%\n",
      "layer   3  Sparsity: 69.8150%\n",
      "total_backward_count 1008370 real_backward_count 92659   9.189%\n",
      "fc layer 1 self.abs_max_out: 11647.0\n",
      "lif layer 1 self.abs_max_v: 22100.5\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.685004/  1.125376, val:  70.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7052%\n",
      "layer   2  Sparsity: 73.1932%\n",
      "layer   3  Sparsity: 70.0568%\n",
      "total_backward_count 1018160 real_backward_count 93378   9.171%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.688306/  1.106023, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7200%\n",
      "layer   2  Sparsity: 72.8491%\n",
      "layer   3  Sparsity: 69.9084%\n",
      "total_backward_count 1027950 real_backward_count 94106   9.155%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.705525/  1.109893, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7174%\n",
      "layer   2  Sparsity: 72.4937%\n",
      "layer   3  Sparsity: 70.0779%\n",
      "total_backward_count 1037740 real_backward_count 94835   9.139%\n",
      "fc layer 1 self.abs_max_out: 11659.0\n",
      "lif layer 1 self.abs_max_v: 22124.5\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.708479/  1.201681, val:  67.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7112%\n",
      "layer   2  Sparsity: 72.6444%\n",
      "layer   3  Sparsity: 71.7352%\n",
      "total_backward_count 1047530 real_backward_count 95522   9.119%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.713749/  1.273143, val:  57.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7068%\n",
      "layer   2  Sparsity: 72.5331%\n",
      "layer   3  Sparsity: 70.6073%\n",
      "total_backward_count 1057320 real_backward_count 96244   9.103%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.695590/  1.086452, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6956%\n",
      "layer   2  Sparsity: 72.8144%\n",
      "layer   3  Sparsity: 69.8153%\n",
      "total_backward_count 1067110 real_backward_count 96948   9.085%\n",
      "fc layer 1 self.abs_max_out: 11681.0\n",
      "lif layer 1 self.abs_max_v: 22167.5\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.678971/  1.177795, val:  70.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7349%\n",
      "layer   2  Sparsity: 72.7754%\n",
      "layer   3  Sparsity: 69.5016%\n",
      "total_backward_count 1076900 real_backward_count 97630   9.066%\n",
      "fc layer 1 self.abs_max_out: 11696.0\n",
      "lif layer 1 self.abs_max_v: 22197.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.686725/  1.082324, val:  73.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7209%\n",
      "layer   2  Sparsity: 72.6361%\n",
      "layer   3  Sparsity: 68.1208%\n",
      "total_backward_count 1086690 real_backward_count 98353   9.051%\n",
      "fc layer 1 self.abs_max_out: 11697.0\n",
      "lif layer 1 self.abs_max_v: 22199.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.649202/  1.074247, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6994%\n",
      "layer   2  Sparsity: 73.0361%\n",
      "layer   3  Sparsity: 69.6210%\n",
      "total_backward_count 1096480 real_backward_count 98998   9.029%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.664980/  1.110015, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7104%\n",
      "layer   2  Sparsity: 72.8086%\n",
      "layer   3  Sparsity: 70.3761%\n",
      "total_backward_count 1106270 real_backward_count 99704   9.013%\n",
      "fc layer 1 self.abs_max_out: 11706.0\n",
      "lif layer 1 self.abs_max_v: 22217.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.678597/  1.102818, val:  70.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7131%\n",
      "layer   2  Sparsity: 72.4620%\n",
      "layer   3  Sparsity: 70.3145%\n",
      "total_backward_count 1116060 real_backward_count 100414   8.997%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.684358/  1.150545, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7368%\n",
      "layer   2  Sparsity: 72.4712%\n",
      "layer   3  Sparsity: 69.6139%\n",
      "total_backward_count 1125850 real_backward_count 101129   8.982%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.674994/  1.143883, val:  74.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7297%\n",
      "layer   2  Sparsity: 72.7846%\n",
      "layer   3  Sparsity: 68.2753%\n",
      "total_backward_count 1135640 real_backward_count 101835   8.967%\n",
      "fc layer 1 self.abs_max_out: 11722.0\n",
      "lif layer 1 self.abs_max_v: 22248.5\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.680157/  1.192235, val:  60.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7396%\n",
      "layer   2  Sparsity: 72.5721%\n",
      "layer   3  Sparsity: 68.3379%\n",
      "total_backward_count 1145430 real_backward_count 102554   8.953%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.670131/  1.114423, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 92.7164%\n",
      "layer   2  Sparsity: 73.0434%\n",
      "layer   3  Sparsity: 69.6233%\n",
      "total_backward_count 1155220 real_backward_count 103158   8.930%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.672558/  1.073850, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7160%\n",
      "layer   2  Sparsity: 72.6477%\n",
      "layer   3  Sparsity: 70.3381%\n",
      "total_backward_count 1165010 real_backward_count 103829   8.912%\n",
      "fc layer 1 self.abs_max_out: 11733.0\n",
      "lif layer 1 self.abs_max_v: 22270.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.681544/  1.091254, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7333%\n",
      "layer   2  Sparsity: 72.7275%\n",
      "layer   3  Sparsity: 71.8505%\n",
      "total_backward_count 1174800 real_backward_count 104561   8.900%\n",
      "fc layer 1 self.abs_max_out: 11741.0\n",
      "lif layer 1 self.abs_max_v: 22286.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.695714/  1.187244, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7236%\n",
      "layer   2  Sparsity: 72.7501%\n",
      "layer   3  Sparsity: 71.4961%\n",
      "total_backward_count 1184590 real_backward_count 105282   8.888%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.674885/  1.129326, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7193%\n",
      "layer   2  Sparsity: 72.6267%\n",
      "layer   3  Sparsity: 71.0002%\n",
      "total_backward_count 1194380 real_backward_count 105963   8.872%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.680783/  1.067930, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7359%\n",
      "layer   2  Sparsity: 73.2200%\n",
      "layer   3  Sparsity: 70.5846%\n",
      "total_backward_count 1204170 real_backward_count 106672   8.859%\n",
      "lif layer 2 self.abs_max_v: 8985.0\n",
      "lif layer 2 self.abs_max_v: 9006.5\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.668377/  1.082229, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7478%\n",
      "layer   2  Sparsity: 72.6993%\n",
      "layer   3  Sparsity: 70.5038%\n",
      "total_backward_count 1213960 real_backward_count 107347   8.843%\n",
      "fc layer 1 self.abs_max_out: 11742.0\n",
      "lif layer 1 self.abs_max_v: 22288.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.644689/  1.062637, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7154%\n",
      "layer   2  Sparsity: 72.6374%\n",
      "layer   3  Sparsity: 70.6815%\n",
      "total_backward_count 1223750 real_backward_count 108011   8.826%\n",
      "fc layer 1 self.abs_max_out: 11745.0\n",
      "lif layer 1 self.abs_max_v: 22294.0\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.668037/  1.069772, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 72.7351%\n",
      "layer   3  Sparsity: 70.9377%\n",
      "total_backward_count 1233540 real_backward_count 108704   8.812%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.669309/  1.217344, val:  64.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7194%\n",
      "layer   2  Sparsity: 72.7059%\n",
      "layer   3  Sparsity: 70.8265%\n",
      "total_backward_count 1243330 real_backward_count 109365   8.796%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.659877/  1.053931, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7020%\n",
      "layer   2  Sparsity: 72.9336%\n",
      "layer   3  Sparsity: 71.2521%\n",
      "total_backward_count 1253120 real_backward_count 110006   8.779%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.661566/  1.095274, val:  78.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7155%\n",
      "layer   2  Sparsity: 73.1292%\n",
      "layer   3  Sparsity: 71.5170%\n",
      "total_backward_count 1262910 real_backward_count 110651   8.762%\n",
      "fc layer 1 self.abs_max_out: 11756.0\n",
      "lif layer 1 self.abs_max_v: 22315.5\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.677160/  1.099469, val:  72.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7375%\n",
      "layer   2  Sparsity: 73.5146%\n",
      "layer   3  Sparsity: 71.3114%\n",
      "total_backward_count 1272700 real_backward_count 111362   8.750%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.690385/  1.149364, val:  68.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7286%\n",
      "layer   2  Sparsity: 73.2043%\n",
      "layer   3  Sparsity: 72.2934%\n",
      "total_backward_count 1282490 real_backward_count 112012   8.734%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.671698/  1.159487, val:  70.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7383%\n",
      "layer   2  Sparsity: 73.1394%\n",
      "layer   3  Sparsity: 71.4681%\n",
      "total_backward_count 1292280 real_backward_count 112708   8.722%\n",
      "fc layer 1 self.abs_max_out: 11767.0\n",
      "lif layer 1 self.abs_max_v: 22337.0\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.639798/  1.124586, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7374%\n",
      "layer   2  Sparsity: 72.9419%\n",
      "layer   3  Sparsity: 69.2462%\n",
      "total_backward_count 1302070 real_backward_count 113399   8.709%\n",
      "fc layer 1 self.abs_max_out: 11775.0\n",
      "lif layer 1 self.abs_max_v: 22353.0\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.636445/  1.077618, val:  78.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7108%\n",
      "layer   2  Sparsity: 72.9114%\n",
      "layer   3  Sparsity: 70.2829%\n",
      "total_backward_count 1311860 real_backward_count 114047   8.694%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.626980/  1.039066, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7323%\n",
      "layer   2  Sparsity: 73.2774%\n",
      "layer   3  Sparsity: 70.3551%\n",
      "total_backward_count 1321650 real_backward_count 114686   8.677%\n",
      "fc layer 1 self.abs_max_out: 11802.0\n",
      "lif layer 1 self.abs_max_v: 22408.0\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.647574/  1.091247, val:  69.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7037%\n",
      "layer   2  Sparsity: 73.1434%\n",
      "layer   3  Sparsity: 69.4476%\n",
      "total_backward_count 1331440 real_backward_count 115391   8.667%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.635147/  1.016420, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7277%\n",
      "layer   2  Sparsity: 72.8686%\n",
      "layer   3  Sparsity: 70.3331%\n",
      "total_backward_count 1341230 real_backward_count 116021   8.650%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.643610/  1.059007, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6969%\n",
      "layer   2  Sparsity: 72.5493%\n",
      "layer   3  Sparsity: 69.9515%\n",
      "total_backward_count 1351020 real_backward_count 116676   8.636%\n",
      "lif layer 2 self.abs_max_v: 9146.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.665639/  1.129936, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7043%\n",
      "layer   2  Sparsity: 72.6074%\n",
      "layer   3  Sparsity: 69.5699%\n",
      "total_backward_count 1360810 real_backward_count 117324   8.622%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.658413/  1.052113, val:  78.33%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7337%\n",
      "layer   2  Sparsity: 72.5677%\n",
      "layer   3  Sparsity: 69.1163%\n",
      "total_backward_count 1370600 real_backward_count 117980   8.608%\n",
      "fc layer 1 self.abs_max_out: 11810.0\n",
      "lif layer 1 self.abs_max_v: 22424.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.631265/  1.034388, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7386%\n",
      "layer   2  Sparsity: 72.6146%\n",
      "layer   3  Sparsity: 69.4157%\n",
      "total_backward_count 1380390 real_backward_count 118654   8.596%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.610362/  1.069332, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6986%\n",
      "layer   2  Sparsity: 72.4805%\n",
      "layer   3  Sparsity: 68.1497%\n",
      "total_backward_count 1390180 real_backward_count 119338   8.584%\n",
      "fc layer 1 self.abs_max_out: 11834.0\n",
      "lif layer 1 self.abs_max_v: 22471.5\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.624052/  1.126540, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7351%\n",
      "layer   2  Sparsity: 72.6539%\n",
      "layer   3  Sparsity: 70.3907%\n",
      "total_backward_count 1399970 real_backward_count 120006   8.572%\n",
      "fc layer 1 self.abs_max_out: 11845.0\n",
      "lif layer 1 self.abs_max_v: 22493.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.604303/  1.020775, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7565%\n",
      "layer   2  Sparsity: 73.1117%\n",
      "layer   3  Sparsity: 70.4045%\n",
      "total_backward_count 1409760 real_backward_count 120639   8.557%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.597484/  1.044175, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7321%\n",
      "layer   2  Sparsity: 73.1725%\n",
      "layer   3  Sparsity: 70.3745%\n",
      "total_backward_count 1419550 real_backward_count 121332   8.547%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.612114/  1.143916, val:  64.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7036%\n",
      "layer   2  Sparsity: 73.1704%\n",
      "layer   3  Sparsity: 71.1240%\n",
      "total_backward_count 1429340 real_backward_count 122009   8.536%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.624697/  1.062738, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7285%\n",
      "layer   2  Sparsity: 73.1370%\n",
      "layer   3  Sparsity: 70.3319%\n",
      "total_backward_count 1439130 real_backward_count 122666   8.524%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.604364/  1.051197, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7325%\n",
      "layer   2  Sparsity: 73.2398%\n",
      "layer   3  Sparsity: 70.6451%\n",
      "total_backward_count 1448920 real_backward_count 123289   8.509%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.608836/  1.104930, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7453%\n",
      "layer   2  Sparsity: 73.0641%\n",
      "layer   3  Sparsity: 69.8498%\n",
      "total_backward_count 1458710 real_backward_count 123917   8.495%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.612907/  1.120486, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7209%\n",
      "layer   2  Sparsity: 72.9458%\n",
      "layer   3  Sparsity: 70.0587%\n",
      "total_backward_count 1468500 real_backward_count 124580   8.483%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.597511/  1.108491, val:  75.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7214%\n",
      "layer   2  Sparsity: 72.8826%\n",
      "layer   3  Sparsity: 71.0918%\n",
      "total_backward_count 1478290 real_backward_count 125139   8.465%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.601154/  1.006858, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6934%\n",
      "layer   2  Sparsity: 73.1921%\n",
      "layer   3  Sparsity: 71.3125%\n",
      "total_backward_count 1488080 real_backward_count 125735   8.449%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.603007/  1.036316, val:  75.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7199%\n",
      "layer   2  Sparsity: 72.9778%\n",
      "layer   3  Sparsity: 72.0117%\n",
      "total_backward_count 1497870 real_backward_count 126341   8.435%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.622020/  1.008205, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6654%\n",
      "layer   2  Sparsity: 72.9048%\n",
      "layer   3  Sparsity: 71.7946%\n",
      "total_backward_count 1507660 real_backward_count 126998   8.424%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.591955/  1.078014, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7030%\n",
      "layer   2  Sparsity: 72.7473%\n",
      "layer   3  Sparsity: 70.2639%\n",
      "total_backward_count 1517450 real_backward_count 127635   8.411%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.608127/  1.060260, val:  70.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7150%\n",
      "layer   2  Sparsity: 73.0039%\n",
      "layer   3  Sparsity: 70.6009%\n",
      "total_backward_count 1527240 real_backward_count 128262   8.398%\n",
      "fc layer 1 self.abs_max_out: 11849.0\n",
      "lif layer 1 self.abs_max_v: 22500.5\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.597701/  1.070217, val:  70.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7007%\n",
      "layer   2  Sparsity: 73.1633%\n",
      "layer   3  Sparsity: 69.1810%\n",
      "total_backward_count 1537030 real_backward_count 128876   8.385%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.587816/  1.054101, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7293%\n",
      "layer   2  Sparsity: 73.4158%\n",
      "layer   3  Sparsity: 70.8968%\n",
      "total_backward_count 1546820 real_backward_count 129447   8.369%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.626418/  1.056602, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7130%\n",
      "layer   2  Sparsity: 73.0678%\n",
      "layer   3  Sparsity: 71.7901%\n",
      "total_backward_count 1556610 real_backward_count 130053   8.355%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.625148/  1.103639, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7197%\n",
      "layer   2  Sparsity: 73.2605%\n",
      "layer   3  Sparsity: 70.9240%\n",
      "total_backward_count 1566400 real_backward_count 130636   8.340%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.627383/  1.148848, val:  70.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7155%\n",
      "layer   2  Sparsity: 73.4711%\n",
      "layer   3  Sparsity: 71.0176%\n",
      "total_backward_count 1576190 real_backward_count 131260   8.328%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.646280/  1.068859, val:  79.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7085%\n",
      "layer   2  Sparsity: 73.2659%\n",
      "layer   3  Sparsity: 70.5346%\n",
      "total_backward_count 1585980 real_backward_count 131912   8.317%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.627844/  1.080509, val:  79.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.6969%\n",
      "layer   2  Sparsity: 73.3634%\n",
      "layer   3  Sparsity: 71.4129%\n",
      "total_backward_count 1595770 real_backward_count 132527   8.305%\n",
      "fc layer 2 self.abs_max_out: 4894.0\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.621295/  1.216848, val:  63.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6937%\n",
      "layer   2  Sparsity: 73.4716%\n",
      "layer   3  Sparsity: 70.5685%\n",
      "total_backward_count 1605560 real_backward_count 133105   8.290%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.637848/  1.049371, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7072%\n",
      "layer   2  Sparsity: 72.9648%\n",
      "layer   3  Sparsity: 70.9239%\n",
      "total_backward_count 1615350 real_backward_count 133726   8.278%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.598124/  1.066976, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7102%\n",
      "layer   2  Sparsity: 72.8269%\n",
      "layer   3  Sparsity: 70.3037%\n",
      "total_backward_count 1625140 real_backward_count 134370   8.268%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.605216/  1.041170, val:  76.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7175%\n",
      "layer   2  Sparsity: 72.8754%\n",
      "layer   3  Sparsity: 71.1493%\n",
      "total_backward_count 1634930 real_backward_count 134944   8.254%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.593428/  1.143669, val:  67.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7361%\n",
      "layer   2  Sparsity: 73.1728%\n",
      "layer   3  Sparsity: 69.9190%\n",
      "total_backward_count 1644720 real_backward_count 135585   8.244%\n",
      "fc layer 3 self.abs_max_out: 2650.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.605597/  1.187743, val:  63.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7104%\n",
      "layer   2  Sparsity: 73.2080%\n",
      "layer   3  Sparsity: 70.2568%\n",
      "total_backward_count 1654510 real_backward_count 136223   8.233%\n",
      "fc layer 1 self.abs_max_out: 11853.0\n",
      "lif layer 1 self.abs_max_v: 22508.5\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.612987/  1.040958, val:  77.08%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7301%\n",
      "layer   2  Sparsity: 73.6934%\n",
      "layer   3  Sparsity: 71.7862%\n",
      "total_backward_count 1664300 real_backward_count 136829   8.221%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.606897/  1.061944, val:  77.08%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6948%\n",
      "layer   2  Sparsity: 73.5202%\n",
      "layer   3  Sparsity: 71.0386%\n",
      "total_backward_count 1674090 real_backward_count 137469   8.212%\n",
      "fc layer 1 self.abs_max_out: 11861.0\n",
      "lif layer 1 self.abs_max_v: 22524.5\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.605717/  1.119788, val:  71.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6972%\n",
      "layer   2  Sparsity: 73.6548%\n",
      "layer   3  Sparsity: 71.7288%\n",
      "total_backward_count 1683880 real_backward_count 138054   8.199%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.621635/  1.036391, val:  83.33%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7335%\n",
      "layer   2  Sparsity: 73.3957%\n",
      "layer   3  Sparsity: 71.5906%\n",
      "total_backward_count 1693670 real_backward_count 138711   8.190%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.612319/  1.112130, val:  77.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7326%\n",
      "layer   2  Sparsity: 73.4623%\n",
      "layer   3  Sparsity: 71.7653%\n",
      "total_backward_count 1703460 real_backward_count 139330   8.179%\n",
      "fc layer 2 self.abs_max_out: 4896.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.648700/  1.089234, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7334%\n",
      "layer   2  Sparsity: 73.5272%\n",
      "layer   3  Sparsity: 72.4754%\n",
      "total_backward_count 1713250 real_backward_count 139941   8.168%\n",
      "fc layer 1 self.abs_max_out: 11873.0\n",
      "lif layer 1 self.abs_max_v: 22548.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.616921/  1.076994, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6814%\n",
      "layer   2  Sparsity: 73.6117%\n",
      "layer   3  Sparsity: 72.3012%\n",
      "total_backward_count 1723040 real_backward_count 140575   8.159%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.628677/  1.103538, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7270%\n",
      "layer   2  Sparsity: 73.7913%\n",
      "layer   3  Sparsity: 71.3525%\n",
      "total_backward_count 1732830 real_backward_count 141191   8.148%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.620198/  1.035533, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7396%\n",
      "layer   2  Sparsity: 73.5458%\n",
      "layer   3  Sparsity: 72.1568%\n",
      "total_backward_count 1742620 real_backward_count 141798   8.137%\n",
      "fc layer 1 self.abs_max_out: 11881.0\n",
      "lif layer 1 self.abs_max_v: 22564.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.628545/  1.138824, val:  70.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7252%\n",
      "layer   2  Sparsity: 73.5653%\n",
      "layer   3  Sparsity: 72.0011%\n",
      "total_backward_count 1752410 real_backward_count 142402   8.126%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.643901/  1.088247, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7143%\n",
      "layer   2  Sparsity: 73.7116%\n",
      "layer   3  Sparsity: 72.3292%\n",
      "total_backward_count 1762200 real_backward_count 143024   8.116%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.634168/  1.082236, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7361%\n",
      "layer   2  Sparsity: 73.6040%\n",
      "layer   3  Sparsity: 72.6744%\n",
      "total_backward_count 1771990 real_backward_count 143627   8.105%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.627429/  1.079329, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7017%\n",
      "layer   2  Sparsity: 73.3974%\n",
      "layer   3  Sparsity: 72.7808%\n",
      "total_backward_count 1781780 real_backward_count 144227   8.095%\n",
      "fc layer 1 self.abs_max_out: 11888.0\n",
      "lif layer 1 self.abs_max_v: 22578.0\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.604828/  1.095764, val:  81.25%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7178%\n",
      "layer   2  Sparsity: 72.9793%\n",
      "layer   3  Sparsity: 72.2685%\n",
      "total_backward_count 1791570 real_backward_count 144870   8.086%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.633174/  1.187860, val:  66.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.6997%\n",
      "layer   2  Sparsity: 73.4267%\n",
      "layer   3  Sparsity: 72.1304%\n",
      "total_backward_count 1801360 real_backward_count 145473   8.076%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.623131/  1.108844, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7251%\n",
      "layer   2  Sparsity: 73.8052%\n",
      "layer   3  Sparsity: 72.3098%\n",
      "total_backward_count 1811150 real_backward_count 146045   8.064%\n",
      "fc layer 1 self.abs_max_out: 11899.0\n",
      "lif layer 1 self.abs_max_v: 22599.5\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.637436/  1.167494, val:  67.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7043%\n",
      "layer   2  Sparsity: 73.9027%\n",
      "layer   3  Sparsity: 72.0829%\n",
      "total_backward_count 1820940 real_backward_count 146652   8.054%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.614946/  1.072068, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7206%\n",
      "layer   2  Sparsity: 73.7576%\n",
      "layer   3  Sparsity: 71.3128%\n",
      "total_backward_count 1830730 real_backward_count 147240   8.043%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.623758/  1.076172, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7022%\n",
      "layer   2  Sparsity: 73.4407%\n",
      "layer   3  Sparsity: 70.4421%\n",
      "total_backward_count 1840520 real_backward_count 147845   8.033%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.625642/  1.108200, val:  75.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7107%\n",
      "layer   2  Sparsity: 73.8894%\n",
      "layer   3  Sparsity: 71.2524%\n",
      "total_backward_count 1850310 real_backward_count 148424   8.022%\n",
      "fc layer 1 self.abs_max_out: 11910.0\n",
      "lif layer 1 self.abs_max_v: 22621.0\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.687262/  1.065973, val:  76.25%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7271%\n",
      "layer   2  Sparsity: 73.8489%\n",
      "layer   3  Sparsity: 73.2947%\n",
      "total_backward_count 1860100 real_backward_count 149116   8.017%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.653017/  1.023982, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.7126%\n",
      "layer   2  Sparsity: 73.9942%\n",
      "layer   3  Sparsity: 73.2928%\n",
      "total_backward_count 1869890 real_backward_count 149754   8.009%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.641277/  1.075883, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7011%\n",
      "layer   2  Sparsity: 73.4491%\n",
      "layer   3  Sparsity: 72.7676%\n",
      "total_backward_count 1879680 real_backward_count 150343   7.998%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.631407/  1.071927, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7052%\n",
      "layer   2  Sparsity: 73.1487%\n",
      "layer   3  Sparsity: 72.0509%\n",
      "total_backward_count 1889470 real_backward_count 150967   7.990%\n",
      "fc layer 1 self.abs_max_out: 11921.0\n",
      "lif layer 1 self.abs_max_v: 22642.5\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.632525/  1.078644, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7161%\n",
      "layer   2  Sparsity: 73.3859%\n",
      "layer   3  Sparsity: 71.3421%\n",
      "total_backward_count 1899260 real_backward_count 151541   7.979%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.635042/  1.112235, val:  67.08%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7081%\n",
      "layer   2  Sparsity: 73.4285%\n",
      "layer   3  Sparsity: 71.7776%\n",
      "total_backward_count 1909050 real_backward_count 152114   7.968%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.629844/  1.080271, val:  74.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7338%\n",
      "layer   2  Sparsity: 73.3043%\n",
      "layer   3  Sparsity: 71.7997%\n",
      "total_backward_count 1918840 real_backward_count 152718   7.959%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.640788/  1.119727, val:  75.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7226%\n",
      "layer   2  Sparsity: 73.3347%\n",
      "layer   3  Sparsity: 73.3248%\n",
      "total_backward_count 1928630 real_backward_count 153337   7.951%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.685891/  1.083920, val:  75.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.6952%\n",
      "layer   2  Sparsity: 73.4636%\n",
      "layer   3  Sparsity: 73.1619%\n",
      "total_backward_count 1938420 real_backward_count 153937   7.941%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.663342/  1.114759, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7340%\n",
      "layer   2  Sparsity: 73.4936%\n",
      "layer   3  Sparsity: 73.6138%\n",
      "total_backward_count 1948210 real_backward_count 154532   7.932%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.649777/  1.063927, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7234%\n",
      "layer   2  Sparsity: 73.4459%\n",
      "layer   3  Sparsity: 72.8335%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6667dc318b413b8eb0dfc020bde909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÖ‚ñÖ‚ñá‚ñà‚ñÖ‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÖ‚ñÖ‚ñá‚ñà‚ñÖ‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.64978</td></tr><tr><td>val_acc_best</td><td>0.85</td></tr><tr><td>val_acc_now</td><td>0.77083</td></tr><tr><td>val_loss</td><td>1.06393</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-sweep-120</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ut70wscx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ut70wscx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_235433-ut70wscx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4ad1jewp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_041045-4ad1jewp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4ad1jewp' target=\"_blank\">swift-sweep-126</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4ad1jewp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4ad1jewp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251117_041053_957', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1221.0\n",
      "lif layer 1 self.abs_max_v: 1221.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "lif layer 2 self.abs_max_v: 1654.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "fc layer 1 self.abs_max_out: 1341.0\n",
      "lif layer 1 self.abs_max_v: 1707.5\n",
      "lif layer 2 self.abs_max_v: 1897.0\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "lif layer 1 self.abs_max_v: 1826.0\n",
      "fc layer 2 self.abs_max_out: 1691.0\n",
      "lif layer 2 self.abs_max_v: 2426.0\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "lif layer 2 self.abs_max_v: 2905.0\n",
      "fc layer 1 self.abs_max_out: 1413.0\n",
      "lif layer 2 self.abs_max_v: 3144.5\n",
      "fc layer 1 self.abs_max_out: 1612.0\n",
      "lif layer 1 self.abs_max_v: 2415.0\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "fc layer 2 self.abs_max_out: 2239.0\n",
      "lif layer 2 self.abs_max_v: 3674.5\n",
      "fc layer 1 self.abs_max_out: 1666.0\n",
      "lif layer 1 self.abs_max_v: 2786.0\n",
      "fc layer 1 self.abs_max_out: 1693.0\n",
      "fc layer 1 self.abs_max_out: 1767.0\n",
      "fc layer 3 self.abs_max_out: 779.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "fc layer 1 self.abs_max_out: 2251.0\n",
      "fc layer 2 self.abs_max_out: 2583.0\n",
      "fc layer 1 self.abs_max_out: 2291.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 1 self.abs_max_out: 2372.0\n",
      "lif layer 1 self.abs_max_v: 2931.0\n",
      "fc layer 1 self.abs_max_out: 2491.0\n",
      "lif layer 1 self.abs_max_v: 3090.5\n",
      "fc layer 1 self.abs_max_out: 2576.0\n",
      "fc layer 1 self.abs_max_out: 2654.0\n",
      "lif layer 1 self.abs_max_v: 3221.5\n",
      "lif layer 2 self.abs_max_v: 3779.5\n",
      "fc layer 1 self.abs_max_out: 3225.0\n",
      "lif layer 1 self.abs_max_v: 3225.0\n",
      "fc layer 2 self.abs_max_out: 2758.0\n",
      "fc layer 3 self.abs_max_out: 948.0\n",
      "lif layer 1 self.abs_max_v: 3319.0\n",
      "lif layer 1 self.abs_max_v: 3334.5\n",
      "lif layer 1 self.abs_max_v: 3449.0\n",
      "lif layer 1 self.abs_max_v: 3786.5\n",
      "lif layer 2 self.abs_max_v: 4108.0\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "fc layer 3 self.abs_max_out: 1002.0\n",
      "fc layer 3 self.abs_max_out: 1014.0\n",
      "fc layer 3 self.abs_max_out: 1153.0\n",
      "fc layer 3 self.abs_max_out: 1162.0\n",
      "lif layer 1 self.abs_max_v: 3876.5\n",
      "lif layer 1 self.abs_max_v: 3944.5\n",
      "lif layer 1 self.abs_max_v: 4258.0\n",
      "lif layer 2 self.abs_max_v: 4168.5\n",
      "lif layer 2 self.abs_max_v: 4177.5\n",
      "lif layer 1 self.abs_max_v: 4484.5\n",
      "lif layer 1 self.abs_max_v: 4931.0\n",
      "lif layer 2 self.abs_max_v: 4408.0\n",
      "lif layer 2 self.abs_max_v: 4470.5\n",
      "lif layer 2 self.abs_max_v: 4726.5\n",
      "fc layer 1 self.abs_max_out: 3363.0\n",
      "fc layer 1 self.abs_max_out: 3369.0\n",
      "lif layer 1 self.abs_max_v: 5139.0\n",
      "lif layer 1 self.abs_max_v: 5271.5\n",
      "fc layer 2 self.abs_max_out: 2855.0\n",
      "fc layer 2 self.abs_max_out: 3008.0\n",
      "lif layer 2 self.abs_max_v: 5020.5\n",
      "lif layer 1 self.abs_max_v: 5411.5\n",
      "lif layer 1 self.abs_max_v: 5915.5\n",
      "lif layer 1 self.abs_max_v: 5929.0\n",
      "fc layer 1 self.abs_max_out: 3463.0\n",
      "lif layer 1 self.abs_max_v: 5952.5\n",
      "lif layer 1 self.abs_max_v: 6135.5\n",
      "fc layer 1 self.abs_max_out: 3528.0\n",
      "fc layer 1 self.abs_max_out: 3624.0\n",
      "fc layer 1 self.abs_max_out: 3979.0\n",
      "lif layer 1 self.abs_max_v: 6879.0\n",
      "fc layer 2 self.abs_max_out: 3080.0\n",
      "fc layer 1 self.abs_max_out: 4035.0\n",
      "lif layer 1 self.abs_max_v: 7199.0\n",
      "lif layer 1 self.abs_max_v: 7437.5\n",
      "fc layer 1 self.abs_max_out: 4339.0\n",
      "lif layer 1 self.abs_max_v: 7482.0\n",
      "fc layer 1 self.abs_max_out: 4440.0\n",
      "lif layer 2 self.abs_max_v: 5059.5\n",
      "lif layer 1 self.abs_max_v: 7533.0\n",
      "fc layer 1 self.abs_max_out: 4697.0\n",
      "lif layer 1 self.abs_max_v: 8127.0\n",
      "lif layer 1 self.abs_max_v: 8577.0\n",
      "lif layer 2 self.abs_max_v: 5232.0\n",
      "fc layer 1 self.abs_max_out: 4699.0\n",
      "lif layer 1 self.abs_max_v: 8593.5\n",
      "lif layer 1 self.abs_max_v: 8737.0\n",
      "fc layer 1 self.abs_max_out: 4937.0\n",
      "lif layer 1 self.abs_max_v: 8785.5\n",
      "fc layer 1 self.abs_max_out: 4965.0\n",
      "lif layer 1 self.abs_max_v: 9064.0\n",
      "fc layer 1 self.abs_max_out: 5001.0\n",
      "fc layer 1 self.abs_max_out: 5620.0\n",
      "fc layer 2 self.abs_max_out: 3085.0\n",
      "lif layer 1 self.abs_max_v: 9356.0\n",
      "fc layer 3 self.abs_max_out: 1177.0\n",
      "lif layer 1 self.abs_max_v: 9447.0\n",
      "lif layer 1 self.abs_max_v: 9472.5\n",
      "lif layer 1 self.abs_max_v: 9664.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.969245/  2.019361, val:  43.75%, val_best:  43.75%, tr:  80.39%, tr_best:  80.39%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7038%\n",
      "layer   3  Sparsity: 70.2842%\n",
      "total_backward_count 9790 real_backward_count 3766  38.468%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 9813.5\n",
      "lif layer 1 self.abs_max_v: 10125.5\n",
      "fc layer 1 self.abs_max_out: 5694.0\n",
      "lif layer 1 self.abs_max_v: 10611.5\n",
      "lif layer 1 self.abs_max_v: 10765.0\n",
      "fc layer 1 self.abs_max_out: 5805.0\n",
      "lif layer 1 self.abs_max_v: 10833.5\n",
      "lif layer 1 self.abs_max_v: 10932.0\n",
      "fc layer 1 self.abs_max_out: 5838.0\n",
      "fc layer 1 self.abs_max_out: 6110.0\n",
      "lif layer 1 self.abs_max_v: 11479.0\n",
      "lif layer 1 self.abs_max_v: 11653.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.919693/  2.024899, val:  48.33%, val_best:  48.33%, tr:  93.36%, tr_best:  93.36%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 76.5988%\n",
      "layer   3  Sparsity: 69.8012%\n",
      "total_backward_count 19580 real_backward_count 6175  31.537%\n",
      "fc layer 2 self.abs_max_out: 3106.0\n",
      "fc layer 2 self.abs_max_out: 3110.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.926338/  2.021528, val:  55.00%, val_best:  55.00%, tr:  95.91%, tr_best:  95.91%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 76.8779%\n",
      "layer   3  Sparsity: 69.5111%\n",
      "total_backward_count 29370 real_backward_count 8306  28.281%\n",
      "fc layer 1 self.abs_max_out: 6332.0\n",
      "fc layer 1 self.abs_max_out: 6967.0\n",
      "lif layer 1 self.abs_max_v: 11952.5\n",
      "fc layer 2 self.abs_max_out: 3271.0\n",
      "lif layer 1 self.abs_max_v: 12001.5\n",
      "lif layer 1 self.abs_max_v: 12135.5\n",
      "lif layer 1 self.abs_max_v: 12858.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.919614/  2.022929, val:  52.50%, val_best:  55.00%, tr:  97.24%, tr_best:  97.24%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 76.8792%\n",
      "layer   3  Sparsity: 68.2193%\n",
      "total_backward_count 39160 real_backward_count 10152  25.924%\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.924606/  2.026882, val:  57.50%, val_best:  57.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 76.7163%\n",
      "layer   3  Sparsity: 67.5219%\n",
      "total_backward_count 48950 real_backward_count 11836  24.180%\n",
      "lif layer 1 self.abs_max_v: 12943.0\n",
      "lif layer 1 self.abs_max_v: 13177.5\n",
      "fc layer 1 self.abs_max_out: 7029.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.929871/  2.028208, val:  56.67%, val_best:  57.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.7920%\n",
      "layer   3  Sparsity: 66.2919%\n",
      "total_backward_count 58740 real_backward_count 13328  22.690%\n",
      "fc layer 1 self.abs_max_out: 7143.0\n",
      "lif layer 1 self.abs_max_v: 13205.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.924667/  2.022393, val:  51.67%, val_best:  57.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 76.1736%\n",
      "layer   3  Sparsity: 65.5749%\n",
      "total_backward_count 68530 real_backward_count 14796  21.591%\n",
      "lif layer 1 self.abs_max_v: 13342.5\n",
      "fc layer 1 self.abs_max_out: 7220.0\n",
      "fc layer 1 self.abs_max_out: 7342.0\n",
      "fc layer 1 self.abs_max_out: 7807.0\n",
      "lif layer 1 self.abs_max_v: 13831.0\n",
      "lif layer 1 self.abs_max_v: 13930.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.925744/  2.011294, val:  52.08%, val_best:  57.50%, tr:  99.39%, tr_best:  99.39%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.8510%\n",
      "layer   3  Sparsity: 65.3907%\n",
      "total_backward_count 78320 real_backward_count 16073  20.522%\n",
      "lif layer 1 self.abs_max_v: 14184.0\n",
      "lif layer 2 self.abs_max_v: 5488.5\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.923323/  2.005877, val:  59.17%, val_best:  59.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.5671%\n",
      "layer   3  Sparsity: 66.6026%\n",
      "total_backward_count 88110 real_backward_count 17438  19.791%\n",
      "lif layer 2 self.abs_max_v: 5515.5\n",
      "lif layer 2 self.abs_max_v: 5734.5\n",
      "lif layer 2 self.abs_max_v: 5752.5\n",
      "lif layer 2 self.abs_max_v: 5801.0\n",
      "lif layer 2 self.abs_max_v: 5806.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.922759/  2.024598, val:  52.92%, val_best:  59.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.8000%\n",
      "layer   3  Sparsity: 66.7322%\n",
      "total_backward_count 97900 real_backward_count 18597  18.996%\n",
      "fc layer 2 self.abs_max_out: 3283.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.916924/  2.019500, val:  56.25%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.2960%\n",
      "layer   3  Sparsity: 66.8753%\n",
      "total_backward_count 107690 real_backward_count 19770  18.358%\n",
      "fc layer 2 self.abs_max_out: 3314.0\n",
      "lif layer 1 self.abs_max_v: 14324.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.916571/  2.000843, val:  57.92%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.6895%\n",
      "layer   3  Sparsity: 67.2283%\n",
      "total_backward_count 117480 real_backward_count 20943  17.827%\n",
      "lif layer 2 self.abs_max_v: 5809.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.909571/  2.008121, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9840%\n",
      "layer   3  Sparsity: 67.2002%\n",
      "total_backward_count 127270 real_backward_count 22064  17.336%\n",
      "fc layer 2 self.abs_max_out: 3402.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.907745/  2.010317, val:  51.67%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.1295%\n",
      "layer   3  Sparsity: 68.1253%\n",
      "total_backward_count 137060 real_backward_count 23144  16.886%\n",
      "fc layer 2 self.abs_max_out: 3492.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.907156/  1.999195, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.1107%\n",
      "layer   3  Sparsity: 68.2855%\n",
      "total_backward_count 146850 real_backward_count 24186  16.470%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.910205/  2.010703, val:  60.00%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.4600%\n",
      "layer   3  Sparsity: 68.0929%\n",
      "total_backward_count 156640 real_backward_count 25253  16.122%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.904547/  1.990712, val:  61.25%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.7733%\n",
      "layer   3  Sparsity: 68.2434%\n",
      "total_backward_count 166430 real_backward_count 26280  15.790%\n",
      "lif layer 1 self.abs_max_v: 14831.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.902099/  2.000080, val:  60.83%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.0705%\n",
      "layer   3  Sparsity: 68.4110%\n",
      "total_backward_count 176220 real_backward_count 27265  15.472%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.898267/  1.979262, val:  61.25%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.3836%\n",
      "layer   3  Sparsity: 67.6372%\n",
      "total_backward_count 186010 real_backward_count 28244  15.184%\n",
      "fc layer 1 self.abs_max_out: 7851.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.885816/  1.979184, val:  59.58%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.6931%\n",
      "layer   3  Sparsity: 68.3942%\n",
      "total_backward_count 195800 real_backward_count 29186  14.906%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.886847/  1.994922, val:  62.08%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.1743%\n",
      "layer   3  Sparsity: 69.4210%\n",
      "total_backward_count 205590 real_backward_count 30122  14.651%\n",
      "fc layer 1 self.abs_max_out: 7889.0\n",
      "fc layer 1 self.abs_max_out: 8082.0\n",
      "lif layer 1 self.abs_max_v: 15421.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.892672/  1.993248, val:  63.75%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.3308%\n",
      "layer   3  Sparsity: 69.1396%\n",
      "total_backward_count 215380 real_backward_count 31091  14.435%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.888676/  1.967553, val:  63.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.3575%\n",
      "layer   3  Sparsity: 69.0821%\n",
      "total_backward_count 225170 real_backward_count 32058  14.237%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.879076/  1.970905, val:  65.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.6705%\n",
      "layer   3  Sparsity: 69.2514%\n",
      "total_backward_count 234960 real_backward_count 32994  14.042%\n",
      "fc layer 1 self.abs_max_out: 8296.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.873241/  1.957723, val:  66.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.5926%\n",
      "layer   3  Sparsity: 68.9789%\n",
      "total_backward_count 244750 real_backward_count 33883  13.844%\n",
      "lif layer 1 self.abs_max_v: 15674.5\n",
      "fc layer 1 self.abs_max_out: 8473.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.867224/  1.973359, val:  65.42%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.5289%\n",
      "layer   3  Sparsity: 69.1412%\n",
      "total_backward_count 254540 real_backward_count 34770  13.660%\n",
      "lif layer 2 self.abs_max_v: 5922.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.865598/  1.975608, val:  63.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.1438%\n",
      "layer   3  Sparsity: 69.2000%\n",
      "total_backward_count 264330 real_backward_count 35593  13.465%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.865272/  1.961719, val:  65.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.2346%\n",
      "layer   3  Sparsity: 68.8902%\n",
      "total_backward_count 274120 real_backward_count 36386  13.274%\n",
      "lif layer 2 self.abs_max_v: 6006.5\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.862411/  1.963405, val:  68.33%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.2909%\n",
      "layer   3  Sparsity: 69.4178%\n",
      "total_backward_count 283910 real_backward_count 37153  13.086%\n",
      "fc layer 1 self.abs_max_out: 8488.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.859020/  1.965459, val:  57.92%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8665%\n",
      "layer   3  Sparsity: 69.4706%\n",
      "total_backward_count 293700 real_backward_count 37957  12.924%\n",
      "lif layer 1 self.abs_max_v: 15981.5\n",
      "lif layer 1 self.abs_max_v: 16003.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.857472/  1.959175, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5926%\n",
      "layer   3  Sparsity: 69.6511%\n",
      "total_backward_count 303490 real_backward_count 38752  12.769%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.854034/  1.959569, val:  67.92%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7959%\n",
      "layer   3  Sparsity: 70.3228%\n",
      "total_backward_count 313280 real_backward_count 39467  12.598%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.844290/  1.945984, val:  63.75%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5056%\n",
      "layer   3  Sparsity: 70.4071%\n",
      "total_backward_count 323070 real_backward_count 40155  12.429%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.840259/  1.940650, val:  64.17%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8790%\n",
      "layer   3  Sparsity: 70.5808%\n",
      "total_backward_count 332860 real_backward_count 40896  12.286%\n",
      "fc layer 2 self.abs_max_out: 3551.0\n",
      "lif layer 2 self.abs_max_v: 6053.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.842808/  1.954988, val:  65.00%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1843%\n",
      "layer   3  Sparsity: 70.3181%\n",
      "total_backward_count 342650 real_backward_count 41580  12.135%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.842769/  1.944666, val:  69.58%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9907%\n",
      "layer   3  Sparsity: 70.3158%\n",
      "total_backward_count 352440 real_backward_count 42252  11.988%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.845270/  1.957455, val:  67.50%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.0357%\n",
      "layer   3  Sparsity: 70.6530%\n",
      "total_backward_count 362230 real_backward_count 42916  11.848%\n",
      "fc layer 1 self.abs_max_out: 8567.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.838902/  1.955490, val:  60.83%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3185%\n",
      "layer   3  Sparsity: 70.6870%\n",
      "total_backward_count 372020 real_backward_count 43556  11.708%\n",
      "fc layer 1 self.abs_max_out: 8710.0\n",
      "fc layer 1 self.abs_max_out: 9208.0\n",
      "fc layer 1 self.abs_max_out: 9553.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.834228/  1.938832, val:  67.92%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2043%\n",
      "layer   3  Sparsity: 70.9649%\n",
      "total_backward_count 381810 real_backward_count 44254  11.591%\n",
      "lif layer 2 self.abs_max_v: 6055.5\n",
      "lif layer 1 self.abs_max_v: 16062.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.833677/  1.962742, val:  67.08%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8758%\n",
      "layer   3  Sparsity: 71.1357%\n",
      "total_backward_count 391600 real_backward_count 44881  11.461%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.842569/  1.956659, val:  62.50%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8124%\n",
      "layer   3  Sparsity: 70.7946%\n",
      "total_backward_count 401390 real_backward_count 45529  11.343%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.831988/  1.943671, val:  69.17%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9632%\n",
      "layer   3  Sparsity: 70.5881%\n",
      "total_backward_count 411180 real_backward_count 46146  11.223%\n",
      "lif layer 2 self.abs_max_v: 6137.5\n",
      "lif layer 2 self.abs_max_v: 6166.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.821571/  1.938151, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9299%\n",
      "layer   3  Sparsity: 70.8253%\n",
      "total_backward_count 420970 real_backward_count 46722  11.099%\n",
      "lif layer 1 self.abs_max_v: 16131.5\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.822728/  1.949734, val:  67.92%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3353%\n",
      "layer   3  Sparsity: 71.9176%\n",
      "total_backward_count 430760 real_backward_count 47288  10.978%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.823777/  1.938465, val:  69.58%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9436%\n",
      "layer   3  Sparsity: 71.3411%\n",
      "total_backward_count 440550 real_backward_count 47852  10.862%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.829307/  1.948134, val:  67.50%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3140%\n",
      "layer   3  Sparsity: 71.5616%\n",
      "total_backward_count 450340 real_backward_count 48436  10.755%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.820985/  1.935039, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6970%\n",
      "layer   3  Sparsity: 71.7042%\n",
      "total_backward_count 460130 real_backward_count 48962  10.641%\n",
      "fc layer 2 self.abs_max_out: 3560.0\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.819938/  1.938841, val:  67.08%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5332%\n",
      "layer   3  Sparsity: 71.2467%\n",
      "total_backward_count 469920 real_backward_count 49479  10.529%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.818883/  1.946146, val:  71.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6753%\n",
      "layer   3  Sparsity: 71.9229%\n",
      "total_backward_count 479710 real_backward_count 49996  10.422%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.815785/  1.948372, val:  63.33%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2675%\n",
      "layer   3  Sparsity: 72.3258%\n",
      "total_backward_count 489500 real_backward_count 50552  10.327%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.817637/  1.943257, val:  66.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8468%\n",
      "layer   3  Sparsity: 72.3309%\n",
      "total_backward_count 499290 real_backward_count 51056  10.226%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.817645/  1.932017, val:  70.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9396%\n",
      "layer   3  Sparsity: 72.0079%\n",
      "total_backward_count 509080 real_backward_count 51569  10.130%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.813444/  1.941985, val:  64.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6126%\n",
      "layer   3  Sparsity: 71.8311%\n",
      "total_backward_count 518870 real_backward_count 52087  10.039%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.812673/  1.934940, val:  60.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4991%\n",
      "layer   3  Sparsity: 71.9352%\n",
      "total_backward_count 528660 real_backward_count 52576   9.945%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.809608/  1.929435, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5630%\n",
      "layer   3  Sparsity: 72.3153%\n",
      "total_backward_count 538450 real_backward_count 53032   9.849%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.809378/  1.927325, val:  70.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3502%\n",
      "layer   3  Sparsity: 72.2731%\n",
      "total_backward_count 548240 real_backward_count 53501   9.759%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.800591/  1.926610, val:  65.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4499%\n",
      "layer   3  Sparsity: 72.5853%\n",
      "total_backward_count 558030 real_backward_count 53968   9.671%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.795972/  1.929085, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1945%\n",
      "layer   3  Sparsity: 72.7323%\n",
      "total_backward_count 567820 real_backward_count 54387   9.578%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.799572/  1.924819, val:  70.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2336%\n",
      "layer   3  Sparsity: 72.5932%\n",
      "total_backward_count 577610 real_backward_count 54803   9.488%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.794752/  1.927287, val:  65.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2815%\n",
      "layer   3  Sparsity: 72.3043%\n",
      "total_backward_count 587400 real_backward_count 55198   9.397%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.794080/  1.915013, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1710%\n",
      "layer   3  Sparsity: 72.5530%\n",
      "total_backward_count 597190 real_backward_count 55609   9.312%\n",
      "lif layer 1 self.abs_max_v: 16150.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.787727/  1.922878, val:  64.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3757%\n",
      "layer   3  Sparsity: 72.8986%\n",
      "total_backward_count 606980 real_backward_count 56005   9.227%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.789739/  1.924552, val:  71.25%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3351%\n",
      "layer   3  Sparsity: 73.1534%\n",
      "total_backward_count 616770 real_backward_count 56382   9.141%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.785722/  1.926515, val:  69.17%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5178%\n",
      "layer   3  Sparsity: 73.3453%\n",
      "total_backward_count 626560 real_backward_count 56745   9.057%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.787497/  1.916266, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1804%\n",
      "layer   3  Sparsity: 73.0975%\n",
      "total_backward_count 636350 real_backward_count 57104   8.974%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.780264/  1.917534, val:  72.08%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3356%\n",
      "layer   3  Sparsity: 72.7073%\n",
      "total_backward_count 646140 real_backward_count 57467   8.894%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.778929/  1.920940, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1653%\n",
      "layer   3  Sparsity: 73.0260%\n",
      "total_backward_count 655930 real_backward_count 57807   8.813%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.780106/  1.918594, val:  67.92%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1848%\n",
      "layer   3  Sparsity: 72.7515%\n",
      "total_backward_count 665720 real_backward_count 58137   8.733%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.779649/  1.914107, val:  72.92%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1166%\n",
      "layer   3  Sparsity: 72.2659%\n",
      "total_backward_count 675510 real_backward_count 58437   8.651%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.776404/  1.913334, val:  67.50%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1881%\n",
      "layer   3  Sparsity: 72.4625%\n",
      "total_backward_count 685300 real_backward_count 58714   8.568%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.774625/  1.906171, val:  70.83%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2113%\n",
      "layer   3  Sparsity: 72.6991%\n",
      "total_backward_count 695090 real_backward_count 59051   8.495%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.769746/  1.911795, val:  67.50%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2724%\n",
      "layer   3  Sparsity: 73.0925%\n",
      "total_backward_count 704880 real_backward_count 59384   8.425%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.765090/  1.899901, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5162%\n",
      "layer   3  Sparsity: 73.3018%\n",
      "total_backward_count 714670 real_backward_count 59656   8.347%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.762170/  1.906757, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5434%\n",
      "layer   3  Sparsity: 73.3438%\n",
      "total_backward_count 724460 real_backward_count 59926   8.272%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.767183/  1.896253, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5289%\n",
      "layer   3  Sparsity: 73.6897%\n",
      "total_backward_count 734250 real_backward_count 60230   8.203%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.764683/  1.899073, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3805%\n",
      "layer   3  Sparsity: 73.9243%\n",
      "total_backward_count 744040 real_backward_count 60508   8.132%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.764732/  1.902197, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5887%\n",
      "layer   3  Sparsity: 73.6948%\n",
      "total_backward_count 753830 real_backward_count 60785   8.063%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.765625/  1.902642, val:  68.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1817%\n",
      "layer   3  Sparsity: 73.6498%\n",
      "total_backward_count 763620 real_backward_count 61047   7.994%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.766218/  1.906356, val:  68.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1341%\n",
      "layer   3  Sparsity: 73.9392%\n",
      "total_backward_count 773410 real_backward_count 61307   7.927%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.764281/  1.898073, val:  73.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5401%\n",
      "layer   3  Sparsity: 73.9513%\n",
      "total_backward_count 783200 real_backward_count 61518   7.855%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.764813/  1.905787, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2487%\n",
      "layer   3  Sparsity: 73.4425%\n",
      "total_backward_count 792990 real_backward_count 61769   7.789%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.765507/  1.899757, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7174%\n",
      "layer   3  Sparsity: 73.7191%\n",
      "total_backward_count 802780 real_backward_count 62018   7.725%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.758853/  1.897127, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7292%\n",
      "layer   3  Sparsity: 73.2209%\n",
      "total_backward_count 812570 real_backward_count 62244   7.660%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.759288/  1.903542, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5507%\n",
      "layer   3  Sparsity: 73.7545%\n",
      "total_backward_count 822360 real_backward_count 62461   7.595%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.764708/  1.905511, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2857%\n",
      "layer   3  Sparsity: 73.7076%\n",
      "total_backward_count 832150 real_backward_count 62717   7.537%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.762969/  1.903195, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4710%\n",
      "layer   3  Sparsity: 73.7755%\n",
      "total_backward_count 841940 real_backward_count 62926   7.474%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.761897/  1.918733, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4043%\n",
      "layer   3  Sparsity: 74.2361%\n",
      "total_backward_count 851730 real_backward_count 63097   7.408%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.762930/  1.918034, val:  68.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4859%\n",
      "layer   3  Sparsity: 73.7461%\n",
      "total_backward_count 861520 real_backward_count 63302   7.348%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.756075/  1.898021, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4672%\n",
      "layer   3  Sparsity: 73.8442%\n",
      "total_backward_count 871310 real_backward_count 63505   7.288%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.751791/  1.898772, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4199%\n",
      "layer   3  Sparsity: 74.0568%\n",
      "total_backward_count 881100 real_backward_count 63701   7.230%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.753853/  1.894754, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4840%\n",
      "layer   3  Sparsity: 74.2740%\n",
      "total_backward_count 890890 real_backward_count 63896   7.172%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.751607/  1.895730, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7031%\n",
      "layer   3  Sparsity: 74.3491%\n",
      "total_backward_count 900680 real_backward_count 64083   7.115%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.746204/  1.889835, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8345%\n",
      "layer   3  Sparsity: 74.3196%\n",
      "total_backward_count 910470 real_backward_count 64287   7.061%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.745566/  1.890870, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5806%\n",
      "layer   3  Sparsity: 74.4199%\n",
      "total_backward_count 920260 real_backward_count 64483   7.007%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.742062/  1.888507, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6001%\n",
      "layer   3  Sparsity: 74.1943%\n",
      "total_backward_count 930050 real_backward_count 64648   6.951%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.742614/  1.893978, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8161%\n",
      "layer   3  Sparsity: 73.9507%\n",
      "total_backward_count 939840 real_backward_count 64829   6.898%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.743240/  1.887639, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7474%\n",
      "layer   3  Sparsity: 73.9364%\n",
      "total_backward_count 949630 real_backward_count 64999   6.845%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.738067/  1.884074, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6893%\n",
      "layer   3  Sparsity: 73.9217%\n",
      "total_backward_count 959420 real_backward_count 65171   6.793%\n",
      "fc layer 2 self.abs_max_out: 3604.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.743863/  1.888188, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2684%\n",
      "layer   3  Sparsity: 73.9416%\n",
      "total_backward_count 969210 real_backward_count 65323   6.740%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.742043/  1.886141, val:  69.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2870%\n",
      "layer   3  Sparsity: 73.9155%\n",
      "total_backward_count 979000 real_backward_count 65502   6.691%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.739794/  1.884275, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4722%\n",
      "layer   3  Sparsity: 74.0695%\n",
      "total_backward_count 988790 real_backward_count 65630   6.637%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.738925/  1.883674, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5292%\n",
      "layer   3  Sparsity: 73.6498%\n",
      "total_backward_count 998580 real_backward_count 65778   6.587%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.738816/  1.891041, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5169%\n",
      "layer   3  Sparsity: 74.0428%\n",
      "total_backward_count 1008370 real_backward_count 65904   6.536%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.738128/  1.888465, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.0102%\n",
      "layer   3  Sparsity: 74.2494%\n",
      "total_backward_count 1018160 real_backward_count 66016   6.484%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.735679/  1.881728, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8843%\n",
      "layer   3  Sparsity: 74.3310%\n",
      "total_backward_count 1027950 real_backward_count 66139   6.434%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.733093/  1.884317, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7687%\n",
      "layer   3  Sparsity: 73.8212%\n",
      "total_backward_count 1037740 real_backward_count 66251   6.384%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.736253/  1.889378, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6014%\n",
      "layer   3  Sparsity: 74.1789%\n",
      "total_backward_count 1047530 real_backward_count 66387   6.337%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.737910/  1.891484, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6606%\n",
      "layer   3  Sparsity: 74.1428%\n",
      "total_backward_count 1057320 real_backward_count 66517   6.291%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.736465/  1.888320, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9354%\n",
      "layer   3  Sparsity: 74.3807%\n",
      "total_backward_count 1067110 real_backward_count 66584   6.240%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.730891/  1.893909, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8863%\n",
      "layer   3  Sparsity: 74.3112%\n",
      "total_backward_count 1076900 real_backward_count 66668   6.191%\n",
      "fc layer 2 self.abs_max_out: 3732.0\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.735149/  1.895910, val:  69.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6733%\n",
      "layer   3  Sparsity: 74.7089%\n",
      "total_backward_count 1086690 real_backward_count 66784   6.146%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.737572/  1.897178, val:  69.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7082%\n",
      "layer   3  Sparsity: 74.6416%\n",
      "total_backward_count 1096480 real_backward_count 66914   6.103%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.734885/  1.885779, val:  69.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6217%\n",
      "layer   3  Sparsity: 74.4387%\n",
      "total_backward_count 1106270 real_backward_count 67017   6.058%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.732362/  1.890676, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4766%\n",
      "layer   3  Sparsity: 74.2009%\n",
      "total_backward_count 1116060 real_backward_count 67127   6.015%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.731941/  1.881558, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4447%\n",
      "layer   3  Sparsity: 74.4535%\n",
      "total_backward_count 1125850 real_backward_count 67239   5.972%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.729094/  1.895490, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6210%\n",
      "layer   3  Sparsity: 74.8825%\n",
      "total_backward_count 1135640 real_backward_count 67333   5.929%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.732469/  1.885948, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6729%\n",
      "layer   3  Sparsity: 74.7111%\n",
      "total_backward_count 1145430 real_backward_count 67439   5.888%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.729154/  1.883482, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7826%\n",
      "layer   3  Sparsity: 74.5565%\n",
      "total_backward_count 1155220 real_backward_count 67525   5.845%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.728480/  1.894698, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8529%\n",
      "layer   3  Sparsity: 74.9671%\n",
      "total_backward_count 1165010 real_backward_count 67603   5.803%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.726678/  1.880553, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6573%\n",
      "layer   3  Sparsity: 74.9210%\n",
      "total_backward_count 1174800 real_backward_count 67692   5.762%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.722105/  1.877329, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5465%\n",
      "layer   3  Sparsity: 74.8069%\n",
      "total_backward_count 1184590 real_backward_count 67776   5.721%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.723264/  1.884681, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6038%\n",
      "layer   3  Sparsity: 74.7399%\n",
      "total_backward_count 1194380 real_backward_count 67859   5.682%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.722894/  1.876824, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5976%\n",
      "layer   3  Sparsity: 74.8973%\n",
      "total_backward_count 1204170 real_backward_count 67941   5.642%\n",
      "lif layer 2 self.abs_max_v: 6228.0\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.725905/  1.879686, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4903%\n",
      "layer   3  Sparsity: 74.8689%\n",
      "total_backward_count 1213960 real_backward_count 68024   5.603%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.721874/  1.883755, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5912%\n",
      "layer   3  Sparsity: 74.8451%\n",
      "total_backward_count 1223750 real_backward_count 68109   5.566%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.726290/  1.888469, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8304%\n",
      "layer   3  Sparsity: 74.9485%\n",
      "total_backward_count 1233540 real_backward_count 68189   5.528%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.729427/  1.891252, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7299%\n",
      "layer   3  Sparsity: 75.1138%\n",
      "total_backward_count 1243330 real_backward_count 68243   5.489%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.727216/  1.886944, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6065%\n",
      "layer   3  Sparsity: 75.0317%\n",
      "total_backward_count 1253120 real_backward_count 68337   5.453%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.724041/  1.882192, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7016%\n",
      "layer   3  Sparsity: 75.2142%\n",
      "total_backward_count 1262910 real_backward_count 68397   5.416%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.721775/  1.881081, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7573%\n",
      "layer   3  Sparsity: 75.3446%\n",
      "total_backward_count 1272700 real_backward_count 68464   5.379%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.719133/  1.880415, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7645%\n",
      "layer   3  Sparsity: 75.0107%\n",
      "total_backward_count 1282490 real_backward_count 68532   5.344%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.722411/  1.883025, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5893%\n",
      "layer   3  Sparsity: 74.5230%\n",
      "total_backward_count 1292280 real_backward_count 68615   5.310%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.719314/  1.874729, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4900%\n",
      "layer   3  Sparsity: 74.7098%\n",
      "total_backward_count 1302070 real_backward_count 68673   5.274%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.719888/  1.875942, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6381%\n",
      "layer   3  Sparsity: 74.8354%\n",
      "total_backward_count 1311860 real_backward_count 68736   5.240%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.715948/  1.876375, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6774%\n",
      "layer   3  Sparsity: 74.9289%\n",
      "total_backward_count 1321650 real_backward_count 68793   5.205%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.715143/  1.881643, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5129%\n",
      "layer   3  Sparsity: 74.9887%\n",
      "total_backward_count 1331440 real_backward_count 68891   5.174%\n",
      "fc layer 2 self.abs_max_out: 3743.0\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.716937/  1.883952, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4550%\n",
      "layer   3  Sparsity: 75.2467%\n",
      "total_backward_count 1341230 real_backward_count 68960   5.142%\n",
      "fc layer 2 self.abs_max_out: 3837.0\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.718450/  1.878622, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4493%\n",
      "layer   3  Sparsity: 75.2787%\n",
      "total_backward_count 1351020 real_backward_count 69050   5.111%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.717739/  1.882357, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2740%\n",
      "layer   3  Sparsity: 75.4142%\n",
      "total_backward_count 1360810 real_backward_count 69112   5.079%\n",
      "fc layer 2 self.abs_max_out: 3850.0\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.719341/  1.883357, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4293%\n",
      "layer   3  Sparsity: 75.3702%\n",
      "total_backward_count 1370600 real_backward_count 69188   5.048%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.716151/  1.883935, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3322%\n",
      "layer   3  Sparsity: 75.1576%\n",
      "total_backward_count 1380390 real_backward_count 69254   5.017%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.711569/  1.879316, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2980%\n",
      "layer   3  Sparsity: 75.1962%\n",
      "total_backward_count 1390180 real_backward_count 69304   4.985%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.710959/  1.878344, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5349%\n",
      "layer   3  Sparsity: 75.1196%\n",
      "total_backward_count 1399970 real_backward_count 69368   4.955%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.706749/  1.882393, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6742%\n",
      "layer   3  Sparsity: 75.1947%\n",
      "total_backward_count 1409760 real_backward_count 69423   4.924%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.709359/  1.880133, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5209%\n",
      "layer   3  Sparsity: 75.3799%\n",
      "total_backward_count 1419550 real_backward_count 69488   4.895%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.709516/  1.874164, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4139%\n",
      "layer   3  Sparsity: 75.2472%\n",
      "total_backward_count 1429340 real_backward_count 69549   4.866%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.705119/  1.877644, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4737%\n",
      "layer   3  Sparsity: 75.2241%\n",
      "total_backward_count 1439130 real_backward_count 69609   4.837%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.705553/  1.881899, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5250%\n",
      "layer   3  Sparsity: 75.6255%\n",
      "total_backward_count 1448920 real_backward_count 69658   4.808%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.704914/  1.878331, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3525%\n",
      "layer   3  Sparsity: 75.4153%\n",
      "total_backward_count 1458710 real_backward_count 69716   4.779%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.706458/  1.875736, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5813%\n",
      "layer   3  Sparsity: 75.2192%\n",
      "total_backward_count 1468500 real_backward_count 69773   4.751%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.704534/  1.868246, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7519%\n",
      "layer   3  Sparsity: 75.2051%\n",
      "total_backward_count 1478290 real_backward_count 69845   4.725%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.706089/  1.871715, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4587%\n",
      "layer   3  Sparsity: 75.1821%\n",
      "total_backward_count 1488080 real_backward_count 69907   4.698%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.707219/  1.876644, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5861%\n",
      "layer   3  Sparsity: 75.3726%\n",
      "total_backward_count 1497870 real_backward_count 69950   4.670%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.708777/  1.870200, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7226%\n",
      "layer   3  Sparsity: 75.3549%\n",
      "total_backward_count 1507660 real_backward_count 69993   4.642%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.703471/  1.867700, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5822%\n",
      "layer   3  Sparsity: 75.3683%\n",
      "total_backward_count 1517450 real_backward_count 70043   4.616%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.707852/  1.871279, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4079%\n",
      "layer   3  Sparsity: 75.4793%\n",
      "total_backward_count 1527240 real_backward_count 70088   4.589%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.707511/  1.872320, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5169%\n",
      "layer   3  Sparsity: 75.2973%\n",
      "total_backward_count 1537030 real_backward_count 70136   4.563%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.709197/  1.873035, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5626%\n",
      "layer   3  Sparsity: 74.9842%\n",
      "total_backward_count 1546820 real_backward_count 70181   4.537%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.710471/  1.875197, val:  70.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5747%\n",
      "layer   3  Sparsity: 75.1555%\n",
      "total_backward_count 1556610 real_backward_count 70234   4.512%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.707150/  1.876717, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4952%\n",
      "layer   3  Sparsity: 75.5322%\n",
      "total_backward_count 1566400 real_backward_count 70286   4.487%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.707691/  1.877124, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5595%\n",
      "layer   3  Sparsity: 75.4894%\n",
      "total_backward_count 1576190 real_backward_count 70320   4.461%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.706739/  1.880792, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4418%\n",
      "layer   3  Sparsity: 75.5486%\n",
      "total_backward_count 1585980 real_backward_count 70366   4.437%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.707246/  1.879227, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7124%\n",
      "layer   3  Sparsity: 75.4594%\n",
      "total_backward_count 1595770 real_backward_count 70402   4.412%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.705228/  1.875448, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8145%\n",
      "layer   3  Sparsity: 75.5055%\n",
      "total_backward_count 1605560 real_backward_count 70422   4.386%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.701194/  1.864114, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9687%\n",
      "layer   3  Sparsity: 75.4247%\n",
      "total_backward_count 1615350 real_backward_count 70463   4.362%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.697102/  1.870552, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.0702%\n",
      "layer   3  Sparsity: 75.8277%\n",
      "total_backward_count 1625140 real_backward_count 70497   4.338%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.698408/  1.873303, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.0979%\n",
      "layer   3  Sparsity: 75.6804%\n",
      "total_backward_count 1634930 real_backward_count 70535   4.314%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.700657/  1.875667, val:  68.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9020%\n",
      "layer   3  Sparsity: 75.8036%\n",
      "total_backward_count 1644720 real_backward_count 70574   4.291%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.700720/  1.871143, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7298%\n",
      "layer   3  Sparsity: 76.0066%\n",
      "total_backward_count 1654510 real_backward_count 70599   4.267%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.698741/  1.878571, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8143%\n",
      "layer   3  Sparsity: 76.0302%\n",
      "total_backward_count 1664300 real_backward_count 70636   4.244%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.700408/  1.879187, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9962%\n",
      "layer   3  Sparsity: 76.0584%\n",
      "total_backward_count 1674090 real_backward_count 70668   4.221%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.701283/  1.873929, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8577%\n",
      "layer   3  Sparsity: 75.9533%\n",
      "total_backward_count 1683880 real_backward_count 70693   4.198%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.701503/  1.870497, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8811%\n",
      "layer   3  Sparsity: 75.7144%\n",
      "total_backward_count 1693670 real_backward_count 70717   4.175%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.705852/  1.878442, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9110%\n",
      "layer   3  Sparsity: 75.7812%\n",
      "total_backward_count 1703460 real_backward_count 70752   4.153%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.704126/  1.872950, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7567%\n",
      "layer   3  Sparsity: 75.8368%\n",
      "total_backward_count 1713250 real_backward_count 70779   4.131%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.699717/  1.871399, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6356%\n",
      "layer   3  Sparsity: 75.8668%\n",
      "total_backward_count 1723040 real_backward_count 70795   4.109%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.698942/  1.871087, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7937%\n",
      "layer   3  Sparsity: 75.8724%\n",
      "total_backward_count 1732830 real_backward_count 70821   4.087%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.699645/  1.875621, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8811%\n",
      "layer   3  Sparsity: 75.8980%\n",
      "total_backward_count 1742620 real_backward_count 70859   4.066%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.701821/  1.870198, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9239%\n",
      "layer   3  Sparsity: 75.9863%\n",
      "total_backward_count 1752410 real_backward_count 70881   4.045%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.701614/  1.871057, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9501%\n",
      "layer   3  Sparsity: 76.1508%\n",
      "total_backward_count 1762200 real_backward_count 70909   4.024%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.700515/  1.873135, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8770%\n",
      "layer   3  Sparsity: 76.3734%\n",
      "total_backward_count 1771990 real_backward_count 70928   4.003%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.699996/  1.877514, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7237%\n",
      "layer   3  Sparsity: 76.2523%\n",
      "total_backward_count 1781780 real_backward_count 70957   3.982%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.701870/  1.875790, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4915%\n",
      "layer   3  Sparsity: 76.6137%\n",
      "total_backward_count 1791570 real_backward_count 70984   3.962%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.704113/  1.877869, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3409%\n",
      "layer   3  Sparsity: 76.5403%\n",
      "total_backward_count 1801360 real_backward_count 71010   3.942%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.702392/  1.872731, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2707%\n",
      "layer   3  Sparsity: 76.4833%\n",
      "total_backward_count 1811150 real_backward_count 71027   3.922%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.699221/  1.868589, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4015%\n",
      "layer   3  Sparsity: 76.4543%\n",
      "total_backward_count 1820940 real_backward_count 71051   3.902%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.698993/  1.874123, val:  68.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5640%\n",
      "layer   3  Sparsity: 76.3361%\n",
      "total_backward_count 1830730 real_backward_count 71078   3.882%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.699380/  1.871330, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7104%\n",
      "layer   3  Sparsity: 76.1470%\n",
      "total_backward_count 1840520 real_backward_count 71122   3.864%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.697545/  1.872987, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7472%\n",
      "layer   3  Sparsity: 76.0907%\n",
      "total_backward_count 1850310 real_backward_count 71134   3.844%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.697315/  1.874163, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7159%\n",
      "layer   3  Sparsity: 76.0131%\n",
      "total_backward_count 1860100 real_backward_count 71156   3.825%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.697363/  1.873377, val:  70.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6153%\n",
      "layer   3  Sparsity: 75.9439%\n",
      "total_backward_count 1869890 real_backward_count 71190   3.807%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.698068/  1.882301, val:  70.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6850%\n",
      "layer   3  Sparsity: 76.2899%\n",
      "total_backward_count 1879680 real_backward_count 71212   3.789%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.694198/  1.873959, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6851%\n",
      "layer   3  Sparsity: 76.2320%\n",
      "total_backward_count 1889470 real_backward_count 71247   3.771%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.696843/  1.870220, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8886%\n",
      "layer   3  Sparsity: 76.1121%\n",
      "total_backward_count 1899260 real_backward_count 71275   3.753%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.695749/  1.870908, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9308%\n",
      "layer   3  Sparsity: 75.9575%\n",
      "total_backward_count 1909050 real_backward_count 71289   3.734%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.693245/  1.868987, val:  70.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.9228%\n",
      "layer   3  Sparsity: 75.9353%\n",
      "total_backward_count 1918840 real_backward_count 71317   3.717%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.696555/  1.877406, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8456%\n",
      "layer   3  Sparsity: 75.8967%\n",
      "total_backward_count 1928630 real_backward_count 71336   3.699%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.696181/  1.873947, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8632%\n",
      "layer   3  Sparsity: 75.8744%\n",
      "total_backward_count 1938420 real_backward_count 71347   3.681%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.696339/  1.877688, val:  70.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7662%\n",
      "layer   3  Sparsity: 76.0083%\n",
      "total_backward_count 1948210 real_backward_count 71367   3.663%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.697235/  1.875569, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8709%\n",
      "layer   3  Sparsity: 76.2482%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2379bf6eb154a93964260ea62da9925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.69724</td></tr><tr><td>val_acc_best</td><td>0.76667</td></tr><tr><td>val_acc_now</td><td>0.7375</td></tr><tr><td>val_loss</td><td>1.87557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sweep-126</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4ad1jewp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4ad1jewp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_041045-4ad1jewp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v5edd9mw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_082521-v5edd9mw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v5edd9mw' target=\"_blank\">northern-sweep-131</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v5edd9mw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v5edd9mw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251117_082530_495', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 168.0\n",
      "lif layer 1 self.abs_max_v: 168.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 93.0\n",
      "lif layer 2 self.abs_max_v: 93.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 186.0\n",
      "lif layer 1 self.abs_max_v: 211.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 230.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 269.0\n",
      "fc layer 2 self.abs_max_out: 218.0\n",
      "lif layer 2 self.abs_max_v: 263.0\n",
      "fc layer 1 self.abs_max_out: 224.0\n",
      "lif layer 1 self.abs_max_v: 304.0\n",
      "lif layer 2 self.abs_max_v: 302.0\n",
      "fc layer 1 self.abs_max_out: 389.0\n",
      "lif layer 1 self.abs_max_v: 389.0\n",
      "fc layer 2 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 434.5\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 550.5\n",
      "fc layer 2 self.abs_max_out: 324.0\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 749.0\n",
      "lif layer 2 self.abs_max_v: 464.0\n",
      "lif layer 1 self.abs_max_v: 762.0\n",
      "fc layer 2 self.abs_max_out: 378.0\n",
      "lif layer 2 self.abs_max_v: 563.5\n",
      "lif layer 1 self.abs_max_v: 806.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 565.0\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "fc layer 1 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 567.0\n",
      "lif layer 2 self.abs_max_v: 643.5\n",
      "lif layer 2 self.abs_max_v: 651.5\n",
      "lif layer 1 self.abs_max_v: 900.0\n",
      "lif layer 2 self.abs_max_v: 745.0\n",
      "fc layer 1 self.abs_max_out: 1058.0\n",
      "lif layer 1 self.abs_max_v: 1058.0\n",
      "fc layer 1 self.abs_max_out: 1296.0\n",
      "lif layer 1 self.abs_max_v: 1296.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "lif layer 2 self.abs_max_v: 749.0\n",
      "lif layer 2 self.abs_max_v: 781.5\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "lif layer 2 self.abs_max_v: 858.5\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "lif layer 2 self.abs_max_v: 1062.5\n",
      "lif layer 1 self.abs_max_v: 1373.5\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "fc layer 1 self.abs_max_out: 1621.0\n",
      "lif layer 1 self.abs_max_v: 1621.0\n",
      "fc layer 1 self.abs_max_out: 1690.0\n",
      "lif layer 1 self.abs_max_v: 1690.0\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "lif layer 1 self.abs_max_v: 1944.0\n",
      "fc layer 2 self.abs_max_out: 842.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "lif layer 2 self.abs_max_v: 1219.5\n",
      "lif layer 2 self.abs_max_v: 1359.0\n",
      "lif layer 2 self.abs_max_v: 1373.5\n",
      "fc layer 2 self.abs_max_out: 987.0\n",
      "lif layer 2 self.abs_max_v: 1380.0\n",
      "lif layer 2 self.abs_max_v: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "lif layer 2 self.abs_max_v: 1459.0\n",
      "lif layer 2 self.abs_max_v: 1466.5\n",
      "lif layer 2 self.abs_max_v: 1526.5\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "lif layer 2 self.abs_max_v: 1529.5\n",
      "fc layer 1 self.abs_max_out: 1719.0\n",
      "lif layer 1 self.abs_max_v: 2554.5\n",
      "lif layer 2 self.abs_max_v: 1632.0\n",
      "fc layer 1 self.abs_max_out: 2193.0\n",
      "lif layer 1 self.abs_max_v: 3080.5\n",
      "lif layer 2 self.abs_max_v: 1695.5\n",
      "fc layer 2 self.abs_max_out: 1079.0\n",
      "fc layer 2 self.abs_max_out: 1156.0\n",
      "lif layer 2 self.abs_max_v: 1967.5\n",
      "lif layer 2 self.abs_max_v: 2044.5\n",
      "lif layer 2 self.abs_max_v: 2049.5\n",
      "lif layer 2 self.abs_max_v: 2103.0\n",
      "fc layer 2 self.abs_max_out: 1249.0\n",
      "lif layer 2 self.abs_max_v: 2300.5\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "fc layer 2 self.abs_max_out: 1254.0\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "fc layer 2 self.abs_max_out: 1368.0\n",
      "lif layer 2 self.abs_max_v: 2375.0\n",
      "lif layer 2 self.abs_max_v: 2457.0\n",
      "lif layer 2 self.abs_max_v: 2508.5\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 2 self.abs_max_out: 1436.0\n",
      "lif layer 2 self.abs_max_v: 2539.0\n",
      "lif layer 2 self.abs_max_v: 2697.5\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "fc layer 2 self.abs_max_out: 1453.0\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "lif layer 1 self.abs_max_v: 3111.0\n",
      "lif layer 1 self.abs_max_v: 3125.0\n",
      "fc layer 2 self.abs_max_out: 1533.0\n",
      "fc layer 2 self.abs_max_out: 1557.0\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "fc layer 2 self.abs_max_out: 1652.0\n",
      "fc layer 2 self.abs_max_out: 1700.0\n",
      "lif layer 2 self.abs_max_v: 2732.5\n",
      "lif layer 2 self.abs_max_v: 2819.5\n",
      "lif layer 2 self.abs_max_v: 2928.5\n",
      "lif layer 2 self.abs_max_v: 2945.0\n",
      "lif layer 2 self.abs_max_v: 2972.0\n",
      "fc layer 1 self.abs_max_out: 2394.0\n",
      "lif layer 1 self.abs_max_v: 3133.0\n",
      "fc layer 1 self.abs_max_out: 2615.0\n",
      "lif layer 2 self.abs_max_v: 3022.0\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "fc layer 2 self.abs_max_out: 1743.0\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 2 self.abs_max_out: 1758.0\n",
      "fc layer 2 self.abs_max_out: 1840.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "lif layer 1 self.abs_max_v: 3346.5\n",
      "lif layer 1 self.abs_max_v: 3508.0\n",
      "lif layer 2 self.abs_max_v: 3024.0\n",
      "lif layer 2 self.abs_max_v: 3124.0\n",
      "lif layer 2 self.abs_max_v: 3165.0\n",
      "lif layer 2 self.abs_max_v: 3179.0\n",
      "lif layer 2 self.abs_max_v: 3235.5\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "lif layer 2 self.abs_max_v: 3253.5\n",
      "lif layer 2 self.abs_max_v: 3373.5\n",
      "lif layer 2 self.abs_max_v: 3379.0\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 3560.5\n",
      "lif layer 1 self.abs_max_v: 3906.5\n",
      "fc layer 3 self.abs_max_out: 620.0\n",
      "lif layer 1 self.abs_max_v: 4110.0\n",
      "fc layer 2 self.abs_max_out: 2070.0\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "lif layer 2 self.abs_max_v: 3433.5\n",
      "lif layer 2 self.abs_max_v: 3540.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 1 self.abs_max_out: 2652.0\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "lif layer 1 self.abs_max_v: 4119.5\n",
      "lif layer 1 self.abs_max_v: 4521.0\n",
      "fc layer 1 self.abs_max_out: 2654.0\n",
      "fc layer 1 self.abs_max_out: 2905.0\n",
      "lif layer 2 self.abs_max_v: 3554.0\n",
      "lif layer 2 self.abs_max_v: 3600.0\n",
      "lif layer 2 self.abs_max_v: 3607.0\n",
      "lif layer 2 self.abs_max_v: 3643.5\n",
      "lif layer 1 self.abs_max_v: 4580.0\n",
      "lif layer 1 self.abs_max_v: 4813.5\n",
      "lif layer 1 self.abs_max_v: 5150.0\n",
      "fc layer 1 self.abs_max_out: 3044.0\n",
      "fc layer 2 self.abs_max_out: 2177.0\n",
      "fc layer 2 self.abs_max_out: 2183.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "fc layer 1 self.abs_max_out: 3051.0\n",
      "lif layer 1 self.abs_max_v: 5257.5\n",
      "fc layer 2 self.abs_max_out: 2229.0\n",
      "fc layer 2 self.abs_max_out: 2283.0\n",
      "fc layer 1 self.abs_max_out: 3107.0\n",
      "fc layer 1 self.abs_max_out: 3509.0\n",
      "lif layer 2 self.abs_max_v: 3661.0\n",
      "lif layer 1 self.abs_max_v: 5272.5\n",
      "lif layer 2 self.abs_max_v: 3704.0\n",
      "lif layer 2 self.abs_max_v: 3724.5\n",
      "lif layer 2 self.abs_max_v: 3822.0\n",
      "fc layer 2 self.abs_max_out: 2297.0\n",
      "lif layer 2 self.abs_max_v: 4006.5\n",
      "lif layer 2 self.abs_max_v: 4008.5\n",
      "lif layer 1 self.abs_max_v: 5707.5\n",
      "lif layer 1 self.abs_max_v: 5802.0\n",
      "lif layer 1 self.abs_max_v: 5862.5\n",
      "lif layer 1 self.abs_max_v: 5939.5\n",
      "fc layer 2 self.abs_max_out: 2306.0\n",
      "fc layer 2 self.abs_max_out: 2307.0\n",
      "fc layer 2 self.abs_max_out: 2309.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.396441/  1.863554, val:  31.67%, val_best:  31.67%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 67.6180%\n",
      "layer   3  Sparsity: 63.5064%\n",
      "total_backward_count 9790 real_backward_count 1537  15.700%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "lif layer 1 self.abs_max_v: 6058.5\n",
      "fc layer 2 self.abs_max_out: 2315.0\n",
      "fc layer 1 self.abs_max_out: 3885.0\n",
      "fc layer 2 self.abs_max_out: 2369.0\n",
      "fc layer 2 self.abs_max_out: 2593.0\n",
      "fc layer 2 self.abs_max_out: 2804.0\n",
      "lif layer 2 self.abs_max_v: 4064.0\n",
      "lif layer 2 self.abs_max_v: 4373.0\n",
      "lif layer 2 self.abs_max_v: 4382.0\n",
      "lif layer 2 self.abs_max_v: 4597.5\n",
      "lif layer 2 self.abs_max_v: 4748.5\n",
      "lif layer 2 self.abs_max_v: 4775.0\n",
      "lif layer 2 self.abs_max_v: 4864.5\n",
      "lif layer 2 self.abs_max_v: 4924.5\n",
      "lif layer 2 self.abs_max_v: 4968.5\n",
      "fc layer 2 self.abs_max_out: 3018.0\n",
      "lif layer 2 self.abs_max_v: 5021.5\n",
      "lif layer 1 self.abs_max_v: 6414.5\n",
      "lif layer 2 self.abs_max_v: 5160.0\n",
      "lif layer 2 self.abs_max_v: 5371.0\n",
      "lif layer 2 self.abs_max_v: 5389.5\n",
      "fc layer 1 self.abs_max_out: 4037.0\n",
      "fc layer 1 self.abs_max_out: 4257.0\n",
      "lif layer 1 self.abs_max_v: 6595.5\n",
      "lif layer 1 self.abs_max_v: 6638.0\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "lif layer 1 self.abs_max_v: 7025.0\n",
      "lif layer 1 self.abs_max_v: 7306.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.317065/  1.786235, val:  38.33%, val_best:  38.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.3624%\n",
      "layer   3  Sparsity: 68.2552%\n",
      "total_backward_count 19580 real_backward_count 2939  15.010%\n",
      "fc layer 1 self.abs_max_out: 4707.0\n",
      "lif layer 1 self.abs_max_v: 7424.5\n",
      "lif layer 2 self.abs_max_v: 5478.0\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 2 self.abs_max_out: 3026.0\n",
      "fc layer 2 self.abs_max_out: 3042.0\n",
      "lif layer 1 self.abs_max_v: 7527.0\n",
      "lif layer 1 self.abs_max_v: 7620.5\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "fc layer 1 self.abs_max_out: 4993.0\n",
      "lif layer 1 self.abs_max_v: 8459.0\n",
      "fc layer 2 self.abs_max_out: 3071.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.288572/  1.723729, val:  45.00%, val_best:  45.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5605%\n",
      "layer   3  Sparsity: 69.1037%\n",
      "total_backward_count 29370 real_backward_count 4374  14.893%\n",
      "fc layer 2 self.abs_max_out: 3119.0\n",
      "fc layer 2 self.abs_max_out: 3127.0\n",
      "fc layer 3 self.abs_max_out: 747.0\n",
      "fc layer 3 self.abs_max_out: 751.0\n",
      "fc layer 2 self.abs_max_out: 3141.0\n",
      "fc layer 2 self.abs_max_out: 3165.0\n",
      "fc layer 2 self.abs_max_out: 3336.0\n",
      "fc layer 1 self.abs_max_out: 5107.0\n",
      "lif layer 1 self.abs_max_v: 8792.0\n",
      "lif layer 1 self.abs_max_v: 9093.5\n",
      "lif layer 1 self.abs_max_v: 9158.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.199773/  1.687304, val:  45.42%, val_best:  45.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.9041%\n",
      "layer   3  Sparsity: 66.3710%\n",
      "total_backward_count 39160 real_backward_count 5688  14.525%\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "fc layer 2 self.abs_max_out: 3431.0\n",
      "fc layer 1 self.abs_max_out: 5167.0\n",
      "lif layer 2 self.abs_max_v: 5555.0\n",
      "lif layer 1 self.abs_max_v: 9247.5\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 1 self.abs_max_out: 5270.0\n",
      "fc layer 1 self.abs_max_out: 5357.0\n",
      "lif layer 1 self.abs_max_v: 9362.0\n",
      "fc layer 1 self.abs_max_out: 5555.0\n",
      "lif layer 1 self.abs_max_v: 9411.5\n",
      "lif layer 1 self.abs_max_v: 9861.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.158994/  1.652711, val:  48.75%, val_best:  48.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.3095%\n",
      "layer   3  Sparsity: 66.7368%\n",
      "total_backward_count 48950 real_backward_count 6937  14.172%\n",
      "lif layer 2 self.abs_max_v: 5633.0\n",
      "fc layer 3 self.abs_max_out: 820.0\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "fc layer 1 self.abs_max_out: 5696.0\n",
      "fc layer 1 self.abs_max_out: 5842.0\n",
      "lif layer 1 self.abs_max_v: 10267.0\n",
      "lif layer 2 self.abs_max_v: 5687.5\n",
      "fc layer 1 self.abs_max_out: 6092.0\n",
      "fc layer 1 self.abs_max_out: 6117.0\n",
      "lif layer 1 self.abs_max_v: 10460.0\n",
      "lif layer 2 self.abs_max_v: 5802.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.096069/  1.646310, val:  51.67%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.9041%\n",
      "layer   3  Sparsity: 66.2898%\n",
      "total_backward_count 58740 real_backward_count 8163  13.897%\n",
      "fc layer 2 self.abs_max_out: 3481.0\n",
      "fc layer 2 self.abs_max_out: 3526.0\n",
      "fc layer 2 self.abs_max_out: 3787.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "fc layer 2 self.abs_max_out: 3992.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.092695/  1.548305, val:  56.67%, val_best:  56.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.0985%\n",
      "layer   3  Sparsity: 67.3556%\n",
      "total_backward_count 68530 real_backward_count 9302  13.574%\n",
      "fc layer 1 self.abs_max_out: 6167.0\n",
      "lif layer 1 self.abs_max_v: 10751.0\n",
      "fc layer 1 self.abs_max_out: 6330.0\n",
      "lif layer 1 self.abs_max_v: 10922.0\n",
      "lif layer 1 self.abs_max_v: 10975.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.042244/  1.602667, val:  50.42%, val_best:  56.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.4743%\n",
      "layer   3  Sparsity: 67.4763%\n",
      "total_backward_count 78320 real_backward_count 10465  13.362%\n",
      "lif layer 2 self.abs_max_v: 5817.5\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "lif layer 1 self.abs_max_v: 11673.0\n",
      "lif layer 1 self.abs_max_v: 11695.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.011579/  1.473253, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.1592%\n",
      "layer   3  Sparsity: 67.1697%\n",
      "total_backward_count 88110 real_backward_count 11642  13.213%\n",
      "lif layer 2 self.abs_max_v: 6024.0\n",
      "fc layer 1 self.abs_max_out: 6444.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.999021/  1.497856, val:  50.42%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.7373%\n",
      "layer   3  Sparsity: 67.8651%\n",
      "total_backward_count 97900 real_backward_count 12741  13.014%\n",
      "lif layer 2 self.abs_max_v: 6203.0\n",
      "fc layer 1 self.abs_max_out: 6694.0\n",
      "lif layer 2 self.abs_max_v: 6209.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.936872/  1.491493, val:  53.33%, val_best:  57.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4635%\n",
      "layer   3  Sparsity: 69.0766%\n",
      "total_backward_count 107690 real_backward_count 13834  12.846%\n",
      "fc layer 1 self.abs_max_out: 6704.0\n",
      "lif layer 1 self.abs_max_v: 11749.5\n",
      "fc layer 3 self.abs_max_out: 992.0\n",
      "lif layer 1 self.abs_max_v: 11867.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.924442/  1.495425, val:  53.75%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0003%\n",
      "layer   3  Sparsity: 68.0913%\n",
      "total_backward_count 117480 real_backward_count 14914  12.695%\n",
      "fc layer 3 self.abs_max_out: 1017.0\n",
      "lif layer 1 self.abs_max_v: 12290.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.883887/  1.392130, val:  53.33%, val_best:  57.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.4524%\n",
      "layer   3  Sparsity: 68.2961%\n",
      "total_backward_count 127270 real_backward_count 15910  12.501%\n",
      "fc layer 3 self.abs_max_out: 1037.0\n",
      "fc layer 1 self.abs_max_out: 6774.0\n",
      "fc layer 3 self.abs_max_out: 1068.0\n",
      "fc layer 3 self.abs_max_out: 1074.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.882204/  1.393540, val:  53.33%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.2590%\n",
      "layer   3  Sparsity: 68.7520%\n",
      "total_backward_count 137060 real_backward_count 16896  12.327%\n",
      "fc layer 3 self.abs_max_out: 1092.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.851690/  1.385299, val:  58.75%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.4220%\n",
      "layer   3  Sparsity: 69.4962%\n",
      "total_backward_count 146850 real_backward_count 17831  12.142%\n",
      "lif layer 1 self.abs_max_v: 12525.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.826712/  1.361847, val:  52.92%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.9757%\n",
      "layer   3  Sparsity: 69.4708%\n",
      "total_backward_count 156640 real_backward_count 18730  11.957%\n",
      "lif layer 2 self.abs_max_v: 6263.5\n",
      "lif layer 2 self.abs_max_v: 6531.5\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.808452/  1.247153, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.3717%\n",
      "layer   3  Sparsity: 70.1978%\n",
      "total_backward_count 166430 real_backward_count 19550  11.747%\n",
      "fc layer 3 self.abs_max_out: 1123.0\n",
      "fc layer 1 self.abs_max_out: 6935.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.794530/  1.286504, val:  70.00%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.9483%\n",
      "layer   3  Sparsity: 70.3729%\n",
      "total_backward_count 176220 real_backward_count 20387  11.569%\n",
      "fc layer 1 self.abs_max_out: 7086.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.761803/  1.376347, val:  61.25%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.5411%\n",
      "layer   3  Sparsity: 70.0020%\n",
      "total_backward_count 186010 real_backward_count 21241  11.419%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.733971/  1.253784, val:  65.83%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.7219%\n",
      "layer   3  Sparsity: 70.5860%\n",
      "total_backward_count 195800 real_backward_count 21982  11.227%\n",
      "lif layer 2 self.abs_max_v: 6823.5\n",
      "fc layer 1 self.abs_max_out: 7099.0\n",
      "lif layer 1 self.abs_max_v: 13036.0\n",
      "lif layer 1 self.abs_max_v: 13214.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.737256/  1.259817, val:  59.17%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.5430%\n",
      "layer   3  Sparsity: 70.7792%\n",
      "total_backward_count 205590 real_backward_count 22756  11.069%\n",
      "fc layer 3 self.abs_max_out: 1134.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.706485/  1.228844, val:  72.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.2607%\n",
      "layer   3  Sparsity: 71.7806%\n",
      "total_backward_count 215380 real_backward_count 23503  10.912%\n",
      "fc layer 3 self.abs_max_out: 1172.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.699005/  1.147129, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.7831%\n",
      "layer   3  Sparsity: 71.2613%\n",
      "total_backward_count 225170 real_backward_count 24236  10.763%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.670767/  1.164314, val:  76.25%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.6132%\n",
      "layer   3  Sparsity: 71.8885%\n",
      "total_backward_count 234960 real_backward_count 24882  10.590%\n",
      "fc layer 3 self.abs_max_out: 1197.0\n",
      "fc layer 1 self.abs_max_out: 7129.0\n",
      "lif layer 1 self.abs_max_v: 13266.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.689419/  1.133408, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.0313%\n",
      "layer   3  Sparsity: 71.7510%\n",
      "total_backward_count 244750 real_backward_count 25509  10.422%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.664716/  1.142007, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 72.1082%\n",
      "layer   3  Sparsity: 71.9200%\n",
      "total_backward_count 254540 real_backward_count 26159  10.277%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.655824/  1.176416, val:  71.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.8938%\n",
      "layer   3  Sparsity: 72.0799%\n",
      "total_backward_count 264330 real_backward_count 26782  10.132%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.639778/  1.102154, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.6597%\n",
      "layer   3  Sparsity: 72.3279%\n",
      "total_backward_count 274120 real_backward_count 27340   9.974%\n",
      "fc layer 3 self.abs_max_out: 1216.0\n",
      "fc layer 1 self.abs_max_out: 7152.0\n",
      "fc layer 1 self.abs_max_out: 7217.0\n",
      "fc layer 1 self.abs_max_out: 7232.0\n",
      "lif layer 1 self.abs_max_v: 13444.5\n",
      "lif layer 1 self.abs_max_v: 13541.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.623066/  1.112282, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.6834%\n",
      "layer   3  Sparsity: 72.5938%\n",
      "total_backward_count 283910 real_backward_count 27881   9.820%\n",
      "fc layer 3 self.abs_max_out: 1226.0\n",
      "fc layer 3 self.abs_max_out: 1241.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.623623/  1.100186, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.2630%\n",
      "layer   3  Sparsity: 72.2652%\n",
      "total_backward_count 293700 real_backward_count 28417   9.676%\n",
      "fc layer 1 self.abs_max_out: 7383.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.597359/  1.072832, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1063%\n",
      "layer   3  Sparsity: 72.4979%\n",
      "total_backward_count 303490 real_backward_count 28889   9.519%\n",
      "lif layer 2 self.abs_max_v: 7004.5\n",
      "fc layer 3 self.abs_max_out: 1253.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.588615/  1.149468, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1765%\n",
      "layer   3  Sparsity: 72.1772%\n",
      "total_backward_count 313280 real_backward_count 29395   9.383%\n",
      "fc layer 3 self.abs_max_out: 1276.0\n",
      "fc layer 1 self.abs_max_out: 7465.0\n",
      "lif layer 1 self.abs_max_v: 13786.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.578975/  1.083265, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.4103%\n",
      "layer   3  Sparsity: 72.2831%\n",
      "total_backward_count 323070 real_backward_count 29826   9.232%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.582515/  1.046540, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1331%\n",
      "layer   3  Sparsity: 72.4249%\n",
      "total_backward_count 332860 real_backward_count 30235   9.083%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.559382/  1.056427, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1936%\n",
      "layer   3  Sparsity: 72.9482%\n",
      "total_backward_count 342650 real_backward_count 30642   8.943%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.547614/  1.045073, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1827%\n",
      "layer   3  Sparsity: 72.0591%\n",
      "total_backward_count 352440 real_backward_count 31049   8.810%\n",
      "fc layer 3 self.abs_max_out: 1281.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.541753/  1.057192, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1580%\n",
      "layer   3  Sparsity: 72.4210%\n",
      "total_backward_count 362230 real_backward_count 31457   8.684%\n",
      "fc layer 1 self.abs_max_out: 7508.0\n",
      "fc layer 2 self.abs_max_out: 4017.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.523399/  1.097079, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9302%\n",
      "layer   3  Sparsity: 72.7406%\n",
      "total_backward_count 372020 real_backward_count 31811   8.551%\n",
      "fc layer 2 self.abs_max_out: 4066.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.519024/  1.027229, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9365%\n",
      "layer   3  Sparsity: 72.7060%\n",
      "total_backward_count 381810 real_backward_count 32160   8.423%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.512859/  1.070801, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 71.1825%\n",
      "layer   3  Sparsity: 73.5262%\n",
      "total_backward_count 391600 real_backward_count 32522   8.305%\n",
      "fc layer 3 self.abs_max_out: 1306.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.512143/  1.011265, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5445%\n",
      "layer   3  Sparsity: 73.1408%\n",
      "total_backward_count 401390 real_backward_count 32861   8.187%\n",
      "lif layer 2 self.abs_max_v: 7051.5\n",
      "lif layer 2 self.abs_max_v: 7099.5\n",
      "fc layer 2 self.abs_max_out: 4182.0\n",
      "lif layer 2 self.abs_max_v: 7732.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.506377/  1.009436, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3272%\n",
      "layer   3  Sparsity: 73.5978%\n",
      "total_backward_count 411180 real_backward_count 33196   8.073%\n",
      "lif layer 2 self.abs_max_v: 7815.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.490032/  0.991278, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5739%\n",
      "layer   3  Sparsity: 73.5725%\n",
      "total_backward_count 420970 real_backward_count 33502   7.958%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.474216/  0.996683, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7902%\n",
      "layer   3  Sparsity: 73.3518%\n",
      "total_backward_count 430760 real_backward_count 33786   7.843%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.468444/  0.982191, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8948%\n",
      "layer   3  Sparsity: 73.3189%\n",
      "total_backward_count 440550 real_backward_count 34063   7.732%\n",
      "fc layer 2 self.abs_max_out: 4287.0\n",
      "lif layer 2 self.abs_max_v: 8018.5\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.455099/  1.022404, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7744%\n",
      "layer   3  Sparsity: 73.6278%\n",
      "total_backward_count 450340 real_backward_count 34330   7.623%\n",
      "fc layer 3 self.abs_max_out: 1366.0\n",
      "fc layer 3 self.abs_max_out: 1387.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.444037/  0.971923, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6192%\n",
      "layer   3  Sparsity: 73.9761%\n",
      "total_backward_count 460130 real_backward_count 34571   7.513%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.447654/  1.027330, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8200%\n",
      "layer   3  Sparsity: 73.5882%\n",
      "total_backward_count 469920 real_backward_count 34779   7.401%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.448785/  0.948086, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7467%\n",
      "layer   3  Sparsity: 73.9959%\n",
      "total_backward_count 479710 real_backward_count 35033   7.303%\n",
      "fc layer 2 self.abs_max_out: 4319.0\n",
      "lif layer 2 self.abs_max_v: 8097.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.457286/  0.944463, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7840%\n",
      "layer   3  Sparsity: 73.5486%\n",
      "total_backward_count 489500 real_backward_count 35283   7.208%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.454057/  0.944140, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5128%\n",
      "layer   3  Sparsity: 73.8507%\n",
      "total_backward_count 499290 real_backward_count 35486   7.107%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.440097/  0.986454, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9545%\n",
      "layer   3  Sparsity: 73.6696%\n",
      "total_backward_count 509080 real_backward_count 35705   7.014%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.423842/  0.952900, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9402%\n",
      "layer   3  Sparsity: 73.1358%\n",
      "total_backward_count 518870 real_backward_count 35889   6.917%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.396663/  0.961092, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6834%\n",
      "layer   3  Sparsity: 73.3133%\n",
      "total_backward_count 528660 real_backward_count 36066   6.822%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.398744/  0.913362, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5424%\n",
      "layer   3  Sparsity: 73.2058%\n",
      "total_backward_count 538450 real_backward_count 36295   6.741%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.399689/  0.924220, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7893%\n",
      "layer   3  Sparsity: 73.7078%\n",
      "total_backward_count 548240 real_backward_count 36484   6.655%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.400448/  0.978188, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9421%\n",
      "layer   3  Sparsity: 73.9774%\n",
      "total_backward_count 558030 real_backward_count 36654   6.568%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.394858/  0.934190, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8677%\n",
      "layer   3  Sparsity: 73.4881%\n",
      "total_backward_count 567820 real_backward_count 36839   6.488%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.391083/  0.923358, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6882%\n",
      "layer   3  Sparsity: 73.4465%\n",
      "total_backward_count 577610 real_backward_count 37020   6.409%\n",
      "fc layer 1 self.abs_max_out: 7518.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.392848/  0.980686, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5339%\n",
      "layer   3  Sparsity: 73.7840%\n",
      "total_backward_count 587400 real_backward_count 37169   6.328%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.409358/  0.943726, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5192%\n",
      "layer   3  Sparsity: 73.5857%\n",
      "total_backward_count 597190 real_backward_count 37339   6.252%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.394191/  0.911415, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2269%\n",
      "layer   3  Sparsity: 73.1938%\n",
      "total_backward_count 606980 real_backward_count 37469   6.173%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.381435/  0.952169, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.1991%\n",
      "layer   3  Sparsity: 73.7160%\n",
      "total_backward_count 616770 real_backward_count 37600   6.096%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.379122/  0.911958, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4381%\n",
      "layer   3  Sparsity: 73.9703%\n",
      "total_backward_count 626560 real_backward_count 37771   6.028%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.374575/  0.899731, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4916%\n",
      "layer   3  Sparsity: 74.1645%\n",
      "total_backward_count 636350 real_backward_count 37912   5.958%\n",
      "fc layer 3 self.abs_max_out: 1413.0\n",
      "fc layer 1 self.abs_max_out: 7558.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.370323/  0.896542, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4902%\n",
      "layer   3  Sparsity: 74.4041%\n",
      "total_backward_count 646140 real_backward_count 38049   5.889%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.369442/  0.942736, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4167%\n",
      "layer   3  Sparsity: 74.2587%\n",
      "total_backward_count 655930 real_backward_count 38186   5.822%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.371405/  0.915905, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2701%\n",
      "layer   3  Sparsity: 73.7114%\n",
      "total_backward_count 665720 real_backward_count 38315   5.755%\n",
      "fc layer 1 self.abs_max_out: 7565.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.364032/  0.919578, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2707%\n",
      "layer   3  Sparsity: 73.5776%\n",
      "total_backward_count 675510 real_backward_count 38421   5.688%\n",
      "fc layer 1 self.abs_max_out: 7673.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.361341/  0.916130, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4371%\n",
      "layer   3  Sparsity: 74.2475%\n",
      "total_backward_count 685300 real_backward_count 38538   5.624%\n",
      "fc layer 3 self.abs_max_out: 1417.0\n",
      "fc layer 3 self.abs_max_out: 1446.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.356822/  0.936140, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5810%\n",
      "layer   3  Sparsity: 74.0992%\n",
      "total_backward_count 695090 real_backward_count 38660   5.562%\n",
      "fc layer 1 self.abs_max_out: 7675.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.356284/  0.921621, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7459%\n",
      "layer   3  Sparsity: 73.2436%\n",
      "total_backward_count 704880 real_backward_count 38767   5.500%\n",
      "fc layer 3 self.abs_max_out: 1461.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.345683/  0.922163, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5135%\n",
      "layer   3  Sparsity: 73.5445%\n",
      "total_backward_count 714670 real_backward_count 38867   5.438%\n",
      "fc layer 3 self.abs_max_out: 1476.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.339051/  0.898514, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4792%\n",
      "layer   3  Sparsity: 73.1611%\n",
      "total_backward_count 724460 real_backward_count 38958   5.378%\n",
      "fc layer 1 self.abs_max_out: 7709.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.342663/  0.915373, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4395%\n",
      "layer   3  Sparsity: 73.2520%\n",
      "total_backward_count 734250 real_backward_count 39075   5.322%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.344939/  0.919219, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5045%\n",
      "layer   3  Sparsity: 73.4143%\n",
      "total_backward_count 744040 real_backward_count 39186   5.267%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.316834/  0.884747, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2771%\n",
      "layer   3  Sparsity: 73.6066%\n",
      "total_backward_count 753830 real_backward_count 39272   5.210%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.322190/  0.897439, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5885%\n",
      "layer   3  Sparsity: 73.1208%\n",
      "total_backward_count 763620 real_backward_count 39349   5.153%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.325774/  0.917161, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3625%\n",
      "layer   3  Sparsity: 73.3412%\n",
      "total_backward_count 773410 real_backward_count 39443   5.100%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.316148/  0.911596, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3404%\n",
      "layer   3  Sparsity: 73.8450%\n",
      "total_backward_count 783200 real_backward_count 39521   5.046%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.318302/  0.903770, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3473%\n",
      "layer   3  Sparsity: 73.9073%\n",
      "total_backward_count 792990 real_backward_count 39623   4.997%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.310270/  0.899225, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4783%\n",
      "layer   3  Sparsity: 73.9000%\n",
      "total_backward_count 802780 real_backward_count 39690   4.944%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.304410/  0.895707, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6322%\n",
      "layer   3  Sparsity: 74.2825%\n",
      "total_backward_count 812570 real_backward_count 39782   4.896%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.293018/  0.906594, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5338%\n",
      "layer   3  Sparsity: 74.2177%\n",
      "total_backward_count 822360 real_backward_count 39848   4.846%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.299323/  0.887679, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4304%\n",
      "layer   3  Sparsity: 74.5059%\n",
      "total_backward_count 832150 real_backward_count 39927   4.798%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.296876/  0.904129, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4932%\n",
      "layer   3  Sparsity: 74.5777%\n",
      "total_backward_count 841940 real_backward_count 40011   4.752%\n",
      "fc layer 3 self.abs_max_out: 1478.0\n",
      "fc layer 3 self.abs_max_out: 1491.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.288702/  0.874970, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4594%\n",
      "layer   3  Sparsity: 74.2787%\n",
      "total_backward_count 851730 real_backward_count 40095   4.707%\n",
      "fc layer 3 self.abs_max_out: 1511.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.284388/  0.896513, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4380%\n",
      "layer   3  Sparsity: 74.3229%\n",
      "total_backward_count 861520 real_backward_count 40180   4.664%\n",
      "fc layer 3 self.abs_max_out: 1514.0\n",
      "fc layer 3 self.abs_max_out: 1515.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.288574/  0.881227, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2354%\n",
      "layer   3  Sparsity: 74.2924%\n",
      "total_backward_count 871310 real_backward_count 40239   4.618%\n",
      "fc layer 3 self.abs_max_out: 1516.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.278625/  0.873960, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3301%\n",
      "layer   3  Sparsity: 74.1689%\n",
      "total_backward_count 881100 real_backward_count 40313   4.575%\n",
      "fc layer 1 self.abs_max_out: 7761.0\n",
      "lif layer 1 self.abs_max_v: 14041.0\n",
      "lif layer 1 self.abs_max_v: 14128.5\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.281586/  0.890120, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2938%\n",
      "layer   3  Sparsity: 74.0959%\n",
      "total_backward_count 890890 real_backward_count 40369   4.531%\n",
      "fc layer 3 self.abs_max_out: 1519.0\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.291593/  0.905081, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5948%\n",
      "layer   3  Sparsity: 73.9497%\n",
      "total_backward_count 900680 real_backward_count 40434   4.489%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.282034/  0.884350, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6971%\n",
      "layer   3  Sparsity: 73.9607%\n",
      "total_backward_count 910470 real_backward_count 40495   4.448%\n",
      "fc layer 3 self.abs_max_out: 1528.0\n",
      "fc layer 1 self.abs_max_out: 7782.0\n",
      "fc layer 1 self.abs_max_out: 7943.0\n",
      "lif layer 1 self.abs_max_v: 14383.5\n",
      "lif layer 1 self.abs_max_v: 14429.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.283967/  0.875085, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7221%\n",
      "layer   3  Sparsity: 73.6315%\n",
      "total_backward_count 920260 real_backward_count 40570   4.409%\n",
      "fc layer 3 self.abs_max_out: 1531.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.265314/  0.887309, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6566%\n",
      "layer   3  Sparsity: 73.9513%\n",
      "total_backward_count 930050 real_backward_count 40609   4.366%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.266159/  0.882328, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5006%\n",
      "layer   3  Sparsity: 74.0375%\n",
      "total_backward_count 939840 real_backward_count 40644   4.325%\n",
      "fc layer 3 self.abs_max_out: 1549.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.271960/  0.920872, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3382%\n",
      "layer   3  Sparsity: 74.4235%\n",
      "total_backward_count 949630 real_backward_count 40693   4.285%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.280333/  0.911099, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3470%\n",
      "layer   3  Sparsity: 74.5231%\n",
      "total_backward_count 959420 real_backward_count 40760   4.248%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.283905/  0.920346, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2386%\n",
      "layer   3  Sparsity: 74.4389%\n",
      "total_backward_count 969210 real_backward_count 40825   4.212%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.280872/  0.890659, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5186%\n",
      "layer   3  Sparsity: 74.6539%\n",
      "total_backward_count 979000 real_backward_count 40880   4.176%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.276317/  0.905616, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6395%\n",
      "layer   3  Sparsity: 74.5253%\n",
      "total_backward_count 988790 real_backward_count 40936   4.140%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.272276/  0.877532, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7173%\n",
      "layer   3  Sparsity: 74.7425%\n",
      "total_backward_count 998580 real_backward_count 40988   4.105%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.273473/  0.899000, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7772%\n",
      "layer   3  Sparsity: 74.4122%\n",
      "total_backward_count 1008370 real_backward_count 41031   4.069%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.269703/  0.910208, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6694%\n",
      "layer   3  Sparsity: 74.0445%\n",
      "total_backward_count 1018160 real_backward_count 41079   4.035%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.273772/  0.891351, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4797%\n",
      "layer   3  Sparsity: 74.2704%\n",
      "total_backward_count 1027950 real_backward_count 41125   4.001%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.275861/  0.871633, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4067%\n",
      "layer   3  Sparsity: 74.1055%\n",
      "total_backward_count 1037740 real_backward_count 41190   3.969%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.270770/  0.872505, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5009%\n",
      "layer   3  Sparsity: 74.5442%\n",
      "total_backward_count 1047530 real_backward_count 41235   3.936%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.265960/  0.873216, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4519%\n",
      "layer   3  Sparsity: 74.0767%\n",
      "total_backward_count 1057320 real_backward_count 41280   3.904%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.273207/  0.859536, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6305%\n",
      "layer   3  Sparsity: 73.8879%\n",
      "total_backward_count 1067110 real_backward_count 41311   3.871%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.267270/  0.880037, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6775%\n",
      "layer   3  Sparsity: 74.0710%\n",
      "total_backward_count 1076900 real_backward_count 41361   3.841%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.266774/  0.863892, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4086%\n",
      "layer   3  Sparsity: 74.1553%\n",
      "total_backward_count 1086690 real_backward_count 41412   3.811%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.266098/  0.846970, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5499%\n",
      "layer   3  Sparsity: 74.5189%\n",
      "total_backward_count 1096480 real_backward_count 41439   3.779%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.268218/  0.877055, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5243%\n",
      "layer   3  Sparsity: 74.2546%\n",
      "total_backward_count 1106270 real_backward_count 41501   3.751%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.274773/  0.872047, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4848%\n",
      "layer   3  Sparsity: 74.3271%\n",
      "total_backward_count 1116060 real_backward_count 41563   3.724%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.272917/  0.878875, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4015%\n",
      "layer   3  Sparsity: 74.3388%\n",
      "total_backward_count 1125850 real_backward_count 41616   3.696%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.264467/  0.890193, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4762%\n",
      "layer   3  Sparsity: 74.0467%\n",
      "total_backward_count 1135640 real_backward_count 41645   3.667%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.253960/  0.907691, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6222%\n",
      "layer   3  Sparsity: 74.2838%\n",
      "total_backward_count 1145430 real_backward_count 41675   3.638%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.246830/  0.884548, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7201%\n",
      "layer   3  Sparsity: 74.2797%\n",
      "total_backward_count 1155220 real_backward_count 41692   3.609%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.249918/  0.901930, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7958%\n",
      "layer   3  Sparsity: 74.2135%\n",
      "total_backward_count 1165010 real_backward_count 41720   3.581%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.254322/  0.914823, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6610%\n",
      "layer   3  Sparsity: 74.3132%\n",
      "total_backward_count 1174800 real_backward_count 41761   3.555%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.255871/  0.900862, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4881%\n",
      "layer   3  Sparsity: 74.6813%\n",
      "total_backward_count 1184590 real_backward_count 41802   3.529%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.263562/  0.887409, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4566%\n",
      "layer   3  Sparsity: 73.9833%\n",
      "total_backward_count 1194380 real_backward_count 41860   3.505%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.261898/  0.901550, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3847%\n",
      "layer   3  Sparsity: 74.3048%\n",
      "total_backward_count 1204170 real_backward_count 41891   3.479%\n",
      "fc layer 3 self.abs_max_out: 1552.0\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.259992/  0.867748, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3953%\n",
      "layer   3  Sparsity: 74.0774%\n",
      "total_backward_count 1213960 real_backward_count 41940   3.455%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.257436/  0.856201, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3839%\n",
      "layer   3  Sparsity: 73.6025%\n",
      "total_backward_count 1223750 real_backward_count 41983   3.431%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.262157/  0.871491, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3967%\n",
      "layer   3  Sparsity: 73.9584%\n",
      "total_backward_count 1233540 real_backward_count 42021   3.407%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.255009/  0.858206, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5057%\n",
      "layer   3  Sparsity: 73.9105%\n",
      "total_backward_count 1243330 real_backward_count 42046   3.382%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.251475/  0.892686, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4792%\n",
      "layer   3  Sparsity: 73.8855%\n",
      "total_backward_count 1253120 real_backward_count 42078   3.358%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.250167/  0.861033, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3233%\n",
      "layer   3  Sparsity: 74.3743%\n",
      "total_backward_count 1262910 real_backward_count 42117   3.335%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.247939/  0.855760, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2354%\n",
      "layer   3  Sparsity: 74.6972%\n",
      "total_backward_count 1272700 real_backward_count 42133   3.311%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.248124/  0.875516, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4165%\n",
      "layer   3  Sparsity: 74.3833%\n",
      "total_backward_count 1282490 real_backward_count 42162   3.288%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.241725/  0.889866, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3762%\n",
      "layer   3  Sparsity: 74.0466%\n",
      "total_backward_count 1292280 real_backward_count 42191   3.265%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.245426/  0.858181, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4029%\n",
      "layer   3  Sparsity: 73.8548%\n",
      "total_backward_count 1302070 real_backward_count 42227   3.243%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.246385/  0.879580, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3263%\n",
      "layer   3  Sparsity: 74.2506%\n",
      "total_backward_count 1311860 real_backward_count 42270   3.222%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.243175/  0.919468, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3881%\n",
      "layer   3  Sparsity: 74.6493%\n",
      "total_backward_count 1321650 real_backward_count 42295   3.200%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.243149/  0.881749, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3185%\n",
      "layer   3  Sparsity: 74.5134%\n",
      "total_backward_count 1331440 real_backward_count 42325   3.179%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.242468/  0.897660, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.1591%\n",
      "layer   3  Sparsity: 74.5521%\n",
      "total_backward_count 1341230 real_backward_count 42358   3.158%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.241907/  0.855401, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.1712%\n",
      "layer   3  Sparsity: 74.3496%\n",
      "total_backward_count 1351020 real_backward_count 42381   3.137%\n",
      "fc layer 3 self.abs_max_out: 1565.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.234763/  0.881385, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4167%\n",
      "layer   3  Sparsity: 74.5071%\n",
      "total_backward_count 1360810 real_backward_count 42419   3.117%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.234758/  0.858354, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4248%\n",
      "layer   3  Sparsity: 74.6061%\n",
      "total_backward_count 1370600 real_backward_count 42441   3.097%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.234263/  0.863248, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3466%\n",
      "layer   3  Sparsity: 74.3180%\n",
      "total_backward_count 1380390 real_backward_count 42456   3.076%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.237732/  0.883324, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3787%\n",
      "layer   3  Sparsity: 74.0991%\n",
      "total_backward_count 1390180 real_backward_count 42484   3.056%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.237531/  0.850209, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5295%\n",
      "layer   3  Sparsity: 74.0007%\n",
      "total_backward_count 1399970 real_backward_count 42508   3.036%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.238187/  0.848049, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7739%\n",
      "layer   3  Sparsity: 73.7844%\n",
      "total_backward_count 1409760 real_backward_count 42530   3.017%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.236288/  0.863087, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6690%\n",
      "layer   3  Sparsity: 74.2738%\n",
      "total_backward_count 1419550 real_backward_count 42555   2.998%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.236004/  0.860300, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5470%\n",
      "layer   3  Sparsity: 74.3541%\n",
      "total_backward_count 1429340 real_backward_count 42578   2.979%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.237164/  0.834608, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4806%\n",
      "layer   3  Sparsity: 74.1511%\n",
      "total_backward_count 1439130 real_backward_count 42600   2.960%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.231938/  0.839278, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5151%\n",
      "layer   3  Sparsity: 74.2290%\n",
      "total_backward_count 1448920 real_backward_count 42609   2.941%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.240168/  0.884581, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5329%\n",
      "layer   3  Sparsity: 74.0016%\n",
      "total_backward_count 1458710 real_backward_count 42643   2.923%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.235088/  0.859969, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6950%\n",
      "layer   3  Sparsity: 74.1200%\n",
      "total_backward_count 1468500 real_backward_count 42672   2.906%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.231506/  0.841842, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8025%\n",
      "layer   3  Sparsity: 74.4113%\n",
      "total_backward_count 1478290 real_backward_count 42698   2.888%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.234034/  0.854477, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8689%\n",
      "layer   3  Sparsity: 74.2883%\n",
      "total_backward_count 1488080 real_backward_count 42722   2.871%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.233703/  0.847593, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9930%\n",
      "layer   3  Sparsity: 74.2852%\n",
      "total_backward_count 1497870 real_backward_count 42748   2.854%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.232848/  0.854399, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9357%\n",
      "layer   3  Sparsity: 74.7562%\n",
      "total_backward_count 1507660 real_backward_count 42764   2.836%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.235145/  0.882941, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8997%\n",
      "layer   3  Sparsity: 74.9769%\n",
      "total_backward_count 1517450 real_backward_count 42778   2.819%\n",
      "fc layer 3 self.abs_max_out: 1570.0\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.236348/  0.907963, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9055%\n",
      "layer   3  Sparsity: 74.8238%\n",
      "total_backward_count 1527240 real_backward_count 42802   2.803%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.240034/  0.884693, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7668%\n",
      "layer   3  Sparsity: 74.7983%\n",
      "total_backward_count 1537030 real_backward_count 42844   2.787%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.237623/  0.853393, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5950%\n",
      "layer   3  Sparsity: 74.4616%\n",
      "total_backward_count 1546820 real_backward_count 42878   2.772%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.233885/  0.842806, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4539%\n",
      "layer   3  Sparsity: 74.2280%\n",
      "total_backward_count 1556610 real_backward_count 42898   2.756%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.237227/  0.854260, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4811%\n",
      "layer   3  Sparsity: 74.2555%\n",
      "total_backward_count 1566400 real_backward_count 42918   2.740%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.231631/  0.834326, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4617%\n",
      "layer   3  Sparsity: 74.3115%\n",
      "total_backward_count 1576190 real_backward_count 42932   2.724%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.237012/  0.863688, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4556%\n",
      "layer   3  Sparsity: 74.1647%\n",
      "total_backward_count 1585980 real_backward_count 42962   2.709%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.228112/  0.856042, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4187%\n",
      "layer   3  Sparsity: 74.5095%\n",
      "total_backward_count 1595770 real_backward_count 42997   2.694%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.231544/  0.845309, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4628%\n",
      "layer   3  Sparsity: 74.6989%\n",
      "total_backward_count 1605560 real_backward_count 43020   2.679%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.225752/  0.848319, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3976%\n",
      "layer   3  Sparsity: 74.8598%\n",
      "total_backward_count 1615350 real_backward_count 43035   2.664%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.224380/  0.867037, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5508%\n",
      "layer   3  Sparsity: 74.8363%\n",
      "total_backward_count 1625140 real_backward_count 43050   2.649%\n",
      "fc layer 1 self.abs_max_out: 7990.0\n",
      "lif layer 1 self.abs_max_v: 14488.0\n",
      "lif layer 1 self.abs_max_v: 14499.0\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.221955/  0.857972, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5834%\n",
      "layer   3  Sparsity: 74.6973%\n",
      "total_backward_count 1634930 real_backward_count 43061   2.634%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.218044/  0.872113, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5945%\n",
      "layer   3  Sparsity: 74.5817%\n",
      "total_backward_count 1644720 real_backward_count 43068   2.619%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.215447/  0.837046, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5937%\n",
      "layer   3  Sparsity: 74.6733%\n",
      "total_backward_count 1654510 real_backward_count 43078   2.604%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.217982/  0.849416, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5342%\n",
      "layer   3  Sparsity: 74.6700%\n",
      "total_backward_count 1664300 real_backward_count 43089   2.589%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.215251/  0.881126, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4185%\n",
      "layer   3  Sparsity: 74.7934%\n",
      "total_backward_count 1674090 real_backward_count 43094   2.574%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.224441/  0.874383, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2891%\n",
      "layer   3  Sparsity: 74.8998%\n",
      "total_backward_count 1683880 real_backward_count 43117   2.561%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.230285/  0.878021, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2595%\n",
      "layer   3  Sparsity: 74.6098%\n",
      "total_backward_count 1693670 real_backward_count 43142   2.547%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.226938/  0.865312, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.2278%\n",
      "layer   3  Sparsity: 74.6914%\n",
      "total_backward_count 1703460 real_backward_count 43161   2.534%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.225311/  0.860278, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3130%\n",
      "layer   3  Sparsity: 74.7589%\n",
      "total_backward_count 1713250 real_backward_count 43183   2.521%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.221487/  0.847423, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3891%\n",
      "layer   3  Sparsity: 74.8807%\n",
      "total_backward_count 1723040 real_backward_count 43203   2.507%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.222436/  0.836680, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3758%\n",
      "layer   3  Sparsity: 74.6048%\n",
      "total_backward_count 1732830 real_backward_count 43220   2.494%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.217396/  0.843136, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4506%\n",
      "layer   3  Sparsity: 74.6143%\n",
      "total_backward_count 1742620 real_backward_count 43229   2.481%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.219251/  0.878903, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3807%\n",
      "layer   3  Sparsity: 74.4189%\n",
      "total_backward_count 1752410 real_backward_count 43246   2.468%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.218873/  0.870719, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3466%\n",
      "layer   3  Sparsity: 74.2760%\n",
      "total_backward_count 1762200 real_backward_count 43265   2.455%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.216076/  0.855979, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4365%\n",
      "layer   3  Sparsity: 74.7494%\n",
      "total_backward_count 1771990 real_backward_count 43283   2.443%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.218328/  0.866720, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6121%\n",
      "layer   3  Sparsity: 74.6764%\n",
      "total_backward_count 1781780 real_backward_count 43297   2.430%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.220591/  0.838443, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5210%\n",
      "layer   3  Sparsity: 74.7980%\n",
      "total_backward_count 1791570 real_backward_count 43323   2.418%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.212961/  0.825771, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5822%\n",
      "layer   3  Sparsity: 74.6387%\n",
      "total_backward_count 1801360 real_backward_count 43335   2.406%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.211981/  0.829875, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5929%\n",
      "layer   3  Sparsity: 74.7551%\n",
      "total_backward_count 1811150 real_backward_count 43344   2.393%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.212764/  0.856315, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5499%\n",
      "layer   3  Sparsity: 74.6875%\n",
      "total_backward_count 1820940 real_backward_count 43362   2.381%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.219397/  0.866057, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4913%\n",
      "layer   3  Sparsity: 74.7779%\n",
      "total_backward_count 1830730 real_backward_count 43379   2.369%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.218767/  0.886167, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4570%\n",
      "layer   3  Sparsity: 74.9539%\n",
      "total_backward_count 1840520 real_backward_count 43388   2.357%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.220656/  0.836433, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5496%\n",
      "layer   3  Sparsity: 74.8532%\n",
      "total_backward_count 1850310 real_backward_count 43410   2.346%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.214553/  0.851670, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6530%\n",
      "layer   3  Sparsity: 74.7347%\n",
      "total_backward_count 1860100 real_backward_count 43428   2.335%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.215682/  0.841069, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.5292%\n",
      "layer   3  Sparsity: 74.6288%\n",
      "total_backward_count 1869890 real_backward_count 43442   2.323%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.211131/  0.831200, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3379%\n",
      "layer   3  Sparsity: 74.7253%\n",
      "total_backward_count 1879680 real_backward_count 43447   2.311%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.212172/  0.848388, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.3203%\n",
      "layer   3  Sparsity: 74.7867%\n",
      "total_backward_count 1889470 real_backward_count 43455   2.300%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.214655/  0.845351, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.4710%\n",
      "layer   3  Sparsity: 75.0351%\n",
      "total_backward_count 1899260 real_backward_count 43468   2.289%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.215270/  0.856282, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.6991%\n",
      "layer   3  Sparsity: 75.2001%\n",
      "total_backward_count 1909050 real_backward_count 43482   2.278%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.209233/  0.849201, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7923%\n",
      "layer   3  Sparsity: 75.0015%\n",
      "total_backward_count 1918840 real_backward_count 43493   2.267%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.203314/  0.850504, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9413%\n",
      "layer   3  Sparsity: 75.2937%\n",
      "total_backward_count 1928630 real_backward_count 43506   2.256%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.199066/  0.852507, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.9505%\n",
      "layer   3  Sparsity: 75.3423%\n",
      "total_backward_count 1938420 real_backward_count 43509   2.245%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.202153/  0.850176, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.8157%\n",
      "layer   3  Sparsity: 75.2386%\n",
      "total_backward_count 1948210 real_backward_count 43519   2.234%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.202762/  0.834822, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 70.7746%\n",
      "layer   3  Sparsity: 75.2804%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8a4cd292fb43fe8ac90158e066e16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.20276</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>0.83482</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-131</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v5edd9mw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v5edd9mw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_082521-v5edd9mw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6l2kz9od with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_124009-6l2kz9od</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l2kz9od' target=\"_blank\">peachy-sweep-136</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l2kz9od' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l2kz9od</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251117_124018_548', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 146.0\n",
      "lif layer 1 self.abs_max_v: 146.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 67.0\n",
      "lif layer 2 self.abs_max_v: 67.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 207.5\n",
      "fc layer 2 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 180.5\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "fc layer 1 self.abs_max_out: 201.0\n",
      "lif layer 1 self.abs_max_v: 263.5\n",
      "fc layer 2 self.abs_max_out: 208.0\n",
      "lif layer 2 self.abs_max_v: 274.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 1 self.abs_max_out: 221.0\n",
      "lif layer 1 self.abs_max_v: 287.0\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "lif layer 1 self.abs_max_v: 308.5\n",
      "lif layer 1 self.abs_max_v: 342.5\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 347.0\n",
      "lif layer 1 self.abs_max_v: 345.5\n",
      "fc layer 3 self.abs_max_out: 98.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "fc layer 2 self.abs_max_out: 309.0\n",
      "lif layer 2 self.abs_max_v: 361.5\n",
      "lif layer 1 self.abs_max_v: 359.5\n",
      "lif layer 1 self.abs_max_v: 382.0\n",
      "fc layer 2 self.abs_max_out: 339.0\n",
      "lif layer 2 self.abs_max_v: 494.5\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "lif layer 1 self.abs_max_v: 392.0\n",
      "lif layer 2 self.abs_max_v: 541.5\n",
      "fc layer 2 self.abs_max_out: 378.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 1 self.abs_max_out: 303.0\n",
      "fc layer 1 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 2 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "lif layer 1 self.abs_max_v: 421.0\n",
      "fc layer 1 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 501.0\n",
      "lif layer 1 self.abs_max_v: 518.0\n",
      "lif layer 1 self.abs_max_v: 579.0\n",
      "fc layer 2 self.abs_max_out: 431.0\n",
      "fc layer 1 self.abs_max_out: 470.0\n",
      "lif layer 1 self.abs_max_v: 627.0\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "lif layer 2 self.abs_max_v: 698.5\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "lif layer 1 self.abs_max_v: 659.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 663.0\n",
      "lif layer 2 self.abs_max_v: 738.0\n",
      "fc layer 1 self.abs_max_out: 511.0\n",
      "fc layer 2 self.abs_max_out: 527.0\n",
      "lif layer 2 self.abs_max_v: 773.0\n",
      "lif layer 1 self.abs_max_v: 710.0\n",
      "fc layer 2 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "lif layer 1 self.abs_max_v: 790.5\n",
      "lif layer 1 self.abs_max_v: 792.5\n",
      "lif layer 1 self.abs_max_v: 832.0\n",
      "lif layer 2 self.abs_max_v: 801.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.296092/  2.291661, val:  15.42%, val_best:  15.42%, tr:  15.73%, tr_best:  15.73%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9324%\n",
      "layer   2  Sparsity: 91.5610%\n",
      "layer   3  Sparsity: 93.1414%\n",
      "total_backward_count 9790 real_backward_count 8447  86.282%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 578.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "fc layer 1 self.abs_max_out: 518.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.294663/  2.291661, val:  15.42%, val_best:  15.42%, tr:  15.42%, tr_best:  15.73%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9304%\n",
      "layer   2  Sparsity: 91.5655%\n",
      "layer   3  Sparsity: 93.1335%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1433bcaab84a5b8d5fcf45d4ecc503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.15424</td></tr><tr><td>tr_epoch_loss</td><td>2.29466</td></tr><tr><td>val_acc_best</td><td>0.15417</td></tr><tr><td>val_acc_now</td><td>0.15417</td></tr><tr><td>val_loss</td><td>2.29166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-sweep-136</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l2kz9od' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l2kz9od</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_124009-6l2kz9od/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 6l2kz9od errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_17348/82141465.py\", line 114, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_17348/2991991161.py\", line 973, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 6l2kz9od errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_17348/82141465.py\", line 114, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_17348/2991991161.py\", line 973, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9q7yobem with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_124312-9q7yobem</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9q7yobem' target=\"_blank\">cosmic-sweep-137</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9q7yobem' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9q7yobem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251117_124321_956', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 413.0\n",
      "lif layer 1 self.abs_max_v: 413.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 813.0\n",
      "lif layer 2 self.abs_max_v: 813.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "fc layer 1 self.abs_max_out: 525.0\n",
      "lif layer 1 self.abs_max_v: 557.5\n",
      "lif layer 2 self.abs_max_v: 974.0\n",
      "fc layer 1 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 929.5\n",
      "fc layer 2 self.abs_max_out: 977.0\n",
      "lif layer 2 self.abs_max_v: 1464.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 944.5\n",
      "fc layer 1 self.abs_max_out: 807.0\n",
      "lif layer 1 self.abs_max_v: 1064.0\n",
      "fc layer 2 self.abs_max_out: 1014.0\n",
      "lif layer 2 self.abs_max_v: 1717.0\n",
      "lif layer 2 self.abs_max_v: 1787.5\n",
      "fc layer 1 self.abs_max_out: 818.0\n",
      "lif layer 1 self.abs_max_v: 1223.0\n",
      "fc layer 1 self.abs_max_out: 940.0\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "fc layer 1 self.abs_max_out: 1008.0\n",
      "lif layer 1 self.abs_max_v: 1283.5\n",
      "fc layer 2 self.abs_max_out: 1159.0\n",
      "fc layer 3 self.abs_max_out: 424.0\n",
      "lif layer 1 self.abs_max_v: 1389.5\n",
      "lif layer 1 self.abs_max_v: 1501.0\n",
      "lif layer 1 self.abs_max_v: 1642.5\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "fc layer 1 self.abs_max_out: 1232.0\n",
      "fc layer 2 self.abs_max_out: 1235.0\n",
      "lif layer 2 self.abs_max_v: 1871.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 2 self.abs_max_out: 1241.0\n",
      "fc layer 1 self.abs_max_out: 1837.0\n",
      "lif layer 1 self.abs_max_v: 1837.0\n",
      "lif layer 1 self.abs_max_v: 2356.5\n",
      "lif layer 1 self.abs_max_v: 2597.0\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "lif layer 2 self.abs_max_v: 2056.0\n",
      "lif layer 1 self.abs_max_v: 2613.5\n",
      "lif layer 1 self.abs_max_v: 2674.0\n",
      "fc layer 1 self.abs_max_out: 2057.0\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 2095.0\n",
      "lif layer 1 self.abs_max_v: 2700.5\n",
      "lif layer 2 self.abs_max_v: 2101.5\n",
      "fc layer 1 self.abs_max_out: 2258.0\n",
      "lif layer 1 self.abs_max_v: 2874.0\n",
      "lif layer 1 self.abs_max_v: 3523.0\n",
      "lif layer 2 self.abs_max_v: 2236.0\n",
      "fc layer 1 self.abs_max_out: 2626.0\n",
      "lif layer 1 self.abs_max_v: 4015.5\n",
      "fc layer 1 self.abs_max_out: 2702.0\n",
      "lif layer 1 self.abs_max_v: 4205.0\n",
      "lif layer 1 self.abs_max_v: 4453.5\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 1402.0\n",
      "lif layer 2 self.abs_max_v: 2358.0\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "fc layer 2 self.abs_max_out: 1466.0\n",
      "fc layer 2 self.abs_max_out: 1561.0\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 2 self.abs_max_out: 1640.0\n",
      "lif layer 2 self.abs_max_v: 2462.0\n",
      "lif layer 2 self.abs_max_v: 2492.5\n",
      "lif layer 2 self.abs_max_v: 2565.5\n",
      "lif layer 2 self.abs_max_v: 2605.0\n",
      "lif layer 2 self.abs_max_v: 2670.5\n",
      "lif layer 2 self.abs_max_v: 2806.0\n",
      "lif layer 2 self.abs_max_v: 2822.0\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "fc layer 2 self.abs_max_out: 1669.0\n",
      "lif layer 2 self.abs_max_v: 2829.0\n",
      "fc layer 2 self.abs_max_out: 1715.0\n",
      "lif layer 2 self.abs_max_v: 2856.0\n",
      "lif layer 2 self.abs_max_v: 3031.0\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "lif layer 2 self.abs_max_v: 3152.0\n",
      "lif layer 2 self.abs_max_v: 3185.0\n",
      "lif layer 2 self.abs_max_v: 3211.5\n",
      "fc layer 2 self.abs_max_out: 1806.0\n",
      "lif layer 2 self.abs_max_v: 3278.5\n",
      "fc layer 2 self.abs_max_out: 1818.0\n",
      "lif layer 2 self.abs_max_v: 3457.5\n",
      "lif layer 2 self.abs_max_v: 3492.0\n",
      "lif layer 2 self.abs_max_v: 3554.0\n",
      "fc layer 2 self.abs_max_out: 1875.0\n",
      "fc layer 2 self.abs_max_out: 1895.0\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 678.0\n",
      "fc layer 1 self.abs_max_out: 2742.0\n",
      "fc layer 1 self.abs_max_out: 2813.0\n",
      "lif layer 1 self.abs_max_v: 4515.5\n",
      "lif layer 1 self.abs_max_v: 4626.0\n",
      "lif layer 1 self.abs_max_v: 4729.0\n",
      "lif layer 1 self.abs_max_v: 4776.0\n",
      "fc layer 2 self.abs_max_out: 2054.0\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "lif layer 1 self.abs_max_v: 4844.5\n",
      "fc layer 1 self.abs_max_out: 2883.0\n",
      "lif layer 1 self.abs_max_v: 4916.0\n",
      "lif layer 1 self.abs_max_v: 5000.5\n",
      "lif layer 1 self.abs_max_v: 5197.5\n",
      "lif layer 1 self.abs_max_v: 5218.5\n",
      "lif layer 1 self.abs_max_v: 5404.5\n",
      "fc layer 1 self.abs_max_out: 2973.0\n",
      "lif layer 1 self.abs_max_v: 5432.0\n",
      "fc layer 1 self.abs_max_out: 3077.0\n",
      "lif layer 2 self.abs_max_v: 3593.0\n",
      "fc layer 2 self.abs_max_out: 2074.0\n",
      "lif layer 2 self.abs_max_v: 3618.0\n",
      "lif layer 2 self.abs_max_v: 3677.0\n",
      "lif layer 2 self.abs_max_v: 3772.5\n",
      "lif layer 2 self.abs_max_v: 3844.5\n",
      "lif layer 1 self.abs_max_v: 5510.5\n",
      "fc layer 1 self.abs_max_out: 3174.0\n",
      "fc layer 1 self.abs_max_out: 3405.0\n",
      "fc layer 3 self.abs_max_out: 819.0\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "lif layer 1 self.abs_max_v: 5534.5\n",
      "lif layer 1 self.abs_max_v: 5603.0\n",
      "lif layer 1 self.abs_max_v: 5634.0\n",
      "lif layer 1 self.abs_max_v: 5672.5\n",
      "lif layer 1 self.abs_max_v: 5838.0\n",
      "fc layer 1 self.abs_max_out: 3512.0\n",
      "fc layer 1 self.abs_max_out: 3594.0\n",
      "lif layer 1 self.abs_max_v: 5897.0\n",
      "fc layer 1 self.abs_max_out: 3660.0\n",
      "lif layer 1 self.abs_max_v: 6600.5\n",
      "fc layer 1 self.abs_max_out: 3699.0\n",
      "fc layer 1 self.abs_max_out: 3753.0\n",
      "fc layer 1 self.abs_max_out: 3913.0\n",
      "lif layer 1 self.abs_max_v: 7023.5\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.686305/  1.954799, val:  31.25%, val_best:  31.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8686%\n",
      "layer   2  Sparsity: 70.5518%\n",
      "layer   3  Sparsity: 62.1266%\n",
      "total_backward_count 9790 real_backward_count 1639  16.742%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 4046.0\n",
      "fc layer 1 self.abs_max_out: 4077.0\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "fc layer 3 self.abs_max_out: 896.0\n",
      "fc layer 3 self.abs_max_out: 986.0\n",
      "fc layer 3 self.abs_max_out: 1004.0\n",
      "fc layer 1 self.abs_max_out: 4376.0\n",
      "lif layer 1 self.abs_max_v: 7398.5\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "fc layer 2 self.abs_max_out: 2206.0\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "fc layer 2 self.abs_max_out: 2277.0\n",
      "lif layer 1 self.abs_max_v: 7656.5\n",
      "lif layer 1 self.abs_max_v: 7931.5\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2379.0\n",
      "fc layer 2 self.abs_max_out: 2462.0\n",
      "fc layer 1 self.abs_max_out: 4443.0\n",
      "fc layer 1 self.abs_max_out: 4497.0\n",
      "lif layer 1 self.abs_max_v: 8045.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.614087/  1.888407, val:  42.92%, val_best:  42.92%, tr:  98.98%, tr_best:  98.98%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8520%\n",
      "layer   2  Sparsity: 74.7577%\n",
      "layer   3  Sparsity: 65.1132%\n",
      "total_backward_count 19580 real_backward_count 3046  15.557%\n",
      "fc layer 1 self.abs_max_out: 4709.0\n",
      "fc layer 2 self.abs_max_out: 2486.0\n",
      "lif layer 2 self.abs_max_v: 3882.0\n",
      "lif layer 1 self.abs_max_v: 8221.5\n",
      "lif layer 1 self.abs_max_v: 8307.5\n",
      "lif layer 1 self.abs_max_v: 8384.0\n",
      "fc layer 3 self.abs_max_out: 1056.0\n",
      "fc layer 2 self.abs_max_out: 2699.0\n",
      "lif layer 2 self.abs_max_v: 3907.0\n",
      "lif layer 2 self.abs_max_v: 4059.5\n",
      "lif layer 2 self.abs_max_v: 4083.0\n",
      "fc layer 1 self.abs_max_out: 4874.0\n",
      "lif layer 2 self.abs_max_v: 4147.0\n",
      "lif layer 2 self.abs_max_v: 4179.0\n",
      "fc layer 1 self.abs_max_out: 5069.0\n",
      "lif layer 2 self.abs_max_v: 4254.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.607259/  1.899622, val:  46.25%, val_best:  46.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8445%\n",
      "layer   2  Sparsity: 74.2727%\n",
      "layer   3  Sparsity: 65.7573%\n",
      "total_backward_count 29370 real_backward_count 4347  14.801%\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "lif layer 1 self.abs_max_v: 8495.5\n",
      "lif layer 1 self.abs_max_v: 8609.0\n",
      "lif layer 1 self.abs_max_v: 9247.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.594839/  1.903880, val:  42.50%, val_best:  46.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8615%\n",
      "layer   2  Sparsity: 74.2831%\n",
      "layer   3  Sparsity: 66.6842%\n",
      "total_backward_count 39160 real_backward_count 5616  14.341%\n",
      "lif layer 2 self.abs_max_v: 4262.5\n",
      "lif layer 2 self.abs_max_v: 4268.5\n",
      "lif layer 2 self.abs_max_v: 4446.5\n",
      "lif layer 2 self.abs_max_v: 4585.0\n",
      "lif layer 2 self.abs_max_v: 4828.5\n",
      "fc layer 1 self.abs_max_out: 5368.0\n",
      "fc layer 3 self.abs_max_out: 1118.0\n",
      "fc layer 3 self.abs_max_out: 1147.0\n",
      "fc layer 3 self.abs_max_out: 1156.0\n",
      "fc layer 1 self.abs_max_out: 5387.0\n",
      "lif layer 1 self.abs_max_v: 9635.0\n",
      "lif layer 2 self.abs_max_v: 5000.0\n",
      "fc layer 2 self.abs_max_out: 2808.0\n",
      "lif layer 2 self.abs_max_v: 5251.0\n",
      "fc layer 1 self.abs_max_out: 5732.0\n",
      "lif layer 1 self.abs_max_v: 9695.5\n",
      "lif layer 1 self.abs_max_v: 9804.0\n",
      "lif layer 1 self.abs_max_v: 9860.5\n",
      "fc layer 3 self.abs_max_out: 1157.0\n",
      "fc layer 1 self.abs_max_out: 5854.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.566653/  1.826142, val:  48.33%, val_best:  48.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8524%\n",
      "layer   2  Sparsity: 74.1315%\n",
      "layer   3  Sparsity: 65.9002%\n",
      "total_backward_count 48950 real_backward_count 6861  14.016%\n",
      "lif layer 1 self.abs_max_v: 9989.5\n",
      "fc layer 2 self.abs_max_out: 3108.0\n",
      "fc layer 1 self.abs_max_out: 6032.0\n",
      "lif layer 1 self.abs_max_v: 10015.5\n",
      "lif layer 1 self.abs_max_v: 10939.0\n",
      "lif layer 1 self.abs_max_v: 10960.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.528315/  1.811406, val:  48.33%, val_best:  48.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8369%\n",
      "layer   2  Sparsity: 75.0017%\n",
      "layer   3  Sparsity: 66.0390%\n",
      "total_backward_count 58740 real_backward_count 8102  13.793%\n",
      "fc layer 1 self.abs_max_out: 6110.0\n",
      "lif layer 1 self.abs_max_v: 11033.5\n",
      "fc layer 2 self.abs_max_out: 3113.0\n",
      "fc layer 1 self.abs_max_out: 6232.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.518570/  1.790651, val:  51.67%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8284%\n",
      "layer   2  Sparsity: 74.3741%\n",
      "layer   3  Sparsity: 65.2768%\n",
      "total_backward_count 68530 real_backward_count 9300  13.571%\n",
      "lif layer 1 self.abs_max_v: 11287.5\n",
      "fc layer 1 self.abs_max_out: 6274.0\n",
      "lif layer 1 self.abs_max_v: 11918.0\n",
      "fc layer 1 self.abs_max_out: 6324.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.495627/  1.756060, val:  49.58%, val_best:  51.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8397%\n",
      "layer   2  Sparsity: 74.3644%\n",
      "layer   3  Sparsity: 65.0215%\n",
      "total_backward_count 78320 real_backward_count 10426  13.312%\n",
      "fc layer 1 self.abs_max_out: 6398.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.472905/  1.719236, val:  53.33%, val_best:  53.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8566%\n",
      "layer   2  Sparsity: 74.5752%\n",
      "layer   3  Sparsity: 65.6452%\n",
      "total_backward_count 88110 real_backward_count 11598  13.163%\n",
      "fc layer 3 self.abs_max_out: 1170.0\n",
      "fc layer 1 self.abs_max_out: 6483.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.465862/  1.754343, val:  53.75%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8397%\n",
      "layer   2  Sparsity: 74.6676%\n",
      "layer   3  Sparsity: 64.9893%\n",
      "total_backward_count 97900 real_backward_count 12688  12.960%\n",
      "fc layer 1 self.abs_max_out: 6591.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.417745/  1.756538, val:  50.00%, val_best:  53.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8715%\n",
      "layer   2  Sparsity: 73.9194%\n",
      "layer   3  Sparsity: 65.0911%\n",
      "total_backward_count 107690 real_backward_count 13806  12.820%\n",
      "fc layer 3 self.abs_max_out: 1251.0\n",
      "fc layer 1 self.abs_max_out: 6977.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.424754/  1.726478, val:  58.33%, val_best:  58.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8714%\n",
      "layer   2  Sparsity: 73.7402%\n",
      "layer   3  Sparsity: 65.3284%\n",
      "total_backward_count 117480 real_backward_count 14946  12.722%\n",
      "fc layer 2 self.abs_max_out: 3398.0\n",
      "lif layer 1 self.abs_max_v: 12036.5\n",
      "lif layer 2 self.abs_max_v: 5251.5\n",
      "lif layer 2 self.abs_max_v: 5281.0\n",
      "lif layer 2 self.abs_max_v: 5293.0\n",
      "lif layer 2 self.abs_max_v: 5302.0\n",
      "lif layer 1 self.abs_max_v: 12194.0\n",
      "lif layer 1 self.abs_max_v: 12941.0\n",
      "fc layer 1 self.abs_max_out: 7151.0\n",
      "lif layer 1 self.abs_max_v: 13621.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.413412/  1.713898, val:  48.33%, val_best:  58.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8497%\n",
      "layer   2  Sparsity: 74.6917%\n",
      "layer   3  Sparsity: 65.0711%\n",
      "total_backward_count 127270 real_backward_count 15976  12.553%\n",
      "lif layer 2 self.abs_max_v: 5404.5\n",
      "lif layer 2 self.abs_max_v: 5456.0\n",
      "fc layer 1 self.abs_max_out: 7183.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.410475/  1.760041, val:  45.42%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8605%\n",
      "layer   2  Sparsity: 75.2282%\n",
      "layer   3  Sparsity: 66.2077%\n",
      "total_backward_count 137060 real_backward_count 17062  12.449%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.422523/  1.699597, val:  54.17%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8565%\n",
      "layer   2  Sparsity: 75.1368%\n",
      "layer   3  Sparsity: 67.4104%\n",
      "total_backward_count 146850 real_backward_count 18140  12.353%\n",
      "lif layer 2 self.abs_max_v: 5478.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.411013/  1.652649, val:  59.58%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8452%\n",
      "layer   2  Sparsity: 74.2027%\n",
      "layer   3  Sparsity: 66.4927%\n",
      "total_backward_count 156640 real_backward_count 19246  12.287%\n",
      "fc layer 1 self.abs_max_out: 7711.0\n",
      "lif layer 2 self.abs_max_v: 5487.0\n",
      "lif layer 2 self.abs_max_v: 5658.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.403908/  1.699294, val:  54.58%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8020%\n",
      "layer   2  Sparsity: 74.3077%\n",
      "layer   3  Sparsity: 66.6824%\n",
      "total_backward_count 166430 real_backward_count 20313  12.205%\n",
      "fc layer 2 self.abs_max_out: 3788.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.410247/  1.668770, val:  59.17%, val_best:  59.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8423%\n",
      "layer   2  Sparsity: 73.3526%\n",
      "layer   3  Sparsity: 68.0532%\n",
      "total_backward_count 176220 real_backward_count 21383  12.134%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.381748/  1.652444, val:  53.33%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8535%\n",
      "layer   2  Sparsity: 74.2053%\n",
      "layer   3  Sparsity: 67.7393%\n",
      "total_backward_count 186010 real_backward_count 22491  12.091%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.368018/  1.710821, val:  39.17%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8514%\n",
      "layer   2  Sparsity: 74.8609%\n",
      "layer   3  Sparsity: 66.4956%\n",
      "total_backward_count 195800 real_backward_count 23535  12.020%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.353896/  1.655237, val:  50.83%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8301%\n",
      "layer   2  Sparsity: 74.8716%\n",
      "layer   3  Sparsity: 66.9351%\n",
      "total_backward_count 205590 real_backward_count 24573  11.952%\n",
      "fc layer 1 self.abs_max_out: 7762.0\n",
      "fc layer 1 self.abs_max_out: 7795.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.363759/  1.664128, val:  61.25%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8831%\n",
      "layer   2  Sparsity: 74.5667%\n",
      "layer   3  Sparsity: 67.5372%\n",
      "total_backward_count 215380 real_backward_count 25629  11.899%\n",
      "fc layer 1 self.abs_max_out: 7864.0\n",
      "fc layer 1 self.abs_max_out: 8039.0\n",
      "fc layer 1 self.abs_max_out: 8060.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.360188/  1.596626, val:  55.42%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8612%\n",
      "layer   2  Sparsity: 74.4548%\n",
      "layer   3  Sparsity: 67.7596%\n",
      "total_backward_count 225170 real_backward_count 26671  11.845%\n",
      "fc layer 1 self.abs_max_out: 8425.0\n",
      "fc layer 1 self.abs_max_out: 8519.0\n",
      "fc layer 1 self.abs_max_out: 8837.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.355516/  1.629756, val:  59.17%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8368%\n",
      "layer   2  Sparsity: 75.1765%\n",
      "layer   3  Sparsity: 67.3028%\n",
      "total_backward_count 234960 real_backward_count 27722  11.799%\n",
      "fc layer 1 self.abs_max_out: 8986.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.343544/  1.574856, val:  62.08%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8827%\n",
      "layer   2  Sparsity: 75.2002%\n",
      "layer   3  Sparsity: 67.7739%\n",
      "total_backward_count 244750 real_backward_count 28757  11.750%\n",
      "fc layer 1 self.abs_max_out: 9113.0\n",
      "fc layer 1 self.abs_max_out: 9347.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.328918/  1.602275, val:  61.67%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8638%\n",
      "layer   2  Sparsity: 74.7274%\n",
      "layer   3  Sparsity: 67.4953%\n",
      "total_backward_count 254540 real_backward_count 29837  11.722%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.322435/  1.602748, val:  57.08%, val_best:  62.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8547%\n",
      "layer   2  Sparsity: 74.5675%\n",
      "layer   3  Sparsity: 68.1676%\n",
      "total_backward_count 264330 real_backward_count 30900  11.690%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.307477/  1.560353, val:  62.08%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8502%\n",
      "layer   2  Sparsity: 73.8046%\n",
      "layer   3  Sparsity: 67.3279%\n",
      "total_backward_count 274120 real_backward_count 31891  11.634%\n",
      "fc layer 3 self.abs_max_out: 1289.0\n",
      "fc layer 2 self.abs_max_out: 3799.0\n",
      "fc layer 3 self.abs_max_out: 1293.0\n",
      "fc layer 3 self.abs_max_out: 1295.0\n",
      "fc layer 3 self.abs_max_out: 1407.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.274016/  1.596988, val:  53.75%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8376%\n",
      "layer   2  Sparsity: 74.0228%\n",
      "layer   3  Sparsity: 66.1108%\n",
      "total_backward_count 283910 real_backward_count 32910  11.592%\n",
      "lif layer 1 self.abs_max_v: 13717.5\n",
      "lif layer 1 self.abs_max_v: 14010.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.268112/  1.561080, val:  56.25%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8586%\n",
      "layer   2  Sparsity: 73.7167%\n",
      "layer   3  Sparsity: 65.3935%\n",
      "total_backward_count 293700 real_backward_count 33905  11.544%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.274580/  1.573135, val:  61.25%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8517%\n",
      "layer   2  Sparsity: 74.2819%\n",
      "layer   3  Sparsity: 66.1824%\n",
      "total_backward_count 303490 real_backward_count 34842  11.480%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.302940/  1.639511, val:  55.42%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8916%\n",
      "layer   2  Sparsity: 74.0006%\n",
      "layer   3  Sparsity: 66.4157%\n",
      "total_backward_count 313280 real_backward_count 35789  11.424%\n",
      "lif layer 2 self.abs_max_v: 5838.0\n",
      "lif layer 2 self.abs_max_v: 6055.5\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.323105/  1.639884, val:  58.75%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8241%\n",
      "layer   2  Sparsity: 74.2256%\n",
      "layer   3  Sparsity: 66.0605%\n",
      "total_backward_count 323070 real_backward_count 36735  11.371%\n",
      "lif layer 2 self.abs_max_v: 6426.5\n",
      "fc layer 2 self.abs_max_out: 3872.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.278962/  1.572222, val:  60.83%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8516%\n",
      "layer   2  Sparsity: 73.7571%\n",
      "layer   3  Sparsity: 66.0965%\n",
      "total_backward_count 332860 real_backward_count 37683  11.321%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.258116/  1.540045, val:  60.42%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8336%\n",
      "layer   2  Sparsity: 73.6545%\n",
      "layer   3  Sparsity: 67.3236%\n",
      "total_backward_count 342650 real_backward_count 38655  11.281%\n",
      "lif layer 1 self.abs_max_v: 14795.5\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.271647/  1.566255, val:  56.67%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8746%\n",
      "layer   2  Sparsity: 74.2288%\n",
      "layer   3  Sparsity: 67.7985%\n",
      "total_backward_count 352440 real_backward_count 39565  11.226%\n",
      "lif layer 1 self.abs_max_v: 14887.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.240410/  1.520050, val:  64.17%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8625%\n",
      "layer   2  Sparsity: 73.6494%\n",
      "layer   3  Sparsity: 67.5927%\n",
      "total_backward_count 362230 real_backward_count 40500  11.181%\n",
      "fc layer 3 self.abs_max_out: 1442.0\n",
      "fc layer 3 self.abs_max_out: 1458.0\n",
      "fc layer 1 self.abs_max_out: 9684.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.211590/  1.511714, val:  60.42%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8548%\n",
      "layer   2  Sparsity: 74.0575%\n",
      "layer   3  Sparsity: 66.9721%\n",
      "total_backward_count 372020 real_backward_count 41355  11.116%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.232803/  1.509995, val:  61.25%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8372%\n",
      "layer   2  Sparsity: 74.3915%\n",
      "layer   3  Sparsity: 66.9113%\n",
      "total_backward_count 381810 real_backward_count 42319  11.084%\n",
      "fc layer 1 self.abs_max_out: 9939.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.224608/  1.531812, val:  63.33%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8538%\n",
      "layer   2  Sparsity: 74.6889%\n",
      "layer   3  Sparsity: 67.9228%\n",
      "total_backward_count 391600 real_backward_count 43196  11.031%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.238800/  1.518787, val:  62.50%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8772%\n",
      "layer   2  Sparsity: 74.8311%\n",
      "layer   3  Sparsity: 67.9899%\n",
      "total_backward_count 401390 real_backward_count 44158  11.001%\n",
      "fc layer 1 self.abs_max_out: 9959.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.225543/  1.509332, val:  66.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8535%\n",
      "layer   2  Sparsity: 74.5975%\n",
      "layer   3  Sparsity: 67.5633%\n",
      "total_backward_count 411180 real_backward_count 45043  10.955%\n",
      "lif layer 1 self.abs_max_v: 14983.5\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.182918/  1.484174, val:  64.17%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8583%\n",
      "layer   2  Sparsity: 73.9530%\n",
      "layer   3  Sparsity: 67.1085%\n",
      "total_backward_count 420970 real_backward_count 45935  10.912%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.191851/  1.502666, val:  64.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8586%\n",
      "layer   2  Sparsity: 73.8068%\n",
      "layer   3  Sparsity: 67.1867%\n",
      "total_backward_count 430760 real_backward_count 46813  10.868%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.190437/  1.526752, val:  61.67%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8568%\n",
      "layer   2  Sparsity: 73.8668%\n",
      "layer   3  Sparsity: 67.4181%\n",
      "total_backward_count 440550 real_backward_count 47679  10.823%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.210701/  1.510359, val:  61.67%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8472%\n",
      "layer   2  Sparsity: 74.0500%\n",
      "layer   3  Sparsity: 67.4648%\n",
      "total_backward_count 450340 real_backward_count 48564  10.784%\n",
      "lif layer 1 self.abs_max_v: 15076.5\n",
      "lif layer 1 self.abs_max_v: 15222.5\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.222150/  1.562014, val:  55.83%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8502%\n",
      "layer   2  Sparsity: 74.0859%\n",
      "layer   3  Sparsity: 67.4116%\n",
      "total_backward_count 460130 real_backward_count 49479  10.753%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.203779/  1.496090, val:  57.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8554%\n",
      "layer   2  Sparsity: 73.7966%\n",
      "layer   3  Sparsity: 67.3628%\n",
      "total_backward_count 469920 real_backward_count 50353  10.715%\n",
      "lif layer 1 self.abs_max_v: 15320.0\n",
      "lif layer 1 self.abs_max_v: 15469.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.160145/  1.493715, val:  62.92%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8758%\n",
      "layer   2  Sparsity: 74.4645%\n",
      "layer   3  Sparsity: 67.4714%\n",
      "total_backward_count 479710 real_backward_count 51231  10.680%\n",
      "lif layer 1 self.abs_max_v: 16437.0\n",
      "lif layer 1 self.abs_max_v: 16609.5\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.171648/  1.497766, val:  62.92%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8523%\n",
      "layer   2  Sparsity: 74.3945%\n",
      "layer   3  Sparsity: 67.0083%\n",
      "total_backward_count 489500 real_backward_count 52130  10.650%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.171254/  1.470867, val:  60.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8576%\n",
      "layer   2  Sparsity: 73.3605%\n",
      "layer   3  Sparsity: 67.3482%\n",
      "total_backward_count 499290 real_backward_count 52990  10.613%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.141566/  1.468545, val:  66.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8559%\n",
      "layer   2  Sparsity: 73.7962%\n",
      "layer   3  Sparsity: 66.0937%\n",
      "total_backward_count 509080 real_backward_count 53825  10.573%\n",
      "fc layer 3 self.abs_max_out: 1459.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.160392/  1.434432, val:  71.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8589%\n",
      "layer   2  Sparsity: 73.1811%\n",
      "layer   3  Sparsity: 66.0736%\n",
      "total_backward_count 518870 real_backward_count 54693  10.541%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.128692/  1.443618, val:  62.08%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8336%\n",
      "layer   2  Sparsity: 73.4834%\n",
      "layer   3  Sparsity: 65.9854%\n",
      "total_backward_count 528660 real_backward_count 55530  10.504%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.114445/  1.441968, val:  70.00%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8473%\n",
      "layer   2  Sparsity: 73.7175%\n",
      "layer   3  Sparsity: 66.4021%\n",
      "total_backward_count 538450 real_backward_count 56331  10.462%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.125535/  1.426915, val:  68.75%, val_best:  71.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8438%\n",
      "layer   2  Sparsity: 73.5697%\n",
      "layer   3  Sparsity: 65.4670%\n",
      "total_backward_count 548240 real_backward_count 57190  10.432%\n",
      "fc layer 3 self.abs_max_out: 1479.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.115217/  1.505179, val:  55.42%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8573%\n",
      "layer   2  Sparsity: 73.8270%\n",
      "layer   3  Sparsity: 66.2454%\n",
      "total_backward_count 558030 real_backward_count 58032  10.399%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.105361/  1.451965, val:  60.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8691%\n",
      "layer   2  Sparsity: 74.5450%\n",
      "layer   3  Sparsity: 65.2030%\n",
      "total_backward_count 567820 real_backward_count 58818  10.359%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.103424/  1.405114, val:  63.33%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8565%\n",
      "layer   2  Sparsity: 74.7831%\n",
      "layer   3  Sparsity: 65.6410%\n",
      "total_backward_count 577610 real_backward_count 59597  10.318%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.088086/  1.471502, val:  57.50%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8738%\n",
      "layer   2  Sparsity: 74.9246%\n",
      "layer   3  Sparsity: 66.2235%\n",
      "total_backward_count 587400 real_backward_count 60400  10.283%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.081137/  1.417204, val:  62.92%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8665%\n",
      "layer   2  Sparsity: 74.0985%\n",
      "layer   3  Sparsity: 65.4024%\n",
      "total_backward_count 597190 real_backward_count 61204  10.249%\n",
      "fc layer 3 self.abs_max_out: 1481.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.059813/  1.380760, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8635%\n",
      "layer   2  Sparsity: 73.8959%\n",
      "layer   3  Sparsity: 65.3984%\n",
      "total_backward_count 606980 real_backward_count 61983  10.212%\n",
      "fc layer 3 self.abs_max_out: 1490.0\n",
      "fc layer 3 self.abs_max_out: 1501.0\n",
      "fc layer 3 self.abs_max_out: 1514.0\n",
      "fc layer 3 self.abs_max_out: 1545.0\n",
      "fc layer 3 self.abs_max_out: 1578.0\n",
      "fc layer 3 self.abs_max_out: 1599.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.058225/  1.418605, val:  61.25%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8110%\n",
      "layer   2  Sparsity: 73.9689%\n",
      "layer   3  Sparsity: 65.6436%\n",
      "total_backward_count 616770 real_backward_count 62816  10.185%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.074949/  1.437559, val:  60.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8426%\n",
      "layer   2  Sparsity: 74.0852%\n",
      "layer   3  Sparsity: 65.7518%\n",
      "total_backward_count 626560 real_backward_count 63611  10.152%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.088615/  1.458097, val:  62.50%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8870%\n",
      "layer   2  Sparsity: 73.9727%\n",
      "layer   3  Sparsity: 65.9257%\n",
      "total_backward_count 636350 real_backward_count 64392  10.119%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.072894/  1.514253, val:  55.42%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8373%\n",
      "layer   2  Sparsity: 73.7978%\n",
      "layer   3  Sparsity: 66.3586%\n",
      "total_backward_count 646140 real_backward_count 65194  10.090%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.045049/  1.373907, val:  72.08%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8470%\n",
      "layer   2  Sparsity: 74.2599%\n",
      "layer   3  Sparsity: 64.2377%\n",
      "total_backward_count 655930 real_backward_count 65973  10.058%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.034271/  1.375810, val:  64.17%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8312%\n",
      "layer   2  Sparsity: 74.5726%\n",
      "layer   3  Sparsity: 64.8001%\n",
      "total_backward_count 665720 real_backward_count 66724  10.023%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.025611/  1.351540, val:  65.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8700%\n",
      "layer   2  Sparsity: 74.5266%\n",
      "layer   3  Sparsity: 64.1740%\n",
      "total_backward_count 675510 real_backward_count 67488   9.991%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.011759/  1.375317, val:  72.08%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8339%\n",
      "layer   2  Sparsity: 75.0393%\n",
      "layer   3  Sparsity: 64.7015%\n",
      "total_backward_count 685300 real_backward_count 68292   9.965%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.028706/  1.370611, val:  68.75%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8509%\n",
      "layer   2  Sparsity: 74.4987%\n",
      "layer   3  Sparsity: 65.0283%\n",
      "total_backward_count 695090 real_backward_count 69139   9.947%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.021428/  1.353711, val:  67.92%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8529%\n",
      "layer   2  Sparsity: 74.8461%\n",
      "layer   3  Sparsity: 64.7648%\n",
      "total_backward_count 704880 real_backward_count 69899   9.916%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.019748/  1.347641, val:  65.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8569%\n",
      "layer   2  Sparsity: 74.6341%\n",
      "layer   3  Sparsity: 64.6643%\n",
      "total_backward_count 714670 real_backward_count 70643   9.885%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.025044/  1.414362, val:  58.33%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8250%\n",
      "layer   2  Sparsity: 75.0383%\n",
      "layer   3  Sparsity: 65.0285%\n",
      "total_backward_count 724460 real_backward_count 71392   9.855%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.000813/  1.315384, val:  67.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8195%\n",
      "layer   2  Sparsity: 74.9071%\n",
      "layer   3  Sparsity: 64.2696%\n",
      "total_backward_count 734250 real_backward_count 72194   9.832%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.981726/  1.305634, val:  70.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8138%\n",
      "layer   2  Sparsity: 74.9043%\n",
      "layer   3  Sparsity: 64.5390%\n",
      "total_backward_count 744040 real_backward_count 72894   9.797%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.962889/  1.354464, val:  61.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8687%\n",
      "layer   2  Sparsity: 74.6039%\n",
      "layer   3  Sparsity: 64.0969%\n",
      "total_backward_count 753830 real_backward_count 73629   9.767%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.964351/  1.429837, val:  56.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8552%\n",
      "layer   2  Sparsity: 74.5477%\n",
      "layer   3  Sparsity: 63.3133%\n",
      "total_backward_count 763620 real_backward_count 74333   9.734%\n",
      "fc layer 3 self.abs_max_out: 1608.0\n",
      "fc layer 3 self.abs_max_out: 1618.0\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.954999/  1.310029, val:  70.00%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8638%\n",
      "layer   2  Sparsity: 74.8872%\n",
      "layer   3  Sparsity: 63.4691%\n",
      "total_backward_count 773410 real_backward_count 75064   9.706%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.943748/  1.306211, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8362%\n",
      "layer   2  Sparsity: 74.7984%\n",
      "layer   3  Sparsity: 63.9396%\n",
      "total_backward_count 783200 real_backward_count 75793   9.677%\n",
      "fc layer 3 self.abs_max_out: 1643.0\n",
      "fc layer 3 self.abs_max_out: 1664.0\n",
      "fc layer 3 self.abs_max_out: 1681.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.956557/  1.300780, val:  72.08%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8560%\n",
      "layer   2  Sparsity: 74.5125%\n",
      "layer   3  Sparsity: 64.0783%\n",
      "total_backward_count 792990 real_backward_count 76523   9.650%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.960651/  1.372156, val:  56.25%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8588%\n",
      "layer   2  Sparsity: 74.3628%\n",
      "layer   3  Sparsity: 64.2074%\n",
      "total_backward_count 802780 real_backward_count 77228   9.620%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.943352/  1.305954, val:  65.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8559%\n",
      "layer   2  Sparsity: 74.7712%\n",
      "layer   3  Sparsity: 64.3347%\n",
      "total_backward_count 812570 real_backward_count 77907   9.588%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.947117/  1.354931, val:  62.08%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8492%\n",
      "layer   2  Sparsity: 74.7851%\n",
      "layer   3  Sparsity: 64.4253%\n",
      "total_backward_count 822360 real_backward_count 78605   9.558%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.942047/  1.335579, val:  63.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8579%\n",
      "layer   2  Sparsity: 74.2390%\n",
      "layer   3  Sparsity: 64.9368%\n",
      "total_backward_count 832150 real_backward_count 79306   9.530%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.932068/  1.331302, val:  67.08%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8760%\n",
      "layer   2  Sparsity: 74.2438%\n",
      "layer   3  Sparsity: 64.2801%\n",
      "total_backward_count 841940 real_backward_count 80020   9.504%\n",
      "fc layer 3 self.abs_max_out: 1685.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.921569/  1.316627, val:  65.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8239%\n",
      "layer   2  Sparsity: 74.3323%\n",
      "layer   3  Sparsity: 64.2087%\n",
      "total_backward_count 851730 real_backward_count 80735   9.479%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.937355/  1.331848, val:  68.75%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8592%\n",
      "layer   2  Sparsity: 74.6075%\n",
      "layer   3  Sparsity: 64.1697%\n",
      "total_backward_count 861520 real_backward_count 81441   9.453%\n",
      "fc layer 3 self.abs_max_out: 1713.0\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.925327/  1.270249, val:  72.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8726%\n",
      "layer   2  Sparsity: 74.2050%\n",
      "layer   3  Sparsity: 63.7764%\n",
      "total_backward_count 871310 real_backward_count 82110   9.424%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.922270/  1.287633, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8500%\n",
      "layer   2  Sparsity: 73.6890%\n",
      "layer   3  Sparsity: 63.4861%\n",
      "total_backward_count 881100 real_backward_count 82827   9.400%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.921302/  1.297640, val:  67.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8548%\n",
      "layer   2  Sparsity: 73.3565%\n",
      "layer   3  Sparsity: 62.6739%\n",
      "total_backward_count 890890 real_backward_count 83560   9.379%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.909550/  1.275749, val:  66.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8119%\n",
      "layer   2  Sparsity: 73.6571%\n",
      "layer   3  Sparsity: 62.6856%\n",
      "total_backward_count 900680 real_backward_count 84244   9.353%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.896321/  1.238525, val:  66.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8456%\n",
      "layer   2  Sparsity: 73.4659%\n",
      "layer   3  Sparsity: 62.5689%\n",
      "total_backward_count 910470 real_backward_count 84892   9.324%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.875819/  1.277820, val:  70.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8522%\n",
      "layer   2  Sparsity: 73.7429%\n",
      "layer   3  Sparsity: 62.8687%\n",
      "total_backward_count 920260 real_backward_count 85577   9.299%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.902224/  1.285447, val:  65.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8416%\n",
      "layer   2  Sparsity: 73.8772%\n",
      "layer   3  Sparsity: 62.2786%\n",
      "total_backward_count 930050 real_backward_count 86228   9.271%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.886707/  1.253030, val:  70.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8085%\n",
      "layer   2  Sparsity: 73.8300%\n",
      "layer   3  Sparsity: 62.3920%\n",
      "total_backward_count 939840 real_backward_count 86934   9.250%\n",
      "fc layer 3 self.abs_max_out: 1741.0\n",
      "fc layer 3 self.abs_max_out: 1763.0\n",
      "fc layer 3 self.abs_max_out: 1796.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.869718/  1.324275, val:  58.75%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8181%\n",
      "layer   2  Sparsity: 73.6064%\n",
      "layer   3  Sparsity: 63.0252%\n",
      "total_backward_count 949630 real_backward_count 87574   9.222%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.884966/  1.298315, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8378%\n",
      "layer   2  Sparsity: 73.4575%\n",
      "layer   3  Sparsity: 62.7527%\n",
      "total_backward_count 959420 real_backward_count 88209   9.194%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.871454/  1.272009, val:  70.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8140%\n",
      "layer   2  Sparsity: 73.7025%\n",
      "layer   3  Sparsity: 62.6771%\n",
      "total_backward_count 969210 real_backward_count 88834   9.166%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.863124/  1.222269, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8403%\n",
      "layer   2  Sparsity: 73.4994%\n",
      "layer   3  Sparsity: 62.7975%\n",
      "total_backward_count 979000 real_backward_count 89458   9.138%\n",
      "fc layer 1 self.abs_max_out: 10498.0\n",
      "lif layer 1 self.abs_max_v: 17553.5\n",
      "fc layer 3 self.abs_max_out: 1814.0\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.852466/  1.218494, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8706%\n",
      "layer   2  Sparsity: 73.7584%\n",
      "layer   3  Sparsity: 62.7406%\n",
      "total_backward_count 988790 real_backward_count 90099   9.112%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.864662/  1.253653, val:  67.92%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8525%\n",
      "layer   2  Sparsity: 74.1103%\n",
      "layer   3  Sparsity: 62.6208%\n",
      "total_backward_count 998580 real_backward_count 90744   9.087%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.867553/  1.224364, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8500%\n",
      "layer   2  Sparsity: 73.3625%\n",
      "layer   3  Sparsity: 63.2842%\n",
      "total_backward_count 1008370 real_backward_count 91408   9.065%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.863496/  1.238616, val:  65.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8353%\n",
      "layer   2  Sparsity: 73.4523%\n",
      "layer   3  Sparsity: 62.7887%\n",
      "total_backward_count 1018160 real_backward_count 92021   9.038%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.841275/  1.221770, val:  70.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8523%\n",
      "layer   2  Sparsity: 73.2057%\n",
      "layer   3  Sparsity: 62.0863%\n",
      "total_backward_count 1027950 real_backward_count 92651   9.013%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.837184/  1.269967, val:  65.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8632%\n",
      "layer   2  Sparsity: 73.6450%\n",
      "layer   3  Sparsity: 62.2474%\n",
      "total_backward_count 1037740 real_backward_count 93292   8.990%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.840778/  1.282233, val:  62.08%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 87.8361%\n",
      "layer   2  Sparsity: 73.6065%\n",
      "layer   3  Sparsity: 62.2917%\n",
      "total_backward_count 1047530 real_backward_count 93905   8.964%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.856453/  1.275873, val:  65.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 87.8555%\n",
      "layer   2  Sparsity: 73.1230%\n",
      "layer   3  Sparsity: 62.7258%\n",
      "total_backward_count 1057320 real_backward_count 94594   8.947%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.854504/  1.229960, val:  68.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8193%\n",
      "layer   2  Sparsity: 73.4289%\n",
      "layer   3  Sparsity: 62.7317%\n",
      "total_backward_count 1067110 real_backward_count 95237   8.925%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.841178/  1.262988, val:  68.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8334%\n",
      "layer   2  Sparsity: 74.1812%\n",
      "layer   3  Sparsity: 63.2124%\n",
      "total_backward_count 1076900 real_backward_count 95914   8.906%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.835960/  1.313038, val:  62.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8770%\n",
      "layer   2  Sparsity: 74.3165%\n",
      "layer   3  Sparsity: 63.0223%\n",
      "total_backward_count 1086690 real_backward_count 96509   8.881%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.831453/  1.248580, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8212%\n",
      "layer   2  Sparsity: 73.9555%\n",
      "layer   3  Sparsity: 63.6275%\n",
      "total_backward_count 1096480 real_backward_count 97156   8.861%\n",
      "fc layer 3 self.abs_max_out: 1817.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.812013/  1.227667, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8698%\n",
      "layer   2  Sparsity: 73.6686%\n",
      "layer   3  Sparsity: 63.7793%\n",
      "total_backward_count 1106270 real_backward_count 97727   8.834%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.810842/  1.226770, val:  67.50%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8488%\n",
      "layer   2  Sparsity: 73.6763%\n",
      "layer   3  Sparsity: 64.0393%\n",
      "total_backward_count 1116060 real_backward_count 98317   8.809%\n",
      "fc layer 3 self.abs_max_out: 1822.0\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.817024/  1.289186, val:  60.42%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8449%\n",
      "layer   2  Sparsity: 73.3546%\n",
      "layer   3  Sparsity: 63.3358%\n",
      "total_backward_count 1125850 real_backward_count 98910   8.785%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.821281/  1.246199, val:  66.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 87.8264%\n",
      "layer   2  Sparsity: 73.0973%\n",
      "layer   3  Sparsity: 63.8789%\n",
      "total_backward_count 1135640 real_backward_count 99508   8.762%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.820814/  1.228581, val:  73.33%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8377%\n",
      "layer   2  Sparsity: 73.1308%\n",
      "layer   3  Sparsity: 62.8829%\n",
      "total_backward_count 1145430 real_backward_count 100136   8.742%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.816281/  1.231315, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8529%\n",
      "layer   2  Sparsity: 73.4178%\n",
      "layer   3  Sparsity: 63.5553%\n",
      "total_backward_count 1155220 real_backward_count 100698   8.717%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.803272/  1.272923, val:  66.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8515%\n",
      "layer   2  Sparsity: 73.3242%\n",
      "layer   3  Sparsity: 64.1094%\n",
      "total_backward_count 1165010 real_backward_count 101280   8.693%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.808320/  1.194239, val:  77.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8927%\n",
      "layer   2  Sparsity: 73.2574%\n",
      "layer   3  Sparsity: 62.8126%\n",
      "total_backward_count 1174800 real_backward_count 101872   8.671%\n",
      "fc layer 3 self.abs_max_out: 1826.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.803664/  1.217335, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8502%\n",
      "layer   2  Sparsity: 72.9368%\n",
      "layer   3  Sparsity: 62.5456%\n",
      "total_backward_count 1184590 real_backward_count 102429   8.647%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.792620/  1.210683, val:  68.33%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8591%\n",
      "layer   2  Sparsity: 73.0820%\n",
      "layer   3  Sparsity: 62.5377%\n",
      "total_backward_count 1194380 real_backward_count 103004   8.624%\n",
      "fc layer 3 self.abs_max_out: 1840.0\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.801730/  1.235960, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8707%\n",
      "layer   2  Sparsity: 72.7577%\n",
      "layer   3  Sparsity: 63.2249%\n",
      "total_backward_count 1204170 real_backward_count 103629   8.606%\n",
      "fc layer 3 self.abs_max_out: 1867.0\n",
      "fc layer 3 self.abs_max_out: 1904.0\n",
      "fc layer 3 self.abs_max_out: 1919.0\n",
      "fc layer 3 self.abs_max_out: 2012.0\n",
      "fc layer 3 self.abs_max_out: 2074.0\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.793162/  1.196087, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8347%\n",
      "layer   2  Sparsity: 72.7095%\n",
      "layer   3  Sparsity: 62.9518%\n",
      "total_backward_count 1213960 real_backward_count 104220   8.585%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.786124/  1.187001, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8489%\n",
      "layer   2  Sparsity: 72.7993%\n",
      "layer   3  Sparsity: 63.5249%\n",
      "total_backward_count 1223750 real_backward_count 104812   8.565%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.786075/  1.210527, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8778%\n",
      "layer   2  Sparsity: 72.8805%\n",
      "layer   3  Sparsity: 63.6922%\n",
      "total_backward_count 1233540 real_backward_count 105350   8.540%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.787350/  1.188042, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8937%\n",
      "layer   2  Sparsity: 72.5209%\n",
      "layer   3  Sparsity: 62.5565%\n",
      "total_backward_count 1243330 real_backward_count 105924   8.519%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.792814/  1.207125, val:  66.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8456%\n",
      "layer   2  Sparsity: 72.5607%\n",
      "layer   3  Sparsity: 61.9361%\n",
      "total_backward_count 1253120 real_backward_count 106503   8.499%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.786756/  1.227592, val:  69.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8691%\n",
      "layer   2  Sparsity: 72.3568%\n",
      "layer   3  Sparsity: 63.0680%\n",
      "total_backward_count 1262910 real_backward_count 107029   8.475%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.777607/  1.204220, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8273%\n",
      "layer   2  Sparsity: 72.3895%\n",
      "layer   3  Sparsity: 64.2862%\n",
      "total_backward_count 1272700 real_backward_count 107539   8.450%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.789984/  1.231748, val:  67.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8710%\n",
      "layer   2  Sparsity: 72.7021%\n",
      "layer   3  Sparsity: 62.9767%\n",
      "total_backward_count 1282490 real_backward_count 108107   8.429%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.788585/  1.203493, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8530%\n",
      "layer   2  Sparsity: 72.7841%\n",
      "layer   3  Sparsity: 63.5479%\n",
      "total_backward_count 1292280 real_backward_count 108670   8.409%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.763538/  1.278585, val:  66.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8430%\n",
      "layer   2  Sparsity: 73.0498%\n",
      "layer   3  Sparsity: 63.2660%\n",
      "total_backward_count 1302070 real_backward_count 109117   8.380%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.785098/  1.207236, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8534%\n",
      "layer   2  Sparsity: 72.9748%\n",
      "layer   3  Sparsity: 63.1052%\n",
      "total_backward_count 1311860 real_backward_count 109643   8.358%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.785620/  1.199016, val:  70.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8773%\n",
      "layer   2  Sparsity: 72.8411%\n",
      "layer   3  Sparsity: 63.1716%\n",
      "total_backward_count 1321650 real_backward_count 110200   8.338%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.780517/  1.205877, val:  69.58%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8546%\n",
      "layer   2  Sparsity: 72.6601%\n",
      "layer   3  Sparsity: 64.0347%\n",
      "total_backward_count 1331440 real_backward_count 110742   8.317%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.769593/  1.220690, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8631%\n",
      "layer   2  Sparsity: 72.7371%\n",
      "layer   3  Sparsity: 64.0740%\n",
      "total_backward_count 1341230 real_backward_count 111292   8.298%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.768919/  1.182055, val:  67.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8614%\n",
      "layer   2  Sparsity: 72.5861%\n",
      "layer   3  Sparsity: 63.0292%\n",
      "total_backward_count 1351020 real_backward_count 111790   8.274%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.757680/  1.205003, val:  69.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 87.8438%\n",
      "layer   2  Sparsity: 72.7785%\n",
      "layer   3  Sparsity: 63.0027%\n",
      "total_backward_count 1360810 real_backward_count 112353   8.256%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.754572/  1.152320, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.55 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 87.8566%\n",
      "layer   2  Sparsity: 72.8671%\n",
      "layer   3  Sparsity: 63.1748%\n",
      "total_backward_count 1370600 real_backward_count 112886   8.236%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.751530/  1.123758, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.53 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 87.8370%\n",
      "layer   2  Sparsity: 72.5924%\n",
      "layer   3  Sparsity: 62.8207%\n",
      "total_backward_count 1380390 real_backward_count 113402   8.215%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.747952/  1.155197, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.37 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 87.8721%\n",
      "layer   2  Sparsity: 72.7484%\n",
      "layer   3  Sparsity: 63.2521%\n",
      "total_backward_count 1390180 real_backward_count 113918   8.194%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.770163/  1.170241, val:  71.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 87.8634%\n",
      "layer   2  Sparsity: 72.5532%\n",
      "layer   3  Sparsity: 63.1136%\n",
      "total_backward_count 1399970 real_backward_count 114469   8.177%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.749640/  1.189152, val:  64.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8076%\n",
      "layer   2  Sparsity: 72.3589%\n",
      "layer   3  Sparsity: 63.1231%\n",
      "total_backward_count 1409760 real_backward_count 115010   8.158%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.738829/  1.146213, val:  72.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8556%\n",
      "layer   2  Sparsity: 72.9108%\n",
      "layer   3  Sparsity: 63.3907%\n",
      "total_backward_count 1419550 real_backward_count 115575   8.142%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.731947/  1.138535, val:  72.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8519%\n",
      "layer   2  Sparsity: 72.9841%\n",
      "layer   3  Sparsity: 62.8742%\n",
      "total_backward_count 1429340 real_backward_count 116088   8.122%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.733304/  1.171467, val:  69.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8489%\n",
      "layer   2  Sparsity: 72.6341%\n",
      "layer   3  Sparsity: 63.0815%\n",
      "total_backward_count 1439130 real_backward_count 116655   8.106%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.723299/  1.131187, val:  72.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8526%\n",
      "layer   2  Sparsity: 72.8349%\n",
      "layer   3  Sparsity: 63.1844%\n",
      "total_backward_count 1448920 real_backward_count 117111   8.083%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.725720/  1.127384, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8401%\n",
      "layer   2  Sparsity: 72.7256%\n",
      "layer   3  Sparsity: 63.7072%\n",
      "total_backward_count 1458710 real_backward_count 117593   8.061%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.708148/  1.145742, val:  72.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8580%\n",
      "layer   2  Sparsity: 73.1416%\n",
      "layer   3  Sparsity: 62.6164%\n",
      "total_backward_count 1468500 real_backward_count 118105   8.043%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.709050/  1.166112, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8529%\n",
      "layer   2  Sparsity: 72.6127%\n",
      "layer   3  Sparsity: 63.0690%\n",
      "total_backward_count 1478290 real_backward_count 118600   8.023%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.713218/  1.148749, val:  72.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8612%\n",
      "layer   2  Sparsity: 72.6674%\n",
      "layer   3  Sparsity: 63.5183%\n",
      "total_backward_count 1488080 real_backward_count 119100   8.004%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.708338/  1.129319, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8402%\n",
      "layer   2  Sparsity: 72.7091%\n",
      "layer   3  Sparsity: 63.2992%\n",
      "total_backward_count 1497870 real_backward_count 119578   7.983%\n",
      "lif layer 1 self.abs_max_v: 17589.5\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.720607/  1.135236, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8503%\n",
      "layer   2  Sparsity: 73.0149%\n",
      "layer   3  Sparsity: 62.8452%\n",
      "total_backward_count 1507660 real_backward_count 120072   7.964%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.708048/  1.125181, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8748%\n",
      "layer   2  Sparsity: 72.7012%\n",
      "layer   3  Sparsity: 63.3025%\n",
      "total_backward_count 1517450 real_backward_count 120521   7.942%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.706295/  1.150913, val:  72.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8228%\n",
      "layer   2  Sparsity: 72.7605%\n",
      "layer   3  Sparsity: 63.2260%\n",
      "total_backward_count 1527240 real_backward_count 121012   7.924%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.707470/  1.153978, val:  72.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8628%\n",
      "layer   2  Sparsity: 72.4994%\n",
      "layer   3  Sparsity: 62.0789%\n",
      "total_backward_count 1537030 real_backward_count 121536   7.907%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.723004/  1.191272, val:  69.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.8181%\n",
      "layer   2  Sparsity: 72.5239%\n",
      "layer   3  Sparsity: 63.4623%\n",
      "total_backward_count 1546820 real_backward_count 121997   7.887%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.727954/  1.129256, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8552%\n",
      "layer   2  Sparsity: 72.4216%\n",
      "layer   3  Sparsity: 63.0087%\n",
      "total_backward_count 1556610 real_backward_count 122492   7.869%\n",
      "lif layer 1 self.abs_max_v: 17659.5\n",
      "lif layer 1 self.abs_max_v: 18222.0\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.713484/  1.159817, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8450%\n",
      "layer   2  Sparsity: 72.7452%\n",
      "layer   3  Sparsity: 62.7475%\n",
      "total_backward_count 1566400 real_backward_count 122960   7.850%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.721946/  1.130799, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8625%\n",
      "layer   2  Sparsity: 72.3984%\n",
      "layer   3  Sparsity: 63.5957%\n",
      "total_backward_count 1576190 real_backward_count 123428   7.831%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.714345/  1.143228, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8252%\n",
      "layer   2  Sparsity: 72.5332%\n",
      "layer   3  Sparsity: 62.8382%\n",
      "total_backward_count 1585980 real_backward_count 123914   7.813%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.702413/  1.111659, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8559%\n",
      "layer   2  Sparsity: 72.4087%\n",
      "layer   3  Sparsity: 62.9607%\n",
      "total_backward_count 1595770 real_backward_count 124394   7.795%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.697586/  1.121062, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8583%\n",
      "layer   2  Sparsity: 72.4077%\n",
      "layer   3  Sparsity: 62.8485%\n",
      "total_backward_count 1605560 real_backward_count 124827   7.775%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.691331/  1.106558, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8162%\n",
      "layer   2  Sparsity: 72.3804%\n",
      "layer   3  Sparsity: 62.5103%\n",
      "total_backward_count 1615350 real_backward_count 125282   7.756%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.686062/  1.109818, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8296%\n",
      "layer   2  Sparsity: 72.4973%\n",
      "layer   3  Sparsity: 62.3659%\n",
      "total_backward_count 1625140 real_backward_count 125714   7.736%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.685322/  1.163557, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8341%\n",
      "layer   2  Sparsity: 72.4618%\n",
      "layer   3  Sparsity: 62.5233%\n",
      "total_backward_count 1634930 real_backward_count 126150   7.716%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.693471/  1.144287, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8492%\n",
      "layer   2  Sparsity: 72.3626%\n",
      "layer   3  Sparsity: 62.6891%\n",
      "total_backward_count 1644720 real_backward_count 126584   7.696%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.698438/  1.124174, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8865%\n",
      "layer   2  Sparsity: 72.5856%\n",
      "layer   3  Sparsity: 62.9065%\n",
      "total_backward_count 1654510 real_backward_count 127018   7.677%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.690868/  1.126033, val:  75.83%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8733%\n",
      "layer   2  Sparsity: 72.5890%\n",
      "layer   3  Sparsity: 64.1320%\n",
      "total_backward_count 1664300 real_backward_count 127488   7.660%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.694234/  1.146596, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8614%\n",
      "layer   2  Sparsity: 72.6706%\n",
      "layer   3  Sparsity: 64.2268%\n",
      "total_backward_count 1674090 real_backward_count 127926   7.642%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.699468/  1.153187, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8893%\n",
      "layer   2  Sparsity: 72.8550%\n",
      "layer   3  Sparsity: 63.5466%\n",
      "total_backward_count 1683880 real_backward_count 128380   7.624%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.691715/  1.180375, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8757%\n",
      "layer   2  Sparsity: 72.9036%\n",
      "layer   3  Sparsity: 63.6654%\n",
      "total_backward_count 1693670 real_backward_count 128868   7.609%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.684248/  1.143570, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8585%\n",
      "layer   2  Sparsity: 72.8398%\n",
      "layer   3  Sparsity: 64.5557%\n",
      "total_backward_count 1703460 real_backward_count 129283   7.589%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.689758/  1.132141, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8639%\n",
      "layer   2  Sparsity: 72.5479%\n",
      "layer   3  Sparsity: 63.6509%\n",
      "total_backward_count 1713250 real_backward_count 129716   7.571%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.689708/  1.141473, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8205%\n",
      "layer   2  Sparsity: 72.5601%\n",
      "layer   3  Sparsity: 63.9195%\n",
      "total_backward_count 1723040 real_backward_count 130140   7.553%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.680533/  1.160563, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8541%\n",
      "layer   2  Sparsity: 72.6408%\n",
      "layer   3  Sparsity: 63.9949%\n",
      "total_backward_count 1732830 real_backward_count 130594   7.536%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.687844/  1.158083, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8356%\n",
      "layer   2  Sparsity: 72.7325%\n",
      "layer   3  Sparsity: 63.5505%\n",
      "total_backward_count 1742620 real_backward_count 131079   7.522%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.688365/  1.158430, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8641%\n",
      "layer   2  Sparsity: 72.8121%\n",
      "layer   3  Sparsity: 64.0983%\n",
      "total_backward_count 1752410 real_backward_count 131475   7.503%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.686266/  1.117028, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8645%\n",
      "layer   2  Sparsity: 72.6182%\n",
      "layer   3  Sparsity: 63.7761%\n",
      "total_backward_count 1762200 real_backward_count 131912   7.486%\n",
      "fc layer 3 self.abs_max_out: 2110.0\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.689413/  1.118677, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8666%\n",
      "layer   2  Sparsity: 72.6680%\n",
      "layer   3  Sparsity: 63.5828%\n",
      "total_backward_count 1771990 real_backward_count 132332   7.468%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.690880/  1.168979, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8385%\n",
      "layer   2  Sparsity: 72.8475%\n",
      "layer   3  Sparsity: 64.0327%\n",
      "total_backward_count 1781780 real_backward_count 132742   7.450%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.689367/  1.130712, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8532%\n",
      "layer   2  Sparsity: 72.8767%\n",
      "layer   3  Sparsity: 64.2450%\n",
      "total_backward_count 1791570 real_backward_count 133181   7.434%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.682146/  1.133794, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.8425%\n",
      "layer   2  Sparsity: 72.6483%\n",
      "layer   3  Sparsity: 63.7101%\n",
      "total_backward_count 1801360 real_backward_count 133576   7.415%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.681453/  1.183913, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8664%\n",
      "layer   2  Sparsity: 72.6362%\n",
      "layer   3  Sparsity: 64.0132%\n",
      "total_backward_count 1811150 real_backward_count 133936   7.395%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.675722/  1.150986, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8455%\n",
      "layer   2  Sparsity: 72.7765%\n",
      "layer   3  Sparsity: 64.0598%\n",
      "total_backward_count 1820940 real_backward_count 134357   7.378%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.670972/  1.066652, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8561%\n",
      "layer   2  Sparsity: 72.8348%\n",
      "layer   3  Sparsity: 63.2449%\n",
      "total_backward_count 1830730 real_backward_count 134787   7.362%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.653447/  1.114439, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8555%\n",
      "layer   2  Sparsity: 73.0738%\n",
      "layer   3  Sparsity: 64.2703%\n",
      "total_backward_count 1840520 real_backward_count 135177   7.345%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.660704/  1.164606, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8505%\n",
      "layer   2  Sparsity: 73.0708%\n",
      "layer   3  Sparsity: 63.8489%\n",
      "total_backward_count 1850310 real_backward_count 135606   7.329%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.684909/  1.133081, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8560%\n",
      "layer   2  Sparsity: 72.9850%\n",
      "layer   3  Sparsity: 63.6199%\n",
      "total_backward_count 1860100 real_backward_count 136008   7.312%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.676414/  1.111997, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8578%\n",
      "layer   2  Sparsity: 72.7066%\n",
      "layer   3  Sparsity: 63.4067%\n",
      "total_backward_count 1869890 real_backward_count 136410   7.295%\n",
      "fc layer 1 self.abs_max_out: 10542.0\n",
      "fc layer 1 self.abs_max_out: 10583.0\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.682270/  1.171492, val:  67.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.8373%\n",
      "layer   2  Sparsity: 72.4086%\n",
      "layer   3  Sparsity: 64.3085%\n",
      "total_backward_count 1879680 real_backward_count 136786   7.277%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.677962/  1.138702, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8652%\n",
      "layer   2  Sparsity: 72.5424%\n",
      "layer   3  Sparsity: 63.5578%\n",
      "total_backward_count 1889470 real_backward_count 137188   7.261%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.684165/  1.119928, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8537%\n",
      "layer   2  Sparsity: 72.6083%\n",
      "layer   3  Sparsity: 63.1089%\n",
      "total_backward_count 1899260 real_backward_count 137597   7.245%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.688220/  1.125017, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8415%\n",
      "layer   2  Sparsity: 72.7141%\n",
      "layer   3  Sparsity: 62.3684%\n",
      "total_backward_count 1909050 real_backward_count 137987   7.228%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.676932/  1.145531, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8382%\n",
      "layer   2  Sparsity: 72.7785%\n",
      "layer   3  Sparsity: 62.5188%\n",
      "total_backward_count 1918840 real_backward_count 138416   7.214%\n",
      "fc layer 1 self.abs_max_out: 10607.0\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.671317/  1.118930, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8449%\n",
      "layer   2  Sparsity: 72.8277%\n",
      "layer   3  Sparsity: 63.3656%\n",
      "total_backward_count 1928630 real_backward_count 138818   7.198%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.659950/  1.106941, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8764%\n",
      "layer   2  Sparsity: 72.4113%\n",
      "layer   3  Sparsity: 63.9363%\n",
      "total_backward_count 1938420 real_backward_count 139207   7.181%\n",
      "fc layer 1 self.abs_max_out: 10674.0\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.666876/  1.121709, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8408%\n",
      "layer   2  Sparsity: 72.2085%\n",
      "layer   3  Sparsity: 62.8215%\n",
      "total_backward_count 1948210 real_backward_count 139609   7.166%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.664222/  1.134157, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8493%\n",
      "layer   2  Sparsity: 72.1775%\n",
      "layer   3  Sparsity: 63.6708%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3b64ef839d4b38848aab546deb34bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.66422</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.68333</td></tr><tr><td>val_loss</td><td>1.13416</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-sweep-137</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9q7yobem' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9q7yobem</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_124312-9q7yobem/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z6983la2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_165848-z6983la2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z6983la2' target=\"_blank\">worldly-sweep-143</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z6983la2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z6983la2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251117_165857_787', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1481.0\n",
      "lif layer 1 self.abs_max_v: 1481.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 373.0\n",
      "lif layer 2 self.abs_max_v: 373.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1487.0\n",
      "lif layer 1 self.abs_max_v: 2052.5\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 953.0\n",
      "lif layer 1 self.abs_max_v: 2298.5\n",
      "lif layer 2 self.abs_max_v: 1005.0\n",
      "fc layer 1 self.abs_max_out: 1886.0\n",
      "lif layer 1 self.abs_max_v: 2387.0\n",
      "lif layer 2 self.abs_max_v: 1378.5\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "lif layer 1 self.abs_max_v: 2922.0\n",
      "fc layer 2 self.abs_max_out: 1118.0\n",
      "lif layer 2 self.abs_max_v: 1587.0\n",
      "fc layer 1 self.abs_max_out: 3511.0\n",
      "lif layer 1 self.abs_max_v: 3511.0\n",
      "fc layer 1 self.abs_max_out: 4276.0\n",
      "lif layer 1 self.abs_max_v: 4276.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "lif layer 2 self.abs_max_v: 1812.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "fc layer 1 self.abs_max_out: 4290.0\n",
      "lif layer 1 self.abs_max_v: 4290.0\n",
      "fc layer 1 self.abs_max_out: 4792.0\n",
      "lif layer 1 self.abs_max_v: 4792.0\n",
      "fc layer 1 self.abs_max_out: 5227.0\n",
      "lif layer 1 self.abs_max_v: 5227.0\n",
      "fc layer 1 self.abs_max_out: 5447.0\n",
      "lif layer 1 self.abs_max_v: 5447.0\n",
      "fc layer 1 self.abs_max_out: 5907.0\n",
      "lif layer 1 self.abs_max_v: 5907.0\n",
      "lif layer 2 self.abs_max_v: 1949.0\n",
      "fc layer 1 self.abs_max_out: 6347.0\n",
      "lif layer 1 self.abs_max_v: 6347.0\n",
      "lif layer 2 self.abs_max_v: 2036.5\n",
      "fc layer 1 self.abs_max_out: 6525.0\n",
      "lif layer 1 self.abs_max_v: 6525.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "lif layer 2 self.abs_max_v: 2061.5\n",
      "lif layer 2 self.abs_max_v: 2180.0\n",
      "fc layer 2 self.abs_max_out: 1614.0\n",
      "lif layer 2 self.abs_max_v: 2445.5\n",
      "lif layer 2 self.abs_max_v: 2750.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 1 self.abs_max_out: 6776.0\n",
      "lif layer 1 self.abs_max_v: 6776.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 2 self.abs_max_out: 1617.0\n",
      "fc layer 2 self.abs_max_out: 1665.0\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1785.0\n",
      "fc layer 2 self.abs_max_out: 1847.0\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "fc layer 2 self.abs_max_out: 1951.0\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 1 self.abs_max_out: 7671.0\n",
      "lif layer 1 self.abs_max_v: 7671.0\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "fc layer 2 self.abs_max_out: 2230.0\n",
      "lif layer 2 self.abs_max_v: 2920.5\n",
      "lif layer 1 self.abs_max_v: 7897.0\n",
      "lif layer 2 self.abs_max_v: 3269.5\n",
      "fc layer 2 self.abs_max_out: 2436.0\n",
      "fc layer 2 self.abs_max_out: 2620.0\n",
      "fc layer 2 self.abs_max_out: 2730.0\n",
      "fc layer 2 self.abs_max_out: 2734.0\n",
      "fc layer 1 self.abs_max_out: 7994.0\n",
      "lif layer 1 self.abs_max_v: 7994.0\n",
      "lif layer 2 self.abs_max_v: 3290.5\n",
      "lif layer 2 self.abs_max_v: 3416.5\n",
      "lif layer 2 self.abs_max_v: 3439.5\n",
      "lif layer 2 self.abs_max_v: 3705.5\n",
      "lif layer 2 self.abs_max_v: 3718.0\n",
      "lif layer 1 self.abs_max_v: 8338.0\n",
      "fc layer 2 self.abs_max_out: 2759.0\n",
      "fc layer 1 self.abs_max_out: 8248.0\n",
      "fc layer 2 self.abs_max_out: 2890.0\n",
      "fc layer 2 self.abs_max_out: 3219.0\n",
      "lif layer 1 self.abs_max_v: 8403.0\n",
      "lif layer 1 self.abs_max_v: 8663.5\n",
      "lif layer 1 self.abs_max_v: 9042.5\n",
      "lif layer 1 self.abs_max_v: 9141.5\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "lif layer 1 self.abs_max_v: 9202.0\n",
      "lif layer 1 self.abs_max_v: 9247.0\n",
      "lif layer 1 self.abs_max_v: 9645.5\n",
      "lif layer 1 self.abs_max_v: 10742.5\n",
      "lif layer 1 self.abs_max_v: 11093.0\n",
      "lif layer 1 self.abs_max_v: 11155.5\n",
      "lif layer 1 self.abs_max_v: 11842.0\n",
      "lif layer 1 self.abs_max_v: 12872.5\n",
      "lif layer 1 self.abs_max_v: 13013.5\n",
      "fc layer 1 self.abs_max_out: 9721.0\n",
      "fc layer 1 self.abs_max_out: 9915.0\n",
      "lif layer 1 self.abs_max_v: 14164.5\n",
      "lif layer 1 self.abs_max_v: 14544.5\n",
      "fc layer 1 self.abs_max_out: 10010.0\n",
      "lif layer 1 self.abs_max_v: 14574.5\n",
      "lif layer 1 self.abs_max_v: 14772.5\n",
      "lif layer 1 self.abs_max_v: 14960.5\n",
      "lif layer 1 self.abs_max_v: 16066.0\n",
      "lif layer 1 self.abs_max_v: 16704.0\n",
      "lif layer 1 self.abs_max_v: 17114.0\n",
      "lif layer 1 self.abs_max_v: 17447.0\n",
      "lif layer 1 self.abs_max_v: 18137.5\n",
      "fc layer 1 self.abs_max_out: 10140.0\n",
      "fc layer 1 self.abs_max_out: 10878.0\n",
      "lif layer 1 self.abs_max_v: 18300.5\n",
      "fc layer 1 self.abs_max_out: 11204.0\n",
      "lif layer 1 self.abs_max_v: 20308.5\n",
      "fc layer 1 self.abs_max_out: 11219.0\n",
      "lif layer 1 self.abs_max_v: 21373.5\n",
      "lif layer 1 self.abs_max_v: 21787.0\n",
      "fc layer 1 self.abs_max_out: 11618.0\n",
      "fc layer 1 self.abs_max_out: 11770.0\n",
      "fc layer 1 self.abs_max_out: 12185.0\n",
      "fc layer 1 self.abs_max_out: 12345.0\n",
      "lif layer 1 self.abs_max_v: 22604.5\n",
      "fc layer 1 self.abs_max_out: 12873.0\n",
      "fc layer 1 self.abs_max_out: 13441.0\n",
      "lif layer 1 self.abs_max_v: 22651.0\n",
      "lif layer 1 self.abs_max_v: 22942.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.181293/  2.221200, val:  30.83%, val_best:  30.83%, tr:  72.73%, tr_best:  72.73%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 90.1496%\n",
      "layer   3  Sparsity: 94.4821%\n",
      "total_backward_count 9790 real_backward_count 4588  46.864%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 752.0\n",
      "fc layer 3 self.abs_max_out: 754.0\n",
      "fc layer 1 self.abs_max_out: 14362.0\n",
      "fc layer 1 self.abs_max_out: 14530.0\n",
      "fc layer 1 self.abs_max_out: 15984.0\n",
      "lif layer 1 self.abs_max_v: 24000.5\n",
      "fc layer 1 self.abs_max_out: 17305.0\n",
      "fc layer 1 self.abs_max_out: 18512.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.145080/  2.179229, val:  45.00%, val_best:  45.00%, tr:  89.38%, tr_best:  89.38%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 90.5904%\n",
      "layer   3  Sparsity: 92.4038%\n",
      "total_backward_count 19580 real_backward_count 7956  40.633%\n",
      "fc layer 3 self.abs_max_out: 766.0\n",
      "lif layer 1 self.abs_max_v: 25868.0\n",
      "lif layer 1 self.abs_max_v: 28727.0\n",
      "lif layer 1 self.abs_max_v: 29819.5\n",
      "lif layer 1 self.abs_max_v: 29824.0\n",
      "lif layer 2 self.abs_max_v: 3816.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  2.111857/  2.187702, val:  47.50%, val_best:  47.50%, tr:  93.16%, tr_best:  93.16%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 88.2498%\n",
      "layer   3  Sparsity: 90.2437%\n",
      "total_backward_count 29370 real_backward_count 10791  36.742%\n",
      "lif layer 2 self.abs_max_v: 3945.5\n",
      "lif layer 2 self.abs_max_v: 4104.0\n",
      "lif layer 2 self.abs_max_v: 4201.0\n",
      "lif layer 2 self.abs_max_v: 4268.5\n",
      "lif layer 2 self.abs_max_v: 4303.5\n",
      "lif layer 2 self.abs_max_v: 4417.0\n",
      "lif layer 2 self.abs_max_v: 4439.5\n",
      "lif layer 2 self.abs_max_v: 4597.0\n",
      "fc layer 2 self.abs_max_out: 3236.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  2.097737/  2.194792, val:  34.58%, val_best:  47.50%, tr:  95.61%, tr_best:  95.61%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1357%\n",
      "layer   3  Sparsity: 88.9411%\n",
      "total_backward_count 39160 real_backward_count 13291  33.940%\n",
      "fc layer 2 self.abs_max_out: 3299.0\n",
      "lif layer 2 self.abs_max_v: 4684.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  2.089687/  2.186613, val:  40.42%, val_best:  47.50%, tr:  96.42%, tr_best:  96.42%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.2386%\n",
      "layer   3  Sparsity: 87.7920%\n",
      "total_backward_count 48950 real_backward_count 15775  32.227%\n",
      "lif layer 2 self.abs_max_v: 4733.5\n",
      "lif layer 1 self.abs_max_v: 30525.0\n",
      "lif layer 1 self.abs_max_v: 31647.5\n",
      "lif layer 1 self.abs_max_v: 31780.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  2.078700/  2.191452, val:  40.42%, val_best:  47.50%, tr:  96.12%, tr_best:  96.42%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.3470%\n",
      "layer   3  Sparsity: 87.0305%\n",
      "total_backward_count 58740 real_backward_count 18258  31.083%\n",
      "fc layer 3 self.abs_max_out: 818.0\n",
      "fc layer 1 self.abs_max_out: 18797.0\n",
      "fc layer 3 self.abs_max_out: 860.0\n",
      "fc layer 2 self.abs_max_out: 3353.0\n",
      "lif layer 2 self.abs_max_v: 4737.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  2.042278/  2.169049, val:  36.25%, val_best:  47.50%, tr:  97.34%, tr_best:  97.34%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.0582%\n",
      "layer   3  Sparsity: 84.7425%\n",
      "total_backward_count 68530 real_backward_count 20510  29.928%\n",
      "lif layer 2 self.abs_max_v: 4743.5\n",
      "lif layer 2 self.abs_max_v: 4791.5\n",
      "lif layer 2 self.abs_max_v: 4925.0\n",
      "lif layer 2 self.abs_max_v: 5034.5\n",
      "lif layer 2 self.abs_max_v: 5110.0\n",
      "fc layer 3 self.abs_max_out: 914.0\n",
      "fc layer 3 self.abs_max_out: 924.0\n",
      "fc layer 2 self.abs_max_out: 3371.0\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "fc layer 2 self.abs_max_out: 3499.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  2.031224/  2.098161, val:  45.00%, val_best:  47.50%, tr:  97.45%, tr_best:  97.45%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.4181%\n",
      "layer   3  Sparsity: 84.0546%\n",
      "total_backward_count 78320 real_backward_count 22539  28.778%\n",
      "lif layer 2 self.abs_max_v: 5125.0\n",
      "lif layer 2 self.abs_max_v: 5127.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  2.017980/  2.132540, val:  51.25%, val_best:  51.25%, tr:  98.16%, tr_best:  98.16%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.8400%\n",
      "layer   3  Sparsity: 83.9947%\n",
      "total_backward_count 88110 real_backward_count 24644  27.970%\n",
      "lif layer 2 self.abs_max_v: 5451.5\n",
      "fc layer 1 self.abs_max_out: 19024.0\n",
      "fc layer 1 self.abs_max_out: 19502.0\n",
      "fc layer 1 self.abs_max_out: 20513.0\n",
      "fc layer 1 self.abs_max_out: 20716.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  2.025900/  2.156798, val:  41.25%, val_best:  51.25%, tr:  98.57%, tr_best:  98.57%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4972%\n",
      "layer   3  Sparsity: 85.1112%\n",
      "total_backward_count 97900 real_backward_count 26747  27.321%\n",
      "lif layer 2 self.abs_max_v: 5655.0\n",
      "lif layer 2 self.abs_max_v: 5670.5\n",
      "fc layer 3 self.abs_max_out: 1004.0\n",
      "fc layer 3 self.abs_max_out: 1187.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  2.010907/  2.168553, val:  39.58%, val_best:  51.25%, tr:  98.26%, tr_best:  98.57%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.6076%\n",
      "layer   3  Sparsity: 83.8008%\n",
      "total_backward_count 107690 real_backward_count 28787  26.731%\n",
      "lif layer 2 self.abs_max_v: 5720.0\n",
      "lif layer 1 self.abs_max_v: 32726.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.999388/  2.155059, val:  28.75%, val_best:  51.25%, tr:  98.88%, tr_best:  98.88%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.9868%\n",
      "layer   3  Sparsity: 83.3559%\n",
      "total_backward_count 117480 real_backward_count 30740  26.166%\n",
      "lif layer 2 self.abs_max_v: 5766.0\n",
      "lif layer 2 self.abs_max_v: 5891.0\n",
      "fc layer 1 self.abs_max_out: 20933.0\n",
      "fc layer 1 self.abs_max_out: 21101.0\n",
      "lif layer 1 self.abs_max_v: 36015.0\n",
      "lif layer 1 self.abs_max_v: 38667.5\n",
      "lif layer 1 self.abs_max_v: 39390.0\n",
      "lif layer 1 self.abs_max_v: 40252.0\n",
      "lif layer 1 self.abs_max_v: 41166.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.971517/  2.148008, val:  34.58%, val_best:  51.25%, tr:  98.67%, tr_best:  98.88%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.6410%\n",
      "layer   3  Sparsity: 82.1863%\n",
      "total_backward_count 127270 real_backward_count 32611  25.623%\n",
      "fc layer 2 self.abs_max_out: 3553.0\n",
      "fc layer 2 self.abs_max_out: 3601.0\n",
      "fc layer 2 self.abs_max_out: 3685.0\n",
      "fc layer 2 self.abs_max_out: 3709.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.987215/  2.145714, val:  32.92%, val_best:  51.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.3449%\n",
      "layer   3  Sparsity: 81.9521%\n",
      "total_backward_count 137060 real_backward_count 34496  25.169%\n",
      "fc layer 1 self.abs_max_out: 21222.0\n",
      "lif layer 1 self.abs_max_v: 41451.5\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.969254/  2.150359, val:  34.58%, val_best:  51.25%, tr:  99.08%, tr_best:  99.28%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 79.4177%\n",
      "layer   3  Sparsity: 81.6376%\n",
      "total_backward_count 146850 real_backward_count 36302  24.720%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.965760/  2.129624, val:  45.00%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 79.8622%\n",
      "layer   3  Sparsity: 82.2469%\n",
      "total_backward_count 156640 real_backward_count 38128  24.341%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.971822/  2.084542, val:  51.67%, val_best:  51.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.4870%\n",
      "layer   3  Sparsity: 82.8339%\n",
      "total_backward_count 166430 real_backward_count 39878  23.961%\n",
      "fc layer 2 self.abs_max_out: 3838.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.992409/  2.120652, val:  33.33%, val_best:  51.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.2265%\n",
      "layer   3  Sparsity: 83.7646%\n",
      "total_backward_count 176220 real_backward_count 41837  23.741%\n",
      "lif layer 2 self.abs_max_v: 5894.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.964219/  2.109825, val:  47.08%, val_best:  51.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 79.8260%\n",
      "layer   3  Sparsity: 81.5228%\n",
      "total_backward_count 186010 real_backward_count 43679  23.482%\n",
      "fc layer 2 self.abs_max_out: 3871.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.981297/  2.120760, val:  37.92%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.9817%\n",
      "layer   3  Sparsity: 82.5406%\n",
      "total_backward_count 195800 real_backward_count 45467  23.221%\n",
      "fc layer 1 self.abs_max_out: 21331.0\n",
      "fc layer 1 self.abs_max_out: 22159.0\n",
      "lif layer 1 self.abs_max_v: 41970.0\n",
      "lif layer 1 self.abs_max_v: 42144.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.984723/  2.133786, val:  39.58%, val_best:  51.67%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.3262%\n",
      "layer   3  Sparsity: 83.5704%\n",
      "total_backward_count 205590 real_backward_count 47255  22.985%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.980057/  2.138571, val:  39.17%, val_best:  51.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.0034%\n",
      "layer   3  Sparsity: 82.7518%\n",
      "total_backward_count 215380 real_backward_count 49078  22.787%\n",
      "lif layer 2 self.abs_max_v: 6280.0\n",
      "lif layer 2 self.abs_max_v: 6681.0\n",
      "fc layer 2 self.abs_max_out: 4282.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.964659/  2.077008, val:  51.67%, val_best:  51.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.2151%\n",
      "layer   3  Sparsity: 82.1442%\n",
      "total_backward_count 225170 real_backward_count 50904  22.607%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.976274/  2.107167, val:  57.50%, val_best:  57.50%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.0180%\n",
      "layer   3  Sparsity: 83.4143%\n",
      "total_backward_count 234960 real_backward_count 52672  22.417%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  2.005021/  2.124736, val:  49.17%, val_best:  57.50%, tr:  98.06%, tr_best:  99.59%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.9176%\n",
      "layer   3  Sparsity: 84.7301%\n",
      "total_backward_count 244750 real_backward_count 54538  22.283%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  2.015122/  2.118406, val:  60.00%, val_best:  60.00%, tr:  97.85%, tr_best:  99.59%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.6210%\n",
      "layer   3  Sparsity: 84.1415%\n",
      "total_backward_count 254540 real_backward_count 56525  22.207%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.986163/  2.092983, val:  51.67%, val_best:  60.00%, tr:  98.77%, tr_best:  99.59%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 80.6492%\n",
      "layer   3  Sparsity: 82.3549%\n",
      "total_backward_count 264330 real_backward_count 58277  22.047%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.973154/  2.115444, val:  46.67%, val_best:  60.00%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.8090%\n",
      "layer   3  Sparsity: 83.1806%\n",
      "total_backward_count 274120 real_backward_count 60108  21.928%\n",
      "fc layer 3 self.abs_max_out: 1209.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.991968/  2.084405, val:  42.50%, val_best:  60.00%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.0321%\n",
      "layer   3  Sparsity: 82.9675%\n",
      "total_backward_count 283910 real_backward_count 61840  21.782%\n",
      "fc layer 3 self.abs_max_out: 1212.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.984913/  2.115929, val:  39.17%, val_best:  60.00%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.0229%\n",
      "layer   3  Sparsity: 82.4431%\n",
      "total_backward_count 293700 real_backward_count 63614  21.660%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.971411/  2.106719, val:  51.25%, val_best:  60.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.5105%\n",
      "layer   3  Sparsity: 82.0247%\n",
      "total_backward_count 303490 real_backward_count 65320  21.523%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  2.003949/  2.129265, val:  46.67%, val_best:  60.00%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.3398%\n",
      "layer   3  Sparsity: 84.0601%\n",
      "total_backward_count 313280 real_backward_count 67143  21.432%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  2.000176/  2.127768, val:  42.50%, val_best:  60.00%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.7821%\n",
      "layer   3  Sparsity: 84.5526%\n",
      "total_backward_count 323070 real_backward_count 68955  21.344%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  2.002908/  2.117364, val:  47.92%, val_best:  60.00%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.6494%\n",
      "layer   3  Sparsity: 83.8690%\n",
      "total_backward_count 332860 real_backward_count 70775  21.263%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.974735/  2.094186, val:  47.92%, val_best:  60.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.2546%\n",
      "layer   3  Sparsity: 82.6237%\n",
      "total_backward_count 342650 real_backward_count 72433  21.139%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.983342/  2.117615, val:  41.67%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.2764%\n",
      "layer   3  Sparsity: 82.3994%\n",
      "total_backward_count 352440 real_backward_count 74103  21.026%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.985429/  2.087565, val:  47.92%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.0146%\n",
      "layer   3  Sparsity: 83.2678%\n",
      "total_backward_count 362230 real_backward_count 75790  20.923%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.970666/  2.136655, val:  34.58%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.4857%\n",
      "layer   3  Sparsity: 83.0105%\n",
      "total_backward_count 372020 real_backward_count 77428  20.813%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.978058/  2.116606, val:  43.33%, val_best:  60.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.4747%\n",
      "layer   3  Sparsity: 82.8947%\n",
      "total_backward_count 381810 real_backward_count 79191  20.741%\n",
      "fc layer 3 self.abs_max_out: 1237.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.948000/  2.113413, val:  38.33%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.6061%\n",
      "layer   3  Sparsity: 82.8599%\n",
      "total_backward_count 391600 real_backward_count 80760  20.623%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.967099/  2.099977, val:  38.75%, val_best:  60.00%, tr:  98.67%, tr_best:  99.69%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.2351%\n",
      "layer   3  Sparsity: 82.9813%\n",
      "total_backward_count 401390 real_backward_count 82483  20.549%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.937273/  2.067852, val:  52.08%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.3865%\n",
      "layer   3  Sparsity: 81.7046%\n",
      "total_backward_count 411180 real_backward_count 84152  20.466%\n",
      "fc layer 1 self.abs_max_out: 23173.0\n",
      "fc layer 1 self.abs_max_out: 23331.0\n",
      "fc layer 1 self.abs_max_out: 23567.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.934214/  2.095087, val:  44.17%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.5872%\n",
      "layer   3  Sparsity: 82.4004%\n",
      "total_backward_count 420970 real_backward_count 85784  20.378%\n",
      "fc layer 1 self.abs_max_out: 23653.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.968547/  2.108108, val:  44.17%, val_best:  60.00%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 81.9484%\n",
      "layer   3  Sparsity: 82.7381%\n",
      "total_backward_count 430760 real_backward_count 87476  20.307%\n",
      "fc layer 1 self.abs_max_out: 23879.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.961774/  2.103383, val:  31.67%, val_best:  60.00%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.3497%\n",
      "layer   3  Sparsity: 82.7256%\n",
      "total_backward_count 440550 real_backward_count 89269  20.263%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.966812/  2.139259, val:  47.50%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.5569%\n",
      "layer   3  Sparsity: 83.6756%\n",
      "total_backward_count 450340 real_backward_count 90977  20.202%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.961156/  2.097558, val:  45.42%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.4340%\n",
      "layer   3  Sparsity: 82.2000%\n",
      "total_backward_count 460130 real_backward_count 92689  20.144%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.973273/  2.118500, val:  47.92%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.6200%\n",
      "layer   3  Sparsity: 83.1180%\n",
      "total_backward_count 469920 real_backward_count 94315  20.070%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.958172/  2.106595, val:  45.00%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.4347%\n",
      "layer   3  Sparsity: 83.0864%\n",
      "total_backward_count 479710 real_backward_count 95877  19.986%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.970562/  2.084762, val:  50.00%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.8046%\n",
      "layer   3  Sparsity: 83.5493%\n",
      "total_backward_count 489500 real_backward_count 97567  19.932%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.996130/  2.125552, val:  46.25%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.1396%\n",
      "layer   3  Sparsity: 84.7315%\n",
      "total_backward_count 499290 real_backward_count 99307  19.890%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.976724/  2.126427, val:  47.08%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.9287%\n",
      "layer   3  Sparsity: 83.7352%\n",
      "total_backward_count 509080 real_backward_count 100966  19.833%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.968681/  2.109273, val:  47.08%, val_best:  60.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.4002%\n",
      "layer   3  Sparsity: 83.3546%\n",
      "total_backward_count 518870 real_backward_count 102661  19.785%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.976166/  2.092814, val:  51.67%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.1975%\n",
      "layer   3  Sparsity: 83.8162%\n",
      "total_backward_count 528660 real_backward_count 104379  19.744%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.990658/  2.108298, val:  50.83%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.0011%\n",
      "layer   3  Sparsity: 84.1272%\n",
      "total_backward_count 538450 real_backward_count 106032  19.692%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.984273/  2.129563, val:  42.92%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.5878%\n",
      "layer   3  Sparsity: 84.4432%\n",
      "total_backward_count 548240 real_backward_count 107720  19.648%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.999509/  2.137725, val:  31.67%, val_best:  60.00%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.7768%\n",
      "layer   3  Sparsity: 84.6428%\n",
      "total_backward_count 558030 real_backward_count 109405  19.606%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.990118/  2.139026, val:  36.25%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.1753%\n",
      "layer   3  Sparsity: 83.7737%\n",
      "total_backward_count 567820 real_backward_count 111026  19.553%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.992550/  2.114707, val:  48.75%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.0261%\n",
      "layer   3  Sparsity: 84.1568%\n",
      "total_backward_count 577610 real_backward_count 112641  19.501%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.958386/  2.108692, val:  44.17%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.4040%\n",
      "layer   3  Sparsity: 82.8518%\n",
      "total_backward_count 587400 real_backward_count 114271  19.454%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.945323/  2.110162, val:  48.33%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.3069%\n",
      "layer   3  Sparsity: 82.4829%\n",
      "total_backward_count 597190 real_backward_count 115945  19.415%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.974547/  2.106603, val:  44.58%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.6324%\n",
      "layer   3  Sparsity: 83.4187%\n",
      "total_backward_count 606980 real_backward_count 117595  19.374%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.962801/  2.105477, val:  44.17%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.8841%\n",
      "layer   3  Sparsity: 83.0964%\n",
      "total_backward_count 616770 real_backward_count 119284  19.340%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.962476/  2.075500, val:  44.58%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.1258%\n",
      "layer   3  Sparsity: 83.0941%\n",
      "total_backward_count 626560 real_backward_count 120917  19.299%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.972435/  2.082607, val:  47.92%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.1058%\n",
      "layer   3  Sparsity: 83.2877%\n",
      "total_backward_count 636350 real_backward_count 122595  19.265%\n",
      "lif layer 1 self.abs_max_v: 43716.0\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.981052/  2.146572, val:  35.00%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.8457%\n",
      "layer   3  Sparsity: 84.3187%\n",
      "total_backward_count 646140 real_backward_count 124295  19.237%\n",
      "lif layer 1 self.abs_max_v: 43856.5\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.974062/  2.101883, val:  44.58%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.1851%\n",
      "layer   3  Sparsity: 84.0794%\n",
      "total_backward_count 655930 real_backward_count 125982  19.207%\n",
      "fc layer 3 self.abs_max_out: 1274.0\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.949359/  2.111780, val:  46.25%, val_best:  60.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.4370%\n",
      "layer   3  Sparsity: 82.7332%\n",
      "total_backward_count 665720 real_backward_count 127608  19.168%\n",
      "fc layer 3 self.abs_max_out: 1336.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.956548/  2.119797, val:  57.50%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.2655%\n",
      "layer   3  Sparsity: 83.4122%\n",
      "total_backward_count 675510 real_backward_count 129251  19.134%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.973668/  2.159904, val:  35.83%, val_best:  60.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.8842%\n",
      "layer   3  Sparsity: 84.1982%\n",
      "total_backward_count 685300 real_backward_count 130852  19.094%\n",
      "lif layer 1 self.abs_max_v: 44003.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.964451/  2.062858, val:  52.92%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.0541%\n",
      "layer   3  Sparsity: 82.4175%\n",
      "total_backward_count 695090 real_backward_count 132536  19.067%\n",
      "lif layer 1 self.abs_max_v: 44169.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.946648/  2.110090, val:  40.42%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.1472%\n",
      "layer   3  Sparsity: 83.0162%\n",
      "total_backward_count 704880 real_backward_count 134192  19.038%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.950587/  2.052373, val:  58.33%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.9792%\n",
      "layer   3  Sparsity: 82.9063%\n",
      "total_backward_count 714670 real_backward_count 135844  19.008%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.934443/  2.080668, val:  42.92%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.8641%\n",
      "layer   3  Sparsity: 82.3977%\n",
      "total_backward_count 724460 real_backward_count 137401  18.966%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.919558/  2.051547, val:  45.42%, val_best:  60.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.7226%\n",
      "layer   3  Sparsity: 81.5959%\n",
      "total_backward_count 734250 real_backward_count 139076  18.941%\n",
      "fc layer 3 self.abs_max_out: 1471.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.918549/  2.086209, val:  46.67%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 82.1185%\n",
      "layer   3  Sparsity: 81.9979%\n",
      "total_backward_count 744040 real_backward_count 140727  18.914%\n",
      "lif layer 1 self.abs_max_v: 44727.5\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.956224/  2.073754, val:  51.67%, val_best:  60.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.7852%\n",
      "layer   3  Sparsity: 83.0345%\n",
      "total_backward_count 753830 real_backward_count 142405  18.891%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.938637/  2.096663, val:  45.83%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.0059%\n",
      "layer   3  Sparsity: 83.0219%\n",
      "total_backward_count 763620 real_backward_count 144062  18.866%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.913014/  2.091902, val:  40.00%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.4751%\n",
      "layer   3  Sparsity: 81.1165%\n",
      "total_backward_count 773410 real_backward_count 145649  18.832%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.930278/  2.078302, val:  58.75%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.3223%\n",
      "layer   3  Sparsity: 82.3662%\n",
      "total_backward_count 783200 real_backward_count 147292  18.806%\n",
      "lif layer 1 self.abs_max_v: 44856.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.959736/  2.097491, val:  52.92%, val_best:  60.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.5473%\n",
      "layer   3  Sparsity: 84.1444%\n",
      "total_backward_count 792990 real_backward_count 148940  18.782%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.973614/  2.132883, val:  30.42%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.7282%\n",
      "layer   3  Sparsity: 83.9188%\n",
      "total_backward_count 802780 real_backward_count 150600  18.760%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.951112/  2.088876, val:  56.67%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.6926%\n",
      "layer   3  Sparsity: 83.8128%\n",
      "total_backward_count 812570 real_backward_count 152301  18.743%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.962290/  2.099082, val:  47.50%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.1220%\n",
      "layer   3  Sparsity: 84.1413%\n",
      "total_backward_count 822360 real_backward_count 153970  18.723%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.983637/  2.125815, val:  44.58%, val_best:  60.00%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4257%\n",
      "layer   3  Sparsity: 84.4007%\n",
      "total_backward_count 832150 real_backward_count 155685  18.709%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.976709/  2.134255, val:  42.08%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.3394%\n",
      "layer   3  Sparsity: 84.3079%\n",
      "total_backward_count 841940 real_backward_count 157379  18.692%\n",
      "fc layer 2 self.abs_max_out: 4335.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.961837/  2.086681, val:  55.00%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4679%\n",
      "layer   3  Sparsity: 83.6919%\n",
      "total_backward_count 851730 real_backward_count 159080  18.677%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.969422/  2.138167, val:  45.83%, val_best:  60.00%, tr:  98.57%, tr_best:  99.69%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.0463%\n",
      "layer   3  Sparsity: 84.0458%\n",
      "total_backward_count 861520 real_backward_count 160776  18.662%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.965143/  2.103556, val:  52.08%, val_best:  60.00%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.6509%\n",
      "layer   3  Sparsity: 84.1325%\n",
      "total_backward_count 871310 real_backward_count 162451  18.644%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.992885/  2.094534, val:  50.83%, val_best:  60.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.5094%\n",
      "layer   3  Sparsity: 84.7506%\n",
      "total_backward_count 881100 real_backward_count 164226  18.639%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.953513/  2.134090, val:  45.83%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.1244%\n",
      "layer   3  Sparsity: 83.4588%\n",
      "total_backward_count 890890 real_backward_count 165902  18.622%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.967103/  2.083017, val:  56.25%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 83.7578%\n",
      "layer   3  Sparsity: 83.3977%\n",
      "total_backward_count 900680 real_backward_count 167561  18.604%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.974212/  2.106113, val:  52.08%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4310%\n",
      "layer   3  Sparsity: 83.9379%\n",
      "total_backward_count 910470 real_backward_count 169275  18.592%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.960401/  2.098248, val:  45.00%, val_best:  60.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.5224%\n",
      "layer   3  Sparsity: 83.0065%\n",
      "total_backward_count 920260 real_backward_count 170958  18.577%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.964311/  2.113650, val:  49.58%, val_best:  60.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.3175%\n",
      "layer   3  Sparsity: 84.3794%\n",
      "total_backward_count 930050 real_backward_count 172657  18.564%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.964860/  2.110228, val:  50.42%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.5886%\n",
      "layer   3  Sparsity: 83.9176%\n",
      "total_backward_count 939840 real_backward_count 174359  18.552%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.962148/  2.085276, val:  50.42%, val_best:  60.00%, tr:  98.88%, tr_best:  99.69%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.3147%\n",
      "layer   3  Sparsity: 84.0663%\n",
      "total_backward_count 949630 real_backward_count 176035  18.537%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.960728/  2.089427, val:  49.58%, val_best:  60.00%, tr:  98.37%, tr_best:  99.69%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.7365%\n",
      "layer   3  Sparsity: 83.6411%\n",
      "total_backward_count 959420 real_backward_count 177736  18.525%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.969269/  2.166766, val:  42.92%, val_best:  60.00%, tr:  98.16%, tr_best:  99.69%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7048%\n",
      "layer   3  Sparsity: 84.0292%\n",
      "total_backward_count 969210 real_backward_count 179472  18.517%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.962215/  2.107995, val:  55.42%, val_best:  60.00%, tr:  98.57%, tr_best:  99.69%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.3549%\n",
      "layer   3  Sparsity: 83.5582%\n",
      "total_backward_count 979000 real_backward_count 181124  18.501%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.999513/  2.145735, val:  43.33%, val_best:  60.00%, tr:  97.85%, tr_best:  99.69%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7276%\n",
      "layer   3  Sparsity: 85.9138%\n",
      "total_backward_count 988790 real_backward_count 182952  18.503%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.981496/  2.084215, val:  56.25%, val_best:  60.00%, tr:  98.37%, tr_best:  99.69%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4376%\n",
      "layer   3  Sparsity: 84.3863%\n",
      "total_backward_count 998580 real_backward_count 184775  18.504%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.984723/  2.124455, val:  37.92%, val_best:  60.00%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.8259%\n",
      "layer   3  Sparsity: 84.8851%\n",
      "total_backward_count 1008370 real_backward_count 186534  18.499%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.954700/  2.098958, val:  50.00%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.6110%\n",
      "layer   3  Sparsity: 83.3958%\n",
      "total_backward_count 1018160 real_backward_count 188193  18.484%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.959303/  2.101357, val:  39.58%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.1519%\n",
      "layer   3  Sparsity: 83.4866%\n",
      "total_backward_count 1027950 real_backward_count 189840  18.468%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.977455/  2.107445, val:  56.67%, val_best:  60.00%, tr:  98.47%, tr_best:  99.69%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.3247%\n",
      "layer   3  Sparsity: 83.7507%\n",
      "total_backward_count 1037740 real_backward_count 191652  18.468%\n",
      "fc layer 1 self.abs_max_out: 24020.0\n",
      "lif layer 1 self.abs_max_v: 45308.5\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.975225/  2.132848, val:  45.83%, val_best:  60.00%, tr:  98.67%, tr_best:  99.69%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1386%\n",
      "layer   3  Sparsity: 83.9873%\n",
      "total_backward_count 1047530 real_backward_count 193412  18.464%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.968135/  2.069762, val:  47.50%, val_best:  60.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7388%\n",
      "layer   3  Sparsity: 82.7035%\n",
      "total_backward_count 1057320 real_backward_count 195132  18.455%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.953992/  2.106511, val:  42.50%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.2388%\n",
      "layer   3  Sparsity: 83.5700%\n",
      "total_backward_count 1067110 real_backward_count 196797  18.442%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.947068/  2.091069, val:  44.17%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7657%\n",
      "layer   3  Sparsity: 82.5528%\n",
      "total_backward_count 1076900 real_backward_count 198426  18.426%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.992623/  2.090303, val:  51.67%, val_best:  60.00%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.6541%\n",
      "layer   3  Sparsity: 84.6889%\n",
      "total_backward_count 1086690 real_backward_count 200215  18.424%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.983992/  2.092642, val:  55.42%, val_best:  60.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.5705%\n",
      "layer   3  Sparsity: 83.7329%\n",
      "total_backward_count 1096480 real_backward_count 201974  18.420%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.988382/  2.118751, val:  50.42%, val_best:  60.00%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.6241%\n",
      "layer   3  Sparsity: 84.6820%\n",
      "total_backward_count 1106270 real_backward_count 203758  18.418%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.981397/  2.117304, val:  46.25%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.4902%\n",
      "layer   3  Sparsity: 84.8111%\n",
      "total_backward_count 1116060 real_backward_count 205510  18.414%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.975725/  2.099465, val:  51.67%, val_best:  60.00%, tr:  98.26%, tr_best:  99.69%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7626%\n",
      "layer   3  Sparsity: 83.9694%\n",
      "total_backward_count 1125850 real_backward_count 207304  18.413%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.980418/  2.128637, val:  50.42%, val_best:  60.00%, tr:  98.06%, tr_best:  99.69%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6337%\n",
      "layer   3  Sparsity: 84.5668%\n",
      "total_backward_count 1135640 real_backward_count 209079  18.411%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.977576/  2.089644, val:  49.58%, val_best:  60.00%, tr:  98.47%, tr_best:  99.69%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.0412%\n",
      "layer   3  Sparsity: 83.6212%\n",
      "total_backward_count 1145430 real_backward_count 210901  18.412%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.952057/  2.074583, val:  47.92%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.2533%\n",
      "layer   3  Sparsity: 82.9474%\n",
      "total_backward_count 1155220 real_backward_count 212674  18.410%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.965515/  2.143474, val:  42.92%, val_best:  60.00%, tr:  97.85%, tr_best:  99.69%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.0581%\n",
      "layer   3  Sparsity: 84.2432%\n",
      "total_backward_count 1165010 real_backward_count 214522  18.414%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.962502/  2.046126, val:  53.33%, val_best:  60.00%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.0473%\n",
      "layer   3  Sparsity: 83.1767%\n",
      "total_backward_count 1174800 real_backward_count 216232  18.406%\n",
      "fc layer 2 self.abs_max_out: 4346.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.954877/  2.113436, val:  45.42%, val_best:  60.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.5929%\n",
      "layer   3  Sparsity: 83.5377%\n",
      "total_backward_count 1184590 real_backward_count 217924  18.397%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.952787/  2.108707, val:  40.00%, val_best:  60.00%, tr:  97.14%, tr_best:  99.69%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.9766%\n",
      "layer   3  Sparsity: 83.6381%\n",
      "total_backward_count 1194380 real_backward_count 219755  18.399%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.952602/  2.101470, val:  50.83%, val_best:  60.00%, tr:  98.16%, tr_best:  99.69%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.6043%\n",
      "layer   3  Sparsity: 83.0807%\n",
      "total_backward_count 1204170 real_backward_count 221522  18.396%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.968513/  2.120638, val:  42.92%, val_best:  60.00%, tr:  98.06%, tr_best:  99.69%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.9674%\n",
      "layer   3  Sparsity: 83.5574%\n",
      "total_backward_count 1213960 real_backward_count 223311  18.395%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.958139/  2.077142, val:  52.08%, val_best:  60.00%, tr:  98.57%, tr_best:  99.69%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1289%\n",
      "layer   3  Sparsity: 83.2480%\n",
      "total_backward_count 1223750 real_backward_count 225093  18.394%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.970384/  2.070462, val:  50.83%, val_best:  60.00%, tr:  97.65%, tr_best:  99.69%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7163%\n",
      "layer   3  Sparsity: 84.0096%\n",
      "total_backward_count 1233540 real_backward_count 226960  18.399%\n",
      "fc layer 3 self.abs_max_out: 1479.0\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.965520/  2.114840, val:  47.92%, val_best:  60.00%, tr:  97.85%, tr_best:  99.69%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1528%\n",
      "layer   3  Sparsity: 83.7685%\n",
      "total_backward_count 1243330 real_backward_count 228780  18.401%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.981827/  2.107799, val:  51.67%, val_best:  60.00%, tr:  97.55%, tr_best:  99.69%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.9105%\n",
      "layer   3  Sparsity: 84.5075%\n",
      "total_backward_count 1253120 real_backward_count 230549  18.398%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.995955/  2.152282, val:  57.92%, val_best:  60.00%, tr:  97.24%, tr_best:  99.69%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.8258%\n",
      "layer   3  Sparsity: 85.6089%\n",
      "total_backward_count 1262910 real_backward_count 232387  18.401%\n",
      "fc layer 1 self.abs_max_out: 24081.0\n",
      "lif layer 1 self.abs_max_v: 45420.5\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.993374/  2.123749, val:  49.58%, val_best:  60.00%, tr:  97.75%, tr_best:  99.69%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.2436%\n",
      "layer   3  Sparsity: 85.0514%\n",
      "total_backward_count 1272700 real_backward_count 234191  18.401%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.998748/  2.120519, val:  40.00%, val_best:  60.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.4184%\n",
      "layer   3  Sparsity: 84.7128%\n",
      "total_backward_count 1282490 real_backward_count 235959  18.399%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.991358/  2.105195, val:  49.58%, val_best:  60.00%, tr:  98.06%, tr_best:  99.69%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7669%\n",
      "layer   3  Sparsity: 84.3779%\n",
      "total_backward_count 1292280 real_backward_count 237819  18.403%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.976023/  2.117923, val:  47.92%, val_best:  60.00%, tr:  97.96%, tr_best:  99.69%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.9305%\n",
      "layer   3  Sparsity: 84.3940%\n",
      "total_backward_count 1302070 real_backward_count 239608  18.402%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.971798/  2.125594, val:  45.00%, val_best:  60.00%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6808%\n",
      "layer   3  Sparsity: 83.9591%\n",
      "total_backward_count 1311860 real_backward_count 241415  18.402%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.968119/  2.116850, val:  43.33%, val_best:  60.00%, tr:  98.16%, tr_best:  99.69%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7802%\n",
      "layer   3  Sparsity: 83.6962%\n",
      "total_backward_count 1321650 real_backward_count 243245  18.405%\n",
      "fc layer 2 self.abs_max_out: 4449.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.976193/  2.063713, val:  60.42%, val_best:  60.42%, tr:  97.55%, tr_best:  99.69%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1274%\n",
      "layer   3  Sparsity: 84.0658%\n",
      "total_backward_count 1331440 real_backward_count 245085  18.408%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.951817/  2.106265, val:  49.58%, val_best:  60.42%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.1995%\n",
      "layer   3  Sparsity: 83.3106%\n",
      "total_backward_count 1341230 real_backward_count 246811  18.402%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.971348/  2.085083, val:  48.33%, val_best:  60.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1422%\n",
      "layer   3  Sparsity: 83.1855%\n",
      "total_backward_count 1351020 real_backward_count 248512  18.394%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.960796/  2.104435, val:  55.00%, val_best:  60.42%, tr:  98.77%, tr_best:  99.69%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1984%\n",
      "layer   3  Sparsity: 84.4439%\n",
      "total_backward_count 1360810 real_backward_count 250230  18.388%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.996965/  2.126819, val:  52.08%, val_best:  60.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.4795%\n",
      "layer   3  Sparsity: 84.9138%\n",
      "total_backward_count 1370600 real_backward_count 251999  18.386%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.973423/  2.115413, val:  44.58%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1305%\n",
      "layer   3  Sparsity: 84.3454%\n",
      "total_backward_count 1380390 real_backward_count 253796  18.386%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.984548/  2.116032, val:  44.17%, val_best:  60.42%, tr:  97.75%, tr_best:  99.69%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7978%\n",
      "layer   3  Sparsity: 84.7110%\n",
      "total_backward_count 1390180 real_backward_count 255624  18.388%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.999953/  2.150517, val:  35.42%, val_best:  60.42%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.2813%\n",
      "layer   3  Sparsity: 85.0758%\n",
      "total_backward_count 1399970 real_backward_count 257488  18.392%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.986702/  2.100091, val:  54.17%, val_best:  60.42%, tr:  98.06%, tr_best:  99.69%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1199%\n",
      "layer   3  Sparsity: 83.7482%\n",
      "total_backward_count 1409760 real_backward_count 259294  18.393%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.986990/  2.112076, val:  37.50%, val_best:  60.42%, tr:  97.34%, tr_best:  99.69%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1757%\n",
      "layer   3  Sparsity: 84.4378%\n",
      "total_backward_count 1419550 real_backward_count 261082  18.392%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.975886/  2.111854, val:  48.75%, val_best:  60.42%, tr:  97.85%, tr_best:  99.69%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6565%\n",
      "layer   3  Sparsity: 84.6043%\n",
      "total_backward_count 1429340 real_backward_count 262786  18.385%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.987560/  2.136418, val:  45.42%, val_best:  60.42%, tr:  97.85%, tr_best:  99.69%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1737%\n",
      "layer   3  Sparsity: 84.6462%\n",
      "total_backward_count 1439130 real_backward_count 264578  18.385%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.971718/  2.139509, val:  40.00%, val_best:  60.42%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1453%\n",
      "layer   3  Sparsity: 83.5201%\n",
      "total_backward_count 1448920 real_backward_count 266216  18.373%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.995261/  2.096582, val:  54.17%, val_best:  60.42%, tr:  97.34%, tr_best:  99.69%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.4135%\n",
      "layer   3  Sparsity: 84.6716%\n",
      "total_backward_count 1458710 real_backward_count 267981  18.371%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.974927/  2.094054, val:  50.83%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7731%\n",
      "layer   3  Sparsity: 83.8100%\n",
      "total_backward_count 1468500 real_backward_count 269790  18.372%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.952221/  2.110524, val:  52.92%, val_best:  60.42%, tr:  98.47%, tr_best:  99.69%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.8018%\n",
      "layer   3  Sparsity: 82.9269%\n",
      "total_backward_count 1478290 real_backward_count 271496  18.366%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.961689/  2.100865, val:  44.17%, val_best:  60.42%, tr:  99.18%, tr_best:  99.69%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.2021%\n",
      "layer   3  Sparsity: 83.0235%\n",
      "total_backward_count 1488080 real_backward_count 273188  18.358%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.971231/  2.118939, val:  51.67%, val_best:  60.42%, tr:  98.26%, tr_best:  99.69%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6767%\n",
      "layer   3  Sparsity: 85.0854%\n",
      "total_backward_count 1497870 real_backward_count 274932  18.355%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.986995/  2.090117, val:  45.42%, val_best:  60.42%, tr:  97.04%, tr_best:  99.69%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.0261%\n",
      "layer   3  Sparsity: 84.0886%\n",
      "total_backward_count 1507660 real_backward_count 276819  18.361%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.958301/  2.118479, val:  54.17%, val_best:  60.42%, tr:  97.34%, tr_best:  99.69%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6391%\n",
      "layer   3  Sparsity: 84.5104%\n",
      "total_backward_count 1517450 real_backward_count 278678  18.365%\n",
      "fc layer 2 self.abs_max_out: 4491.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.973782/  2.124192, val:  47.50%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.4950%\n",
      "layer   3  Sparsity: 84.0790%\n",
      "total_backward_count 1527240 real_backward_count 280495  18.366%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.976655/  2.092141, val:  50.42%, val_best:  60.42%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.3381%\n",
      "layer   3  Sparsity: 83.3656%\n",
      "total_backward_count 1537030 real_backward_count 282298  18.366%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.943293/  2.111064, val:  45.00%, val_best:  60.42%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7857%\n",
      "layer   3  Sparsity: 82.8473%\n",
      "total_backward_count 1546820 real_backward_count 283937  18.356%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.942308/  2.087310, val:  52.50%, val_best:  60.42%, tr:  98.26%, tr_best:  99.69%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7126%\n",
      "layer   3  Sparsity: 83.1975%\n",
      "total_backward_count 1556610 real_backward_count 285622  18.349%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.983193/  2.089916, val:  53.75%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.4920%\n",
      "layer   3  Sparsity: 84.9385%\n",
      "total_backward_count 1566400 real_backward_count 287433  18.350%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.969436/  2.088747, val:  59.17%, val_best:  60.42%, tr:  97.55%, tr_best:  99.69%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6907%\n",
      "layer   3  Sparsity: 84.0952%\n",
      "total_backward_count 1576190 real_backward_count 289268  18.352%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.970464/  2.099245, val:  56.67%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.5765%\n",
      "layer   3  Sparsity: 84.0420%\n",
      "total_backward_count 1585980 real_backward_count 291032  18.350%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.980018/  2.104404, val:  48.75%, val_best:  60.42%, tr:  97.55%, tr_best:  99.69%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.9928%\n",
      "layer   3  Sparsity: 84.1145%\n",
      "total_backward_count 1595770 real_backward_count 292938  18.357%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.963639/  2.137737, val:  44.17%, val_best:  60.42%, tr:  98.37%, tr_best:  99.69%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7389%\n",
      "layer   3  Sparsity: 84.0811%\n",
      "total_backward_count 1605560 real_backward_count 294617  18.350%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.994341/  2.113397, val:  42.92%, val_best:  60.42%, tr:  98.57%, tr_best:  99.69%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7874%\n",
      "layer   3  Sparsity: 83.8278%\n",
      "total_backward_count 1615350 real_backward_count 296467  18.353%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.976294/  2.105866, val:  40.83%, val_best:  60.42%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.8716%\n",
      "layer   3  Sparsity: 83.0413%\n",
      "total_backward_count 1625140 real_backward_count 298280  18.354%\n",
      "fc layer 2 self.abs_max_out: 4522.0\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.949578/  2.071314, val:  49.17%, val_best:  60.42%, tr:  98.47%, tr_best:  99.69%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.2272%\n",
      "layer   3  Sparsity: 82.7717%\n",
      "total_backward_count 1634930 real_backward_count 300047  18.352%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.978416/  2.079468, val:  37.92%, val_best:  60.42%, tr:  97.85%, tr_best:  99.69%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7438%\n",
      "layer   3  Sparsity: 83.8397%\n",
      "total_backward_count 1644720 real_backward_count 301897  18.356%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.992639/  2.156101, val:  37.50%, val_best:  60.42%, tr:  98.37%, tr_best:  99.69%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.9403%\n",
      "layer   3  Sparsity: 85.3346%\n",
      "total_backward_count 1654510 real_backward_count 303787  18.361%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.984345/  2.106063, val:  47.92%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 87.0196%\n",
      "layer   3  Sparsity: 84.6203%\n",
      "total_backward_count 1664300 real_backward_count 305653  18.365%\n",
      "fc layer 2 self.abs_max_out: 4549.0\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.948640/  2.071014, val:  50.83%, val_best:  60.42%, tr:  98.77%, tr_best:  99.69%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.2111%\n",
      "layer   3  Sparsity: 82.9781%\n",
      "total_backward_count 1674090 real_backward_count 307353  18.359%\n",
      "fc layer 2 self.abs_max_out: 4715.0\n",
      "fc layer 2 self.abs_max_out: 4738.0\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.987251/  2.148885, val:  38.75%, val_best:  60.42%, tr:  97.45%, tr_best:  99.69%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.8232%\n",
      "layer   3  Sparsity: 85.1387%\n",
      "total_backward_count 1683880 real_backward_count 309240  18.365%\n",
      "fc layer 2 self.abs_max_out: 4900.0\n",
      "fc layer 2 self.abs_max_out: 5169.0\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.975407/  2.073620, val:  51.25%, val_best:  60.42%, tr:  97.96%, tr_best:  99.69%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7068%\n",
      "layer   3  Sparsity: 84.1728%\n",
      "total_backward_count 1693670 real_backward_count 311134  18.370%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.963859/  2.119772, val:  48.33%, val_best:  60.42%, tr:  97.45%, tr_best:  99.69%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 87.1210%\n",
      "layer   3  Sparsity: 84.8822%\n",
      "total_backward_count 1703460 real_backward_count 312976  18.373%\n",
      "fc layer 2 self.abs_max_out: 5363.0\n",
      "fc layer 2 self.abs_max_out: 5425.0\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.988533/  2.095326, val:  52.92%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.6338%\n",
      "layer   3  Sparsity: 84.8945%\n",
      "total_backward_count 1713250 real_backward_count 314852  18.377%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.985442/  2.086699, val:  57.50%, val_best:  60.42%, tr:  98.16%, tr_best:  99.69%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.3113%\n",
      "layer   3  Sparsity: 84.7885%\n",
      "total_backward_count 1723040 real_backward_count 316593  18.374%\n",
      "fc layer 2 self.abs_max_out: 5530.0\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.979628/  2.088802, val:  62.50%, val_best:  62.50%, tr:  98.16%, tr_best:  99.69%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.8273%\n",
      "layer   3  Sparsity: 84.1199%\n",
      "total_backward_count 1732830 real_backward_count 318337  18.371%\n",
      "lif layer 2 self.abs_max_v: 6745.5\n",
      "fc layer 2 self.abs_max_out: 5585.0\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.966156/  2.093114, val:  51.67%, val_best:  62.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4890%\n",
      "layer   3  Sparsity: 83.4534%\n",
      "total_backward_count 1742620 real_backward_count 319933  18.359%\n",
      "fc layer 2 self.abs_max_out: 5681.0\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.965655/  2.113154, val:  47.08%, val_best:  62.50%, tr:  97.85%, tr_best:  99.69%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.0588%\n",
      "layer   3  Sparsity: 83.6355%\n",
      "total_backward_count 1752410 real_backward_count 321611  18.352%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.976865/  2.122046, val:  42.08%, val_best:  62.50%, tr:  97.96%, tr_best:  99.69%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.4753%\n",
      "layer   3  Sparsity: 84.2585%\n",
      "total_backward_count 1762200 real_backward_count 323321  18.348%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.975498/  2.111814, val:  56.25%, val_best:  62.50%, tr:  97.96%, tr_best:  99.69%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.3333%\n",
      "layer   3  Sparsity: 84.0450%\n",
      "total_backward_count 1771990 real_backward_count 325011  18.342%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.954888/  2.103698, val:  57.92%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.8175%\n",
      "layer   3  Sparsity: 83.9499%\n",
      "total_backward_count 1781780 real_backward_count 326689  18.335%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.952554/  2.081789, val:  56.25%, val_best:  62.50%, tr:  98.47%, tr_best:  99.69%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.6653%\n",
      "layer   3  Sparsity: 83.6223%\n",
      "total_backward_count 1791570 real_backward_count 328408  18.331%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.983953/  2.093604, val:  47.92%, val_best:  62.50%, tr:  98.57%, tr_best:  99.69%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.1373%\n",
      "layer   3  Sparsity: 84.8817%\n",
      "total_backward_count 1801360 real_backward_count 330146  18.328%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.977659/  2.118665, val:  48.33%, val_best:  62.50%, tr:  97.96%, tr_best:  99.69%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.7667%\n",
      "layer   3  Sparsity: 84.8049%\n",
      "total_backward_count 1811150 real_backward_count 331835  18.322%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.979779/  2.112390, val:  36.67%, val_best:  62.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.4008%\n",
      "layer   3  Sparsity: 83.9952%\n",
      "total_backward_count 1820940 real_backward_count 333604  18.320%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.994610/  2.105836, val:  47.08%, val_best:  62.50%, tr:  98.26%, tr_best:  99.69%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 86.3265%\n",
      "layer   3  Sparsity: 84.9815%\n",
      "total_backward_count 1830730 real_backward_count 335387  18.320%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.935575/  2.108525, val:  48.75%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.5655%\n",
      "layer   3  Sparsity: 82.6843%\n",
      "total_backward_count 1840520 real_backward_count 336994  18.310%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.960919/  2.114043, val:  52.08%, val_best:  62.50%, tr:  97.75%, tr_best:  99.69%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.9715%\n",
      "layer   3  Sparsity: 83.9155%\n",
      "total_backward_count 1850310 real_backward_count 338684  18.304%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.956517/  2.081950, val:  48.33%, val_best:  62.50%, tr:  99.08%, tr_best:  99.69%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.2648%\n",
      "layer   3  Sparsity: 83.5204%\n",
      "total_backward_count 1860100 real_backward_count 340275  18.293%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.944057/  2.079107, val:  57.92%, val_best:  62.50%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.6516%\n",
      "layer   3  Sparsity: 82.9936%\n",
      "total_backward_count 1869890 real_backward_count 341886  18.284%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.957066/  2.094966, val:  46.25%, val_best:  62.50%, tr:  98.67%, tr_best:  99.69%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.0655%\n",
      "layer   3  Sparsity: 84.0786%\n",
      "total_backward_count 1879680 real_backward_count 343595  18.279%\n",
      "lif layer 2 self.abs_max_v: 6796.5\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.918499/  2.106801, val:  56.67%, val_best:  62.50%, tr:  98.98%, tr_best:  99.69%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.5544%\n",
      "layer   3  Sparsity: 82.3201%\n",
      "total_backward_count 1889470 real_backward_count 345177  18.268%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.948834/  2.113161, val:  43.75%, val_best:  62.50%, tr:  98.57%, tr_best:  99.69%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.8022%\n",
      "layer   3  Sparsity: 83.2962%\n",
      "total_backward_count 1899260 real_backward_count 346799  18.260%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.959755/  2.070148, val:  38.33%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.4149%\n",
      "layer   3  Sparsity: 82.8429%\n",
      "total_backward_count 1909050 real_backward_count 348508  18.256%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.941881/  2.099096, val:  48.33%, val_best:  62.50%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.7445%\n",
      "layer   3  Sparsity: 82.7419%\n",
      "total_backward_count 1918840 real_backward_count 350164  18.249%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.938846/  2.083980, val:  54.17%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.4402%\n",
      "layer   3  Sparsity: 82.7387%\n",
      "total_backward_count 1928630 real_backward_count 351781  18.240%\n",
      "lif layer 2 self.abs_max_v: 6814.0\n",
      "lif layer 2 self.abs_max_v: 6815.0\n",
      "lif layer 2 self.abs_max_v: 6851.0\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.927731/  2.076914, val:  52.92%, val_best:  62.50%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 84.4731%\n",
      "layer   3  Sparsity: 82.6377%\n",
      "total_backward_count 1938420 real_backward_count 353358  18.229%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.928647/  2.097814, val:  45.00%, val_best:  62.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.0450%\n",
      "layer   3  Sparsity: 82.4010%\n",
      "total_backward_count 1948210 real_backward_count 354918  18.218%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.946683/  2.085008, val:  61.25%, val_best:  62.50%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 76.2810%\n",
      "layer   2  Sparsity: 85.5878%\n",
      "layer   3  Sparsity: 83.4495%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686c3ae855fc454abf8fc737fd26258d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99081</td></tr><tr><td>tr_epoch_loss</td><td>1.94668</td></tr><tr><td>val_acc_best</td><td>0.625</td></tr><tr><td>val_acc_now</td><td>0.6125</td></tr><tr><td>val_loss</td><td>2.08501</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-143</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z6983la2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z6983la2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_165848-z6983la2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zbdzpzhq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_211614-zbdzpzhq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zbdzpzhq' target=\"_blank\">pious-sweep-148</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zbdzpzhq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zbdzpzhq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251117_211623_552', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 475.0\n",
      "lif layer 1 self.abs_max_v: 475.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 612.0\n",
      "lif layer 1 self.abs_max_v: 748.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "lif layer 2 self.abs_max_v: 547.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 1 self.abs_max_out: 651.0\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "fc layer 2 self.abs_max_out: 787.0\n",
      "lif layer 2 self.abs_max_v: 985.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "fc layer 1 self.abs_max_out: 768.0\n",
      "lif layer 1 self.abs_max_v: 999.5\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1111.5\n",
      "fc layer 2 self.abs_max_out: 836.0\n",
      "lif layer 2 self.abs_max_v: 1010.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 1 self.abs_max_out: 1542.0\n",
      "lif layer 1 self.abs_max_v: 1542.0\n",
      "fc layer 2 self.abs_max_out: 1019.0\n",
      "lif layer 2 self.abs_max_v: 1423.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 1544.0\n",
      "lif layer 1 self.abs_max_v: 1544.0\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "lif layer 2 self.abs_max_v: 1524.5\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 1 self.abs_max_out: 1769.0\n",
      "lif layer 1 self.abs_max_v: 1769.0\n",
      "fc layer 2 self.abs_max_out: 1345.0\n",
      "lif layer 2 self.abs_max_v: 1820.5\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "fc layer 1 self.abs_max_out: 1820.0\n",
      "lif layer 1 self.abs_max_v: 1820.0\n",
      "lif layer 2 self.abs_max_v: 1946.5\n",
      "lif layer 2 self.abs_max_v: 2041.0\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "fc layer 2 self.abs_max_out: 1649.0\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 1 self.abs_max_out: 1999.0\n",
      "lif layer 1 self.abs_max_v: 2227.0\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "lif layer 1 self.abs_max_v: 3106.0\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "lif layer 2 self.abs_max_v: 2099.0\n",
      "lif layer 2 self.abs_max_v: 2101.0\n",
      "lif layer 2 self.abs_max_v: 2155.0\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "lif layer 2 self.abs_max_v: 2374.5\n",
      "fc layer 3 self.abs_max_out: 888.0\n",
      "fc layer 1 self.abs_max_out: 3151.0\n",
      "lif layer 1 self.abs_max_v: 3151.0\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "lif layer 2 self.abs_max_v: 2647.0\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "fc layer 1 self.abs_max_out: 3258.0\n",
      "lif layer 1 self.abs_max_v: 3258.0\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "fc layer 2 self.abs_max_out: 2148.0\n",
      "fc layer 1 self.abs_max_out: 3329.0\n",
      "lif layer 1 self.abs_max_v: 3329.0\n",
      "fc layer 2 self.abs_max_out: 2160.0\n",
      "fc layer 1 self.abs_max_out: 3725.0\n",
      "lif layer 1 self.abs_max_v: 3725.0\n",
      "lif layer 2 self.abs_max_v: 2673.5\n",
      "lif layer 2 self.abs_max_v: 2962.5\n",
      "lif layer 2 self.abs_max_v: 3193.5\n",
      "lif layer 2 self.abs_max_v: 3518.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "fc layer 2 self.abs_max_out: 2327.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "fc layer 3 self.abs_max_out: 971.0\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "fc layer 2 self.abs_max_out: 2965.0\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "fc layer 3 self.abs_max_out: 1187.0\n",
      "fc layer 1 self.abs_max_out: 3859.0\n",
      "lif layer 1 self.abs_max_v: 3859.0\n",
      "fc layer 1 self.abs_max_out: 4139.0\n",
      "lif layer 1 self.abs_max_v: 4303.0\n",
      "fc layer 1 self.abs_max_out: 4490.0\n",
      "lif layer 1 self.abs_max_v: 4490.0\n",
      "fc layer 2 self.abs_max_out: 3192.0\n",
      "fc layer 1 self.abs_max_out: 4907.0\n",
      "lif layer 1 self.abs_max_v: 4907.0\n",
      "lif layer 2 self.abs_max_v: 3572.5\n",
      "lif layer 2 self.abs_max_v: 3912.0\n",
      "lif layer 2 self.abs_max_v: 4084.0\n",
      "lif layer 2 self.abs_max_v: 4117.0\n",
      "lif layer 2 self.abs_max_v: 4454.5\n",
      "fc layer 1 self.abs_max_out: 5297.0\n",
      "lif layer 1 self.abs_max_v: 5297.0\n",
      "fc layer 2 self.abs_max_out: 3832.0\n",
      "lif layer 2 self.abs_max_v: 4676.0\n",
      "lif layer 1 self.abs_max_v: 5640.5\n",
      "fc layer 2 self.abs_max_out: 4174.0\n",
      "fc layer 1 self.abs_max_out: 5764.0\n",
      "lif layer 1 self.abs_max_v: 5764.0\n",
      "lif layer 2 self.abs_max_v: 4905.5\n",
      "lif layer 2 self.abs_max_v: 5188.0\n",
      "lif layer 2 self.abs_max_v: 5218.5\n",
      "fc layer 3 self.abs_max_out: 1200.0\n",
      "fc layer 3 self.abs_max_out: 1268.0\n",
      "lif layer 2 self.abs_max_v: 5318.5\n",
      "fc layer 2 self.abs_max_out: 4233.0\n",
      "fc layer 3 self.abs_max_out: 1271.0\n",
      "fc layer 3 self.abs_max_out: 1340.0\n",
      "lif layer 2 self.abs_max_v: 5460.5\n",
      "lif layer 2 self.abs_max_v: 5850.0\n",
      "lif layer 2 self.abs_max_v: 5869.5\n",
      "lif layer 2 self.abs_max_v: 5916.5\n",
      "lif layer 2 self.abs_max_v: 6158.5\n",
      "lif layer 2 self.abs_max_v: 6439.5\n",
      "fc layer 3 self.abs_max_out: 1458.0\n",
      "fc layer 3 self.abs_max_out: 1463.0\n",
      "lif layer 1 self.abs_max_v: 6163.5\n",
      "fc layer 2 self.abs_max_out: 4333.0\n",
      "lif layer 1 self.abs_max_v: 6466.0\n",
      "lif layer 1 self.abs_max_v: 6875.0\n",
      "lif layer 2 self.abs_max_v: 6991.0\n",
      "fc layer 3 self.abs_max_out: 1530.0\n",
      "fc layer 2 self.abs_max_out: 4723.0\n",
      "fc layer 2 self.abs_max_out: 4833.0\n",
      "lif layer 1 self.abs_max_v: 7245.5\n",
      "lif layer 2 self.abs_max_v: 7567.5\n",
      "lif layer 2 self.abs_max_v: 8021.5\n",
      "fc layer 2 self.abs_max_out: 5132.0\n",
      "fc layer 2 self.abs_max_out: 5156.0\n",
      "fc layer 2 self.abs_max_out: 5572.0\n",
      "lif layer 2 self.abs_max_v: 8103.0\n",
      "lif layer 1 self.abs_max_v: 7493.0\n",
      "lif layer 1 self.abs_max_v: 7548.5\n",
      "lif layer 1 self.abs_max_v: 7567.5\n",
      "lif layer 1 self.abs_max_v: 8173.0\n",
      "lif layer 1 self.abs_max_v: 8335.5\n",
      "lif layer 1 self.abs_max_v: 8350.0\n",
      "lif layer 2 self.abs_max_v: 8295.0\n",
      "lif layer 2 self.abs_max_v: 8588.0\n",
      "fc layer 3 self.abs_max_out: 1625.0\n",
      "fc layer 3 self.abs_max_out: 1801.0\n",
      "lif layer 2 self.abs_max_v: 8664.5\n",
      "lif layer 2 self.abs_max_v: 8803.5\n",
      "lif layer 2 self.abs_max_v: 9156.0\n",
      "lif layer 2 self.abs_max_v: 9371.0\n",
      "lif layer 2 self.abs_max_v: 9391.5\n",
      "fc layer 3 self.abs_max_out: 1861.0\n",
      "fc layer 1 self.abs_max_out: 5873.0\n",
      "fc layer 1 self.abs_max_out: 6132.0\n",
      "fc layer 1 self.abs_max_out: 6374.0\n",
      "lif layer 1 self.abs_max_v: 8659.0\n",
      "fc layer 1 self.abs_max_out: 6629.0\n",
      "fc layer 3 self.abs_max_out: 2094.0\n",
      "lif layer 1 self.abs_max_v: 8763.5\n",
      "fc layer 1 self.abs_max_out: 7105.0\n",
      "lif layer 1 self.abs_max_v: 9016.0\n",
      "lif layer 1 self.abs_max_v: 9215.0\n",
      "lif layer 1 self.abs_max_v: 9659.5\n",
      "fc layer 1 self.abs_max_out: 7161.0\n",
      "fc layer 2 self.abs_max_out: 5914.0\n",
      "fc layer 1 self.abs_max_out: 7208.0\n",
      "lif layer 1 self.abs_max_v: 9727.5\n",
      "lif layer 1 self.abs_max_v: 9946.0\n",
      "fc layer 1 self.abs_max_out: 7219.0\n",
      "fc layer 1 self.abs_max_out: 7297.0\n",
      "fc layer 1 self.abs_max_out: 7483.0\n",
      "fc layer 1 self.abs_max_out: 7682.0\n",
      "lif layer 1 self.abs_max_v: 10075.0\n",
      "fc layer 1 self.abs_max_out: 8114.0\n",
      "lif layer 1 self.abs_max_v: 10484.0\n",
      "lif layer 1 self.abs_max_v: 11046.0\n",
      "lif layer 1 self.abs_max_v: 11683.0\n",
      "lif layer 1 self.abs_max_v: 11713.0\n",
      "lif layer 1 self.abs_max_v: 12970.5\n",
      "lif layer 1 self.abs_max_v: 14299.5\n",
      "lif layer 1 self.abs_max_v: 14850.0\n",
      "lif layer 2 self.abs_max_v: 9492.5\n",
      "lif layer 2 self.abs_max_v: 9631.5\n",
      "fc layer 2 self.abs_max_out: 5934.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.591417/  1.948492, val:  27.50%, val_best:  27.50%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 73.6100%\n",
      "layer   3  Sparsity: 73.4114%\n",
      "total_backward_count 9790 real_backward_count 1760  17.978%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 2130.0\n",
      "lif layer 2 self.abs_max_v: 9726.5\n",
      "fc layer 2 self.abs_max_out: 6151.0\n",
      "fc layer 1 self.abs_max_out: 8996.0\n",
      "fc layer 3 self.abs_max_out: 2177.0\n",
      "lif layer 2 self.abs_max_v: 9887.5\n",
      "lif layer 2 self.abs_max_v: 9936.0\n",
      "fc layer 2 self.abs_max_out: 6189.0\n",
      "fc layer 2 self.abs_max_out: 6203.0\n",
      "fc layer 2 self.abs_max_out: 6258.0\n",
      "fc layer 2 self.abs_max_out: 6268.0\n",
      "lif layer 2 self.abs_max_v: 10228.5\n",
      "fc layer 3 self.abs_max_out: 2265.0\n",
      "fc layer 3 self.abs_max_out: 2338.0\n",
      "lif layer 2 self.abs_max_v: 10390.0\n",
      "lif layer 2 self.abs_max_v: 10435.0\n",
      "lif layer 2 self.abs_max_v: 10555.5\n",
      "lif layer 2 self.abs_max_v: 10661.0\n",
      "lif layer 2 self.abs_max_v: 10897.5\n",
      "fc layer 2 self.abs_max_out: 6274.0\n",
      "fc layer 1 self.abs_max_out: 9250.0\n",
      "fc layer 3 self.abs_max_out: 2426.0\n",
      "lif layer 1 self.abs_max_v: 14872.0\n",
      "lif layer 1 self.abs_max_v: 15519.5\n",
      "lif layer 2 self.abs_max_v: 10993.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.422767/  1.836189, val:  40.83%, val_best:  40.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 74.1732%\n",
      "layer   3  Sparsity: 70.7609%\n",
      "total_backward_count 19580 real_backward_count 3238  16.537%\n",
      "lif layer 1 self.abs_max_v: 16246.0\n",
      "fc layer 1 self.abs_max_out: 10227.0\n",
      "fc layer 2 self.abs_max_out: 6470.0\n",
      "lif layer 1 self.abs_max_v: 17690.5\n",
      "lif layer 1 self.abs_max_v: 18932.5\n",
      "lif layer 1 self.abs_max_v: 19677.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.367748/  1.803999, val:  40.83%, val_best:  40.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 75.6604%\n",
      "layer   3  Sparsity: 70.6827%\n",
      "total_backward_count 29370 real_backward_count 4736  16.125%\n",
      "fc layer 3 self.abs_max_out: 2614.0\n",
      "fc layer 3 self.abs_max_out: 2733.0\n",
      "lif layer 2 self.abs_max_v: 11084.5\n",
      "lif layer 2 self.abs_max_v: 11343.5\n",
      "fc layer 1 self.abs_max_out: 10912.0\n",
      "fc layer 1 self.abs_max_out: 11548.0\n",
      "lif layer 1 self.abs_max_v: 20929.0\n",
      "fc layer 1 self.abs_max_out: 12567.0\n",
      "lif layer 1 self.abs_max_v: 23031.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.327546/  1.840126, val:  39.17%, val_best:  40.83%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 75.5667%\n",
      "layer   3  Sparsity: 70.0657%\n",
      "total_backward_count 39160 real_backward_count 6187  15.799%\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.345412/  1.783676, val:  43.75%, val_best:  43.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.6975%\n",
      "layer   3  Sparsity: 72.1725%\n",
      "total_backward_count 48950 real_backward_count 7653  15.634%\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.379929/  1.852763, val:  40.00%, val_best:  43.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 75.4258%\n",
      "layer   3  Sparsity: 72.7932%\n",
      "total_backward_count 58740 real_backward_count 9117  15.521%\n",
      "fc layer 3 self.abs_max_out: 2977.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.361296/  1.876852, val:  37.50%, val_best:  43.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.8703%\n",
      "layer   3  Sparsity: 73.4089%\n",
      "total_backward_count 68530 real_backward_count 10602  15.471%\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.401111/  1.786775, val:  42.08%, val_best:  43.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.1948%\n",
      "layer   3  Sparsity: 74.7907%\n",
      "total_backward_count 78320 real_backward_count 12115  15.469%\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.497087/  1.800959, val:  55.00%, val_best:  55.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.4347%\n",
      "layer   3  Sparsity: 79.1676%\n",
      "total_backward_count 88110 real_backward_count 13845  15.713%\n",
      "fc layer 2 self.abs_max_out: 6657.0\n",
      "lif layer 2 self.abs_max_v: 11637.0\n",
      "fc layer 1 self.abs_max_out: 13143.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.519804/  1.887753, val:  44.58%, val_best:  55.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.3366%\n",
      "layer   3  Sparsity: 79.7509%\n",
      "total_backward_count 97900 real_backward_count 15474  15.806%\n",
      "fc layer 2 self.abs_max_out: 6682.0\n",
      "lif layer 2 self.abs_max_v: 12245.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.598859/  1.952927, val:  36.67%, val_best:  55.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.4151%\n",
      "layer   3  Sparsity: 81.9259%\n",
      "total_backward_count 107690 real_backward_count 17115  15.893%\n",
      "lif layer 1 self.abs_max_v: 23652.0\n",
      "lif layer 1 self.abs_max_v: 23867.5\n",
      "lif layer 1 self.abs_max_v: 24312.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.600974/  1.852980, val:  46.67%, val_best:  55.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.1197%\n",
      "layer   3  Sparsity: 81.1528%\n",
      "total_backward_count 117480 real_backward_count 18795  15.998%\n",
      "fc layer 2 self.abs_max_out: 6836.0\n",
      "lif layer 2 self.abs_max_v: 12261.5\n",
      "lif layer 1 self.abs_max_v: 24603.0\n",
      "fc layer 1 self.abs_max_out: 13855.0\n",
      "lif layer 1 self.abs_max_v: 26156.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.580847/  1.866784, val:  40.42%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.2941%\n",
      "layer   3  Sparsity: 81.4733%\n",
      "total_backward_count 127270 real_backward_count 20423  16.047%\n",
      "fc layer 2 self.abs_max_out: 6985.0\n",
      "lif layer 2 self.abs_max_v: 12583.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.581633/  1.959044, val:  39.58%, val_best:  55.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.5930%\n",
      "layer   3  Sparsity: 82.5842%\n",
      "total_backward_count 137060 real_backward_count 22068  16.101%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.595144/  1.905159, val:  44.58%, val_best:  55.00%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.9440%\n",
      "layer   3  Sparsity: 81.8486%\n",
      "total_backward_count 146850 real_backward_count 23680  16.125%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.595363/  1.915276, val:  37.92%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.2944%\n",
      "layer   3  Sparsity: 81.7890%\n",
      "total_backward_count 156640 real_backward_count 25310  16.158%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.662271/  1.943145, val:  47.92%, val_best:  55.00%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4291%\n",
      "layer   3  Sparsity: 82.9520%\n",
      "total_backward_count 166430 real_backward_count 27014  16.231%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.575406/  1.914367, val:  33.33%, val_best:  55.00%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.1229%\n",
      "layer   3  Sparsity: 79.8176%\n",
      "total_backward_count 176220 real_backward_count 28655  16.261%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.575570/  1.950657, val:  45.42%, val_best:  55.00%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.1572%\n",
      "layer   3  Sparsity: 81.2587%\n",
      "total_backward_count 186010 real_backward_count 30382  16.334%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.620117/  1.968699, val:  39.17%, val_best:  55.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.2078%\n",
      "layer   3  Sparsity: 82.0274%\n",
      "total_backward_count 195800 real_backward_count 32015  16.351%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.616271/  1.963138, val:  35.00%, val_best:  55.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.0445%\n",
      "layer   3  Sparsity: 81.3501%\n",
      "total_backward_count 205590 real_backward_count 33589  16.338%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.637518/  1.978631, val:  41.25%, val_best:  55.00%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.9976%\n",
      "layer   3  Sparsity: 82.3881%\n",
      "total_backward_count 215380 real_backward_count 35271  16.376%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.640494/  1.847199, val:  50.42%, val_best:  55.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.0968%\n",
      "layer   3  Sparsity: 80.8281%\n",
      "total_backward_count 225170 real_backward_count 36950  16.410%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.585285/  1.883245, val:  62.08%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5160%\n",
      "layer   3  Sparsity: 81.0676%\n",
      "total_backward_count 234960 real_backward_count 38563  16.413%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.666776/  1.905499, val:  51.25%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4927%\n",
      "layer   3  Sparsity: 82.5544%\n",
      "total_backward_count 244750 real_backward_count 40183  16.418%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.671535/  1.857850, val:  53.75%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6345%\n",
      "layer   3  Sparsity: 82.2237%\n",
      "total_backward_count 254540 real_backward_count 41876  16.452%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.633290/  1.909589, val:  50.83%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4624%\n",
      "layer   3  Sparsity: 81.4436%\n",
      "total_backward_count 264330 real_backward_count 43510  16.460%\n",
      "fc layer 1 self.abs_max_out: 13888.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.556906/  1.907268, val:  47.08%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.1183%\n",
      "layer   3  Sparsity: 79.6249%\n",
      "total_backward_count 274120 real_backward_count 45112  16.457%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.589915/  1.937977, val:  44.17%, val_best:  62.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.9345%\n",
      "layer   3  Sparsity: 80.2939%\n",
      "total_backward_count 283910 real_backward_count 46708  16.452%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.620128/  2.039843, val:  26.25%, val_best:  62.08%, tr:  98.47%, tr_best:  99.80%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.9106%\n",
      "layer   3  Sparsity: 81.6666%\n",
      "total_backward_count 293700 real_backward_count 48429  16.489%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.655423/  1.935550, val:  47.08%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.1054%\n",
      "layer   3  Sparsity: 81.6018%\n",
      "total_backward_count 303490 real_backward_count 50114  16.513%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.637770/  1.957794, val:  42.08%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.2183%\n",
      "layer   3  Sparsity: 81.3307%\n",
      "total_backward_count 313280 real_backward_count 51761  16.522%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.675473/  1.974904, val:  42.50%, val_best:  62.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.8247%\n",
      "layer   3  Sparsity: 81.8913%\n",
      "total_backward_count 323070 real_backward_count 53436  16.540%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.629544/  1.945540, val:  40.00%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.6826%\n",
      "layer   3  Sparsity: 81.5615%\n",
      "total_backward_count 332860 real_backward_count 55120  16.560%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.606870/  1.882446, val:  53.33%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.0378%\n",
      "layer   3  Sparsity: 80.5708%\n",
      "total_backward_count 342650 real_backward_count 56734  16.557%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.610410/  1.885042, val:  54.58%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.0061%\n",
      "layer   3  Sparsity: 81.0329%\n",
      "total_backward_count 352440 real_backward_count 58328  16.550%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.649504/  1.913309, val:  44.58%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4152%\n",
      "layer   3  Sparsity: 82.1465%\n",
      "total_backward_count 362230 real_backward_count 59941  16.548%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.678483/  1.971988, val:  41.67%, val_best:  62.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7190%\n",
      "layer   3  Sparsity: 83.4612%\n",
      "total_backward_count 372020 real_backward_count 61538  16.542%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.672258/  1.981717, val:  42.92%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5109%\n",
      "layer   3  Sparsity: 82.7359%\n",
      "total_backward_count 381810 real_backward_count 63207  16.555%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.688287/  1.961516, val:  42.50%, val_best:  62.08%, tr:  98.88%, tr_best:  99.80%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6131%\n",
      "layer   3  Sparsity: 83.7701%\n",
      "total_backward_count 391600 real_backward_count 64836  16.557%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.700189/  1.962495, val:  41.67%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.8386%\n",
      "layer   3  Sparsity: 82.6547%\n",
      "total_backward_count 401390 real_backward_count 66528  16.574%\n",
      "lif layer 2 self.abs_max_v: 12585.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.641701/  1.888660, val:  47.08%, val_best:  62.08%, tr:  98.47%, tr_best:  99.80%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5399%\n",
      "layer   3  Sparsity: 81.1433%\n",
      "total_backward_count 411180 real_backward_count 68124  16.568%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.653313/  1.983015, val:  42.08%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 76.9012%\n",
      "layer   3  Sparsity: 82.6502%\n",
      "total_backward_count 420970 real_backward_count 69757  16.571%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.662388/  1.977385, val:  45.42%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.2262%\n",
      "layer   3  Sparsity: 82.4357%\n",
      "total_backward_count 430760 real_backward_count 71386  16.572%\n",
      "fc layer 1 self.abs_max_out: 14150.0\n",
      "lif layer 1 self.abs_max_v: 26165.5\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.659503/  1.948014, val:  39.17%, val_best:  62.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4525%\n",
      "layer   3  Sparsity: 83.1258%\n",
      "total_backward_count 440550 real_backward_count 73015  16.574%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.682291/  2.007243, val:  43.75%, val_best:  62.08%, tr:  98.88%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8833%\n",
      "layer   3  Sparsity: 83.5206%\n",
      "total_backward_count 450340 real_backward_count 74691  16.585%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.732844/  1.957805, val:  52.08%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8690%\n",
      "layer   3  Sparsity: 84.1058%\n",
      "total_backward_count 460130 real_backward_count 76353  16.594%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.630704/  1.923064, val:  38.33%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3232%\n",
      "layer   3  Sparsity: 81.0821%\n",
      "total_backward_count 469920 real_backward_count 77947  16.587%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.600231/  2.017163, val:  33.75%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5788%\n",
      "layer   3  Sparsity: 81.7909%\n",
      "total_backward_count 479710 real_backward_count 79519  16.576%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.673463/  1.984750, val:  52.50%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9070%\n",
      "layer   3  Sparsity: 83.4995%\n",
      "total_backward_count 489500 real_backward_count 81185  16.585%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.694869/  1.964616, val:  45.00%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.5039%\n",
      "layer   3  Sparsity: 84.3866%\n",
      "total_backward_count 499290 real_backward_count 82818  16.587%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.660403/  1.877520, val:  49.17%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2522%\n",
      "layer   3  Sparsity: 82.5081%\n",
      "total_backward_count 509080 real_backward_count 84392  16.577%\n",
      "fc layer 2 self.abs_max_out: 7051.0\n",
      "lif layer 2 self.abs_max_v: 12596.0\n",
      "fc layer 2 self.abs_max_out: 7256.0\n",
      "lif layer 2 self.abs_max_v: 13103.5\n",
      "fc layer 1 self.abs_max_out: 14743.0\n",
      "lif layer 1 self.abs_max_v: 27395.5\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.669679/  1.987179, val:  35.83%, val_best:  62.08%, tr:  98.37%, tr_best:  99.80%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7042%\n",
      "layer   3  Sparsity: 83.0046%\n",
      "total_backward_count 518870 real_backward_count 86046  16.583%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.712875/  1.957698, val:  45.00%, val_best:  62.08%, tr:  98.16%, tr_best:  99.80%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3497%\n",
      "layer   3  Sparsity: 84.3506%\n",
      "total_backward_count 528660 real_backward_count 87720  16.593%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.695401/  1.987038, val:  50.00%, val_best:  62.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5271%\n",
      "layer   3  Sparsity: 84.0517%\n",
      "total_backward_count 538450 real_backward_count 89371  16.598%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.656777/  1.940981, val:  42.92%, val_best:  62.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7492%\n",
      "layer   3  Sparsity: 83.4867%\n",
      "total_backward_count 548240 real_backward_count 91019  16.602%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.738154/  1.969076, val:  42.08%, val_best:  62.08%, tr:  98.37%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4918%\n",
      "layer   3  Sparsity: 85.0833%\n",
      "total_backward_count 558030 real_backward_count 92766  16.624%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.672458/  1.919555, val:  48.33%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6403%\n",
      "layer   3  Sparsity: 83.3646%\n",
      "total_backward_count 567820 real_backward_count 94415  16.628%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.704168/  1.976213, val:  45.42%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3479%\n",
      "layer   3  Sparsity: 84.1096%\n",
      "total_backward_count 577610 real_backward_count 95987  16.618%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.733606/  1.970947, val:  45.42%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4627%\n",
      "layer   3  Sparsity: 85.6768%\n",
      "total_backward_count 587400 real_backward_count 97585  16.613%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.725830/  1.992916, val:  44.58%, val_best:  62.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0377%\n",
      "layer   3  Sparsity: 85.0210%\n",
      "total_backward_count 597190 real_backward_count 99198  16.611%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.741961/  1.932904, val:  53.75%, val_best:  62.08%, tr:  98.47%, tr_best:  99.80%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9901%\n",
      "layer   3  Sparsity: 85.4067%\n",
      "total_backward_count 606980 real_backward_count 100909  16.625%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.759947/  2.022250, val:  35.83%, val_best:  62.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.6444%\n",
      "layer   3  Sparsity: 85.9382%\n",
      "total_backward_count 616770 real_backward_count 102649  16.643%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.705516/  1.983248, val:  35.83%, val_best:  62.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5007%\n",
      "layer   3  Sparsity: 84.1470%\n",
      "total_backward_count 626560 real_backward_count 104321  16.650%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.653696/  1.928694, val:  42.50%, val_best:  62.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0697%\n",
      "layer   3  Sparsity: 82.0223%\n",
      "total_backward_count 636350 real_backward_count 105934  16.647%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.647716/  1.953554, val:  32.50%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9695%\n",
      "layer   3  Sparsity: 81.2719%\n",
      "total_backward_count 646140 real_backward_count 107533  16.642%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.618834/  1.851802, val:  45.83%, val_best:  62.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3353%\n",
      "layer   3  Sparsity: 80.2856%\n",
      "total_backward_count 655930 real_backward_count 109025  16.621%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.645987/  1.932627, val:  48.33%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0450%\n",
      "layer   3  Sparsity: 81.3635%\n",
      "total_backward_count 665720 real_backward_count 110641  16.620%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.658463/  1.903074, val:  52.50%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.2470%\n",
      "layer   3  Sparsity: 81.3403%\n",
      "total_backward_count 675510 real_backward_count 112265  16.619%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.645038/  1.987439, val:  40.42%, val_best:  62.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2900%\n",
      "layer   3  Sparsity: 82.1025%\n",
      "total_backward_count 685300 real_backward_count 113884  16.618%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.623194/  1.899507, val:  43.33%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4772%\n",
      "layer   3  Sparsity: 81.1663%\n",
      "total_backward_count 695090 real_backward_count 115542  16.623%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.628496/  1.985042, val:  37.92%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3701%\n",
      "layer   3  Sparsity: 81.0733%\n",
      "total_backward_count 704880 real_backward_count 117145  16.619%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.668516/  1.913314, val:  43.75%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.2580%\n",
      "layer   3  Sparsity: 82.2394%\n",
      "total_backward_count 714670 real_backward_count 118785  16.621%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.648561/  1.979937, val:  43.33%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9461%\n",
      "layer   3  Sparsity: 82.3001%\n",
      "total_backward_count 724460 real_backward_count 120435  16.624%\n",
      "fc layer 1 self.abs_max_out: 15075.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.636846/  1.954159, val:  46.67%, val_best:  62.08%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9722%\n",
      "layer   3  Sparsity: 80.6651%\n",
      "total_backward_count 734250 real_backward_count 122109  16.630%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.649774/  1.873756, val:  52.50%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7454%\n",
      "layer   3  Sparsity: 80.9307%\n",
      "total_backward_count 744040 real_backward_count 123674  16.622%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.599351/  1.917609, val:  55.83%, val_best:  62.08%, tr:  98.77%, tr_best:  99.80%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3756%\n",
      "layer   3  Sparsity: 80.4833%\n",
      "total_backward_count 753830 real_backward_count 125305  16.622%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.597868/  2.024622, val:  32.92%, val_best:  62.08%, tr:  98.88%, tr_best:  99.80%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8378%\n",
      "layer   3  Sparsity: 81.9730%\n",
      "total_backward_count 763620 real_backward_count 126884  16.616%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.625966/  1.925092, val:  45.42%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5976%\n",
      "layer   3  Sparsity: 81.9215%\n",
      "total_backward_count 773410 real_backward_count 128477  16.612%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.699752/  1.989764, val:  49.17%, val_best:  62.08%, tr:  98.37%, tr_best:  99.80%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4115%\n",
      "layer   3  Sparsity: 84.1380%\n",
      "total_backward_count 783200 real_backward_count 130099  16.611%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.686638/  1.853191, val:  53.33%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8899%\n",
      "layer   3  Sparsity: 83.2367%\n",
      "total_backward_count 792990 real_backward_count 131727  16.611%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.681421/  1.999606, val:  26.25%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9040%\n",
      "layer   3  Sparsity: 82.9655%\n",
      "total_backward_count 802780 real_backward_count 133312  16.606%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.640609/  1.843330, val:  48.33%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0178%\n",
      "layer   3  Sparsity: 81.5871%\n",
      "total_backward_count 812570 real_backward_count 134913  16.603%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.657915/  1.994180, val:  41.25%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6250%\n",
      "layer   3  Sparsity: 82.9603%\n",
      "total_backward_count 822360 real_backward_count 136526  16.602%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.697050/  1.954654, val:  41.25%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4884%\n",
      "layer   3  Sparsity: 83.2138%\n",
      "total_backward_count 832150 real_backward_count 138093  16.595%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.667608/  2.025873, val:  35.00%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1300%\n",
      "layer   3  Sparsity: 83.3691%\n",
      "total_backward_count 841940 real_backward_count 139663  16.588%\n",
      "fc layer 2 self.abs_max_out: 7558.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.667346/  1.889352, val:  51.67%, val_best:  62.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7276%\n",
      "layer   3  Sparsity: 82.7980%\n",
      "total_backward_count 851730 real_backward_count 141210  16.579%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.690426/  1.963861, val:  47.08%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0934%\n",
      "layer   3  Sparsity: 82.8145%\n",
      "total_backward_count 861520 real_backward_count 142850  16.581%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.663379/  1.950207, val:  45.83%, val_best:  62.08%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3792%\n",
      "layer   3  Sparsity: 81.8921%\n",
      "total_backward_count 871310 real_backward_count 144419  16.575%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.619129/  1.873028, val:  41.67%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8212%\n",
      "layer   3  Sparsity: 79.6318%\n",
      "total_backward_count 881100 real_backward_count 145994  16.570%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.591098/  1.948886, val:  49.17%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5485%\n",
      "layer   3  Sparsity: 80.7408%\n",
      "total_backward_count 890890 real_backward_count 147543  16.561%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.639280/  1.914538, val:  52.08%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8512%\n",
      "layer   3  Sparsity: 81.4640%\n",
      "total_backward_count 900680 real_backward_count 149146  16.559%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.673717/  1.915559, val:  35.42%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3625%\n",
      "layer   3  Sparsity: 82.3008%\n",
      "total_backward_count 910470 real_backward_count 150735  16.556%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.632124/  1.943082, val:  41.25%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6850%\n",
      "layer   3  Sparsity: 81.8648%\n",
      "total_backward_count 920260 real_backward_count 152346  16.555%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.627279/  1.890971, val:  47.08%, val_best:  62.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6467%\n",
      "layer   3  Sparsity: 81.2293%\n",
      "total_backward_count 930050 real_backward_count 153967  16.555%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.599825/  1.893848, val:  52.08%, val_best:  62.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1929%\n",
      "layer   3  Sparsity: 80.1929%\n",
      "total_backward_count 939840 real_backward_count 155600  16.556%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.584610/  1.940791, val:  48.33%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1121%\n",
      "layer   3  Sparsity: 80.4921%\n",
      "total_backward_count 949630 real_backward_count 157117  16.545%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.673133/  1.934564, val:  44.58%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3352%\n",
      "layer   3  Sparsity: 82.9365%\n",
      "total_backward_count 959420 real_backward_count 158772  16.549%\n",
      "lif layer 2 self.abs_max_v: 13147.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.662865/  1.982524, val:  37.08%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0899%\n",
      "layer   3  Sparsity: 82.7089%\n",
      "total_backward_count 969210 real_backward_count 160399  16.549%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.592101/  1.882604, val:  55.83%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1154%\n",
      "layer   3  Sparsity: 80.8704%\n",
      "total_backward_count 979000 real_backward_count 161922  16.540%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.585386/  1.898886, val:  46.67%, val_best:  62.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2363%\n",
      "layer   3  Sparsity: 80.4219%\n",
      "total_backward_count 988790 real_backward_count 163462  16.532%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.570242/  1.835414, val:  53.75%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1792%\n",
      "layer   3  Sparsity: 80.3324%\n",
      "total_backward_count 998580 real_backward_count 165103  16.534%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.606850/  1.919037, val:  46.67%, val_best:  62.08%, tr:  98.67%, tr_best:  99.80%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2329%\n",
      "layer   3  Sparsity: 80.8899%\n",
      "total_backward_count 1008370 real_backward_count 166704  16.532%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.628001/  1.880483, val:  49.58%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1561%\n",
      "layer   3  Sparsity: 81.4654%\n",
      "total_backward_count 1018160 real_backward_count 168293  16.529%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.561638/  1.888061, val:  45.83%, val_best:  62.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6022%\n",
      "layer   3  Sparsity: 80.2281%\n",
      "total_backward_count 1027950 real_backward_count 169767  16.515%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.642602/  1.886921, val:  55.83%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.4112%\n",
      "layer   3  Sparsity: 81.9550%\n",
      "total_backward_count 1037740 real_backward_count 171420  16.519%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.613184/  1.989096, val:  30.42%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0265%\n",
      "layer   3  Sparsity: 81.9539%\n",
      "total_backward_count 1047530 real_backward_count 172983  16.513%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.663945/  1.949244, val:  44.17%, val_best:  62.08%, tr:  98.57%, tr_best:  99.80%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6320%\n",
      "layer   3  Sparsity: 82.4764%\n",
      "total_backward_count 1057320 real_backward_count 174583  16.512%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.611180/  1.869972, val:  43.33%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9435%\n",
      "layer   3  Sparsity: 80.0784%\n",
      "total_backward_count 1067110 real_backward_count 176136  16.506%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.641490/  1.907509, val:  41.25%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0116%\n",
      "layer   3  Sparsity: 81.8309%\n",
      "total_backward_count 1076900 real_backward_count 177728  16.504%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.636506/  1.877971, val:  46.25%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9616%\n",
      "layer   3  Sparsity: 81.9160%\n",
      "total_backward_count 1086690 real_backward_count 179366  16.506%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.602458/  1.938565, val:  38.75%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2839%\n",
      "layer   3  Sparsity: 80.5573%\n",
      "total_backward_count 1096480 real_backward_count 180934  16.501%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.618430/  1.861453, val:  53.75%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7047%\n",
      "layer   3  Sparsity: 81.2008%\n",
      "total_backward_count 1106270 real_backward_count 182576  16.504%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.618785/  1.927325, val:  38.75%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6646%\n",
      "layer   3  Sparsity: 81.8984%\n",
      "total_backward_count 1116060 real_backward_count 184125  16.498%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.645137/  2.009355, val:  36.25%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7639%\n",
      "layer   3  Sparsity: 81.9163%\n",
      "total_backward_count 1125850 real_backward_count 185709  16.495%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.677353/  1.951723, val:  49.17%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8944%\n",
      "layer   3  Sparsity: 83.2430%\n",
      "total_backward_count 1135640 real_backward_count 187308  16.494%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.683667/  1.956129, val:  45.83%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.8194%\n",
      "layer   3  Sparsity: 83.4759%\n",
      "total_backward_count 1145430 real_backward_count 188984  16.499%\n",
      "fc layer 2 self.abs_max_out: 7669.0\n",
      "lif layer 2 self.abs_max_v: 13343.5\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.686368/  1.905128, val:  45.00%, val_best:  62.08%, tr:  98.98%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1132%\n",
      "layer   3  Sparsity: 82.5862%\n",
      "total_backward_count 1155220 real_backward_count 190602  16.499%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.639933/  1.925766, val:  47.50%, val_best:  62.08%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3478%\n",
      "layer   3  Sparsity: 82.5052%\n",
      "total_backward_count 1165010 real_backward_count 192187  16.497%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.659203/  1.928354, val:  43.33%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.6052%\n",
      "layer   3  Sparsity: 82.7401%\n",
      "total_backward_count 1174800 real_backward_count 193782  16.495%\n",
      "lif layer 2 self.abs_max_v: 13648.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.662355/  1.888240, val:  55.42%, val_best:  62.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3884%\n",
      "layer   3  Sparsity: 81.8892%\n",
      "total_backward_count 1184590 real_backward_count 195267  16.484%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.650977/  1.980229, val:  46.25%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1878%\n",
      "layer   3  Sparsity: 83.1416%\n",
      "total_backward_count 1194380 real_backward_count 196809  16.478%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.659142/  1.924966, val:  53.75%, val_best:  62.08%, tr:  99.18%, tr_best:  99.80%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2380%\n",
      "layer   3  Sparsity: 82.2881%\n",
      "total_backward_count 1204170 real_backward_count 198432  16.479%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.704461/  1.927743, val:  57.08%, val_best:  62.08%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2514%\n",
      "layer   3  Sparsity: 83.6926%\n",
      "total_backward_count 1213960 real_backward_count 200023  16.477%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.756775/  1.983551, val:  42.50%, val_best:  62.08%, tr:  98.88%, tr_best:  99.80%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9797%\n",
      "layer   3  Sparsity: 84.9834%\n",
      "total_backward_count 1223750 real_backward_count 201692  16.481%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.652619/  1.950849, val:  45.83%, val_best:  62.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2215%\n",
      "layer   3  Sparsity: 82.3290%\n",
      "total_backward_count 1233540 real_backward_count 203196  16.473%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.701165/  2.004621, val:  45.00%, val_best:  62.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1043%\n",
      "layer   3  Sparsity: 84.3591%\n",
      "total_backward_count 1243330 real_backward_count 204826  16.474%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.738567/  1.971001, val:  52.92%, val_best:  62.08%, tr:  98.77%, tr_best:  99.90%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3393%\n",
      "layer   3  Sparsity: 83.6496%\n",
      "total_backward_count 1253120 real_backward_count 206447  16.475%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.680053/  1.983271, val:  51.25%, val_best:  62.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8352%\n",
      "layer   3  Sparsity: 82.0411%\n",
      "total_backward_count 1262910 real_backward_count 207968  16.467%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.645111/  1.953444, val:  47.08%, val_best:  62.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9831%\n",
      "layer   3  Sparsity: 81.9410%\n",
      "total_backward_count 1272700 real_backward_count 209441  16.456%\n",
      "fc layer 3 self.abs_max_out: 3050.0\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.614703/  1.875600, val:  49.17%, val_best:  62.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2224%\n",
      "layer   3  Sparsity: 81.2941%\n",
      "total_backward_count 1282490 real_backward_count 210984  16.451%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.664832/  1.965064, val:  51.25%, val_best:  62.08%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3418%\n",
      "layer   3  Sparsity: 84.0464%\n",
      "total_backward_count 1292280 real_backward_count 212553  16.448%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.706042/  1.935945, val:  51.67%, val_best:  62.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2811%\n",
      "layer   3  Sparsity: 84.0085%\n",
      "total_backward_count 1302070 real_backward_count 214075  16.441%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.661694/  1.920287, val:  46.67%, val_best:  62.08%, tr:  98.77%, tr_best:  99.90%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0108%\n",
      "layer   3  Sparsity: 82.0634%\n",
      "total_backward_count 1311860 real_backward_count 215695  16.442%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.635220/  1.964169, val:  42.92%, val_best:  62.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0880%\n",
      "layer   3  Sparsity: 81.2741%\n",
      "total_backward_count 1321650 real_backward_count 217303  16.442%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.680400/  1.943560, val:  45.83%, val_best:  62.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0311%\n",
      "layer   3  Sparsity: 83.1632%\n",
      "total_backward_count 1331440 real_backward_count 218923  16.443%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.681626/  1.914062, val:  46.67%, val_best:  62.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2541%\n",
      "layer   3  Sparsity: 83.3309%\n",
      "total_backward_count 1341230 real_backward_count 220535  16.443%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.717786/  1.896974, val:  46.25%, val_best:  62.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2381%\n",
      "layer   3  Sparsity: 83.5111%\n",
      "total_backward_count 1351020 real_backward_count 222070  16.437%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.706645/  1.904604, val:  54.58%, val_best:  62.08%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.4107%\n",
      "layer   3  Sparsity: 83.9893%\n",
      "total_backward_count 1360810 real_backward_count 223688  16.438%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.700706/  1.978762, val:  42.92%, val_best:  62.08%, tr:  98.77%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.3420%\n",
      "layer   3  Sparsity: 84.0248%\n",
      "total_backward_count 1370600 real_backward_count 225313  16.439%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.695760/  1.979861, val:  39.17%, val_best:  62.08%, tr:  98.67%, tr_best:  99.90%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9607%\n",
      "layer   3  Sparsity: 83.6545%\n",
      "total_backward_count 1380390 real_backward_count 226929  16.439%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.670254/  1.999949, val:  41.67%, val_best:  62.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9393%\n",
      "layer   3  Sparsity: 82.4337%\n",
      "total_backward_count 1390180 real_backward_count 228508  16.437%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.657499/  1.925935, val:  45.83%, val_best:  62.08%, tr:  98.88%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2437%\n",
      "layer   3  Sparsity: 82.4052%\n",
      "total_backward_count 1399970 real_backward_count 230160  16.440%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.636035/  1.888928, val:  40.42%, val_best:  62.08%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.5896%\n",
      "layer   3  Sparsity: 82.0720%\n",
      "total_backward_count 1409760 real_backward_count 231759  16.440%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.600334/  1.951509, val:  41.25%, val_best:  62.08%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9457%\n",
      "layer   3  Sparsity: 81.0812%\n",
      "total_backward_count 1419550 real_backward_count 233270  16.433%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.675949/  1.940589, val:  40.00%, val_best:  62.08%, tr:  98.67%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2898%\n",
      "layer   3  Sparsity: 81.9937%\n",
      "total_backward_count 1429340 real_backward_count 234910  16.435%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.649462/  1.929852, val:  50.00%, val_best:  62.08%, tr:  98.77%, tr_best:  99.90%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9888%\n",
      "layer   3  Sparsity: 82.3421%\n",
      "total_backward_count 1439130 real_backward_count 236496  16.433%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.621055/  1.943643, val:  37.50%, val_best:  62.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.4407%\n",
      "layer   3  Sparsity: 81.6318%\n",
      "total_backward_count 1448920 real_backward_count 238047  16.429%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.622215/  1.913803, val:  44.58%, val_best:  62.08%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0981%\n",
      "layer   3  Sparsity: 81.4128%\n",
      "total_backward_count 1458710 real_backward_count 239659  16.430%\n",
      "fc layer 3 self.abs_max_out: 3115.0\n",
      "fc layer 3 self.abs_max_out: 3142.0\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.616857/  1.881420, val:  55.00%, val_best:  62.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0905%\n",
      "layer   3  Sparsity: 82.5342%\n",
      "total_backward_count 1468500 real_backward_count 241241  16.428%\n",
      "fc layer 3 self.abs_max_out: 3146.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.652903/  2.022233, val:  40.00%, val_best:  62.08%, tr:  98.67%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2822%\n",
      "layer   3  Sparsity: 84.6432%\n",
      "total_backward_count 1478290 real_backward_count 242800  16.424%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.693809/  1.904575, val:  48.33%, val_best:  62.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1103%\n",
      "layer   3  Sparsity: 83.7716%\n",
      "total_backward_count 1488080 real_backward_count 244376  16.422%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.666729/  1.886623, val:  52.50%, val_best:  62.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1895%\n",
      "layer   3  Sparsity: 82.2904%\n",
      "total_backward_count 1497870 real_backward_count 245888  16.416%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.676312/  1.917202, val:  62.92%, val_best:  62.92%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7461%\n",
      "layer   3  Sparsity: 83.3504%\n",
      "total_backward_count 1507660 real_backward_count 247473  16.414%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.720790/  1.911597, val:  45.83%, val_best:  62.92%, tr:  98.77%, tr_best:  99.90%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7599%\n",
      "layer   3  Sparsity: 83.8929%\n",
      "total_backward_count 1517450 real_backward_count 249086  16.415%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.700410/  1.980877, val:  50.42%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1736%\n",
      "layer   3  Sparsity: 82.4512%\n",
      "total_backward_count 1527240 real_backward_count 250636  16.411%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.691758/  1.986923, val:  38.75%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9797%\n",
      "layer   3  Sparsity: 83.6155%\n",
      "total_backward_count 1537030 real_backward_count 252136  16.404%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.681018/  1.925770, val:  61.67%, val_best:  62.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5057%\n",
      "layer   3  Sparsity: 83.6389%\n",
      "total_backward_count 1546820 real_backward_count 253658  16.399%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.715751/  2.005886, val:  39.58%, val_best:  62.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0742%\n",
      "layer   3  Sparsity: 84.3615%\n",
      "total_backward_count 1556610 real_backward_count 255262  16.399%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.696572/  1.987128, val:  43.33%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0500%\n",
      "layer   3  Sparsity: 83.8927%\n",
      "total_backward_count 1566400 real_backward_count 256920  16.402%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.699457/  1.963369, val:  51.25%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1474%\n",
      "layer   3  Sparsity: 83.8232%\n",
      "total_backward_count 1576190 real_backward_count 258479  16.399%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.714088/  1.915069, val:  52.50%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9994%\n",
      "layer   3  Sparsity: 83.7818%\n",
      "total_backward_count 1585980 real_backward_count 260059  16.397%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.672274/  1.975150, val:  44.17%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7393%\n",
      "layer   3  Sparsity: 83.2364%\n",
      "total_backward_count 1595770 real_backward_count 261641  16.396%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.678066/  1.955010, val:  40.00%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.6091%\n",
      "layer   3  Sparsity: 83.7109%\n",
      "total_backward_count 1605560 real_backward_count 263121  16.388%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.684842/  1.974419, val:  45.42%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7534%\n",
      "layer   3  Sparsity: 83.8626%\n",
      "total_backward_count 1615350 real_backward_count 264748  16.390%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.728017/  1.972800, val:  45.00%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9586%\n",
      "layer   3  Sparsity: 84.4530%\n",
      "total_backward_count 1625140 real_backward_count 266323  16.388%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.691378/  1.953897, val:  42.08%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3841%\n",
      "layer   3  Sparsity: 83.1729%\n",
      "total_backward_count 1634930 real_backward_count 267907  16.386%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.657512/  1.931487, val:  39.58%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1730%\n",
      "layer   3  Sparsity: 82.7502%\n",
      "total_backward_count 1644720 real_backward_count 269479  16.384%\n",
      "fc layer 2 self.abs_max_out: 7679.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.697993/  1.963384, val:  38.33%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.2598%\n",
      "layer   3  Sparsity: 84.8712%\n",
      "total_backward_count 1654510 real_backward_count 271049  16.382%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.741591/  1.960914, val:  47.08%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1848%\n",
      "layer   3  Sparsity: 84.3854%\n",
      "total_backward_count 1664300 real_backward_count 272583  16.378%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.699826/  1.960363, val:  38.75%, val_best:  62.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.6048%\n",
      "layer   3  Sparsity: 83.5848%\n",
      "total_backward_count 1674090 real_backward_count 274165  16.377%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.695104/  2.018048, val:  36.67%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1093%\n",
      "layer   3  Sparsity: 83.9278%\n",
      "total_backward_count 1683880 real_backward_count 275798  16.379%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.702142/  1.923190, val:  50.00%, val_best:  62.92%, tr:  98.77%, tr_best:  99.90%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0915%\n",
      "layer   3  Sparsity: 83.8880%\n",
      "total_backward_count 1693670 real_backward_count 277363  16.376%\n",
      "fc layer 3 self.abs_max_out: 3202.0\n",
      "fc layer 3 self.abs_max_out: 3221.0\n",
      "fc layer 3 self.abs_max_out: 3225.0\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.653162/  1.889667, val:  53.75%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1027%\n",
      "layer   3  Sparsity: 82.0944%\n",
      "total_backward_count 1703460 real_backward_count 278848  16.370%\n",
      "fc layer 3 self.abs_max_out: 3286.0\n",
      "fc layer 3 self.abs_max_out: 3297.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.624298/  1.876903, val:  48.75%, val_best:  62.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6904%\n",
      "layer   3  Sparsity: 81.0950%\n",
      "total_backward_count 1713250 real_backward_count 280397  16.366%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.662433/  1.912877, val:  46.67%, val_best:  62.92%, tr:  98.77%, tr_best:  99.90%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4461%\n",
      "layer   3  Sparsity: 82.4969%\n",
      "total_backward_count 1723040 real_backward_count 282072  16.371%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.616238/  1.896350, val:  51.67%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.1153%\n",
      "layer   3  Sparsity: 81.1827%\n",
      "total_backward_count 1732830 real_backward_count 283591  16.366%\n",
      "lif layer 2 self.abs_max_v: 13824.0\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.603651/  1.864181, val:  51.67%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9035%\n",
      "layer   3  Sparsity: 80.5805%\n",
      "total_backward_count 1742620 real_backward_count 285072  16.359%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.604758/  1.967929, val:  39.17%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6610%\n",
      "layer   3  Sparsity: 81.6606%\n",
      "total_backward_count 1752410 real_backward_count 286592  16.354%\n",
      "fc layer 3 self.abs_max_out: 3439.0\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.519271/  1.925244, val:  49.17%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7295%\n",
      "layer   3  Sparsity: 79.4635%\n",
      "total_backward_count 1762200 real_backward_count 288033  16.345%\n",
      "fc layer 3 self.abs_max_out: 3499.0\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.617289/  1.899392, val:  52.92%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5499%\n",
      "layer   3  Sparsity: 80.5972%\n",
      "total_backward_count 1771990 real_backward_count 289488  16.337%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.600185/  1.955835, val:  33.75%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8510%\n",
      "layer   3  Sparsity: 80.4738%\n",
      "total_backward_count 1781780 real_backward_count 290891  16.326%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.581065/  1.895745, val:  47.50%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5450%\n",
      "layer   3  Sparsity: 80.3443%\n",
      "total_backward_count 1791570 real_backward_count 292357  16.318%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.559309/  1.955936, val:  41.25%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7987%\n",
      "layer   3  Sparsity: 80.7065%\n",
      "total_backward_count 1801360 real_backward_count 293824  16.311%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.606605/  1.937858, val:  34.17%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6266%\n",
      "layer   3  Sparsity: 81.2728%\n",
      "total_backward_count 1811150 real_backward_count 295347  16.307%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.606967/  1.859442, val:  43.75%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.4236%\n",
      "layer   3  Sparsity: 80.5212%\n",
      "total_backward_count 1820940 real_backward_count 296872  16.303%\n",
      "lif layer 2 self.abs_max_v: 13965.5\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.597186/  1.924788, val:  45.83%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.3148%\n",
      "layer   3  Sparsity: 81.0819%\n",
      "total_backward_count 1830730 real_backward_count 298443  16.302%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.613309/  1.988926, val:  42.08%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9727%\n",
      "layer   3  Sparsity: 82.4485%\n",
      "total_backward_count 1840520 real_backward_count 300043  16.302%\n",
      "fc layer 2 self.abs_max_out: 7844.0\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.642370/  1.948066, val:  48.75%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.7792%\n",
      "layer   3  Sparsity: 82.1666%\n",
      "total_backward_count 1850310 real_backward_count 301627  16.301%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.610775/  1.857798, val:  52.92%, val_best:  62.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.1998%\n",
      "layer   3  Sparsity: 80.5231%\n",
      "total_backward_count 1860100 real_backward_count 303173  16.299%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.585088/  1.918942, val:  50.42%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6459%\n",
      "layer   3  Sparsity: 80.6061%\n",
      "total_backward_count 1869890 real_backward_count 304701  16.295%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.596781/  1.942816, val:  38.75%, val_best:  62.92%, tr:  98.88%, tr_best:  99.90%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.5183%\n",
      "layer   3  Sparsity: 80.4300%\n",
      "total_backward_count 1879680 real_backward_count 306241  16.292%\n",
      "fc layer 3 self.abs_max_out: 3585.0\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.584535/  1.953990, val:  38.75%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9071%\n",
      "layer   3  Sparsity: 81.0595%\n",
      "total_backward_count 1889470 real_backward_count 307806  16.291%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.596795/  1.971646, val:  38.75%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6868%\n",
      "layer   3  Sparsity: 81.1783%\n",
      "total_backward_count 1899260 real_backward_count 309336  16.287%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.668249/  1.979885, val:  40.83%, val_best:  62.92%, tr:  98.67%, tr_best:  99.90%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.6001%\n",
      "layer   3  Sparsity: 82.5064%\n",
      "total_backward_count 1909050 real_backward_count 311025  16.292%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.635271/  1.984979, val:  45.00%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9934%\n",
      "layer   3  Sparsity: 81.6614%\n",
      "total_backward_count 1918840 real_backward_count 312624  16.292%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.682199/  1.959689, val:  46.67%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.9446%\n",
      "layer   3  Sparsity: 83.0903%\n",
      "total_backward_count 1928630 real_backward_count 314213  16.292%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.652596/  1.886452, val:  46.25%, val_best:  62.92%, tr:  98.57%, tr_best:  99.90%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8658%\n",
      "layer   3  Sparsity: 81.5987%\n",
      "total_backward_count 1938420 real_backward_count 315826  16.293%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.625023/  1.978857, val:  34.58%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 78.0458%\n",
      "layer   3  Sparsity: 81.8769%\n",
      "total_backward_count 1948210 real_backward_count 317347  16.289%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.655902/  1.953653, val:  46.67%, val_best:  62.92%, tr:  98.98%, tr_best:  99.90%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.6801%\n",
      "layer   2  Sparsity: 77.8782%\n",
      "layer   3  Sparsity: 83.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b7eaf7f623468380dcf8fe32cb8965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98979</td></tr><tr><td>tr_epoch_loss</td><td>1.6559</td></tr><tr><td>val_acc_best</td><td>0.62917</td></tr><tr><td>val_acc_now</td><td>0.46667</td></tr><tr><td>val_loss</td><td>1.95365</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-sweep-148</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zbdzpzhq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zbdzpzhq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_211614-zbdzpzhq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nvwg95vb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_013233-nvwg95vb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nvwg95vb' target=\"_blank\">faithful-sweep-153</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nvwg95vb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nvwg95vb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251118_013242_207', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 331.0\n",
      "lif layer 1 self.abs_max_v: 331.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "lif layer 2 self.abs_max_v: 790.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 1 self.abs_max_out: 367.0\n",
      "lif layer 1 self.abs_max_v: 414.5\n",
      "fc layer 2 self.abs_max_out: 1008.0\n",
      "lif layer 2 self.abs_max_v: 1403.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 1 self.abs_max_out: 420.0\n",
      "lif layer 1 self.abs_max_v: 540.5\n",
      "lif layer 1 self.abs_max_v: 582.5\n",
      "fc layer 1 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 605.5\n",
      "lif layer 1 self.abs_max_v: 633.0\n",
      "fc layer 2 self.abs_max_out: 1040.0\n",
      "lif layer 2 self.abs_max_v: 1530.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "fc layer 1 self.abs_max_out: 745.0\n",
      "lif layer 1 self.abs_max_v: 869.5\n",
      "lif layer 2 self.abs_max_v: 1770.0\n",
      "lif layer 2 self.abs_max_v: 1911.0\n",
      "fc layer 2 self.abs_max_out: 1322.0\n",
      "fc layer 1 self.abs_max_out: 809.0\n",
      "lif layer 1 self.abs_max_v: 882.0\n",
      "lif layer 1 self.abs_max_v: 998.5\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "lif layer 1 self.abs_max_v: 1103.5\n",
      "fc layer 1 self.abs_max_out: 872.0\n",
      "fc layer 1 self.abs_max_out: 999.0\n",
      "fc layer 2 self.abs_max_out: 1365.0\n",
      "fc layer 1 self.abs_max_out: 1003.0\n",
      "fc layer 1 self.abs_max_out: 1292.0\n",
      "lif layer 1 self.abs_max_v: 1311.0\n",
      "lif layer 2 self.abs_max_v: 2038.0\n",
      "lif layer 2 self.abs_max_v: 2256.0\n",
      "lif layer 1 self.abs_max_v: 1690.5\n",
      "fc layer 3 self.abs_max_out: 641.0\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "fc layer 1 self.abs_max_out: 1563.0\n",
      "fc layer 1 self.abs_max_out: 1599.0\n",
      "fc layer 2 self.abs_max_out: 1491.0\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "lif layer 2 self.abs_max_v: 2280.5\n",
      "lif layer 2 self.abs_max_v: 2303.0\n",
      "fc layer 2 self.abs_max_out: 1537.0\n",
      "lif layer 1 self.abs_max_v: 1901.5\n",
      "fc layer 2 self.abs_max_out: 1615.0\n",
      "lif layer 2 self.abs_max_v: 2610.0\n",
      "lif layer 1 self.abs_max_v: 2011.0\n",
      "fc layer 1 self.abs_max_out: 1626.0\n",
      "lif layer 1 self.abs_max_v: 2239.0\n",
      "fc layer 1 self.abs_max_out: 1745.0\n",
      "lif layer 1 self.abs_max_v: 2575.5\n",
      "fc layer 1 self.abs_max_out: 1906.0\n",
      "lif layer 1 self.abs_max_v: 2588.5\n",
      "fc layer 2 self.abs_max_out: 1634.0\n",
      "lif layer 1 self.abs_max_v: 2948.5\n",
      "fc layer 2 self.abs_max_out: 1667.0\n",
      "lif layer 2 self.abs_max_v: 2691.0\n",
      "lif layer 2 self.abs_max_v: 2748.5\n",
      "lif layer 2 self.abs_max_v: 2810.5\n",
      "lif layer 2 self.abs_max_v: 2864.5\n",
      "fc layer 1 self.abs_max_out: 1909.0\n",
      "lif layer 1 self.abs_max_v: 3178.5\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "fc layer 2 self.abs_max_out: 1719.0\n",
      "lif layer 2 self.abs_max_v: 3026.5\n",
      "lif layer 2 self.abs_max_v: 3076.5\n",
      "fc layer 2 self.abs_max_out: 1800.0\n",
      "fc layer 2 self.abs_max_out: 1821.0\n",
      "fc layer 2 self.abs_max_out: 1958.0\n",
      "fc layer 1 self.abs_max_out: 1988.0\n",
      "lif layer 2 self.abs_max_v: 3196.5\n",
      "lif layer 2 self.abs_max_v: 3261.5\n",
      "lif layer 2 self.abs_max_v: 3342.0\n",
      "lif layer 2 self.abs_max_v: 3399.0\n",
      "lif layer 2 self.abs_max_v: 3408.5\n",
      "lif layer 2 self.abs_max_v: 3590.0\n",
      "fc layer 2 self.abs_max_out: 2090.0\n",
      "lif layer 2 self.abs_max_v: 3778.5\n",
      "fc layer 1 self.abs_max_out: 2148.0\n",
      "lif layer 1 self.abs_max_v: 3503.0\n",
      "fc layer 1 self.abs_max_out: 2670.0\n",
      "lif layer 1 self.abs_max_v: 3742.5\n",
      "lif layer 1 self.abs_max_v: 3879.0\n",
      "lif layer 1 self.abs_max_v: 4069.5\n",
      "fc layer 3 self.abs_max_out: 674.0\n",
      "fc layer 3 self.abs_max_out: 691.0\n",
      "fc layer 3 self.abs_max_out: 694.0\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "fc layer 3 self.abs_max_out: 796.0\n",
      "fc layer 3 self.abs_max_out: 839.0\n",
      "fc layer 1 self.abs_max_out: 2977.0\n",
      "lif layer 1 self.abs_max_v: 4108.0\n",
      "fc layer 2 self.abs_max_out: 2106.0\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "fc layer 1 self.abs_max_out: 3004.0\n",
      "fc layer 1 self.abs_max_out: 3069.0\n",
      "fc layer 1 self.abs_max_out: 3306.0\n",
      "lif layer 1 self.abs_max_v: 4446.0\n",
      "lif layer 1 self.abs_max_v: 4598.0\n",
      "fc layer 2 self.abs_max_out: 2159.0\n",
      "fc layer 2 self.abs_max_out: 2252.0\n",
      "lif layer 1 self.abs_max_v: 4666.5\n",
      "lif layer 2 self.abs_max_v: 3818.0\n",
      "lif layer 2 self.abs_max_v: 3924.5\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "fc layer 3 self.abs_max_out: 899.0\n",
      "lif layer 2 self.abs_max_v: 3958.5\n",
      "lif layer 2 self.abs_max_v: 4148.5\n",
      "fc layer 1 self.abs_max_out: 3757.0\n",
      "lif layer 1 self.abs_max_v: 5923.5\n",
      "fc layer 2 self.abs_max_out: 2306.0\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "fc layer 3 self.abs_max_out: 950.0\n",
      "lif layer 2 self.abs_max_v: 4159.0\n",
      "lif layer 2 self.abs_max_v: 4163.5\n",
      "lif layer 2 self.abs_max_v: 4224.0\n",
      "fc layer 2 self.abs_max_out: 2353.0\n",
      "lif layer 2 self.abs_max_v: 4282.5\n",
      "lif layer 2 self.abs_max_v: 4310.0\n",
      "lif layer 2 self.abs_max_v: 4336.0\n",
      "fc layer 2 self.abs_max_out: 2394.0\n",
      "lif layer 2 self.abs_max_v: 4371.0\n",
      "lif layer 2 self.abs_max_v: 4432.5\n",
      "fc layer 2 self.abs_max_out: 2530.0\n",
      "lif layer 2 self.abs_max_v: 4746.5\n",
      "lif layer 2 self.abs_max_v: 4747.5\n",
      "lif layer 2 self.abs_max_v: 4793.5\n",
      "fc layer 3 self.abs_max_out: 966.0\n",
      "fc layer 2 self.abs_max_out: 2541.0\n",
      "lif layer 1 self.abs_max_v: 5950.0\n",
      "lif layer 1 self.abs_max_v: 6284.5\n",
      "lif layer 1 self.abs_max_v: 6456.5\n",
      "lif layer 1 self.abs_max_v: 6794.5\n",
      "fc layer 2 self.abs_max_out: 2621.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.642769/  1.950477, val:  30.83%, val_best:  30.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0304%\n",
      "layer   2  Sparsity: 66.9172%\n",
      "layer   3  Sparsity: 58.0084%\n",
      "total_backward_count 9790 real_backward_count 1574  16.078%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2709.0\n",
      "fc layer 2 self.abs_max_out: 2927.0\n",
      "lif layer 2 self.abs_max_v: 4946.0\n",
      "fc layer 1 self.abs_max_out: 3881.0\n",
      "fc layer 1 self.abs_max_out: 4037.0\n",
      "lif layer 1 self.abs_max_v: 7068.0\n",
      "lif layer 2 self.abs_max_v: 5007.0\n",
      "fc layer 3 self.abs_max_out: 973.0\n",
      "lif layer 2 self.abs_max_v: 5084.5\n",
      "lif layer 2 self.abs_max_v: 5200.5\n",
      "lif layer 2 self.abs_max_v: 5325.0\n",
      "fc layer 2 self.abs_max_out: 2982.0\n",
      "fc layer 2 self.abs_max_out: 3020.0\n",
      "lif layer 2 self.abs_max_v: 5547.0\n",
      "lif layer 2 self.abs_max_v: 5759.5\n",
      "fc layer 2 self.abs_max_out: 3033.0\n",
      "fc layer 2 self.abs_max_out: 3054.0\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "fc layer 3 self.abs_max_out: 1025.0\n",
      "fc layer 2 self.abs_max_out: 3177.0\n",
      "lif layer 2 self.abs_max_v: 5812.0\n",
      "lif layer 2 self.abs_max_v: 5974.0\n",
      "lif layer 2 self.abs_max_v: 6139.0\n",
      "fc layer 2 self.abs_max_out: 3207.0\n",
      "lif layer 2 self.abs_max_v: 6273.0\n",
      "fc layer 2 self.abs_max_out: 3529.0\n",
      "fc layer 2 self.abs_max_out: 3550.0\n",
      "lif layer 2 self.abs_max_v: 6481.5\n",
      "fc layer 2 self.abs_max_out: 3572.0\n",
      "lif layer 2 self.abs_max_v: 6721.5\n",
      "lif layer 2 self.abs_max_v: 6728.0\n",
      "fc layer 2 self.abs_max_out: 3627.0\n",
      "lif layer 1 self.abs_max_v: 7070.0\n",
      "fc layer 1 self.abs_max_out: 4052.0\n",
      "lif layer 1 self.abs_max_v: 7490.5\n",
      "fc layer 1 self.abs_max_out: 4135.0\n",
      "fc layer 1 self.abs_max_out: 4592.0\n",
      "lif layer 1 self.abs_max_v: 7988.5\n",
      "lif layer 1 self.abs_max_v: 8009.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.558217/  1.862594, val:  43.33%, val_best:  43.33%, tr:  99.18%, tr_best:  99.59%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0457%\n",
      "layer   2  Sparsity: 69.3582%\n",
      "layer   3  Sparsity: 57.5796%\n",
      "total_backward_count 19580 real_backward_count 2905  14.837%\n",
      "fc layer 3 self.abs_max_out: 1041.0\n",
      "fc layer 3 self.abs_max_out: 1071.0\n",
      "fc layer 3 self.abs_max_out: 1191.0\n",
      "fc layer 2 self.abs_max_out: 3644.0\n",
      "lif layer 2 self.abs_max_v: 6829.5\n",
      "lif layer 2 self.abs_max_v: 6937.0\n",
      "fc layer 2 self.abs_max_out: 3652.0\n",
      "fc layer 2 self.abs_max_out: 4325.0\n",
      "lif layer 2 self.abs_max_v: 6955.5\n",
      "lif layer 2 self.abs_max_v: 7009.0\n",
      "fc layer 1 self.abs_max_out: 5064.0\n",
      "lif layer 1 self.abs_max_v: 9002.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.504806/  1.849459, val:  45.00%, val_best:  45.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0520%\n",
      "layer   2  Sparsity: 69.8761%\n",
      "layer   3  Sparsity: 58.0248%\n",
      "total_backward_count 29370 real_backward_count 4222  14.375%\n",
      "lif layer 2 self.abs_max_v: 7112.0\n",
      "lif layer 2 self.abs_max_v: 7191.0\n",
      "lif layer 2 self.abs_max_v: 7354.5\n",
      "lif layer 2 self.abs_max_v: 7406.0\n",
      "lif layer 2 self.abs_max_v: 7562.0\n",
      "fc layer 2 self.abs_max_out: 4372.0\n",
      "lif layer 2 self.abs_max_v: 7905.0\n",
      "fc layer 2 self.abs_max_out: 4505.0\n",
      "fc layer 2 self.abs_max_out: 4529.0\n",
      "lif layer 2 self.abs_max_v: 7954.5\n",
      "lif layer 2 self.abs_max_v: 7986.5\n",
      "lif layer 2 self.abs_max_v: 8051.0\n",
      "lif layer 2 self.abs_max_v: 8101.0\n",
      "fc layer 1 self.abs_max_out: 5248.0\n",
      "lif layer 1 self.abs_max_v: 9293.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.488390/  1.838526, val:  41.67%, val_best:  45.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0327%\n",
      "layer   2  Sparsity: 70.4861%\n",
      "layer   3  Sparsity: 59.1291%\n",
      "total_backward_count 39160 real_backward_count 5489  14.017%\n",
      "lif layer 2 self.abs_max_v: 8237.5\n",
      "lif layer 2 self.abs_max_v: 8309.0\n",
      "lif layer 2 self.abs_max_v: 8511.0\n",
      "fc layer 2 self.abs_max_out: 4631.0\n",
      "lif layer 2 self.abs_max_v: 8711.0\n",
      "fc layer 2 self.abs_max_out: 4677.0\n",
      "lif layer 2 self.abs_max_v: 9032.5\n",
      "fc layer 2 self.abs_max_out: 4681.0\n",
      "lif layer 2 self.abs_max_v: 9050.0\n",
      "fc layer 2 self.abs_max_out: 4721.0\n",
      "lif layer 2 self.abs_max_v: 9071.5\n",
      "fc layer 2 self.abs_max_out: 4781.0\n",
      "lif layer 1 self.abs_max_v: 9592.5\n",
      "fc layer 1 self.abs_max_out: 5368.0\n",
      "lif layer 1 self.abs_max_v: 9654.5\n",
      "lif layer 1 self.abs_max_v: 9719.5\n",
      "fc layer 1 self.abs_max_out: 5623.0\n",
      "lif layer 1 self.abs_max_v: 9738.5\n",
      "lif layer 1 self.abs_max_v: 9871.5\n",
      "lif layer 2 self.abs_max_v: 9120.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.470813/  1.784345, val:  56.25%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0311%\n",
      "layer   2  Sparsity: 69.7751%\n",
      "layer   3  Sparsity: 59.9674%\n",
      "total_backward_count 48950 real_backward_count 6686  13.659%\n",
      "fc layer 2 self.abs_max_out: 5161.0\n",
      "lif layer 2 self.abs_max_v: 9241.5\n",
      "lif layer 2 self.abs_max_v: 9280.0\n",
      "lif layer 2 self.abs_max_v: 9335.0\n",
      "lif layer 2 self.abs_max_v: 9337.0\n",
      "fc layer 2 self.abs_max_out: 5388.0\n",
      "lif layer 2 self.abs_max_v: 9387.0\n",
      "lif layer 2 self.abs_max_v: 9471.5\n",
      "fc layer 2 self.abs_max_out: 5452.0\n",
      "lif layer 2 self.abs_max_v: 9483.0\n",
      "lif layer 2 self.abs_max_v: 9542.0\n",
      "lif layer 2 self.abs_max_v: 9951.5\n",
      "lif layer 1 self.abs_max_v: 10292.0\n",
      "fc layer 1 self.abs_max_out: 5950.0\n",
      "lif layer 1 self.abs_max_v: 10679.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.462026/  1.790621, val:  51.25%, val_best:  56.25%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0271%\n",
      "layer   2  Sparsity: 69.1502%\n",
      "layer   3  Sparsity: 60.8524%\n",
      "total_backward_count 58740 real_backward_count 7866  13.391%\n",
      "fc layer 3 self.abs_max_out: 1216.0\n",
      "fc layer 1 self.abs_max_out: 5969.0\n",
      "lif layer 1 self.abs_max_v: 10751.0\n",
      "lif layer 1 self.abs_max_v: 10909.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.454082/  1.763634, val:  46.67%, val_best:  56.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0215%\n",
      "layer   2  Sparsity: 69.7728%\n",
      "layer   3  Sparsity: 61.7336%\n",
      "total_backward_count 68530 real_backward_count 9043  13.196%\n",
      "fc layer 3 self.abs_max_out: 1225.0\n",
      "fc layer 3 self.abs_max_out: 1237.0\n",
      "fc layer 1 self.abs_max_out: 6016.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.424514/  1.717969, val:  53.33%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0392%\n",
      "layer   2  Sparsity: 69.6875%\n",
      "layer   3  Sparsity: 62.1252%\n",
      "total_backward_count 78320 real_backward_count 10114  12.914%\n",
      "fc layer 1 self.abs_max_out: 6101.0\n",
      "lif layer 1 self.abs_max_v: 10932.0\n",
      "fc layer 1 self.abs_max_out: 6230.0\n",
      "lif layer 1 self.abs_max_v: 11111.0\n",
      "fc layer 1 self.abs_max_out: 6254.0\n",
      "lif layer 1 self.abs_max_v: 11186.5\n",
      "lif layer 1 self.abs_max_v: 11724.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.408985/  1.716876, val:  52.50%, val_best:  56.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0553%\n",
      "layer   2  Sparsity: 69.5241%\n",
      "layer   3  Sparsity: 62.1533%\n",
      "total_backward_count 88110 real_backward_count 11263  12.783%\n",
      "fc layer 3 self.abs_max_out: 1300.0\n",
      "fc layer 3 self.abs_max_out: 1329.0\n",
      "fc layer 3 self.abs_max_out: 1341.0\n",
      "fc layer 1 self.abs_max_out: 6340.0\n",
      "fc layer 1 self.abs_max_out: 6469.0\n",
      "fc layer 1 self.abs_max_out: 6780.0\n",
      "lif layer 1 self.abs_max_v: 12085.5\n",
      "lif layer 1 self.abs_max_v: 12659.0\n",
      "fc layer 1 self.abs_max_out: 6791.0\n",
      "fc layer 1 self.abs_max_out: 6912.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.377095/  1.759033, val:  50.42%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0407%\n",
      "layer   2  Sparsity: 69.4562%\n",
      "layer   3  Sparsity: 62.4228%\n",
      "total_backward_count 97900 real_backward_count 12352  12.617%\n",
      "lif layer 1 self.abs_max_v: 12746.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.389849/  1.696811, val:  53.33%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0425%\n",
      "layer   2  Sparsity: 68.8060%\n",
      "layer   3  Sparsity: 61.4963%\n",
      "total_backward_count 107690 real_backward_count 13410  12.452%\n",
      "fc layer 3 self.abs_max_out: 1459.0\n",
      "lif layer 2 self.abs_max_v: 10043.0\n",
      "fc layer 1 self.abs_max_out: 7092.0\n",
      "lif layer 1 self.abs_max_v: 13136.5\n",
      "fc layer 1 self.abs_max_out: 7128.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.396503/  1.708980, val:  52.50%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0065%\n",
      "layer   2  Sparsity: 68.3196%\n",
      "layer   3  Sparsity: 62.0248%\n",
      "total_backward_count 117480 real_backward_count 14453  12.303%\n",
      "fc layer 1 self.abs_max_out: 7576.0\n",
      "lif layer 1 self.abs_max_v: 13213.5\n",
      "lif layer 1 self.abs_max_v: 13594.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.360535/  1.676837, val:  48.33%, val_best:  56.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0361%\n",
      "layer   2  Sparsity: 68.5714%\n",
      "layer   3  Sparsity: 62.1814%\n",
      "total_backward_count 127270 real_backward_count 15488  12.169%\n",
      "fc layer 2 self.abs_max_out: 5716.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.366089/  1.708303, val:  52.92%, val_best:  56.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0451%\n",
      "layer   2  Sparsity: 68.3095%\n",
      "layer   3  Sparsity: 61.9386%\n",
      "total_backward_count 137060 real_backward_count 16491  12.032%\n",
      "lif layer 2 self.abs_max_v: 10423.0\n",
      "lif layer 2 self.abs_max_v: 10501.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.340267/  1.598010, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0166%\n",
      "layer   2  Sparsity: 68.0544%\n",
      "layer   3  Sparsity: 63.4062%\n",
      "total_backward_count 146850 real_backward_count 17499  11.916%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.337939/  1.674081, val:  50.83%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0242%\n",
      "layer   2  Sparsity: 68.2873%\n",
      "layer   3  Sparsity: 64.2972%\n",
      "total_backward_count 156640 real_backward_count 18511  11.818%\n",
      "fc layer 2 self.abs_max_out: 5747.0\n",
      "lif layer 2 self.abs_max_v: 10539.0\n",
      "fc layer 1 self.abs_max_out: 7585.0\n",
      "lif layer 1 self.abs_max_v: 14124.5\n",
      "fc layer 1 self.abs_max_out: 7775.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.351549/  1.638864, val:  64.58%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0363%\n",
      "layer   2  Sparsity: 67.7378%\n",
      "layer   3  Sparsity: 65.1293%\n",
      "total_backward_count 166430 real_backward_count 19517  11.727%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.369465/  1.616098, val:  64.17%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0047%\n",
      "layer   2  Sparsity: 67.5055%\n",
      "layer   3  Sparsity: 64.5609%\n",
      "total_backward_count 176220 real_backward_count 20494  11.630%\n",
      "fc layer 2 self.abs_max_out: 5763.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.341020/  1.676481, val:  55.42%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0153%\n",
      "layer   2  Sparsity: 67.6792%\n",
      "layer   3  Sparsity: 64.7249%\n",
      "total_backward_count 186010 real_backward_count 21493  11.555%\n",
      "fc layer 1 self.abs_max_out: 7781.0\n",
      "fc layer 1 self.abs_max_out: 8210.0\n",
      "lif layer 1 self.abs_max_v: 14426.5\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.315411/  1.643480, val:  45.83%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0113%\n",
      "layer   2  Sparsity: 67.5450%\n",
      "layer   3  Sparsity: 65.0506%\n",
      "total_backward_count 195800 real_backward_count 22438  11.460%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.294242/  1.616383, val:  56.67%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0237%\n",
      "layer   2  Sparsity: 67.6623%\n",
      "layer   3  Sparsity: 63.7909%\n",
      "total_backward_count 205590 real_backward_count 23372  11.368%\n",
      "lif layer 2 self.abs_max_v: 10553.0\n",
      "lif layer 2 self.abs_max_v: 10708.5\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.303139/  1.649504, val:  59.17%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0325%\n",
      "layer   2  Sparsity: 67.6554%\n",
      "layer   3  Sparsity: 64.9056%\n",
      "total_backward_count 215380 real_backward_count 24313  11.288%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.294650/  1.557014, val:  67.50%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0207%\n",
      "layer   2  Sparsity: 67.1527%\n",
      "layer   3  Sparsity: 64.3541%\n",
      "total_backward_count 225170 real_backward_count 25265  11.220%\n",
      "fc layer 1 self.abs_max_out: 8457.0\n",
      "lif layer 1 self.abs_max_v: 14719.5\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.286990/  1.581779, val:  58.33%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.9946%\n",
      "layer   2  Sparsity: 67.4655%\n",
      "layer   3  Sparsity: 64.0941%\n",
      "total_backward_count 234960 real_backward_count 26225  11.161%\n",
      "lif layer 2 self.abs_max_v: 10810.5\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.275700/  1.568114, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0406%\n",
      "layer   2  Sparsity: 66.4407%\n",
      "layer   3  Sparsity: 64.9773%\n",
      "total_backward_count 244750 real_backward_count 27094  11.070%\n",
      "fc layer 2 self.abs_max_out: 6033.0\n",
      "fc layer 1 self.abs_max_out: 8632.0\n",
      "lif layer 1 self.abs_max_v: 14778.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.276228/  1.564179, val:  58.33%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0225%\n",
      "layer   2  Sparsity: 66.8383%\n",
      "layer   3  Sparsity: 64.6626%\n",
      "total_backward_count 254540 real_backward_count 28014  11.006%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.277517/  1.530999, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0248%\n",
      "layer   2  Sparsity: 66.8244%\n",
      "layer   3  Sparsity: 65.3873%\n",
      "total_backward_count 264330 real_backward_count 28882  10.926%\n",
      "fc layer 1 self.abs_max_out: 8835.0\n",
      "lif layer 1 self.abs_max_v: 14942.5\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.264131/  1.526297, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.9914%\n",
      "layer   2  Sparsity: 66.5086%\n",
      "layer   3  Sparsity: 64.1401%\n",
      "total_backward_count 274120 real_backward_count 29794  10.869%\n",
      "fc layer 1 self.abs_max_out: 8884.0\n",
      "lif layer 1 self.abs_max_v: 15037.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.240306/  1.591981, val:  55.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0174%\n",
      "layer   2  Sparsity: 66.4535%\n",
      "layer   3  Sparsity: 64.8211%\n",
      "total_backward_count 283910 real_backward_count 30606  10.780%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.247476/  1.501034, val:  69.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0224%\n",
      "layer   2  Sparsity: 66.4225%\n",
      "layer   3  Sparsity: 65.3568%\n",
      "total_backward_count 293700 real_backward_count 31431  10.702%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.249150/  1.502467, val:  79.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0496%\n",
      "layer   2  Sparsity: 66.2255%\n",
      "layer   3  Sparsity: 66.0158%\n",
      "total_backward_count 303490 real_backward_count 32270  10.633%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.242277/  1.530430, val:  64.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0054%\n",
      "layer   2  Sparsity: 66.4093%\n",
      "layer   3  Sparsity: 65.6272%\n",
      "total_backward_count 313280 real_backward_count 33028  10.543%\n",
      "fc layer 3 self.abs_max_out: 1522.0\n",
      "fc layer 3 self.abs_max_out: 1555.0\n",
      "lif layer 1 self.abs_max_v: 15443.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.223595/  1.529528, val:  64.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0209%\n",
      "layer   2  Sparsity: 66.6388%\n",
      "layer   3  Sparsity: 64.9145%\n",
      "total_backward_count 323070 real_backward_count 33844  10.476%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.214193/  1.487347, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0500%\n",
      "layer   2  Sparsity: 66.8250%\n",
      "layer   3  Sparsity: 64.4616%\n",
      "total_backward_count 332860 real_backward_count 34618  10.400%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.218705/  1.514862, val:  60.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0250%\n",
      "layer   2  Sparsity: 66.3016%\n",
      "layer   3  Sparsity: 65.1650%\n",
      "total_backward_count 342650 real_backward_count 35352  10.317%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.194391/  1.499067, val:  70.42%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0162%\n",
      "layer   2  Sparsity: 65.8685%\n",
      "layer   3  Sparsity: 65.1139%\n",
      "total_backward_count 352440 real_backward_count 36138  10.254%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.173745/  1.471319, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0513%\n",
      "layer   2  Sparsity: 66.5564%\n",
      "layer   3  Sparsity: 65.0513%\n",
      "total_backward_count 362230 real_backward_count 36886  10.183%\n",
      "fc layer 1 self.abs_max_out: 8913.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.176610/  1.415787, val:  79.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0475%\n",
      "layer   2  Sparsity: 66.2914%\n",
      "layer   3  Sparsity: 65.3964%\n",
      "total_backward_count 372020 real_backward_count 37617  10.112%\n",
      "lif layer 2 self.abs_max_v: 10823.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.166860/  1.442562, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.9942%\n",
      "layer   2  Sparsity: 66.1087%\n",
      "layer   3  Sparsity: 65.3848%\n",
      "total_backward_count 381810 real_backward_count 38342  10.042%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.140644/  1.420925, val:  74.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0365%\n",
      "layer   2  Sparsity: 66.1724%\n",
      "layer   3  Sparsity: 64.4266%\n",
      "total_backward_count 391600 real_backward_count 39062   9.975%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.148671/  1.443299, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0147%\n",
      "layer   2  Sparsity: 66.3552%\n",
      "layer   3  Sparsity: 65.2154%\n",
      "total_backward_count 401390 real_backward_count 39780   9.911%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.144211/  1.429806, val:  75.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0269%\n",
      "layer   2  Sparsity: 65.8137%\n",
      "layer   3  Sparsity: 64.8007%\n",
      "total_backward_count 411180 real_backward_count 40509   9.852%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.138045/  1.424621, val:  71.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0514%\n",
      "layer   2  Sparsity: 65.8996%\n",
      "layer   3  Sparsity: 65.1792%\n",
      "total_backward_count 420970 real_backward_count 41260   9.801%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.134086/  1.435202, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0566%\n",
      "layer   2  Sparsity: 65.5971%\n",
      "layer   3  Sparsity: 65.0474%\n",
      "total_backward_count 430760 real_backward_count 41941   9.737%\n",
      "lif layer 1 self.abs_max_v: 15858.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.156460/  1.426110, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0703%\n",
      "layer   2  Sparsity: 65.2698%\n",
      "layer   3  Sparsity: 65.7330%\n",
      "total_backward_count 440550 real_backward_count 42643   9.679%\n",
      "fc layer 1 self.abs_max_out: 9241.0\n",
      "lif layer 1 self.abs_max_v: 16518.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.153173/  1.439732, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0111%\n",
      "layer   2  Sparsity: 65.4676%\n",
      "layer   3  Sparsity: 65.7539%\n",
      "total_backward_count 450340 real_backward_count 43314   9.618%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.153420/  1.476106, val:  68.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0121%\n",
      "layer   2  Sparsity: 65.4856%\n",
      "layer   3  Sparsity: 64.3614%\n",
      "total_backward_count 460130 real_backward_count 44009   9.564%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.123383/  1.506009, val:  51.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0516%\n",
      "layer   2  Sparsity: 65.8405%\n",
      "layer   3  Sparsity: 63.3774%\n",
      "total_backward_count 469920 real_backward_count 44650   9.502%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.137546/  1.427661, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.9984%\n",
      "layer   2  Sparsity: 65.4027%\n",
      "layer   3  Sparsity: 64.0290%\n",
      "total_backward_count 479710 real_backward_count 45322   9.448%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.106628/  1.380470, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0127%\n",
      "layer   2  Sparsity: 65.6248%\n",
      "layer   3  Sparsity: 64.5929%\n",
      "total_backward_count 489500 real_backward_count 45966   9.390%\n",
      "fc layer 1 self.abs_max_out: 9257.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.099362/  1.457407, val:  60.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0346%\n",
      "layer   2  Sparsity: 65.5366%\n",
      "layer   3  Sparsity: 65.0701%\n",
      "total_backward_count 499290 real_backward_count 46564   9.326%\n",
      "lif layer 2 self.abs_max_v: 10899.0\n",
      "lif layer 2 self.abs_max_v: 10964.5\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.082757/  1.391032, val:  70.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0005%\n",
      "layer   2  Sparsity: 65.4564%\n",
      "layer   3  Sparsity: 63.2513%\n",
      "total_backward_count 509080 real_backward_count 47187   9.269%\n",
      "fc layer 3 self.abs_max_out: 1567.0\n",
      "fc layer 3 self.abs_max_out: 1583.0\n",
      "fc layer 3 self.abs_max_out: 1610.0\n",
      "fc layer 3 self.abs_max_out: 1614.0\n",
      "fc layer 1 self.abs_max_out: 9311.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.066513/  1.350207, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0270%\n",
      "layer   2  Sparsity: 65.4810%\n",
      "layer   3  Sparsity: 64.3507%\n",
      "total_backward_count 518870 real_backward_count 47848   9.222%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.056273/  1.319761, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0177%\n",
      "layer   2  Sparsity: 65.1256%\n",
      "layer   3  Sparsity: 64.9644%\n",
      "total_backward_count 528660 real_backward_count 48480   9.170%\n",
      "fc layer 3 self.abs_max_out: 1645.0\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.043908/  1.361571, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0287%\n",
      "layer   2  Sparsity: 65.0409%\n",
      "layer   3  Sparsity: 64.3863%\n",
      "total_backward_count 538450 real_backward_count 49093   9.117%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.049992/  1.367542, val:  72.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0368%\n",
      "layer   2  Sparsity: 65.0281%\n",
      "layer   3  Sparsity: 64.0232%\n",
      "total_backward_count 548240 real_backward_count 49724   9.070%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.046681/  1.347210, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0069%\n",
      "layer   2  Sparsity: 65.0306%\n",
      "layer   3  Sparsity: 63.0975%\n",
      "total_backward_count 558030 real_backward_count 50369   9.026%\n",
      "fc layer 3 self.abs_max_out: 1650.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.038639/  1.356308, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0396%\n",
      "layer   2  Sparsity: 65.4805%\n",
      "layer   3  Sparsity: 63.2788%\n",
      "total_backward_count 567820 real_backward_count 51009   8.983%\n",
      "fc layer 1 self.abs_max_out: 9345.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.044881/  1.339689, val:  82.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0286%\n",
      "layer   2  Sparsity: 65.2874%\n",
      "layer   3  Sparsity: 64.4514%\n",
      "total_backward_count 577610 real_backward_count 51620   8.937%\n",
      "fc layer 1 self.abs_max_out: 9608.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.032881/  1.348846, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0384%\n",
      "layer   2  Sparsity: 65.0417%\n",
      "layer   3  Sparsity: 64.9763%\n",
      "total_backward_count 587400 real_backward_count 52194   8.886%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.038673/  1.353926, val:  71.67%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0343%\n",
      "layer   2  Sparsity: 65.1941%\n",
      "layer   3  Sparsity: 64.6420%\n",
      "total_backward_count 597190 real_backward_count 52802   8.842%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.028189/  1.273267, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0094%\n",
      "layer   2  Sparsity: 65.0403%\n",
      "layer   3  Sparsity: 63.6132%\n",
      "total_backward_count 606980 real_backward_count 53403   8.798%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.042207/  1.363205, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0258%\n",
      "layer   2  Sparsity: 64.9883%\n",
      "layer   3  Sparsity: 64.3746%\n",
      "total_backward_count 616770 real_backward_count 54023   8.759%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.022413/  1.311476, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0501%\n",
      "layer   2  Sparsity: 65.0928%\n",
      "layer   3  Sparsity: 64.0151%\n",
      "total_backward_count 626560 real_backward_count 54617   8.717%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.026468/  1.355909, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0498%\n",
      "layer   2  Sparsity: 65.0301%\n",
      "layer   3  Sparsity: 63.6982%\n",
      "total_backward_count 636350 real_backward_count 55213   8.677%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.001655/  1.347103, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.0277%\n",
      "layer   2  Sparsity: 65.0427%\n",
      "layer   3  Sparsity: 63.7635%\n",
      "total_backward_count 646140 real_backward_count 55755   8.629%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.013652/  1.341772, val:  74.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0229%\n",
      "layer   2  Sparsity: 65.1790%\n",
      "layer   3  Sparsity: 63.5431%\n",
      "total_backward_count 655930 real_backward_count 56310   8.585%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.987958/  1.282221, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0088%\n",
      "layer   2  Sparsity: 65.0209%\n",
      "layer   3  Sparsity: 63.9413%\n",
      "total_backward_count 665720 real_backward_count 56863   8.542%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.991769/  1.295915, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0115%\n",
      "layer   2  Sparsity: 65.1472%\n",
      "layer   3  Sparsity: 63.4697%\n",
      "total_backward_count 675510 real_backward_count 57384   8.495%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.986204/  1.348685, val:  72.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0218%\n",
      "layer   2  Sparsity: 65.1675%\n",
      "layer   3  Sparsity: 63.5353%\n",
      "total_backward_count 685300 real_backward_count 57925   8.453%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.998642/  1.313637, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0326%\n",
      "layer   2  Sparsity: 65.3352%\n",
      "layer   3  Sparsity: 64.1122%\n",
      "total_backward_count 695090 real_backward_count 58471   8.412%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.971070/  1.333187, val:  67.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0173%\n",
      "layer   2  Sparsity: 65.0105%\n",
      "layer   3  Sparsity: 63.7628%\n",
      "total_backward_count 704880 real_backward_count 58990   8.369%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.968475/  1.278867, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0294%\n",
      "layer   2  Sparsity: 65.0901%\n",
      "layer   3  Sparsity: 64.3419%\n",
      "total_backward_count 714670 real_backward_count 59584   8.337%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.981466/  1.302543, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0262%\n",
      "layer   2  Sparsity: 65.2623%\n",
      "layer   3  Sparsity: 64.5943%\n",
      "total_backward_count 724460 real_backward_count 60114   8.298%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.989451/  1.280292, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0396%\n",
      "layer   2  Sparsity: 65.2682%\n",
      "layer   3  Sparsity: 64.5701%\n",
      "total_backward_count 734250 real_backward_count 60652   8.260%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.972438/  1.255659, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0153%\n",
      "layer   2  Sparsity: 65.2001%\n",
      "layer   3  Sparsity: 64.3501%\n",
      "total_backward_count 744040 real_backward_count 61152   8.219%\n",
      "fc layer 3 self.abs_max_out: 1657.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.968015/  1.286919, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0384%\n",
      "layer   2  Sparsity: 64.9828%\n",
      "layer   3  Sparsity: 64.0097%\n",
      "total_backward_count 753830 real_backward_count 61687   8.183%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.989134/  1.310004, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0387%\n",
      "layer   2  Sparsity: 64.7545%\n",
      "layer   3  Sparsity: 64.1201%\n",
      "total_backward_count 763620 real_backward_count 62214   8.147%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.959387/  1.267685, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0466%\n",
      "layer   2  Sparsity: 64.6682%\n",
      "layer   3  Sparsity: 64.3416%\n",
      "total_backward_count 773410 real_backward_count 62765   8.115%\n",
      "fc layer 3 self.abs_max_out: 1725.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.937606/  1.271660, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0240%\n",
      "layer   2  Sparsity: 64.8525%\n",
      "layer   3  Sparsity: 64.0062%\n",
      "total_backward_count 783200 real_backward_count 63294   8.081%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.955723/  1.236912, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0051%\n",
      "layer   2  Sparsity: 65.0424%\n",
      "layer   3  Sparsity: 63.9482%\n",
      "total_backward_count 792990 real_backward_count 63825   8.049%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.931411/  1.252256, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0370%\n",
      "layer   2  Sparsity: 65.1913%\n",
      "layer   3  Sparsity: 63.5107%\n",
      "total_backward_count 802780 real_backward_count 64343   8.015%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.933788/  1.295029, val:  74.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0244%\n",
      "layer   2  Sparsity: 65.3583%\n",
      "layer   3  Sparsity: 63.3693%\n",
      "total_backward_count 812570 real_backward_count 64838   7.979%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.917416/  1.288625, val:  73.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0024%\n",
      "layer   2  Sparsity: 64.9128%\n",
      "layer   3  Sparsity: 63.1512%\n",
      "total_backward_count 822360 real_backward_count 65324   7.943%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.929232/  1.245480, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0259%\n",
      "layer   2  Sparsity: 65.0126%\n",
      "layer   3  Sparsity: 63.2131%\n",
      "total_backward_count 832150 real_backward_count 65790   7.906%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.909934/  1.282359, val:  74.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0337%\n",
      "layer   2  Sparsity: 64.8376%\n",
      "layer   3  Sparsity: 63.9977%\n",
      "total_backward_count 841940 real_backward_count 66267   7.871%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.915553/  1.196953, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0158%\n",
      "layer   2  Sparsity: 64.8812%\n",
      "layer   3  Sparsity: 63.7646%\n",
      "total_backward_count 851730 real_backward_count 66729   7.835%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.918040/  1.233016, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.9859%\n",
      "layer   2  Sparsity: 65.1393%\n",
      "layer   3  Sparsity: 65.0718%\n",
      "total_backward_count 861520 real_backward_count 67188   7.799%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.913837/  1.216844, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0060%\n",
      "layer   2  Sparsity: 64.9013%\n",
      "layer   3  Sparsity: 64.3398%\n",
      "total_backward_count 871310 real_backward_count 67654   7.765%\n",
      "fc layer 3 self.abs_max_out: 1768.0\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.908631/  1.188462, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0392%\n",
      "layer   2  Sparsity: 65.0748%\n",
      "layer   3  Sparsity: 63.5811%\n",
      "total_backward_count 881100 real_backward_count 68162   7.736%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.909433/  1.243624, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0318%\n",
      "layer   2  Sparsity: 64.9337%\n",
      "layer   3  Sparsity: 63.6012%\n",
      "total_backward_count 890890 real_backward_count 68643   7.705%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.900073/  1.245633, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0000%\n",
      "layer   2  Sparsity: 64.5869%\n",
      "layer   3  Sparsity: 63.1581%\n",
      "total_backward_count 900680 real_backward_count 69095   7.671%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.896757/  1.213255, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0490%\n",
      "layer   2  Sparsity: 64.8836%\n",
      "layer   3  Sparsity: 63.5860%\n",
      "total_backward_count 910470 real_backward_count 69562   7.640%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.915055/  1.219572, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0396%\n",
      "layer   2  Sparsity: 64.7007%\n",
      "layer   3  Sparsity: 63.4492%\n",
      "total_backward_count 920260 real_backward_count 70045   7.611%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.892063/  1.210520, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0473%\n",
      "layer   2  Sparsity: 64.6239%\n",
      "layer   3  Sparsity: 63.1015%\n",
      "total_backward_count 930050 real_backward_count 70494   7.580%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.906979/  1.192444, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0458%\n",
      "layer   2  Sparsity: 64.5108%\n",
      "layer   3  Sparsity: 64.4239%\n",
      "total_backward_count 939840 real_backward_count 71015   7.556%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.902519/  1.217005, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.9997%\n",
      "layer   2  Sparsity: 64.5885%\n",
      "layer   3  Sparsity: 65.2943%\n",
      "total_backward_count 949630 real_backward_count 71528   7.532%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.901059/  1.240874, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0367%\n",
      "layer   2  Sparsity: 64.7358%\n",
      "layer   3  Sparsity: 64.4829%\n",
      "total_backward_count 959420 real_backward_count 71965   7.501%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.893463/  1.303840, val:  72.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0367%\n",
      "layer   2  Sparsity: 64.5001%\n",
      "layer   3  Sparsity: 63.3489%\n",
      "total_backward_count 969210 real_backward_count 72456   7.476%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.902263/  1.229149, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0316%\n",
      "layer   2  Sparsity: 64.6144%\n",
      "layer   3  Sparsity: 62.9527%\n",
      "total_backward_count 979000 real_backward_count 72921   7.449%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.931441/  1.293705, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0232%\n",
      "layer   2  Sparsity: 64.8532%\n",
      "layer   3  Sparsity: 62.6968%\n",
      "total_backward_count 988790 real_backward_count 73383   7.421%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.901097/  1.208849, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0196%\n",
      "layer   2  Sparsity: 64.5564%\n",
      "layer   3  Sparsity: 62.8077%\n",
      "total_backward_count 998580 real_backward_count 73837   7.394%\n",
      "lif layer 1 self.abs_max_v: 17202.5\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.886804/  1.199332, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0689%\n",
      "layer   2  Sparsity: 64.6464%\n",
      "layer   3  Sparsity: 62.9305%\n",
      "total_backward_count 1008370 real_backward_count 74308   7.369%\n",
      "fc layer 3 self.abs_max_out: 1791.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.890198/  1.223936, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0217%\n",
      "layer   2  Sparsity: 64.7037%\n",
      "layer   3  Sparsity: 63.3022%\n",
      "total_backward_count 1018160 real_backward_count 74815   7.348%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.886888/  1.175537, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0503%\n",
      "layer   2  Sparsity: 64.9233%\n",
      "layer   3  Sparsity: 63.2717%\n",
      "total_backward_count 1027950 real_backward_count 75269   7.322%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.871182/  1.207671, val:  80.83%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0231%\n",
      "layer   2  Sparsity: 64.7762%\n",
      "layer   3  Sparsity: 62.6354%\n",
      "total_backward_count 1037740 real_backward_count 75705   7.295%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.889934/  1.242095, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0337%\n",
      "layer   2  Sparsity: 64.7591%\n",
      "layer   3  Sparsity: 63.4021%\n",
      "total_backward_count 1047530 real_backward_count 76140   7.269%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.887854/  1.236567, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0288%\n",
      "layer   2  Sparsity: 64.7474%\n",
      "layer   3  Sparsity: 64.2468%\n",
      "total_backward_count 1057320 real_backward_count 76597   7.244%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.897242/  1.204138, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0223%\n",
      "layer   2  Sparsity: 64.8630%\n",
      "layer   3  Sparsity: 63.9194%\n",
      "total_backward_count 1067110 real_backward_count 77031   7.219%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.892589/  1.195718, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.9967%\n",
      "layer   2  Sparsity: 65.1000%\n",
      "layer   3  Sparsity: 64.1554%\n",
      "total_backward_count 1076900 real_backward_count 77496   7.196%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.897087/  1.229860, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0427%\n",
      "layer   2  Sparsity: 64.6818%\n",
      "layer   3  Sparsity: 63.9654%\n",
      "total_backward_count 1086690 real_backward_count 77974   7.175%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.916510/  1.240770, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0459%\n",
      "layer   2  Sparsity: 64.6044%\n",
      "layer   3  Sparsity: 64.1907%\n",
      "total_backward_count 1096480 real_backward_count 78392   7.149%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.895566/  1.258001, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0429%\n",
      "layer   2  Sparsity: 64.8881%\n",
      "layer   3  Sparsity: 63.9206%\n",
      "total_backward_count 1106270 real_backward_count 78841   7.127%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.875295/  1.237640, val:  72.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.9891%\n",
      "layer   2  Sparsity: 64.9626%\n",
      "layer   3  Sparsity: 63.9441%\n",
      "total_backward_count 1116060 real_backward_count 79262   7.102%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.894119/  1.212684, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0383%\n",
      "layer   2  Sparsity: 64.6746%\n",
      "layer   3  Sparsity: 64.1546%\n",
      "total_backward_count 1125850 real_backward_count 79680   7.077%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.868964/  1.217268, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0313%\n",
      "layer   2  Sparsity: 64.6719%\n",
      "layer   3  Sparsity: 61.9679%\n",
      "total_backward_count 1135640 real_backward_count 80128   7.056%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.885783/  1.180786, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0232%\n",
      "layer   2  Sparsity: 64.8366%\n",
      "layer   3  Sparsity: 63.0825%\n",
      "total_backward_count 1145430 real_backward_count 80529   7.030%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.881369/  1.202064, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0416%\n",
      "layer   2  Sparsity: 64.9486%\n",
      "layer   3  Sparsity: 62.8478%\n",
      "total_backward_count 1155220 real_backward_count 80945   7.007%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.848009/  1.224093, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0095%\n",
      "layer   2  Sparsity: 64.6835%\n",
      "layer   3  Sparsity: 63.2510%\n",
      "total_backward_count 1165010 real_backward_count 81370   6.984%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.858091/  1.196654, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0178%\n",
      "layer   2  Sparsity: 65.0005%\n",
      "layer   3  Sparsity: 63.8331%\n",
      "total_backward_count 1174800 real_backward_count 81789   6.962%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.863411/  1.231730, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0120%\n",
      "layer   2  Sparsity: 65.0072%\n",
      "layer   3  Sparsity: 63.5297%\n",
      "total_backward_count 1184590 real_backward_count 82198   6.939%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.855359/  1.194842, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0344%\n",
      "layer   2  Sparsity: 65.2734%\n",
      "layer   3  Sparsity: 63.9892%\n",
      "total_backward_count 1194380 real_backward_count 82596   6.915%\n",
      "fc layer 3 self.abs_max_out: 1803.0\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.856949/  1.185357, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0159%\n",
      "layer   2  Sparsity: 64.8905%\n",
      "layer   3  Sparsity: 63.6395%\n",
      "total_backward_count 1204170 real_backward_count 82996   6.892%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.861784/  1.173193, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0233%\n",
      "layer   2  Sparsity: 64.7228%\n",
      "layer   3  Sparsity: 63.5660%\n",
      "total_backward_count 1213960 real_backward_count 83381   6.869%\n",
      "fc layer 3 self.abs_max_out: 1822.0\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.871454/  1.187696, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0447%\n",
      "layer   2  Sparsity: 64.7053%\n",
      "layer   3  Sparsity: 64.5033%\n",
      "total_backward_count 1223750 real_backward_count 83798   6.848%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.875115/  1.198765, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.9984%\n",
      "layer   2  Sparsity: 64.7778%\n",
      "layer   3  Sparsity: 64.3911%\n",
      "total_backward_count 1233540 real_backward_count 84226   6.828%\n",
      "fc layer 3 self.abs_max_out: 1886.0\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.858535/  1.193614, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0503%\n",
      "layer   2  Sparsity: 64.9428%\n",
      "layer   3  Sparsity: 64.1267%\n",
      "total_backward_count 1243330 real_backward_count 84627   6.806%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.834360/  1.180153, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0457%\n",
      "layer   2  Sparsity: 64.9166%\n",
      "layer   3  Sparsity: 63.7604%\n",
      "total_backward_count 1253120 real_backward_count 85059   6.788%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.842335/  1.189493, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0097%\n",
      "layer   2  Sparsity: 64.9302%\n",
      "layer   3  Sparsity: 63.5381%\n",
      "total_backward_count 1262910 real_backward_count 85444   6.766%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.832017/  1.135022, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0246%\n",
      "layer   2  Sparsity: 64.9226%\n",
      "layer   3  Sparsity: 64.1166%\n",
      "total_backward_count 1272700 real_backward_count 85867   6.747%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.833022/  1.162164, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0137%\n",
      "layer   2  Sparsity: 64.7506%\n",
      "layer   3  Sparsity: 63.9716%\n",
      "total_backward_count 1282490 real_backward_count 86281   6.728%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.829074/  1.135862, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0201%\n",
      "layer   2  Sparsity: 64.8413%\n",
      "layer   3  Sparsity: 63.8139%\n",
      "total_backward_count 1292280 real_backward_count 86665   6.706%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.827112/  1.187699, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0380%\n",
      "layer   2  Sparsity: 65.2551%\n",
      "layer   3  Sparsity: 64.2257%\n",
      "total_backward_count 1302070 real_backward_count 87049   6.685%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.829805/  1.173343, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0339%\n",
      "layer   2  Sparsity: 64.9936%\n",
      "layer   3  Sparsity: 64.0014%\n",
      "total_backward_count 1311860 real_backward_count 87459   6.667%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.832151/  1.220319, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0215%\n",
      "layer   2  Sparsity: 64.6733%\n",
      "layer   3  Sparsity: 63.3391%\n",
      "total_backward_count 1321650 real_backward_count 87861   6.648%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.836938/  1.163749, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0084%\n",
      "layer   2  Sparsity: 65.2213%\n",
      "layer   3  Sparsity: 63.6191%\n",
      "total_backward_count 1331440 real_backward_count 88235   6.627%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.821470/  1.162607, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0293%\n",
      "layer   2  Sparsity: 64.8834%\n",
      "layer   3  Sparsity: 64.3221%\n",
      "total_backward_count 1341230 real_backward_count 88610   6.607%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.812085/  1.137501, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0485%\n",
      "layer   2  Sparsity: 64.7365%\n",
      "layer   3  Sparsity: 64.1295%\n",
      "total_backward_count 1351020 real_backward_count 89003   6.588%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.830769/  1.145512, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0129%\n",
      "layer   2  Sparsity: 64.8874%\n",
      "layer   3  Sparsity: 63.9048%\n",
      "total_backward_count 1360810 real_backward_count 89398   6.569%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.813814/  1.130784, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0321%\n",
      "layer   2  Sparsity: 65.0986%\n",
      "layer   3  Sparsity: 63.5804%\n",
      "total_backward_count 1370600 real_backward_count 89769   6.550%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.801169/  1.176319, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.9967%\n",
      "layer   2  Sparsity: 64.8497%\n",
      "layer   3  Sparsity: 62.6569%\n",
      "total_backward_count 1380390 real_backward_count 90147   6.531%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.817858/  1.159598, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0319%\n",
      "layer   2  Sparsity: 64.5050%\n",
      "layer   3  Sparsity: 62.7500%\n",
      "total_backward_count 1390180 real_backward_count 90522   6.512%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.805325/  1.174651, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0101%\n",
      "layer   2  Sparsity: 64.8461%\n",
      "layer   3  Sparsity: 63.9927%\n",
      "total_backward_count 1399970 real_backward_count 90867   6.491%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.804636/  1.196611, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0067%\n",
      "layer   2  Sparsity: 64.9296%\n",
      "layer   3  Sparsity: 63.7899%\n",
      "total_backward_count 1409760 real_backward_count 91243   6.472%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.814533/  1.157321, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0375%\n",
      "layer   2  Sparsity: 65.0088%\n",
      "layer   3  Sparsity: 63.1752%\n",
      "total_backward_count 1419550 real_backward_count 91622   6.454%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.805069/  1.118816, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0462%\n",
      "layer   2  Sparsity: 64.9824%\n",
      "layer   3  Sparsity: 62.5670%\n",
      "total_backward_count 1429340 real_backward_count 92007   6.437%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.812729/  1.194256, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0412%\n",
      "layer   2  Sparsity: 64.8214%\n",
      "layer   3  Sparsity: 62.6077%\n",
      "total_backward_count 1439130 real_backward_count 92397   6.420%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.798781/  1.104246, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0222%\n",
      "layer   2  Sparsity: 64.8832%\n",
      "layer   3  Sparsity: 62.8615%\n",
      "total_backward_count 1448920 real_backward_count 92774   6.403%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.819106/  1.134077, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0359%\n",
      "layer   2  Sparsity: 64.9837%\n",
      "layer   3  Sparsity: 63.0146%\n",
      "total_backward_count 1458710 real_backward_count 93151   6.386%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.795838/  1.150255, val:  80.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0245%\n",
      "layer   2  Sparsity: 64.8336%\n",
      "layer   3  Sparsity: 62.7288%\n",
      "total_backward_count 1468500 real_backward_count 93525   6.369%\n",
      "fc layer 1 self.abs_max_out: 9775.0\n",
      "lif layer 1 self.abs_max_v: 17735.0\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.783924/  1.151499, val:  80.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0677%\n",
      "layer   2  Sparsity: 65.1764%\n",
      "layer   3  Sparsity: 62.8411%\n",
      "total_backward_count 1478290 real_backward_count 93890   6.351%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.791276/  1.136162, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0343%\n",
      "layer   2  Sparsity: 65.2320%\n",
      "layer   3  Sparsity: 63.1853%\n",
      "total_backward_count 1488080 real_backward_count 94270   6.335%\n",
      "fc layer 3 self.abs_max_out: 1927.0\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.788489/  1.162084, val:  75.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0360%\n",
      "layer   2  Sparsity: 64.6581%\n",
      "layer   3  Sparsity: 64.0833%\n",
      "total_backward_count 1497870 real_backward_count 94619   6.317%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.773799/  1.047408, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0173%\n",
      "layer   2  Sparsity: 64.8512%\n",
      "layer   3  Sparsity: 63.2939%\n",
      "total_backward_count 1507660 real_backward_count 95021   6.303%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.779142/  1.118748, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0150%\n",
      "layer   2  Sparsity: 65.0780%\n",
      "layer   3  Sparsity: 63.4295%\n",
      "total_backward_count 1517450 real_backward_count 95384   6.286%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.773723/  1.125983, val:  81.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0423%\n",
      "layer   2  Sparsity: 65.0278%\n",
      "layer   3  Sparsity: 63.4201%\n",
      "total_backward_count 1527240 real_backward_count 95694   6.266%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.765906/  1.129126, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0275%\n",
      "layer   2  Sparsity: 64.8197%\n",
      "layer   3  Sparsity: 62.7756%\n",
      "total_backward_count 1537030 real_backward_count 96054   6.249%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.743384/  1.124292, val:  82.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0156%\n",
      "layer   2  Sparsity: 64.8906%\n",
      "layer   3  Sparsity: 62.2815%\n",
      "total_backward_count 1546820 real_backward_count 96441   6.235%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.751186/  1.116876, val:  81.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0485%\n",
      "layer   2  Sparsity: 65.2178%\n",
      "layer   3  Sparsity: 61.7035%\n",
      "total_backward_count 1556610 real_backward_count 96774   6.217%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.756740/  1.118789, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0413%\n",
      "layer   2  Sparsity: 65.1518%\n",
      "layer   3  Sparsity: 63.2528%\n",
      "total_backward_count 1566400 real_backward_count 97105   6.199%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.757014/  1.147243, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0495%\n",
      "layer   2  Sparsity: 65.0704%\n",
      "layer   3  Sparsity: 63.4172%\n",
      "total_backward_count 1576190 real_backward_count 97467   6.184%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.755975/  1.116229, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 92.0286%\n",
      "layer   2  Sparsity: 65.1064%\n",
      "layer   3  Sparsity: 63.4954%\n",
      "total_backward_count 1585980 real_backward_count 97825   6.168%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.753290/  1.116265, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0211%\n",
      "layer   2  Sparsity: 65.1104%\n",
      "layer   3  Sparsity: 63.6761%\n",
      "total_backward_count 1595770 real_backward_count 98158   6.151%\n",
      "fc layer 3 self.abs_max_out: 1928.0\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.750982/  1.104226, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0255%\n",
      "layer   2  Sparsity: 64.7177%\n",
      "layer   3  Sparsity: 63.8393%\n",
      "total_backward_count 1605560 real_backward_count 98500   6.135%\n",
      "fc layer 3 self.abs_max_out: 1975.0\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.768763/  1.097868, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0160%\n",
      "layer   2  Sparsity: 64.7294%\n",
      "layer   3  Sparsity: 63.5140%\n",
      "total_backward_count 1615350 real_backward_count 98891   6.122%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.773128/  1.100998, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0322%\n",
      "layer   2  Sparsity: 64.9237%\n",
      "layer   3  Sparsity: 64.0419%\n",
      "total_backward_count 1625140 real_backward_count 99230   6.106%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.769793/  1.097462, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0400%\n",
      "layer   2  Sparsity: 65.0078%\n",
      "layer   3  Sparsity: 63.5835%\n",
      "total_backward_count 1634930 real_backward_count 99536   6.088%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.772716/  1.126606, val:  80.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0132%\n",
      "layer   2  Sparsity: 64.7952%\n",
      "layer   3  Sparsity: 62.9678%\n",
      "total_backward_count 1644720 real_backward_count 99897   6.074%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.765388/  1.129957, val:  77.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0573%\n",
      "layer   2  Sparsity: 64.8187%\n",
      "layer   3  Sparsity: 62.8909%\n",
      "total_backward_count 1654510 real_backward_count 100243   6.059%\n",
      "fc layer 3 self.abs_max_out: 1993.0\n",
      "fc layer 3 self.abs_max_out: 2034.0\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.757616/  1.083837, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0309%\n",
      "layer   2  Sparsity: 64.8270%\n",
      "layer   3  Sparsity: 63.5373%\n",
      "total_backward_count 1664300 real_backward_count 100547   6.041%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.748741/  1.050748, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.0336%\n",
      "layer   2  Sparsity: 64.6888%\n",
      "layer   3  Sparsity: 63.5644%\n",
      "total_backward_count 1674090 real_backward_count 100855   6.024%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.740946/  1.115317, val:  82.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0031%\n",
      "layer   2  Sparsity: 65.1199%\n",
      "layer   3  Sparsity: 62.8381%\n",
      "total_backward_count 1683880 real_backward_count 101199   6.010%\n",
      "fc layer 3 self.abs_max_out: 2050.0\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.747463/  1.114739, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0183%\n",
      "layer   2  Sparsity: 65.2213%\n",
      "layer   3  Sparsity: 63.6365%\n",
      "total_backward_count 1693670 real_backward_count 101555   5.996%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.739992/  1.097876, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0111%\n",
      "layer   2  Sparsity: 65.2022%\n",
      "layer   3  Sparsity: 63.9424%\n",
      "total_backward_count 1703460 real_backward_count 101894   5.982%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.754292/  1.087665, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0132%\n",
      "layer   2  Sparsity: 65.0801%\n",
      "layer   3  Sparsity: 63.3249%\n",
      "total_backward_count 1713250 real_backward_count 102268   5.969%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.753496/  1.073718, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.9939%\n",
      "layer   2  Sparsity: 65.1037%\n",
      "layer   3  Sparsity: 63.5206%\n",
      "total_backward_count 1723040 real_backward_count 102615   5.955%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.740120/  1.080222, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0272%\n",
      "layer   2  Sparsity: 64.8925%\n",
      "layer   3  Sparsity: 63.4778%\n",
      "total_backward_count 1732830 real_backward_count 102972   5.942%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.737574/  1.105516, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0250%\n",
      "layer   2  Sparsity: 64.7782%\n",
      "layer   3  Sparsity: 65.1107%\n",
      "total_backward_count 1742620 real_backward_count 103300   5.928%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.756449/  1.094207, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0198%\n",
      "layer   2  Sparsity: 64.6970%\n",
      "layer   3  Sparsity: 64.5149%\n",
      "total_backward_count 1752410 real_backward_count 103640   5.914%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.748291/  1.071243, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0354%\n",
      "layer   2  Sparsity: 64.9221%\n",
      "layer   3  Sparsity: 64.0441%\n",
      "total_backward_count 1762200 real_backward_count 103982   5.901%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.751254/  1.117007, val:  77.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0071%\n",
      "layer   2  Sparsity: 64.9678%\n",
      "layer   3  Sparsity: 63.8491%\n",
      "total_backward_count 1771990 real_backward_count 104323   5.887%\n",
      "fc layer 3 self.abs_max_out: 2055.0\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.736387/  1.061193, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0344%\n",
      "layer   2  Sparsity: 65.0938%\n",
      "layer   3  Sparsity: 64.3593%\n",
      "total_backward_count 1781780 real_backward_count 104640   5.873%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.748131/  1.119965, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.9972%\n",
      "layer   2  Sparsity: 65.1466%\n",
      "layer   3  Sparsity: 64.3043%\n",
      "total_backward_count 1791570 real_backward_count 104977   5.859%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.723699/  1.137808, val:  74.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0207%\n",
      "layer   2  Sparsity: 64.9116%\n",
      "layer   3  Sparsity: 64.7494%\n",
      "total_backward_count 1801360 real_backward_count 105256   5.843%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.733616/  1.101887, val:  79.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0294%\n",
      "layer   2  Sparsity: 64.8948%\n",
      "layer   3  Sparsity: 64.8963%\n",
      "total_backward_count 1811150 real_backward_count 105581   5.830%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.730625/  1.087466, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0018%\n",
      "layer   2  Sparsity: 64.6623%\n",
      "layer   3  Sparsity: 64.4014%\n",
      "total_backward_count 1820940 real_backward_count 105897   5.816%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.734131/  1.060704, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0118%\n",
      "layer   2  Sparsity: 64.7134%\n",
      "layer   3  Sparsity: 64.7413%\n",
      "total_backward_count 1830730 real_backward_count 106204   5.801%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.721716/  1.087278, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0261%\n",
      "layer   2  Sparsity: 64.5395%\n",
      "layer   3  Sparsity: 65.0237%\n",
      "total_backward_count 1840520 real_backward_count 106508   5.787%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.727114/  1.111599, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0204%\n",
      "layer   2  Sparsity: 64.6435%\n",
      "layer   3  Sparsity: 64.5034%\n",
      "total_backward_count 1850310 real_backward_count 106813   5.773%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.714834/  1.075342, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.9993%\n",
      "layer   2  Sparsity: 64.7262%\n",
      "layer   3  Sparsity: 64.6703%\n",
      "total_backward_count 1860100 real_backward_count 107126   5.759%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.727783/  1.052559, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0091%\n",
      "layer   2  Sparsity: 64.6333%\n",
      "layer   3  Sparsity: 65.0592%\n",
      "total_backward_count 1869890 real_backward_count 107426   5.745%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.741413/  1.101811, val:  80.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0194%\n",
      "layer   2  Sparsity: 64.9913%\n",
      "layer   3  Sparsity: 64.9995%\n",
      "total_backward_count 1879680 real_backward_count 107714   5.730%\n",
      "fc layer 1 self.abs_max_out: 9796.0\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.751218/  1.062528, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0128%\n",
      "layer   2  Sparsity: 64.9771%\n",
      "layer   3  Sparsity: 64.8865%\n",
      "total_backward_count 1889470 real_backward_count 108057   5.719%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.724362/  1.081122, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0402%\n",
      "layer   2  Sparsity: 65.0538%\n",
      "layer   3  Sparsity: 65.1108%\n",
      "total_backward_count 1899260 real_backward_count 108361   5.705%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.703895/  1.058750, val:  84.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0292%\n",
      "layer   2  Sparsity: 65.0725%\n",
      "layer   3  Sparsity: 64.7755%\n",
      "total_backward_count 1909050 real_backward_count 108638   5.691%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.719396/  1.073355, val:  80.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.9908%\n",
      "layer   2  Sparsity: 64.8894%\n",
      "layer   3  Sparsity: 64.7469%\n",
      "total_backward_count 1918840 real_backward_count 108942   5.677%\n",
      "fc layer 1 self.abs_max_out: 9813.0\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.716136/  1.066322, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0382%\n",
      "layer   2  Sparsity: 64.6980%\n",
      "layer   3  Sparsity: 65.4023%\n",
      "total_backward_count 1928630 real_backward_count 109224   5.663%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.712289/  1.059133, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0784%\n",
      "layer   2  Sparsity: 64.9278%\n",
      "layer   3  Sparsity: 65.1733%\n",
      "total_backward_count 1938420 real_backward_count 109547   5.651%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.708795/  1.056959, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0168%\n",
      "layer   2  Sparsity: 64.9084%\n",
      "layer   3  Sparsity: 64.5534%\n",
      "total_backward_count 1948210 real_backward_count 109826   5.637%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.707955/  1.086475, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.0336%\n",
      "layer   2  Sparsity: 65.0207%\n",
      "layer   3  Sparsity: 64.0334%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75b83e9419f4c24a78a78cffecad44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.70795</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.84167</td></tr><tr><td>val_loss</td><td>1.08648</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">faithful-sweep-153</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nvwg95vb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nvwg95vb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_013233-nvwg95vb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: scgrb7pa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_054902-scgrb7pa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/scgrb7pa' target=\"_blank\">winter-sweep-158</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/scgrb7pa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/scgrb7pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251118_054911_745', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 111.0\n",
      "lif layer 1 self.abs_max_v: 111.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 148.0\n",
      "lif layer 1 self.abs_max_v: 184.0\n",
      "fc layer 1 self.abs_max_out: 162.0\n",
      "lif layer 1 self.abs_max_v: 252.0\n",
      "fc layer 1 self.abs_max_out: 174.0\n",
      "fc layer 1 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 316.5\n",
      "fc layer 2 self.abs_max_out: 110.0\n",
      "lif layer 2 self.abs_max_v: 110.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 328.5\n",
      "fc layer 1 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "lif layer 1 self.abs_max_v: 367.0\n",
      "fc layer 2 self.abs_max_out: 116.0\n",
      "lif layer 2 self.abs_max_v: 119.0\n",
      "fc layer 1 self.abs_max_out: 433.0\n",
      "lif layer 1 self.abs_max_v: 553.0\n",
      "fc layer 2 self.abs_max_out: 175.0\n",
      "lif layer 2 self.abs_max_v: 186.0\n",
      "fc layer 1 self.abs_max_out: 798.0\n",
      "lif layer 1 self.abs_max_v: 798.0\n",
      "fc layer 2 self.abs_max_out: 213.0\n",
      "lif layer 2 self.abs_max_v: 279.5\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "lif layer 2 self.abs_max_v: 326.5\n",
      "lif layer 2 self.abs_max_v: 335.0\n",
      "fc layer 1 self.abs_max_out: 822.0\n",
      "lif layer 1 self.abs_max_v: 822.0\n",
      "fc layer 2 self.abs_max_out: 342.0\n",
      "lif layer 2 self.abs_max_v: 342.0\n",
      "fc layer 3 self.abs_max_out: 23.0\n",
      "fc layer 1 self.abs_max_out: 892.0\n",
      "lif layer 1 self.abs_max_v: 892.0\n",
      "lif layer 2 self.abs_max_v: 356.0\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 360.5\n",
      "fc layer 3 self.abs_max_out: 43.0\n",
      "lif layer 2 self.abs_max_v: 370.5\n",
      "lif layer 2 self.abs_max_v: 374.0\n",
      "lif layer 2 self.abs_max_v: 408.5\n",
      "lif layer 2 self.abs_max_v: 434.5\n",
      "fc layer 1 self.abs_max_out: 908.0\n",
      "lif layer 1 self.abs_max_v: 908.0\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "fc layer 2 self.abs_max_out: 385.0\n",
      "lif layer 2 self.abs_max_v: 448.0\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 538.5\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 2 self.abs_max_out: 511.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 941.0\n",
      "fc layer 2 self.abs_max_out: 517.0\n",
      "fc layer 2 self.abs_max_out: 526.0\n",
      "lif layer 2 self.abs_max_v: 551.0\n",
      "lif layer 2 self.abs_max_v: 607.0\n",
      "fc layer 1 self.abs_max_out: 949.0\n",
      "lif layer 1 self.abs_max_v: 949.0\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "fc layer 2 self.abs_max_out: 561.0\n",
      "fc layer 2 self.abs_max_out: 597.0\n",
      "lif layer 2 self.abs_max_v: 635.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 1 self.abs_max_out: 1309.0\n",
      "lif layer 1 self.abs_max_v: 1309.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "lif layer 2 self.abs_max_v: 679.0\n",
      "fc layer 1 self.abs_max_out: 1501.0\n",
      "lif layer 1 self.abs_max_v: 1501.0\n",
      "fc layer 2 self.abs_max_out: 633.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 2 self.abs_max_out: 636.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "lif layer 2 self.abs_max_v: 696.0\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "lif layer 2 self.abs_max_v: 697.5\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "fc layer 1 self.abs_max_out: 1513.0\n",
      "lif layer 1 self.abs_max_v: 1513.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 719.0\n",
      "fc layer 2 self.abs_max_out: 755.0\n",
      "lif layer 2 self.abs_max_v: 755.0\n",
      "lif layer 2 self.abs_max_v: 768.5\n",
      "lif layer 2 self.abs_max_v: 804.0\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "lif layer 2 self.abs_max_v: 830.5\n",
      "fc layer 2 self.abs_max_out: 770.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 2 self.abs_max_out: 775.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "lif layer 2 self.abs_max_v: 840.0\n",
      "fc layer 3 self.abs_max_out: 197.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "lif layer 2 self.abs_max_v: 846.0\n",
      "lif layer 2 self.abs_max_v: 853.5\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "lif layer 2 self.abs_max_v: 860.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "lif layer 2 self.abs_max_v: 922.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "lif layer 2 self.abs_max_v: 1010.0\n",
      "fc layer 1 self.abs_max_out: 1672.0\n",
      "lif layer 1 self.abs_max_v: 1672.0\n",
      "fc layer 1 self.abs_max_out: 1753.0\n",
      "lif layer 1 self.abs_max_v: 1753.0\n",
      "fc layer 1 self.abs_max_out: 1777.0\n",
      "lif layer 1 self.abs_max_v: 1777.0\n",
      "fc layer 1 self.abs_max_out: 1855.0\n",
      "lif layer 1 self.abs_max_v: 1855.0\n",
      "lif layer 2 self.abs_max_v: 1035.0\n",
      "fc layer 1 self.abs_max_out: 2083.0\n",
      "lif layer 1 self.abs_max_v: 2083.0\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "lif layer 2 self.abs_max_v: 1063.5\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "fc layer 2 self.abs_max_out: 1044.0\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "lif layer 2 self.abs_max_v: 1071.0\n",
      "fc layer 2 self.abs_max_out: 1103.0\n",
      "lif layer 2 self.abs_max_v: 1173.5\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "fc layer 1 self.abs_max_out: 2179.0\n",
      "lif layer 1 self.abs_max_v: 2179.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "lif layer 2 self.abs_max_v: 1296.0\n",
      "fc layer 2 self.abs_max_out: 1407.0\n",
      "lif layer 2 self.abs_max_v: 1407.0\n",
      "fc layer 2 self.abs_max_out: 1432.0\n",
      "lif layer 2 self.abs_max_v: 1432.0\n",
      "fc layer 1 self.abs_max_out: 2197.0\n",
      "lif layer 1 self.abs_max_v: 2197.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 1 self.abs_max_out: 2337.0\n",
      "lif layer 1 self.abs_max_v: 2337.0\n",
      "fc layer 1 self.abs_max_out: 2338.0\n",
      "lif layer 1 self.abs_max_v: 2338.0\n",
      "fc layer 1 self.abs_max_out: 2544.0\n",
      "lif layer 1 self.abs_max_v: 2544.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 1 self.abs_max_out: 2691.0\n",
      "lif layer 1 self.abs_max_v: 2691.0\n",
      "fc layer 1 self.abs_max_out: 2705.0\n",
      "lif layer 1 self.abs_max_v: 2705.0\n",
      "fc layer 1 self.abs_max_out: 2842.0\n",
      "lif layer 1 self.abs_max_v: 2842.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 3 self.abs_max_out: 265.0\n",
      "fc layer 1 self.abs_max_out: 3025.0\n",
      "lif layer 1 self.abs_max_v: 3025.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  2.025946/  2.113996, val:  33.33%, val_best:  33.33%, tr:  94.08%, tr_best:  94.08%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 87.0691%\n",
      "layer   3  Sparsity: 91.3369%\n",
      "total_backward_count 9790 real_backward_count 2734  27.926%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "lif layer 2 self.abs_max_v: 1567.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 1 self.abs_max_out: 3045.0\n",
      "lif layer 1 self.abs_max_v: 3045.0\n",
      "fc layer 1 self.abs_max_out: 3380.0\n",
      "lif layer 1 self.abs_max_v: 3380.0\n",
      "fc layer 1 self.abs_max_out: 3546.0\n",
      "lif layer 1 self.abs_max_v: 3546.0\n",
      "fc layer 2 self.abs_max_out: 1684.0\n",
      "lif layer 2 self.abs_max_v: 1684.0\n",
      "fc layer 2 self.abs_max_out: 1694.0\n",
      "lif layer 2 self.abs_max_v: 1694.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.917357/  2.089613, val:  37.08%, val_best:  37.08%, tr:  98.37%, tr_best:  98.37%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0042%\n",
      "layer   2  Sparsity: 85.3943%\n",
      "layer   3  Sparsity: 88.6068%\n",
      "total_backward_count 19580 real_backward_count 4624  23.616%\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "lif layer 2 self.abs_max_v: 1707.0\n",
      "fc layer 1 self.abs_max_out: 3568.0\n",
      "lif layer 1 self.abs_max_v: 3568.0\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "lif layer 2 self.abs_max_v: 1714.0\n",
      "lif layer 2 self.abs_max_v: 1849.5\n",
      "lif layer 2 self.abs_max_v: 1933.0\n",
      "lif layer 2 self.abs_max_v: 1947.5\n",
      "lif layer 2 self.abs_max_v: 2081.0\n",
      "lif layer 2 self.abs_max_v: 2126.5\n",
      "lif layer 2 self.abs_max_v: 2155.0\n",
      "fc layer 1 self.abs_max_out: 3671.0\n",
      "lif layer 1 self.abs_max_v: 3671.0\n",
      "fc layer 1 self.abs_max_out: 3679.0\n",
      "lif layer 1 self.abs_max_v: 3679.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.876117/  2.031363, val:  40.83%, val_best:  40.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0101%\n",
      "layer   2  Sparsity: 85.1388%\n",
      "layer   3  Sparsity: 88.0346%\n",
      "total_backward_count 29370 real_backward_count 6521  22.203%\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 1 self.abs_max_out: 3707.0\n",
      "lif layer 1 self.abs_max_v: 3707.0\n",
      "fc layer 1 self.abs_max_out: 3750.0\n",
      "lif layer 1 self.abs_max_v: 3750.0\n",
      "fc layer 1 self.abs_max_out: 3753.0\n",
      "lif layer 1 self.abs_max_v: 3753.0\n",
      "fc layer 1 self.abs_max_out: 3813.0\n",
      "lif layer 1 self.abs_max_v: 3813.0\n",
      "fc layer 1 self.abs_max_out: 4745.0\n",
      "lif layer 1 self.abs_max_v: 4745.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 2 self.abs_max_out: 1738.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.820954/  2.007430, val:  43.75%, val_best:  43.75%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0035%\n",
      "layer   2  Sparsity: 84.6169%\n",
      "layer   3  Sparsity: 87.2350%\n",
      "total_backward_count 39160 real_backward_count 8310  21.221%\n",
      "fc layer 1 self.abs_max_out: 4774.0\n",
      "lif layer 1 self.abs_max_v: 4774.0\n",
      "fc layer 2 self.abs_max_out: 1776.0\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "lif layer 2 self.abs_max_v: 2298.0\n",
      "fc layer 2 self.abs_max_out: 1845.0\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.799477/  1.988922, val:  32.92%, val_best:  43.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0066%\n",
      "layer   2  Sparsity: 84.2861%\n",
      "layer   3  Sparsity: 87.1555%\n",
      "total_backward_count 48950 real_backward_count 9943  20.313%\n",
      "fc layer 1 self.abs_max_out: 5190.0\n",
      "lif layer 1 self.abs_max_v: 5190.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.764290/  1.993329, val:  47.92%, val_best:  47.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0000%\n",
      "layer   2  Sparsity: 84.1284%\n",
      "layer   3  Sparsity: 86.8290%\n",
      "total_backward_count 58740 real_backward_count 11582  19.717%\n",
      "lif layer 2 self.abs_max_v: 2502.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.753556/  2.002424, val:  47.50%, val_best:  47.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0203%\n",
      "layer   2  Sparsity: 83.5309%\n",
      "layer   3  Sparsity: 86.1087%\n",
      "total_backward_count 68530 real_backward_count 13228  19.302%\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 2 self.abs_max_out: 1850.0\n",
      "lif layer 1 self.abs_max_v: 5351.0\n",
      "lif layer 1 self.abs_max_v: 5735.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.704193/  1.934288, val:  46.25%, val_best:  47.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0016%\n",
      "layer   2  Sparsity: 83.3944%\n",
      "layer   3  Sparsity: 85.5084%\n",
      "total_backward_count 78320 real_backward_count 14753  18.837%\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 2 self.abs_max_out: 1894.0\n",
      "lif layer 2 self.abs_max_v: 2661.5\n",
      "fc layer 2 self.abs_max_out: 1907.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.680301/  1.912161, val:  54.58%, val_best:  54.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0119%\n",
      "layer   2  Sparsity: 83.2387%\n",
      "layer   3  Sparsity: 85.3133%\n",
      "total_backward_count 88110 real_backward_count 16333  18.537%\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 2 self.abs_max_out: 1936.0\n",
      "fc layer 2 self.abs_max_out: 1992.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.639832/  1.900137, val:  43.33%, val_best:  54.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9921%\n",
      "layer   2  Sparsity: 83.0775%\n",
      "layer   3  Sparsity: 84.7452%\n",
      "total_backward_count 97900 real_backward_count 17832  18.215%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.618557/  1.920005, val:  46.67%, val_best:  54.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9956%\n",
      "layer   2  Sparsity: 83.2753%\n",
      "layer   3  Sparsity: 85.0445%\n",
      "total_backward_count 107690 real_backward_count 19390  18.005%\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.622252/  1.857230, val:  54.58%, val_best:  54.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 82.8854%\n",
      "layer   3  Sparsity: 84.7353%\n",
      "total_backward_count 117480 real_backward_count 20922  17.809%\n",
      "fc layer 2 self.abs_max_out: 2019.0\n",
      "fc layer 2 self.abs_max_out: 2042.0\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.576174/  1.836609, val:  39.17%, val_best:  54.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0169%\n",
      "layer   2  Sparsity: 83.1807%\n",
      "layer   3  Sparsity: 84.6419%\n",
      "total_backward_count 127270 real_backward_count 22351  17.562%\n",
      "fc layer 2 self.abs_max_out: 2169.0\n",
      "fc layer 1 self.abs_max_out: 5809.0\n",
      "lif layer 1 self.abs_max_v: 5809.0\n",
      "lif layer 1 self.abs_max_v: 5853.5\n",
      "lif layer 2 self.abs_max_v: 2720.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.540228/  1.810256, val:  45.00%, val_best:  54.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0152%\n",
      "layer   2  Sparsity: 83.0262%\n",
      "layer   3  Sparsity: 83.9489%\n",
      "total_backward_count 137060 real_backward_count 23753  17.330%\n",
      "fc layer 1 self.abs_max_out: 5899.0\n",
      "lif layer 1 self.abs_max_v: 5899.0\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 5902.0\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.507182/  1.746638, val:  52.92%, val_best:  54.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0167%\n",
      "layer   2  Sparsity: 82.8011%\n",
      "layer   3  Sparsity: 83.5352%\n",
      "total_backward_count 146850 real_backward_count 25153  17.128%\n",
      "lif layer 1 self.abs_max_v: 5957.5\n",
      "fc layer 2 self.abs_max_out: 2214.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.491167/  1.766774, val:  47.50%, val_best:  54.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9920%\n",
      "layer   2  Sparsity: 82.6262%\n",
      "layer   3  Sparsity: 83.5007%\n",
      "total_backward_count 156640 real_backward_count 26556  16.954%\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 2 self.abs_max_out: 2219.0\n",
      "fc layer 1 self.abs_max_out: 5963.0\n",
      "lif layer 1 self.abs_max_v: 5963.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.468174/  1.753461, val:  60.83%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0048%\n",
      "layer   2  Sparsity: 82.4577%\n",
      "layer   3  Sparsity: 83.5663%\n",
      "total_backward_count 166430 real_backward_count 27916  16.773%\n",
      "lif layer 2 self.abs_max_v: 2772.0\n",
      "lif layer 2 self.abs_max_v: 2826.5\n",
      "fc layer 2 self.abs_max_out: 2257.0\n",
      "lif layer 1 self.abs_max_v: 6610.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.496371/  1.708429, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0057%\n",
      "layer   2  Sparsity: 82.2404%\n",
      "layer   3  Sparsity: 83.7869%\n",
      "total_backward_count 176220 real_backward_count 29302  16.628%\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 2302.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.470847/  1.727960, val:  55.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9975%\n",
      "layer   2  Sparsity: 82.0143%\n",
      "layer   3  Sparsity: 83.8610%\n",
      "total_backward_count 186010 real_backward_count 30644  16.474%\n",
      "fc layer 1 self.abs_max_out: 6116.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.450351/  1.728500, val:  50.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9997%\n",
      "layer   2  Sparsity: 81.8457%\n",
      "layer   3  Sparsity: 83.7760%\n",
      "total_backward_count 195800 real_backward_count 31920  16.302%\n",
      "lif layer 2 self.abs_max_v: 2900.0\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "lif layer 2 self.abs_max_v: 2923.5\n",
      "fc layer 2 self.abs_max_out: 2323.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.455120/  1.738528, val:  56.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9951%\n",
      "layer   2  Sparsity: 82.0927%\n",
      "layer   3  Sparsity: 83.2978%\n",
      "total_backward_count 205590 real_backward_count 33196  16.147%\n",
      "fc layer 2 self.abs_max_out: 2329.0\n",
      "lif layer 2 self.abs_max_v: 3054.5\n",
      "fc layer 2 self.abs_max_out: 2399.0\n",
      "lif layer 1 self.abs_max_v: 6778.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.431816/  1.740096, val:  57.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9890%\n",
      "layer   2  Sparsity: 82.0800%\n",
      "layer   3  Sparsity: 82.6735%\n",
      "total_backward_count 215380 real_backward_count 34502  16.019%\n",
      "lif layer 2 self.abs_max_v: 3259.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.435113/  1.687079, val:  64.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9960%\n",
      "layer   2  Sparsity: 82.0861%\n",
      "layer   3  Sparsity: 82.8145%\n",
      "total_backward_count 225170 real_backward_count 35766  15.884%\n",
      "fc layer 1 self.abs_max_out: 6201.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.423646/  1.680194, val:  75.42%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0115%\n",
      "layer   2  Sparsity: 81.9903%\n",
      "layer   3  Sparsity: 83.5264%\n",
      "total_backward_count 234960 real_backward_count 37011  15.752%\n",
      "fc layer 2 self.abs_max_out: 2433.0\n",
      "fc layer 2 self.abs_max_out: 2493.0\n",
      "fc layer 2 self.abs_max_out: 2502.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.425951/  1.692354, val:  70.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9852%\n",
      "layer   2  Sparsity: 81.7434%\n",
      "layer   3  Sparsity: 83.6088%\n",
      "total_backward_count 244750 real_backward_count 38187  15.602%\n",
      "fc layer 1 self.abs_max_out: 6286.0\n",
      "lif layer 2 self.abs_max_v: 3306.5\n",
      "lif layer 2 self.abs_max_v: 3416.5\n",
      "fc layer 2 self.abs_max_out: 2524.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.438235/  1.638768, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9926%\n",
      "layer   2  Sparsity: 81.5951%\n",
      "layer   3  Sparsity: 83.7466%\n",
      "total_backward_count 254540 real_backward_count 39418  15.486%\n",
      "fc layer 1 self.abs_max_out: 6357.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.416596/  1.657027, val:  68.33%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9977%\n",
      "layer   2  Sparsity: 81.5245%\n",
      "layer   3  Sparsity: 84.0020%\n",
      "total_backward_count 264330 real_backward_count 40584  15.354%\n",
      "lif layer 2 self.abs_max_v: 3557.0\n",
      "lif layer 2 self.abs_max_v: 3748.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.400039/  1.625571, val:  78.33%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 81.5882%\n",
      "layer   3  Sparsity: 83.6009%\n",
      "total_backward_count 274120 real_backward_count 41777  15.240%\n",
      "fc layer 2 self.abs_max_out: 2562.0\n",
      "fc layer 2 self.abs_max_out: 2635.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.378193/  1.656748, val:  67.50%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0136%\n",
      "layer   2  Sparsity: 81.4904%\n",
      "layer   3  Sparsity: 83.2410%\n",
      "total_backward_count 283910 real_backward_count 42922  15.118%\n",
      "lif layer 2 self.abs_max_v: 3757.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.375924/  1.625639, val:  66.25%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0134%\n",
      "layer   2  Sparsity: 81.5613%\n",
      "layer   3  Sparsity: 82.6517%\n",
      "total_backward_count 293700 real_backward_count 44058  15.001%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.371618/  1.584173, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 81.3109%\n",
      "layer   3  Sparsity: 82.4702%\n",
      "total_backward_count 303490 real_backward_count 45231  14.904%\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.331641/  1.591310, val:  71.67%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0110%\n",
      "layer   2  Sparsity: 81.0721%\n",
      "layer   3  Sparsity: 82.3252%\n",
      "total_backward_count 313280 real_backward_count 46380  14.805%\n",
      "fc layer 2 self.abs_max_out: 2683.0\n",
      "fc layer 1 self.abs_max_out: 6560.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.339544/  1.609288, val:  67.50%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9918%\n",
      "layer   2  Sparsity: 81.0395%\n",
      "layer   3  Sparsity: 82.3787%\n",
      "total_backward_count 323070 real_backward_count 47478  14.696%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.301373/  1.579295, val:  73.33%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0060%\n",
      "layer   2  Sparsity: 81.2176%\n",
      "layer   3  Sparsity: 81.9655%\n",
      "total_backward_count 332860 real_backward_count 48551  14.586%\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "lif layer 2 self.abs_max_v: 3837.0\n",
      "lif layer 1 self.abs_max_v: 7012.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.295327/  1.596199, val:  63.75%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0124%\n",
      "layer   2  Sparsity: 81.2064%\n",
      "layer   3  Sparsity: 81.9197%\n",
      "total_backward_count 342650 real_backward_count 49604  14.477%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.296777/  1.533828, val:  75.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0161%\n",
      "layer   2  Sparsity: 81.1468%\n",
      "layer   3  Sparsity: 81.9647%\n",
      "total_backward_count 352440 real_backward_count 50707  14.387%\n",
      "fc layer 2 self.abs_max_out: 2878.0\n",
      "lif layer 1 self.abs_max_v: 7235.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.297081/  1.534898, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0046%\n",
      "layer   2  Sparsity: 81.1956%\n",
      "layer   3  Sparsity: 82.3991%\n",
      "total_backward_count 362230 real_backward_count 51777  14.294%\n",
      "lif layer 2 self.abs_max_v: 3839.5\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 3 self.abs_max_out: 706.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.270433/  1.633951, val:  52.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0011%\n",
      "layer   2  Sparsity: 81.2647%\n",
      "layer   3  Sparsity: 82.5130%\n",
      "total_backward_count 372020 real_backward_count 52752  14.180%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.293070/  1.584751, val:  71.25%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 81.1672%\n",
      "layer   3  Sparsity: 82.4975%\n",
      "total_backward_count 381810 real_backward_count 53842  14.102%\n",
      "lif layer 1 self.abs_max_v: 7347.0\n",
      "lif layer 1 self.abs_max_v: 7378.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.284336/  1.558584, val:  72.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0199%\n",
      "layer   2  Sparsity: 81.0766%\n",
      "layer   3  Sparsity: 82.5657%\n",
      "total_backward_count 391600 real_backward_count 54827  14.001%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.287096/  1.561151, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9917%\n",
      "layer   2  Sparsity: 80.9543%\n",
      "layer   3  Sparsity: 82.3022%\n",
      "total_backward_count 401390 real_backward_count 55835  13.910%\n",
      "lif layer 2 self.abs_max_v: 3913.0\n",
      "lif layer 1 self.abs_max_v: 7479.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.287353/  1.573981, val:  64.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 80.8703%\n",
      "layer   3  Sparsity: 82.3641%\n",
      "total_backward_count 411180 real_backward_count 56833  13.822%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.286078/  1.572973, val:  67.50%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0092%\n",
      "layer   2  Sparsity: 80.7691%\n",
      "layer   3  Sparsity: 82.4242%\n",
      "total_backward_count 420970 real_backward_count 57841  13.740%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.283356/  1.546726, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0120%\n",
      "layer   2  Sparsity: 81.0395%\n",
      "layer   3  Sparsity: 82.6058%\n",
      "total_backward_count 430760 real_backward_count 58900  13.674%\n",
      "lif layer 2 self.abs_max_v: 3944.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.286875/  1.535468, val:  75.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0221%\n",
      "layer   2  Sparsity: 81.1414%\n",
      "layer   3  Sparsity: 82.3907%\n",
      "total_backward_count 440550 real_backward_count 59908  13.598%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.273413/  1.523721, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 81.1100%\n",
      "layer   3  Sparsity: 82.3390%\n",
      "total_backward_count 450340 real_backward_count 60866  13.516%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.285557/  1.532605, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9894%\n",
      "layer   2  Sparsity: 81.1436%\n",
      "layer   3  Sparsity: 82.5321%\n",
      "total_backward_count 460130 real_backward_count 61842  13.440%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.254657/  1.518791, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0072%\n",
      "layer   2  Sparsity: 81.0024%\n",
      "layer   3  Sparsity: 82.6838%\n",
      "total_backward_count 469920 real_backward_count 62791  13.362%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.262602/  1.506011, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9955%\n",
      "layer   2  Sparsity: 81.0490%\n",
      "layer   3  Sparsity: 82.7954%\n",
      "total_backward_count 479710 real_backward_count 63764  13.292%\n",
      "fc layer 1 self.abs_max_out: 6570.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.260228/  1.530720, val:  75.42%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0036%\n",
      "layer   2  Sparsity: 80.9909%\n",
      "layer   3  Sparsity: 82.5862%\n",
      "total_backward_count 489500 real_backward_count 64725  13.223%\n",
      "lif layer 2 self.abs_max_v: 4020.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.237254/  1.516403, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0045%\n",
      "layer   2  Sparsity: 81.1505%\n",
      "layer   3  Sparsity: 82.3943%\n",
      "total_backward_count 499290 real_backward_count 65632  13.145%\n",
      "lif layer 1 self.abs_max_v: 7482.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.222503/  1.494260, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0112%\n",
      "layer   2  Sparsity: 81.2339%\n",
      "layer   3  Sparsity: 82.3495%\n",
      "total_backward_count 509080 real_backward_count 66604  13.083%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.219085/  1.496491, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9967%\n",
      "layer   2  Sparsity: 81.2114%\n",
      "layer   3  Sparsity: 82.3968%\n",
      "total_backward_count 518870 real_backward_count 67557  13.020%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.234048/  1.512830, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 81.1981%\n",
      "layer   3  Sparsity: 82.4330%\n",
      "total_backward_count 528660 real_backward_count 68494  12.956%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.227462/  1.516132, val:  73.75%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0020%\n",
      "layer   2  Sparsity: 81.1782%\n",
      "layer   3  Sparsity: 81.9545%\n",
      "total_backward_count 538450 real_backward_count 69435  12.895%\n",
      "lif layer 1 self.abs_max_v: 7598.0\n",
      "lif layer 1 self.abs_max_v: 7948.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.253418/  1.524053, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9952%\n",
      "layer   2  Sparsity: 80.8894%\n",
      "layer   3  Sparsity: 82.0956%\n",
      "total_backward_count 548240 real_backward_count 70366  12.835%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.226073/  1.484443, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0023%\n",
      "layer   2  Sparsity: 80.6513%\n",
      "layer   3  Sparsity: 82.2854%\n",
      "total_backward_count 558030 real_backward_count 71226  12.764%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.235640/  1.492387, val:  75.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0143%\n",
      "layer   2  Sparsity: 80.8536%\n",
      "layer   3  Sparsity: 82.6347%\n",
      "total_backward_count 567820 real_backward_count 72152  12.707%\n",
      "fc layer 1 self.abs_max_out: 6627.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.214257/  1.511451, val:  75.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 80.7970%\n",
      "layer   3  Sparsity: 82.0102%\n",
      "total_backward_count 577610 real_backward_count 72997  12.638%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.250558/  1.514906, val:  60.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0128%\n",
      "layer   2  Sparsity: 80.7978%\n",
      "layer   3  Sparsity: 82.2526%\n",
      "total_backward_count 587400 real_backward_count 73894  12.580%\n",
      "lif layer 1 self.abs_max_v: 8025.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.228378/  1.488433, val:  70.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 80.9713%\n",
      "layer   3  Sparsity: 82.4020%\n",
      "total_backward_count 597190 real_backward_count 74791  12.524%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.215823/  1.515244, val:  67.92%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9995%\n",
      "layer   2  Sparsity: 81.0226%\n",
      "layer   3  Sparsity: 81.8339%\n",
      "total_backward_count 606980 real_backward_count 75691  12.470%\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.195854/  1.483377, val:  78.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9998%\n",
      "layer   2  Sparsity: 80.8344%\n",
      "layer   3  Sparsity: 81.7870%\n",
      "total_backward_count 616770 real_backward_count 76596  12.419%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.179229/  1.440415, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9953%\n",
      "layer   2  Sparsity: 80.7775%\n",
      "layer   3  Sparsity: 82.5843%\n",
      "total_backward_count 626560 real_backward_count 77452  12.361%\n",
      "fc layer 2 self.abs_max_out: 2887.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.190149/  1.492252, val:  64.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0188%\n",
      "layer   2  Sparsity: 80.8003%\n",
      "layer   3  Sparsity: 82.5710%\n",
      "total_backward_count 636350 real_backward_count 78317  12.307%\n",
      "fc layer 2 self.abs_max_out: 2924.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.184755/  1.495972, val:  75.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0034%\n",
      "layer   2  Sparsity: 80.7853%\n",
      "layer   3  Sparsity: 82.4779%\n",
      "total_backward_count 646140 real_backward_count 79129  12.246%\n",
      "lif layer 1 self.abs_max_v: 8043.0\n",
      "fc layer 2 self.abs_max_out: 2964.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.195480/  1.488681, val:  68.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 80.6775%\n",
      "layer   3  Sparsity: 82.4750%\n",
      "total_backward_count 655930 real_backward_count 79963  12.191%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.186892/  1.484066, val:  74.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 80.6049%\n",
      "layer   3  Sparsity: 82.2309%\n",
      "total_backward_count 665720 real_backward_count 80759  12.131%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.198483/  1.479739, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9912%\n",
      "layer   2  Sparsity: 80.5129%\n",
      "layer   3  Sparsity: 82.4130%\n",
      "total_backward_count 675510 real_backward_count 81646  12.087%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.184805/  1.458037, val:  82.92%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0120%\n",
      "layer   2  Sparsity: 80.6370%\n",
      "layer   3  Sparsity: 82.1763%\n",
      "total_backward_count 685300 real_backward_count 82479  12.035%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.190616/  1.486367, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0206%\n",
      "layer   2  Sparsity: 80.5991%\n",
      "layer   3  Sparsity: 82.5954%\n",
      "total_backward_count 695090 real_backward_count 83329  11.988%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.207543/  1.496642, val:  80.00%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9999%\n",
      "layer   2  Sparsity: 80.6806%\n",
      "layer   3  Sparsity: 82.9783%\n",
      "total_backward_count 704880 real_backward_count 84116  11.933%\n",
      "lif layer 1 self.abs_max_v: 8110.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.215304/  1.481980, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0090%\n",
      "layer   2  Sparsity: 80.6016%\n",
      "layer   3  Sparsity: 82.7393%\n",
      "total_backward_count 714670 real_backward_count 84913  11.881%\n",
      "lif layer 1 self.abs_max_v: 8641.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.189045/  1.466539, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 80.5695%\n",
      "layer   3  Sparsity: 82.8731%\n",
      "total_backward_count 724460 real_backward_count 85735  11.834%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.185299/  1.424389, val:  82.08%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 95.0068%\n",
      "layer   2  Sparsity: 80.5403%\n",
      "layer   3  Sparsity: 82.9109%\n",
      "total_backward_count 734250 real_backward_count 86528  11.785%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.176323/  1.437877, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 80.8578%\n",
      "layer   3  Sparsity: 82.4750%\n",
      "total_backward_count 744040 real_backward_count 87328  11.737%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.177389/  1.426029, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0127%\n",
      "layer   2  Sparsity: 80.7831%\n",
      "layer   3  Sparsity: 82.5536%\n",
      "total_backward_count 753830 real_backward_count 88092  11.686%\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "fc layer 2 self.abs_max_out: 2971.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.152812/  1.483313, val:  72.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9901%\n",
      "layer   2  Sparsity: 80.5970%\n",
      "layer   3  Sparsity: 82.3475%\n",
      "total_backward_count 763620 real_backward_count 88908  11.643%\n",
      "lif layer 2 self.abs_max_v: 4103.5\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.147995/  1.470847, val:  70.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9943%\n",
      "layer   2  Sparsity: 80.5278%\n",
      "layer   3  Sparsity: 82.1162%\n",
      "total_backward_count 773410 real_backward_count 89689  11.597%\n",
      "fc layer 3 self.abs_max_out: 760.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.150052/  1.428098, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0022%\n",
      "layer   2  Sparsity: 80.5613%\n",
      "layer   3  Sparsity: 82.2104%\n",
      "total_backward_count 783200 real_backward_count 90494  11.554%\n",
      "fc layer 1 self.abs_max_out: 6686.0\n",
      "fc layer 2 self.abs_max_out: 3072.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.147298/  1.398235, val:  81.25%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0038%\n",
      "layer   2  Sparsity: 80.3583%\n",
      "layer   3  Sparsity: 82.0909%\n",
      "total_backward_count 792990 real_backward_count 91285  11.511%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.122228/  1.445697, val:  69.58%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9940%\n",
      "layer   2  Sparsity: 80.4438%\n",
      "layer   3  Sparsity: 82.0913%\n",
      "total_backward_count 802780 real_backward_count 92027  11.464%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.139124/  1.403433, val:  82.50%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 80.8539%\n",
      "layer   3  Sparsity: 82.1795%\n",
      "total_backward_count 812570 real_backward_count 92829  11.424%\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "fc layer 3 self.abs_max_out: 778.0\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "fc layer 3 self.abs_max_out: 782.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.131599/  1.430089, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9955%\n",
      "layer   2  Sparsity: 80.5078%\n",
      "layer   3  Sparsity: 81.7765%\n",
      "total_backward_count 822360 real_backward_count 93589  11.381%\n",
      "lif layer 1 self.abs_max_v: 8924.5\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.118053/  1.382687, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0139%\n",
      "layer   2  Sparsity: 80.2710%\n",
      "layer   3  Sparsity: 81.5867%\n",
      "total_backward_count 832150 real_backward_count 94368  11.340%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.117931/  1.406849, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0189%\n",
      "layer   2  Sparsity: 80.5441%\n",
      "layer   3  Sparsity: 81.5194%\n",
      "total_backward_count 841940 real_backward_count 95131  11.299%\n",
      "fc layer 1 self.abs_max_out: 6688.0\n",
      "fc layer 2 self.abs_max_out: 3109.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.130647/  1.426074, val:  73.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0089%\n",
      "layer   2  Sparsity: 80.5934%\n",
      "layer   3  Sparsity: 82.0902%\n",
      "total_backward_count 851730 real_backward_count 95913  11.261%\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.108415/  1.368397, val:  81.25%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0047%\n",
      "layer   2  Sparsity: 80.6770%\n",
      "layer   3  Sparsity: 81.9780%\n",
      "total_backward_count 861520 real_backward_count 96677  11.222%\n",
      "fc layer 1 self.abs_max_out: 6766.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.113527/  1.404337, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0068%\n",
      "layer   2  Sparsity: 80.7008%\n",
      "layer   3  Sparsity: 82.0232%\n",
      "total_backward_count 871310 real_backward_count 97395  11.178%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.128836/  1.392585, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9883%\n",
      "layer   2  Sparsity: 80.8245%\n",
      "layer   3  Sparsity: 82.0035%\n",
      "total_backward_count 881100 real_backward_count 98162  11.141%\n",
      "fc layer 3 self.abs_max_out: 788.0\n",
      "fc layer 3 self.abs_max_out: 818.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.119969/  1.411428, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9873%\n",
      "layer   2  Sparsity: 80.7442%\n",
      "layer   3  Sparsity: 82.4860%\n",
      "total_backward_count 890890 real_backward_count 98910  11.102%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.135742/  1.405125, val:  78.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0023%\n",
      "layer   2  Sparsity: 80.7251%\n",
      "layer   3  Sparsity: 82.3022%\n",
      "total_backward_count 900680 real_backward_count 99601  11.058%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.126446/  1.362164, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9895%\n",
      "layer   2  Sparsity: 80.8077%\n",
      "layer   3  Sparsity: 82.2050%\n",
      "total_backward_count 910470 real_backward_count 100340  11.021%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.120725/  1.418820, val:  77.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0218%\n",
      "layer   2  Sparsity: 80.8831%\n",
      "layer   3  Sparsity: 81.9697%\n",
      "total_backward_count 920260 real_backward_count 101038  10.979%\n",
      "fc layer 3 self.abs_max_out: 834.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.105157/  1.385848, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0031%\n",
      "layer   2  Sparsity: 80.7088%\n",
      "layer   3  Sparsity: 81.9599%\n",
      "total_backward_count 930050 real_backward_count 101777  10.943%\n",
      "lif layer 2 self.abs_max_v: 4173.0\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.111704/  1.367905, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9968%\n",
      "layer   2  Sparsity: 80.6928%\n",
      "layer   3  Sparsity: 82.2866%\n",
      "total_backward_count 939840 real_backward_count 102452  10.901%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.079283/  1.361506, val:  77.08%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0109%\n",
      "layer   2  Sparsity: 80.5039%\n",
      "layer   3  Sparsity: 82.0087%\n",
      "total_backward_count 949630 real_backward_count 103143  10.861%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.087512/  1.355599, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0045%\n",
      "layer   2  Sparsity: 80.6655%\n",
      "layer   3  Sparsity: 82.4104%\n",
      "total_backward_count 959420 real_backward_count 103857  10.825%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.092940/  1.392437, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9992%\n",
      "layer   2  Sparsity: 80.6982%\n",
      "layer   3  Sparsity: 82.2311%\n",
      "total_backward_count 969210 real_backward_count 104528  10.785%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.103197/  1.386132, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0047%\n",
      "layer   2  Sparsity: 80.5344%\n",
      "layer   3  Sparsity: 81.9663%\n",
      "total_backward_count 979000 real_backward_count 105195  10.745%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.073603/  1.394017, val:  75.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0185%\n",
      "layer   2  Sparsity: 80.5480%\n",
      "layer   3  Sparsity: 81.6878%\n",
      "total_backward_count 988790 real_backward_count 105875  10.708%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.063242/  1.336017, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9982%\n",
      "layer   2  Sparsity: 80.6438%\n",
      "layer   3  Sparsity: 81.8864%\n",
      "total_backward_count 998580 real_backward_count 106584  10.674%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.087751/  1.372383, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0070%\n",
      "layer   2  Sparsity: 80.7368%\n",
      "layer   3  Sparsity: 82.5228%\n",
      "total_backward_count 1008370 real_backward_count 107281  10.639%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.081542/  1.356902, val:  80.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9896%\n",
      "layer   2  Sparsity: 80.5557%\n",
      "layer   3  Sparsity: 82.6275%\n",
      "total_backward_count 1018160 real_backward_count 107955  10.603%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.086478/  1.365730, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0007%\n",
      "layer   2  Sparsity: 80.6487%\n",
      "layer   3  Sparsity: 82.4425%\n",
      "total_backward_count 1027950 real_backward_count 108642  10.569%\n",
      "fc layer 1 self.abs_max_out: 6774.0\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.076760/  1.359569, val:  80.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9985%\n",
      "layer   2  Sparsity: 80.5956%\n",
      "layer   3  Sparsity: 82.3893%\n",
      "total_backward_count 1037740 real_backward_count 109301  10.533%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.066615/  1.346147, val:  79.17%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0040%\n",
      "layer   2  Sparsity: 80.6027%\n",
      "layer   3  Sparsity: 82.0253%\n",
      "total_backward_count 1047530 real_backward_count 109991  10.500%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.086343/  1.357706, val:  76.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9978%\n",
      "layer   2  Sparsity: 80.6291%\n",
      "layer   3  Sparsity: 82.0365%\n",
      "total_backward_count 1057320 real_backward_count 110684  10.468%\n",
      "lif layer 1 self.abs_max_v: 9070.5\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.078422/  1.347224, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9842%\n",
      "layer   2  Sparsity: 80.4126%\n",
      "layer   3  Sparsity: 82.3981%\n",
      "total_backward_count 1067110 real_backward_count 111331  10.433%\n",
      "lif layer 1 self.abs_max_v: 9123.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.092640/  1.368216, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0102%\n",
      "layer   2  Sparsity: 80.3111%\n",
      "layer   3  Sparsity: 82.6957%\n",
      "total_backward_count 1076900 real_backward_count 112001  10.400%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.077081/  1.334001, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0078%\n",
      "layer   2  Sparsity: 80.2295%\n",
      "layer   3  Sparsity: 82.6473%\n",
      "total_backward_count 1086690 real_backward_count 112673  10.368%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.078776/  1.345539, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9959%\n",
      "layer   2  Sparsity: 80.3496%\n",
      "layer   3  Sparsity: 82.9862%\n",
      "total_backward_count 1096480 real_backward_count 113300  10.333%\n",
      "fc layer 2 self.abs_max_out: 3178.0\n",
      "fc layer 2 self.abs_max_out: 3187.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.073832/  1.329322, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 80.3537%\n",
      "layer   3  Sparsity: 82.6149%\n",
      "total_backward_count 1106270 real_backward_count 113972  10.302%\n",
      "fc layer 1 self.abs_max_out: 6786.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.071829/  1.363075, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0038%\n",
      "layer   2  Sparsity: 80.4276%\n",
      "layer   3  Sparsity: 82.7698%\n",
      "total_backward_count 1116060 real_backward_count 114603  10.269%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.064266/  1.329355, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0104%\n",
      "layer   2  Sparsity: 80.3537%\n",
      "layer   3  Sparsity: 82.5354%\n",
      "total_backward_count 1125850 real_backward_count 115293  10.241%\n",
      "lif layer 1 self.abs_max_v: 9138.0\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.073165/  1.351510, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0156%\n",
      "layer   2  Sparsity: 80.4365%\n",
      "layer   3  Sparsity: 82.5000%\n",
      "total_backward_count 1135640 real_backward_count 115976  10.212%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.079624/  1.347885, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0163%\n",
      "layer   2  Sparsity: 80.4562%\n",
      "layer   3  Sparsity: 82.4879%\n",
      "total_backward_count 1145430 real_backward_count 116637  10.183%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.076365/  1.371198, val:  79.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0000%\n",
      "layer   2  Sparsity: 80.4179%\n",
      "layer   3  Sparsity: 82.5658%\n",
      "total_backward_count 1155220 real_backward_count 117284  10.153%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.066498/  1.347739, val:  75.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 80.5441%\n",
      "layer   3  Sparsity: 82.6935%\n",
      "total_backward_count 1165010 real_backward_count 117869  10.117%\n",
      "fc layer 2 self.abs_max_out: 3217.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.066188/  1.353549, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0073%\n",
      "layer   2  Sparsity: 80.5016%\n",
      "layer   3  Sparsity: 82.3471%\n",
      "total_backward_count 1174800 real_backward_count 118524  10.089%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.062689/  1.377416, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0042%\n",
      "layer   2  Sparsity: 80.5602%\n",
      "layer   3  Sparsity: 82.5859%\n",
      "total_backward_count 1184590 real_backward_count 119163  10.059%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.055164/  1.324340, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.9971%\n",
      "layer   2  Sparsity: 80.5878%\n",
      "layer   3  Sparsity: 82.7685%\n",
      "total_backward_count 1194380 real_backward_count 119770  10.028%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.073743/  1.363978, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0114%\n",
      "layer   2  Sparsity: 80.4763%\n",
      "layer   3  Sparsity: 82.8537%\n",
      "total_backward_count 1204170 real_backward_count 120400   9.999%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.066680/  1.326525, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0221%\n",
      "layer   2  Sparsity: 80.4879%\n",
      "layer   3  Sparsity: 82.7434%\n",
      "total_backward_count 1213960 real_backward_count 121027   9.970%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.062122/  1.330330, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0009%\n",
      "layer   2  Sparsity: 80.3334%\n",
      "layer   3  Sparsity: 82.9313%\n",
      "total_backward_count 1223750 real_backward_count 121606   9.937%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.080942/  1.338127, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9957%\n",
      "layer   2  Sparsity: 80.1771%\n",
      "layer   3  Sparsity: 83.0287%\n",
      "total_backward_count 1233540 real_backward_count 122117   9.900%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.082312/  1.370601, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0025%\n",
      "layer   2  Sparsity: 80.2813%\n",
      "layer   3  Sparsity: 82.9034%\n",
      "total_backward_count 1243330 real_backward_count 122703   9.869%\n",
      "fc layer 2 self.abs_max_out: 3332.0\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.079632/  1.362573, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9997%\n",
      "layer   2  Sparsity: 80.4975%\n",
      "layer   3  Sparsity: 82.9148%\n",
      "total_backward_count 1253120 real_backward_count 123317   9.841%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.057612/  1.310614, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0015%\n",
      "layer   2  Sparsity: 80.5269%\n",
      "layer   3  Sparsity: 82.6563%\n",
      "total_backward_count 1262910 real_backward_count 123908   9.811%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.064501/  1.325883, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 80.5885%\n",
      "layer   3  Sparsity: 82.5238%\n",
      "total_backward_count 1272700 real_backward_count 124499   9.782%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.053790/  1.325782, val:  77.50%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0067%\n",
      "layer   2  Sparsity: 80.7299%\n",
      "layer   3  Sparsity: 82.2813%\n",
      "total_backward_count 1282490 real_backward_count 125118   9.756%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.034993/  1.308579, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0122%\n",
      "layer   2  Sparsity: 80.3532%\n",
      "layer   3  Sparsity: 82.0400%\n",
      "total_backward_count 1292280 real_backward_count 125738   9.730%\n",
      "fc layer 2 self.abs_max_out: 3346.0\n",
      "fc layer 3 self.abs_max_out: 910.0\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.020386/  1.353430, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0125%\n",
      "layer   2  Sparsity: 80.3220%\n",
      "layer   3  Sparsity: 82.4788%\n",
      "total_backward_count 1302070 real_backward_count 126331   9.702%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.020864/  1.324410, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0041%\n",
      "layer   2  Sparsity: 80.1501%\n",
      "layer   3  Sparsity: 82.4830%\n",
      "total_backward_count 1311860 real_backward_count 126936   9.676%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.012604/  1.315517, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0122%\n",
      "layer   2  Sparsity: 80.1278%\n",
      "layer   3  Sparsity: 82.4291%\n",
      "total_backward_count 1321650 real_backward_count 127522   9.649%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.024527/  1.288299, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9959%\n",
      "layer   2  Sparsity: 80.1236%\n",
      "layer   3  Sparsity: 82.6410%\n",
      "total_backward_count 1331440 real_backward_count 128130   9.623%\n",
      "lif layer 1 self.abs_max_v: 9432.0\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.016011/  1.334811, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0077%\n",
      "layer   2  Sparsity: 80.3035%\n",
      "layer   3  Sparsity: 82.9862%\n",
      "total_backward_count 1341230 real_backward_count 128712   9.597%\n",
      "fc layer 1 self.abs_max_out: 6827.0\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.029075/  1.317621, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9879%\n",
      "layer   2  Sparsity: 80.3676%\n",
      "layer   3  Sparsity: 83.1304%\n",
      "total_backward_count 1351020 real_backward_count 129323   9.572%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.023237/  1.298828, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9909%\n",
      "layer   2  Sparsity: 80.3443%\n",
      "layer   3  Sparsity: 83.0789%\n",
      "total_backward_count 1360810 real_backward_count 129885   9.545%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.025016/  1.300103, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0127%\n",
      "layer   2  Sparsity: 80.3976%\n",
      "layer   3  Sparsity: 82.8337%\n",
      "total_backward_count 1370600 real_backward_count 130454   9.518%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.024685/  1.295145, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0134%\n",
      "layer   2  Sparsity: 80.3584%\n",
      "layer   3  Sparsity: 82.9262%\n",
      "total_backward_count 1380390 real_backward_count 131011   9.491%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.015754/  1.343284, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 80.2893%\n",
      "layer   3  Sparsity: 82.8888%\n",
      "total_backward_count 1390180 real_backward_count 131596   9.466%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.038339/  1.300679, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0090%\n",
      "layer   2  Sparsity: 80.2147%\n",
      "layer   3  Sparsity: 82.6400%\n",
      "total_backward_count 1399970 real_backward_count 132193   9.443%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.025921/  1.318945, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0244%\n",
      "layer   2  Sparsity: 80.2733%\n",
      "layer   3  Sparsity: 82.6140%\n",
      "total_backward_count 1409760 real_backward_count 132782   9.419%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.025643/  1.314144, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0158%\n",
      "layer   2  Sparsity: 80.2603%\n",
      "layer   3  Sparsity: 82.4403%\n",
      "total_backward_count 1419550 real_backward_count 133380   9.396%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.017648/  1.272030, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9974%\n",
      "layer   2  Sparsity: 80.1062%\n",
      "layer   3  Sparsity: 82.6182%\n",
      "total_backward_count 1429340 real_backward_count 133935   9.370%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.001932/  1.277400, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 80.1493%\n",
      "layer   3  Sparsity: 82.6037%\n",
      "total_backward_count 1439130 real_backward_count 134523   9.348%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.968069/  1.274380, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0190%\n",
      "layer   2  Sparsity: 80.1473%\n",
      "layer   3  Sparsity: 82.6279%\n",
      "total_backward_count 1448920 real_backward_count 135099   9.324%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.983583/  1.270707, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0169%\n",
      "layer   2  Sparsity: 80.2256%\n",
      "layer   3  Sparsity: 82.3705%\n",
      "total_backward_count 1458710 real_backward_count 135641   9.299%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.010637/  1.279521, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 80.3546%\n",
      "layer   3  Sparsity: 82.4791%\n",
      "total_backward_count 1468500 real_backward_count 136180   9.273%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.009651/  1.306859, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0057%\n",
      "layer   2  Sparsity: 80.2614%\n",
      "layer   3  Sparsity: 83.0489%\n",
      "total_backward_count 1478290 real_backward_count 136696   9.247%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.011156/  1.308978, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9886%\n",
      "layer   2  Sparsity: 80.2588%\n",
      "layer   3  Sparsity: 83.0119%\n",
      "total_backward_count 1488080 real_backward_count 137247   9.223%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.000260/  1.291742, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 80.2293%\n",
      "layer   3  Sparsity: 83.2294%\n",
      "total_backward_count 1497870 real_backward_count 137753   9.197%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.009637/  1.261413, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.9732%\n",
      "layer   2  Sparsity: 80.2819%\n",
      "layer   3  Sparsity: 83.3752%\n",
      "total_backward_count 1507660 real_backward_count 138293   9.173%\n",
      "fc layer 1 self.abs_max_out: 6832.0\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.002460/  1.289985, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.9915%\n",
      "layer   2  Sparsity: 80.2779%\n",
      "layer   3  Sparsity: 83.3434%\n",
      "total_backward_count 1517450 real_backward_count 138816   9.148%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.013360/  1.275541, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 80.3286%\n",
      "layer   3  Sparsity: 83.2950%\n",
      "total_backward_count 1527240 real_backward_count 139358   9.125%\n",
      "fc layer 1 self.abs_max_out: 6880.0\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.029540/  1.298197, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9867%\n",
      "layer   2  Sparsity: 80.1618%\n",
      "layer   3  Sparsity: 83.3589%\n",
      "total_backward_count 1537030 real_backward_count 139876   9.100%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.017957/  1.291403, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0083%\n",
      "layer   2  Sparsity: 80.1533%\n",
      "layer   3  Sparsity: 83.4396%\n",
      "total_backward_count 1546820 real_backward_count 140368   9.075%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.014805/  1.279712, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9985%\n",
      "layer   2  Sparsity: 80.2199%\n",
      "layer   3  Sparsity: 83.2361%\n",
      "total_backward_count 1556610 real_backward_count 140831   9.047%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.007120/  1.282913, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 80.2489%\n",
      "layer   3  Sparsity: 82.9276%\n",
      "total_backward_count 1566400 real_backward_count 141362   9.025%\n",
      "fc layer 2 self.abs_max_out: 3358.0\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.011933/  1.301664, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 80.2247%\n",
      "layer   3  Sparsity: 83.3774%\n",
      "total_backward_count 1576190 real_backward_count 141879   9.001%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.013346/  1.261041, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 80.2158%\n",
      "layer   3  Sparsity: 83.2269%\n",
      "total_backward_count 1585980 real_backward_count 142463   8.983%\n",
      "fc layer 1 self.abs_max_out: 6909.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.010164/  1.279416, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9948%\n",
      "layer   2  Sparsity: 80.4381%\n",
      "layer   3  Sparsity: 83.5526%\n",
      "total_backward_count 1595770 real_backward_count 142995   8.961%\n",
      "lif layer 1 self.abs_max_v: 9604.5\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.012131/  1.252861, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9878%\n",
      "layer   2  Sparsity: 80.3778%\n",
      "layer   3  Sparsity: 83.3479%\n",
      "total_backward_count 1605560 real_backward_count 143531   8.940%\n",
      "lif layer 1 self.abs_max_v: 9644.0\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.007476/  1.279318, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9946%\n",
      "layer   2  Sparsity: 80.4990%\n",
      "layer   3  Sparsity: 83.5186%\n",
      "total_backward_count 1615350 real_backward_count 144035   8.917%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.002074/  1.279010, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9989%\n",
      "layer   2  Sparsity: 80.1422%\n",
      "layer   3  Sparsity: 83.4089%\n",
      "total_backward_count 1625140 real_backward_count 144569   8.896%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.994682/  1.280358, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9998%\n",
      "layer   2  Sparsity: 80.0595%\n",
      "layer   3  Sparsity: 83.4782%\n",
      "total_backward_count 1634930 real_backward_count 145037   8.871%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.006058/  1.277602, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0173%\n",
      "layer   2  Sparsity: 80.0044%\n",
      "layer   3  Sparsity: 83.1497%\n",
      "total_backward_count 1644720 real_backward_count 145543   8.849%\n",
      "fc layer 1 self.abs_max_out: 6910.0\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.989523/  1.271742, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9902%\n",
      "layer   2  Sparsity: 80.1703%\n",
      "layer   3  Sparsity: 83.3030%\n",
      "total_backward_count 1654510 real_backward_count 146036   8.827%\n",
      "lif layer 1 self.abs_max_v: 9688.0\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.998363/  1.244766, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0158%\n",
      "layer   2  Sparsity: 80.3577%\n",
      "layer   3  Sparsity: 83.4354%\n",
      "total_backward_count 1664300 real_backward_count 146600   8.809%\n",
      "lif layer 1 self.abs_max_v: 10104.5\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.989564/  1.294438, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9892%\n",
      "layer   2  Sparsity: 80.2628%\n",
      "layer   3  Sparsity: 83.3395%\n",
      "total_backward_count 1674090 real_backward_count 147120   8.788%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.978023/  1.265480, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9933%\n",
      "layer   2  Sparsity: 80.2769%\n",
      "layer   3  Sparsity: 83.4001%\n",
      "total_backward_count 1683880 real_backward_count 147606   8.766%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.979525/  1.260969, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 80.2446%\n",
      "layer   3  Sparsity: 83.0660%\n",
      "total_backward_count 1693670 real_backward_count 148110   8.745%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.977261/  1.242046, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0080%\n",
      "layer   2  Sparsity: 80.3316%\n",
      "layer   3  Sparsity: 83.5756%\n",
      "total_backward_count 1703460 real_backward_count 148609   8.724%\n",
      "fc layer 1 self.abs_max_out: 6926.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.998766/  1.304087, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0131%\n",
      "layer   2  Sparsity: 80.1858%\n",
      "layer   3  Sparsity: 83.7934%\n",
      "total_backward_count 1713250 real_backward_count 149096   8.703%\n",
      "fc layer 1 self.abs_max_out: 6928.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.990500/  1.262941, val:  85.00%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.9779%\n",
      "layer   2  Sparsity: 80.1892%\n",
      "layer   3  Sparsity: 83.4529%\n",
      "total_backward_count 1723040 real_backward_count 149647   8.685%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.995466/  1.259823, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 80.2687%\n",
      "layer   3  Sparsity: 83.4494%\n",
      "total_backward_count 1732830 real_backward_count 150167   8.666%\n",
      "fc layer 2 self.abs_max_out: 3376.0\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.987016/  1.277323, val:  74.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 95.0235%\n",
      "layer   2  Sparsity: 80.2386%\n",
      "layer   3  Sparsity: 83.1076%\n",
      "total_backward_count 1742620 real_backward_count 150681   8.647%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.956519/  1.248831, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0021%\n",
      "layer   2  Sparsity: 80.0574%\n",
      "layer   3  Sparsity: 83.1376%\n",
      "total_backward_count 1752410 real_backward_count 151147   8.625%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.960942/  1.242247, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 95.0031%\n",
      "layer   2  Sparsity: 80.0587%\n",
      "layer   3  Sparsity: 83.1587%\n",
      "total_backward_count 1762200 real_backward_count 151627   8.604%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.957482/  1.251855, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0116%\n",
      "layer   2  Sparsity: 80.1647%\n",
      "layer   3  Sparsity: 83.1035%\n",
      "total_backward_count 1771990 real_backward_count 152155   8.587%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.952006/  1.231540, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9977%\n",
      "layer   2  Sparsity: 80.3185%\n",
      "layer   3  Sparsity: 83.5316%\n",
      "total_backward_count 1781780 real_backward_count 152661   8.568%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.965423/  1.253236, val:  83.33%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0041%\n",
      "layer   2  Sparsity: 80.3021%\n",
      "layer   3  Sparsity: 83.8120%\n",
      "total_backward_count 1791570 real_backward_count 153144   8.548%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.982369/  1.311994, val:  77.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9905%\n",
      "layer   2  Sparsity: 80.1635%\n",
      "layer   3  Sparsity: 83.8172%\n",
      "total_backward_count 1801360 real_backward_count 153626   8.528%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.971467/  1.246320, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0118%\n",
      "layer   2  Sparsity: 80.2503%\n",
      "layer   3  Sparsity: 83.7409%\n",
      "total_backward_count 1811150 real_backward_count 154148   8.511%\n",
      "fc layer 2 self.abs_max_out: 3399.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.966092/  1.269392, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 80.1749%\n",
      "layer   3  Sparsity: 83.7046%\n",
      "total_backward_count 1820940 real_backward_count 154653   8.493%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.981525/  1.256454, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0036%\n",
      "layer   2  Sparsity: 80.1526%\n",
      "layer   3  Sparsity: 83.2222%\n",
      "total_backward_count 1830730 real_backward_count 155204   8.478%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.974527/  1.278438, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 80.0951%\n",
      "layer   3  Sparsity: 83.5094%\n",
      "total_backward_count 1840520 real_backward_count 155671   8.458%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.979273/  1.253147, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9926%\n",
      "layer   2  Sparsity: 80.2071%\n",
      "layer   3  Sparsity: 83.0020%\n",
      "total_backward_count 1850310 real_backward_count 156145   8.439%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.967617/  1.228968, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0118%\n",
      "layer   2  Sparsity: 80.2509%\n",
      "layer   3  Sparsity: 82.5371%\n",
      "total_backward_count 1860100 real_backward_count 156684   8.423%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.969336/  1.269681, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9992%\n",
      "layer   2  Sparsity: 80.2523%\n",
      "layer   3  Sparsity: 82.8684%\n",
      "total_backward_count 1869890 real_backward_count 157139   8.404%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.966906/  1.283983, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9890%\n",
      "layer   2  Sparsity: 80.0972%\n",
      "layer   3  Sparsity: 83.3529%\n",
      "total_backward_count 1879680 real_backward_count 157639   8.386%\n",
      "fc layer 2 self.abs_max_out: 3484.0\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.969216/  1.261722, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 79.9546%\n",
      "layer   3  Sparsity: 83.2831%\n",
      "total_backward_count 1889470 real_backward_count 158117   8.368%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.964644/  1.240825, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0055%\n",
      "layer   2  Sparsity: 79.9689%\n",
      "layer   3  Sparsity: 82.8132%\n",
      "total_backward_count 1899260 real_backward_count 158593   8.350%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.965290/  1.253144, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9971%\n",
      "layer   2  Sparsity: 80.0632%\n",
      "layer   3  Sparsity: 82.9720%\n",
      "total_backward_count 1909050 real_backward_count 159054   8.332%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.953742/  1.276492, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0184%\n",
      "layer   2  Sparsity: 80.2034%\n",
      "layer   3  Sparsity: 82.7436%\n",
      "total_backward_count 1918840 real_backward_count 159548   8.315%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.946110/  1.227805, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0071%\n",
      "layer   2  Sparsity: 80.2117%\n",
      "layer   3  Sparsity: 83.0871%\n",
      "total_backward_count 1928630 real_backward_count 159996   8.296%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.930765/  1.228849, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9880%\n",
      "layer   2  Sparsity: 80.1414%\n",
      "layer   3  Sparsity: 83.1945%\n",
      "total_backward_count 1938420 real_backward_count 160460   8.278%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.935257/  1.284715, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0133%\n",
      "layer   2  Sparsity: 80.1450%\n",
      "layer   3  Sparsity: 83.2256%\n",
      "total_backward_count 1948210 real_backward_count 160938   8.261%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.963392/  1.284138, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 80.0832%\n",
      "layer   3  Sparsity: 83.0674%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2779ab4d033740259b20f5de6c7b82cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.96339</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>1.28414</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-158</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/scgrb7pa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/scgrb7pa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_054902-scgrb7pa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y1s9u6e9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_100507-y1s9u6e9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y1s9u6e9' target=\"_blank\">ethereal-sweep-163</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y1s9u6e9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y1s9u6e9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251118_100516_949', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 236.0\n",
      "lif layer 1 self.abs_max_v: 236.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 237.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "fc layer 1 self.abs_max_out: 302.0\n",
      "lif layer 1 self.abs_max_v: 319.5\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "lif layer 2 self.abs_max_v: 344.5\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "lif layer 1 self.abs_max_v: 365.0\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 437.0\n",
      "lif layer 1 self.abs_max_v: 389.5\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 527.0\n",
      "fc layer 1 self.abs_max_out: 376.0\n",
      "lif layer 1 self.abs_max_v: 433.0\n",
      "fc layer 2 self.abs_max_out: 390.0\n",
      "fc layer 1 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 483.5\n",
      "fc layer 1 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "fc layer 1 self.abs_max_out: 597.0\n",
      "lif layer 1 self.abs_max_v: 597.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "lif layer 2 self.abs_max_v: 606.5\n",
      "lif layer 1 self.abs_max_v: 609.0\n",
      "lif layer 2 self.abs_max_v: 645.5\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 1 self.abs_max_out: 635.0\n",
      "lif layer 1 self.abs_max_v: 635.0\n",
      "fc layer 1 self.abs_max_out: 693.0\n",
      "lif layer 1 self.abs_max_v: 693.0\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "lif layer 2 self.abs_max_v: 719.5\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "lif layer 2 self.abs_max_v: 788.5\n",
      "lif layer 2 self.abs_max_v: 804.5\n",
      "fc layer 1 self.abs_max_out: 864.0\n",
      "lif layer 1 self.abs_max_v: 864.0\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "lif layer 1 self.abs_max_v: 869.5\n",
      "lif layer 1 self.abs_max_v: 922.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 1 self.abs_max_out: 936.0\n",
      "fc layer 1 self.abs_max_out: 954.0\n",
      "fc layer 1 self.abs_max_out: 987.0\n",
      "lif layer 1 self.abs_max_v: 987.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 1050.0\n",
      "lif layer 1 self.abs_max_v: 1242.5\n",
      "lif layer 1 self.abs_max_v: 1312.5\n",
      "lif layer 1 self.abs_max_v: 1393.0\n",
      "lif layer 2 self.abs_max_v: 1050.5\n",
      "lif layer 2 self.abs_max_v: 1053.5\n",
      "fc layer 2 self.abs_max_out: 634.0\n",
      "lif layer 2 self.abs_max_v: 1090.0\n",
      "fc layer 1 self.abs_max_out: 1166.0\n",
      "lif layer 1 self.abs_max_v: 1503.5\n",
      "lif layer 1 self.abs_max_v: 1515.5\n",
      "fc layer 1 self.abs_max_out: 1247.0\n",
      "lif layer 1 self.abs_max_v: 1640.0\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 1103.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 2 self.abs_max_out: 725.0\n",
      "lif layer 2 self.abs_max_v: 1180.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 3 self.abs_max_out: 315.0\n",
      "lif layer 1 self.abs_max_v: 1684.0\n",
      "lif layer 1 self.abs_max_v: 1841.0\n",
      "fc layer 1 self.abs_max_out: 1282.0\n",
      "lif layer 1 self.abs_max_v: 2082.0\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "fc layer 1 self.abs_max_out: 1284.0\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "lif layer 1 self.abs_max_v: 2161.5\n",
      "lif layer 1 self.abs_max_v: 2325.0\n",
      "fc layer 1 self.abs_max_out: 1381.0\n",
      "lif layer 1 self.abs_max_v: 2408.0\n",
      "lif layer 2 self.abs_max_v: 1211.0\n",
      "lif layer 2 self.abs_max_v: 1244.5\n",
      "fc layer 1 self.abs_max_out: 1406.0\n",
      "fc layer 1 self.abs_max_out: 1470.0\n",
      "lif layer 1 self.abs_max_v: 2501.0\n",
      "fc layer 2 self.abs_max_out: 846.0\n",
      "fc layer 1 self.abs_max_out: 1498.0\n",
      "fc layer 1 self.abs_max_out: 1512.0\n",
      "fc layer 1 self.abs_max_out: 1545.0\n",
      "fc layer 2 self.abs_max_out: 855.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "lif layer 1 self.abs_max_v: 2691.0\n",
      "lif layer 1 self.abs_max_v: 2743.5\n",
      "fc layer 1 self.abs_max_out: 1563.0\n",
      "fc layer 1 self.abs_max_out: 1565.0\n",
      "fc layer 1 self.abs_max_out: 1692.0\n",
      "fc layer 1 self.abs_max_out: 1735.0\n",
      "lif layer 1 self.abs_max_v: 2778.0\n",
      "fc layer 1 self.abs_max_out: 1831.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.842793/  1.990996, val:  31.67%, val_best:  31.67%, tr:  97.24%, tr_best:  97.24%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0983%\n",
      "layer   3  Sparsity: 76.2441%\n",
      "total_backward_count 9790 real_backward_count 2215  22.625%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 3014.0\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 2 self.abs_max_out: 968.0\n",
      "lif layer 2 self.abs_max_v: 1247.5\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "lif layer 2 self.abs_max_v: 1248.5\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 2 self.abs_max_out: 1014.0\n",
      "lif layer 2 self.abs_max_v: 1261.5\n",
      "lif layer 2 self.abs_max_v: 1323.0\n",
      "lif layer 2 self.abs_max_v: 1326.5\n",
      "fc layer 1 self.abs_max_out: 1919.0\n",
      "lif layer 1 self.abs_max_v: 3226.0\n",
      "lif layer 1 self.abs_max_v: 3423.0\n",
      "fc layer 1 self.abs_max_out: 2035.0\n",
      "lif layer 1 self.abs_max_v: 3544.0\n",
      "lif layer 1 self.abs_max_v: 3793.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.776218/  1.972814, val:  45.42%, val_best:  45.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 77.1755%\n",
      "layer   3  Sparsity: 75.5590%\n",
      "total_backward_count 19580 real_backward_count 3858  19.704%\n",
      "lif layer 2 self.abs_max_v: 1339.0\n",
      "lif layer 2 self.abs_max_v: 1361.5\n",
      "lif layer 2 self.abs_max_v: 1369.0\n",
      "lif layer 2 self.abs_max_v: 1373.5\n",
      "lif layer 2 self.abs_max_v: 1407.5\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "fc layer 1 self.abs_max_out: 2075.0\n",
      "fc layer 1 self.abs_max_out: 2168.0\n",
      "lif layer 1 self.abs_max_v: 3814.0\n",
      "lif layer 1 self.abs_max_v: 3826.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.734875/  1.930224, val:  47.08%, val_best:  47.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 77.1488%\n",
      "layer   3  Sparsity: 74.6389%\n",
      "total_backward_count 29370 real_backward_count 5490  18.693%\n",
      "lif layer 2 self.abs_max_v: 1426.5\n",
      "lif layer 2 self.abs_max_v: 1527.5\n",
      "lif layer 2 self.abs_max_v: 1602.5\n",
      "fc layer 1 self.abs_max_out: 2325.0\n",
      "fc layer 1 self.abs_max_out: 2392.0\n",
      "fc layer 1 self.abs_max_out: 2402.0\n",
      "lif layer 1 self.abs_max_v: 3895.0\n",
      "lif layer 1 self.abs_max_v: 3977.5\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "fc layer 1 self.abs_max_out: 2438.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.696853/  1.928579, val:  36.25%, val_best:  47.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.6230%\n",
      "layer   3  Sparsity: 72.3992%\n",
      "total_backward_count 39160 real_backward_count 6974  17.809%\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 1 self.abs_max_out: 2466.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "fc layer 2 self.abs_max_out: 1130.0\n",
      "fc layer 2 self.abs_max_out: 1169.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "lif layer 1 self.abs_max_v: 4003.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.661821/  1.883262, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.5426%\n",
      "layer   3  Sparsity: 71.9841%\n",
      "total_backward_count 48950 real_backward_count 8332  17.021%\n",
      "lif layer 1 self.abs_max_v: 4043.0\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "lif layer 2 self.abs_max_v: 1617.0\n",
      "lif layer 1 self.abs_max_v: 4072.5\n",
      "lif layer 1 self.abs_max_v: 4166.5\n",
      "lif layer 2 self.abs_max_v: 1755.0\n",
      "lif layer 2 self.abs_max_v: 1872.5\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.629159/  1.907518, val:  41.67%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4982%\n",
      "layer   3  Sparsity: 73.0047%\n",
      "total_backward_count 58740 real_backward_count 9726  16.558%\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "lif layer 1 self.abs_max_v: 4263.5\n",
      "lif layer 1 self.abs_max_v: 4333.0\n",
      "fc layer 2 self.abs_max_out: 1247.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.620713/  1.845027, val:  52.50%, val_best:  55.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.6700%\n",
      "layer   3  Sparsity: 71.5185%\n",
      "total_backward_count 68530 real_backward_count 11082  16.171%\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "fc layer 2 self.abs_max_out: 1385.0\n",
      "fc layer 1 self.abs_max_out: 2510.0\n",
      "fc layer 1 self.abs_max_out: 2547.0\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.601579/  1.814476, val:  55.83%, val_best:  55.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4028%\n",
      "layer   3  Sparsity: 73.4018%\n",
      "total_backward_count 78320 real_backward_count 12416  15.853%\n",
      "lif layer 1 self.abs_max_v: 4357.5\n",
      "fc layer 1 self.abs_max_out: 2577.0\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 1 self.abs_max_v: 4380.5\n",
      "lif layer 1 self.abs_max_v: 4601.0\n",
      "lif layer 1 self.abs_max_v: 4811.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.587364/  1.794706, val:  55.42%, val_best:  55.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0911%\n",
      "layer   3  Sparsity: 73.6731%\n",
      "total_backward_count 88110 real_backward_count 13739  15.593%\n",
      "lif layer 2 self.abs_max_v: 1881.5\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.590827/  1.838642, val:  49.58%, val_best:  55.83%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1804%\n",
      "layer   3  Sparsity: 73.3505%\n",
      "total_backward_count 97900 real_backward_count 15013  15.335%\n",
      "fc layer 1 self.abs_max_out: 2767.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.559403/  1.750522, val:  56.67%, val_best:  56.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4637%\n",
      "layer   3  Sparsity: 73.7523%\n",
      "total_backward_count 107690 real_backward_count 16223  15.065%\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "lif layer 1 self.abs_max_v: 4835.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.554621/  1.778747, val:  55.00%, val_best:  56.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 77.5797%\n",
      "layer   3  Sparsity: 73.7478%\n",
      "total_backward_count 117480 real_backward_count 17489  14.887%\n",
      "lif layer 2 self.abs_max_v: 1905.0\n",
      "lif layer 1 self.abs_max_v: 4887.5\n",
      "lif layer 1 self.abs_max_v: 4899.0\n",
      "lif layer 2 self.abs_max_v: 1920.0\n",
      "lif layer 2 self.abs_max_v: 1945.0\n",
      "lif layer 1 self.abs_max_v: 4932.5\n",
      "lif layer 1 self.abs_max_v: 5130.5\n",
      "lif layer 1 self.abs_max_v: 5134.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.521955/  1.753024, val:  54.17%, val_best:  56.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 77.2235%\n",
      "layer   3  Sparsity: 74.0496%\n",
      "total_backward_count 127270 real_backward_count 18695  14.689%\n",
      "fc layer 1 self.abs_max_out: 3212.0\n",
      "lif layer 2 self.abs_max_v: 1957.5\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "lif layer 2 self.abs_max_v: 1994.5\n",
      "lif layer 1 self.abs_max_v: 5160.0\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.513628/  1.790403, val:  44.58%, val_best:  56.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 77.0557%\n",
      "layer   3  Sparsity: 73.8333%\n",
      "total_backward_count 137060 real_backward_count 19886  14.509%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.505512/  1.723024, val:  60.00%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5523%\n",
      "layer   3  Sparsity: 73.4756%\n",
      "total_backward_count 146850 real_backward_count 21028  14.319%\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.471863/  1.717330, val:  53.75%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3042%\n",
      "layer   3  Sparsity: 73.6814%\n",
      "total_backward_count 156640 real_backward_count 22176  14.157%\n",
      "fc layer 2 self.abs_max_out: 1449.0\n",
      "fc layer 2 self.abs_max_out: 1511.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.458187/  1.681468, val:  59.58%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5813%\n",
      "layer   3  Sparsity: 74.5268%\n",
      "total_backward_count 166430 real_backward_count 23225  13.955%\n",
      "lif layer 1 self.abs_max_v: 5170.0\n",
      "lif layer 1 self.abs_max_v: 5298.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.444645/  1.716518, val:  52.50%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1387%\n",
      "layer   3  Sparsity: 74.2849%\n",
      "total_backward_count 176220 real_backward_count 24289  13.783%\n",
      "lif layer 1 self.abs_max_v: 5354.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.448652/  1.658106, val:  62.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.4801%\n",
      "layer   3  Sparsity: 73.8325%\n",
      "total_backward_count 186010 real_backward_count 25370  13.639%\n",
      "lif layer 1 self.abs_max_v: 5485.0\n",
      "lif layer 1 self.abs_max_v: 5651.5\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.415673/  1.679523, val:  52.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.5614%\n",
      "layer   3  Sparsity: 74.6674%\n",
      "total_backward_count 195800 real_backward_count 26393  13.480%\n",
      "lif layer 2 self.abs_max_v: 2001.0\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.405119/  1.653980, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.7966%\n",
      "layer   3  Sparsity: 74.7850%\n",
      "total_backward_count 205590 real_backward_count 27434  13.344%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.418739/  1.688490, val:  52.92%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8996%\n",
      "layer   3  Sparsity: 74.8506%\n",
      "total_backward_count 215380 real_backward_count 28507  13.236%\n",
      "lif layer 1 self.abs_max_v: 5711.5\n",
      "lif layer 1 self.abs_max_v: 5768.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.415195/  1.639315, val:  63.33%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.7530%\n",
      "layer   3  Sparsity: 75.0703%\n",
      "total_backward_count 225170 real_backward_count 29544  13.121%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.404218/  1.633258, val:  67.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0772%\n",
      "layer   3  Sparsity: 75.5283%\n",
      "total_backward_count 234960 real_backward_count 30585  13.017%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.387558/  1.622271, val:  62.92%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2779%\n",
      "layer   3  Sparsity: 74.9395%\n",
      "total_backward_count 244750 real_backward_count 31541  12.887%\n",
      "lif layer 2 self.abs_max_v: 2008.5\n",
      "lif layer 2 self.abs_max_v: 2035.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.389392/  1.586136, val:  70.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6762%\n",
      "layer   3  Sparsity: 74.9965%\n",
      "total_backward_count 254540 real_backward_count 32517  12.775%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "lif layer 2 self.abs_max_v: 2062.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.372211/  1.599730, val:  56.67%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4728%\n",
      "layer   3  Sparsity: 75.2590%\n",
      "total_backward_count 264330 real_backward_count 33402  12.636%\n",
      "lif layer 2 self.abs_max_v: 2142.0\n",
      "lif layer 2 self.abs_max_v: 2182.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.368527/  1.619017, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7841%\n",
      "layer   3  Sparsity: 75.8482%\n",
      "total_backward_count 274120 real_backward_count 34263  12.499%\n",
      "fc layer 1 self.abs_max_out: 3258.0\n",
      "lif layer 1 self.abs_max_v: 5848.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.374036/  1.637956, val:  65.42%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 77.2808%\n",
      "layer   3  Sparsity: 75.8112%\n",
      "total_backward_count 283910 real_backward_count 35155  12.382%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.361553/  1.603673, val:  56.25%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7971%\n",
      "layer   3  Sparsity: 75.3037%\n",
      "total_backward_count 293700 real_backward_count 36073  12.282%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.350374/  1.581126, val:  74.17%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9726%\n",
      "layer   3  Sparsity: 75.2169%\n",
      "total_backward_count 303490 real_backward_count 36970  12.182%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.323955/  1.593482, val:  59.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9056%\n",
      "layer   3  Sparsity: 75.1053%\n",
      "total_backward_count 313280 real_backward_count 37801  12.066%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.317080/  1.589726, val:  59.58%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3408%\n",
      "layer   3  Sparsity: 75.0619%\n",
      "total_backward_count 323070 real_backward_count 38620  11.954%\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "lif layer 1 self.abs_max_v: 5964.5\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.302007/  1.563656, val:  73.75%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1262%\n",
      "layer   3  Sparsity: 74.9706%\n",
      "total_backward_count 332860 real_backward_count 39444  11.850%\n",
      "fc layer 1 self.abs_max_out: 3317.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.285475/  1.546122, val:  68.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2739%\n",
      "layer   3  Sparsity: 74.8960%\n",
      "total_backward_count 342650 real_backward_count 40225  11.739%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.302720/  1.567660, val:  68.75%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4891%\n",
      "layer   3  Sparsity: 75.5698%\n",
      "total_backward_count 352440 real_backward_count 40994  11.631%\n",
      "fc layer 2 self.abs_max_out: 1586.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.295686/  1.541136, val:  73.75%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2253%\n",
      "layer   3  Sparsity: 74.9951%\n",
      "total_backward_count 362230 real_backward_count 41697  11.511%\n",
      "fc layer 1 self.abs_max_out: 3348.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.267593/  1.555122, val:  65.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.9824%\n",
      "layer   3  Sparsity: 74.2426%\n",
      "total_backward_count 372020 real_backward_count 42320  11.376%\n",
      "fc layer 1 self.abs_max_out: 3479.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.254530/  1.535422, val:  69.58%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4161%\n",
      "layer   3  Sparsity: 74.4169%\n",
      "total_backward_count 381810 real_backward_count 43041  11.273%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.230784/  1.502808, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8074%\n",
      "layer   3  Sparsity: 74.3593%\n",
      "total_backward_count 391600 real_backward_count 43660  11.149%\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 1 self.abs_max_out: 3573.0\n",
      "lif layer 1 self.abs_max_v: 5980.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.219662/  1.493203, val:  70.00%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5346%\n",
      "layer   3  Sparsity: 74.5022%\n",
      "total_backward_count 401390 real_backward_count 44322  11.042%\n",
      "fc layer 2 self.abs_max_out: 1601.0\n",
      "lif layer 2 self.abs_max_v: 2204.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.224773/  1.489552, val:  70.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1655%\n",
      "layer   3  Sparsity: 73.7067%\n",
      "total_backward_count 411180 real_backward_count 44961  10.935%\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "lif layer 2 self.abs_max_v: 2234.5\n",
      "lif layer 2 self.abs_max_v: 2323.5\n",
      "fc layer 1 self.abs_max_out: 4015.0\n",
      "lif layer 1 self.abs_max_v: 6811.5\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.222319/  1.508591, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0479%\n",
      "layer   3  Sparsity: 73.9417%\n",
      "total_backward_count 420970 real_backward_count 45573  10.826%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.218287/  1.503039, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8219%\n",
      "layer   3  Sparsity: 73.7555%\n",
      "total_backward_count 430760 real_backward_count 46123  10.707%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.217222/  1.489591, val:  77.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.6369%\n",
      "layer   3  Sparsity: 73.7548%\n",
      "total_backward_count 440550 real_backward_count 46728  10.607%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.200868/  1.475772, val:  70.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.6801%\n",
      "layer   3  Sparsity: 73.6456%\n",
      "total_backward_count 450340 real_backward_count 47316  10.507%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.184052/  1.509440, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8097%\n",
      "layer   3  Sparsity: 73.8648%\n",
      "total_backward_count 460130 real_backward_count 47831  10.395%\n",
      "fc layer 3 self.abs_max_out: 695.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.184341/  1.504673, val:  68.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8570%\n",
      "layer   3  Sparsity: 74.2249%\n",
      "total_backward_count 469920 real_backward_count 48354  10.290%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.171691/  1.495880, val:  75.42%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1822%\n",
      "layer   3  Sparsity: 74.3349%\n",
      "total_backward_count 479710 real_backward_count 48874  10.188%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.183285/  1.471435, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1785%\n",
      "layer   3  Sparsity: 74.4128%\n",
      "total_backward_count 489500 real_backward_count 49326  10.077%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.159616/  1.467021, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1920%\n",
      "layer   3  Sparsity: 74.7358%\n",
      "total_backward_count 499290 real_backward_count 49801   9.974%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.145595/  1.466444, val:  70.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3039%\n",
      "layer   3  Sparsity: 74.8599%\n",
      "total_backward_count 509080 real_backward_count 50247   9.870%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.145796/  1.444271, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2137%\n",
      "layer   3  Sparsity: 74.8093%\n",
      "total_backward_count 518870 real_backward_count 50713   9.774%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.133872/  1.433497, val:  73.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1468%\n",
      "layer   3  Sparsity: 74.4240%\n",
      "total_backward_count 528660 real_backward_count 51200   9.685%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.115909/  1.434846, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1027%\n",
      "layer   3  Sparsity: 74.3502%\n",
      "total_backward_count 538450 real_backward_count 51661   9.594%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.114766/  1.415764, val:  70.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0223%\n",
      "layer   3  Sparsity: 74.6906%\n",
      "total_backward_count 548240 real_backward_count 52080   9.499%\n",
      "lif layer 2 self.abs_max_v: 2340.5\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.102712/  1.421620, val:  74.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1789%\n",
      "layer   3  Sparsity: 75.2382%\n",
      "total_backward_count 558030 real_backward_count 52498   9.408%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.096023/  1.415505, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1310%\n",
      "layer   3  Sparsity: 75.0329%\n",
      "total_backward_count 567820 real_backward_count 52826   9.303%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.101855/  1.430193, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0472%\n",
      "layer   3  Sparsity: 75.0467%\n",
      "total_backward_count 577610 real_backward_count 53142   9.200%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.106915/  1.435022, val:  70.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0054%\n",
      "layer   3  Sparsity: 74.8981%\n",
      "total_backward_count 587400 real_backward_count 53487   9.106%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.104527/  1.415993, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8109%\n",
      "layer   3  Sparsity: 74.7346%\n",
      "total_backward_count 597190 real_backward_count 53823   9.013%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.101041/  1.422001, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8739%\n",
      "layer   3  Sparsity: 74.2190%\n",
      "total_backward_count 606980 real_backward_count 54175   8.925%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.099612/  1.404618, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.6094%\n",
      "layer   3  Sparsity: 74.6926%\n",
      "total_backward_count 616770 real_backward_count 54520   8.840%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.089291/  1.397522, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.7141%\n",
      "layer   3  Sparsity: 74.9364%\n",
      "total_backward_count 626560 real_backward_count 54836   8.752%\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.085792/  1.421176, val:  75.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.7219%\n",
      "layer   3  Sparsity: 75.1921%\n",
      "total_backward_count 636350 real_backward_count 55146   8.666%\n",
      "fc layer 1 self.abs_max_out: 4036.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.081072/  1.422930, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 75.8216%\n",
      "layer   3  Sparsity: 75.3017%\n",
      "total_backward_count 646140 real_backward_count 55453   8.582%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.072767/  1.398081, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0103%\n",
      "layer   3  Sparsity: 75.6125%\n",
      "total_backward_count 655930 real_backward_count 55716   8.494%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.068067/  1.391851, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3448%\n",
      "layer   3  Sparsity: 75.5457%\n",
      "total_backward_count 665720 real_backward_count 55976   8.408%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.072801/  1.397186, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5875%\n",
      "layer   3  Sparsity: 75.9895%\n",
      "total_backward_count 675510 real_backward_count 56218   8.322%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.077653/  1.393912, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2091%\n",
      "layer   3  Sparsity: 75.8043%\n",
      "total_backward_count 685300 real_backward_count 56465   8.239%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.068825/  1.423609, val:  70.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.0009%\n",
      "layer   3  Sparsity: 75.1485%\n",
      "total_backward_count 695090 real_backward_count 56708   8.158%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.064445/  1.394930, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1132%\n",
      "layer   3  Sparsity: 75.2701%\n",
      "total_backward_count 704880 real_backward_count 56920   8.075%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.062683/  1.396070, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3271%\n",
      "layer   3  Sparsity: 75.3734%\n",
      "total_backward_count 714670 real_backward_count 57145   7.996%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.051185/  1.405199, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3545%\n",
      "layer   3  Sparsity: 75.1516%\n",
      "total_backward_count 724460 real_backward_count 57324   7.913%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.059541/  1.404283, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3966%\n",
      "layer   3  Sparsity: 75.2628%\n",
      "total_backward_count 734250 real_backward_count 57534   7.836%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.053224/  1.398327, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4175%\n",
      "layer   3  Sparsity: 75.0855%\n",
      "total_backward_count 744040 real_backward_count 57755   7.762%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.052471/  1.386014, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5874%\n",
      "layer   3  Sparsity: 75.3151%\n",
      "total_backward_count 753830 real_backward_count 57938   7.686%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.044374/  1.389491, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5087%\n",
      "layer   3  Sparsity: 75.4675%\n",
      "total_backward_count 763620 real_backward_count 58125   7.612%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.041916/  1.393351, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4234%\n",
      "layer   3  Sparsity: 75.7388%\n",
      "total_backward_count 773410 real_backward_count 58313   7.540%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.028388/  1.372740, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6719%\n",
      "layer   3  Sparsity: 75.5556%\n",
      "total_backward_count 783200 real_backward_count 58489   7.468%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.025502/  1.360405, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7177%\n",
      "layer   3  Sparsity: 75.3351%\n",
      "total_backward_count 792990 real_backward_count 58684   7.400%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.024667/  1.392703, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5765%\n",
      "layer   3  Sparsity: 75.7180%\n",
      "total_backward_count 802780 real_backward_count 58848   7.331%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.027542/  1.357318, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5753%\n",
      "layer   3  Sparsity: 75.4960%\n",
      "total_backward_count 812570 real_backward_count 59020   7.263%\n",
      "fc layer 3 self.abs_max_out: 725.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.015005/  1.368844, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4166%\n",
      "layer   3  Sparsity: 75.9162%\n",
      "total_backward_count 822360 real_backward_count 59210   7.200%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.017638/  1.389837, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4216%\n",
      "layer   3  Sparsity: 75.8281%\n",
      "total_backward_count 832150 real_backward_count 59383   7.136%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.006434/  1.351418, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3642%\n",
      "layer   3  Sparsity: 76.0922%\n",
      "total_backward_count 841940 real_backward_count 59544   7.072%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.000254/  1.339254, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4400%\n",
      "layer   3  Sparsity: 76.2252%\n",
      "total_backward_count 851730 real_backward_count 59685   7.008%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  0.994654/  1.342308, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4098%\n",
      "layer   3  Sparsity: 76.2838%\n",
      "total_backward_count 861520 real_backward_count 59831   6.945%\n",
      "fc layer 3 self.abs_max_out: 729.0\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  0.999818/  1.339584, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4740%\n",
      "layer   3  Sparsity: 76.0152%\n",
      "total_backward_count 871310 real_backward_count 59966   6.882%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  0.997431/  1.338721, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5819%\n",
      "layer   3  Sparsity: 76.1356%\n",
      "total_backward_count 881100 real_backward_count 60098   6.821%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  0.990705/  1.354663, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5026%\n",
      "layer   3  Sparsity: 76.1112%\n",
      "total_backward_count 890890 real_backward_count 60215   6.759%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  0.989756/  1.327458, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4529%\n",
      "layer   3  Sparsity: 76.1237%\n",
      "total_backward_count 900680 real_backward_count 60322   6.697%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  0.988762/  1.341714, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4252%\n",
      "layer   3  Sparsity: 76.0930%\n",
      "total_backward_count 910470 real_backward_count 60415   6.636%\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  0.995004/  1.352577, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3642%\n",
      "layer   3  Sparsity: 76.1078%\n",
      "total_backward_count 920260 real_backward_count 60551   6.580%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  0.992503/  1.360610, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3102%\n",
      "layer   3  Sparsity: 76.0043%\n",
      "total_backward_count 930050 real_backward_count 60667   6.523%\n",
      "fc layer 1 self.abs_max_out: 4040.0\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  0.993206/  1.345462, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1432%\n",
      "layer   3  Sparsity: 76.0253%\n",
      "total_backward_count 939840 real_backward_count 60824   6.472%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  0.988720/  1.336619, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1269%\n",
      "layer   3  Sparsity: 76.2356%\n",
      "total_backward_count 949630 real_backward_count 60930   6.416%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  0.980841/  1.336928, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2835%\n",
      "layer   3  Sparsity: 76.1946%\n",
      "total_backward_count 959420 real_backward_count 61044   6.363%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  0.974412/  1.337775, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2822%\n",
      "layer   3  Sparsity: 76.1155%\n",
      "total_backward_count 969210 real_backward_count 61148   6.309%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  0.976375/  1.353352, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.1474%\n",
      "layer   3  Sparsity: 76.3939%\n",
      "total_backward_count 979000 real_backward_count 61265   6.258%\n",
      "fc layer 1 self.abs_max_out: 4067.0\n",
      "fc layer 1 self.abs_max_out: 4165.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  0.977663/  1.336615, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.3044%\n",
      "layer   3  Sparsity: 76.0080%\n",
      "total_backward_count 988790 real_backward_count 61356   6.205%\n",
      "fc layer 1 self.abs_max_out: 4216.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  0.981645/  1.339950, val:  77.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2546%\n",
      "layer   3  Sparsity: 76.0676%\n",
      "total_backward_count 998580 real_backward_count 61448   6.154%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  0.978910/  1.347653, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.2856%\n",
      "layer   3  Sparsity: 76.1700%\n",
      "total_backward_count 1008370 real_backward_count 61534   6.102%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  0.965521/  1.345719, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4467%\n",
      "layer   3  Sparsity: 76.2530%\n",
      "total_backward_count 1018160 real_backward_count 61615   6.052%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  0.966376/  1.323147, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4439%\n",
      "layer   3  Sparsity: 76.0783%\n",
      "total_backward_count 1027950 real_backward_count 61705   6.003%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  0.958626/  1.314024, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5155%\n",
      "layer   3  Sparsity: 75.9587%\n",
      "total_backward_count 1037740 real_backward_count 61785   5.954%\n",
      "lif layer 2 self.abs_max_v: 2414.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  0.955852/  1.319051, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4392%\n",
      "layer   3  Sparsity: 76.1489%\n",
      "total_backward_count 1047530 real_backward_count 61861   5.905%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  0.948484/  1.303375, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4543%\n",
      "layer   3  Sparsity: 76.2576%\n",
      "total_backward_count 1057320 real_backward_count 61940   5.858%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  0.940494/  1.312223, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5416%\n",
      "layer   3  Sparsity: 76.3998%\n",
      "total_backward_count 1067110 real_backward_count 62000   5.810%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  0.940932/  1.318992, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6918%\n",
      "layer   3  Sparsity: 76.7093%\n",
      "total_backward_count 1076900 real_backward_count 62075   5.764%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  0.938657/  1.319441, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8862%\n",
      "layer   3  Sparsity: 76.5421%\n",
      "total_backward_count 1086690 real_backward_count 62146   5.719%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  0.945775/  1.326187, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8526%\n",
      "layer   3  Sparsity: 76.2497%\n",
      "total_backward_count 1096480 real_backward_count 62226   5.675%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  0.942457/  1.314633, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8484%\n",
      "layer   3  Sparsity: 76.0497%\n",
      "total_backward_count 1106270 real_backward_count 62292   5.631%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  0.939586/  1.309666, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7199%\n",
      "layer   3  Sparsity: 76.2008%\n",
      "total_backward_count 1116060 real_backward_count 62397   5.591%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  0.943555/  1.316711, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7244%\n",
      "layer   3  Sparsity: 76.2422%\n",
      "total_backward_count 1125850 real_backward_count 62491   5.551%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  0.936992/  1.313605, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9917%\n",
      "layer   3  Sparsity: 76.4363%\n",
      "total_backward_count 1135640 real_backward_count 62555   5.508%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  0.929710/  1.314138, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8861%\n",
      "layer   3  Sparsity: 76.2266%\n",
      "total_backward_count 1145430 real_backward_count 62619   5.467%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  0.938366/  1.321398, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8000%\n",
      "layer   3  Sparsity: 76.2003%\n",
      "total_backward_count 1155220 real_backward_count 62682   5.426%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  0.940462/  1.331125, val:  76.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6193%\n",
      "layer   3  Sparsity: 76.5802%\n",
      "total_backward_count 1165010 real_backward_count 62722   5.384%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  0.937519/  1.327042, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5645%\n",
      "layer   3  Sparsity: 76.5828%\n",
      "total_backward_count 1174800 real_backward_count 62800   5.346%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  0.928985/  1.324089, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.4539%\n",
      "layer   3  Sparsity: 76.5272%\n",
      "total_backward_count 1184590 real_backward_count 62851   5.306%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  0.929764/  1.301487, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6558%\n",
      "layer   3  Sparsity: 76.2908%\n",
      "total_backward_count 1194380 real_backward_count 62923   5.268%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  0.926364/  1.316126, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8050%\n",
      "layer   3  Sparsity: 76.1674%\n",
      "total_backward_count 1204170 real_backward_count 62992   5.231%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  0.927885/  1.298793, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7632%\n",
      "layer   3  Sparsity: 76.0898%\n",
      "total_backward_count 1213960 real_backward_count 63068   5.195%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  0.922481/  1.307681, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6889%\n",
      "layer   3  Sparsity: 76.3999%\n",
      "total_backward_count 1223750 real_backward_count 63127   5.158%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  0.913657/  1.294450, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8373%\n",
      "layer   3  Sparsity: 76.5596%\n",
      "total_backward_count 1233540 real_backward_count 63181   5.122%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  0.908301/  1.304914, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6133%\n",
      "layer   3  Sparsity: 76.7829%\n",
      "total_backward_count 1243330 real_backward_count 63246   5.087%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  0.910325/  1.292355, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6517%\n",
      "layer   3  Sparsity: 76.7485%\n",
      "total_backward_count 1253120 real_backward_count 63306   5.052%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  0.905934/  1.298866, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5620%\n",
      "layer   3  Sparsity: 76.6288%\n",
      "total_backward_count 1262910 real_backward_count 63348   5.016%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.903265/  1.305094, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5559%\n",
      "layer   3  Sparsity: 76.6458%\n",
      "total_backward_count 1272700 real_backward_count 63393   4.981%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.907841/  1.306954, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5573%\n",
      "layer   3  Sparsity: 76.6048%\n",
      "total_backward_count 1282490 real_backward_count 63444   4.947%\n",
      "lif layer 2 self.abs_max_v: 2417.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.909697/  1.301983, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6037%\n",
      "layer   3  Sparsity: 76.6461%\n",
      "total_backward_count 1292280 real_backward_count 63490   4.913%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.911229/  1.302902, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6438%\n",
      "layer   3  Sparsity: 76.5532%\n",
      "total_backward_count 1302070 real_backward_count 63536   4.880%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.906534/  1.305052, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6774%\n",
      "layer   3  Sparsity: 76.2149%\n",
      "total_backward_count 1311860 real_backward_count 63576   4.846%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.902530/  1.291746, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7819%\n",
      "layer   3  Sparsity: 76.4313%\n",
      "total_backward_count 1321650 real_backward_count 63617   4.813%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.898925/  1.297584, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9212%\n",
      "layer   3  Sparsity: 76.3438%\n",
      "total_backward_count 1331440 real_backward_count 63647   4.780%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.899769/  1.294170, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8795%\n",
      "layer   3  Sparsity: 76.4006%\n",
      "total_backward_count 1341230 real_backward_count 63693   4.749%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.903120/  1.304329, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7975%\n",
      "layer   3  Sparsity: 76.4191%\n",
      "total_backward_count 1351020 real_backward_count 63723   4.717%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.907398/  1.311147, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7368%\n",
      "layer   3  Sparsity: 76.5231%\n",
      "total_backward_count 1360810 real_backward_count 63749   4.685%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.907565/  1.312399, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6746%\n",
      "layer   3  Sparsity: 76.4963%\n",
      "total_backward_count 1370600 real_backward_count 63788   4.654%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.910476/  1.313059, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5327%\n",
      "layer   3  Sparsity: 76.4465%\n",
      "total_backward_count 1380390 real_backward_count 63814   4.623%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.908349/  1.305803, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5894%\n",
      "layer   3  Sparsity: 76.6822%\n",
      "total_backward_count 1390180 real_backward_count 63847   4.593%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.896526/  1.304182, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7004%\n",
      "layer   3  Sparsity: 76.8436%\n",
      "total_backward_count 1399970 real_backward_count 63859   4.561%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.894606/  1.311974, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7418%\n",
      "layer   3  Sparsity: 76.9201%\n",
      "total_backward_count 1409760 real_backward_count 63869   4.530%\n",
      "lif layer 2 self.abs_max_v: 2418.5\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.899339/  1.302724, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8504%\n",
      "layer   3  Sparsity: 77.0409%\n",
      "total_backward_count 1419550 real_backward_count 63905   4.502%\n",
      "lif layer 2 self.abs_max_v: 2430.5\n",
      "lif layer 2 self.abs_max_v: 2431.0\n",
      "lif layer 2 self.abs_max_v: 2454.0\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.897503/  1.299511, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7671%\n",
      "layer   3  Sparsity: 76.8431%\n",
      "total_backward_count 1429340 real_backward_count 63926   4.472%\n",
      "lif layer 2 self.abs_max_v: 2457.5\n",
      "lif layer 2 self.abs_max_v: 2499.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.897481/  1.308013, val:  75.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6579%\n",
      "layer   3  Sparsity: 76.7311%\n",
      "total_backward_count 1439130 real_backward_count 63957   4.444%\n",
      "lif layer 2 self.abs_max_v: 2505.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.900742/  1.300654, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7485%\n",
      "layer   3  Sparsity: 76.7382%\n",
      "total_backward_count 1448920 real_backward_count 63983   4.416%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.892389/  1.296416, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7663%\n",
      "layer   3  Sparsity: 76.7783%\n",
      "total_backward_count 1458710 real_backward_count 64010   4.388%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.886453/  1.302062, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8689%\n",
      "layer   3  Sparsity: 76.9641%\n",
      "total_backward_count 1468500 real_backward_count 64035   4.361%\n",
      "lif layer 2 self.abs_max_v: 2538.5\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.884530/  1.287916, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9031%\n",
      "layer   3  Sparsity: 76.9751%\n",
      "total_backward_count 1478290 real_backward_count 64058   4.333%\n",
      "lif layer 2 self.abs_max_v: 2543.5\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.881740/  1.288672, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8761%\n",
      "layer   3  Sparsity: 76.8792%\n",
      "total_backward_count 1488080 real_backward_count 64078   4.306%\n",
      "lif layer 2 self.abs_max_v: 2551.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.882194/  1.307420, val:  75.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8881%\n",
      "layer   3  Sparsity: 76.7073%\n",
      "total_backward_count 1497870 real_backward_count 64098   4.279%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.883455/  1.299444, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8644%\n",
      "layer   3  Sparsity: 76.8333%\n",
      "total_backward_count 1507660 real_backward_count 64114   4.253%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.887083/  1.286408, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9689%\n",
      "layer   3  Sparsity: 77.0282%\n",
      "total_backward_count 1517450 real_backward_count 64126   4.226%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.883967/  1.287234, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9621%\n",
      "layer   3  Sparsity: 76.9162%\n",
      "total_backward_count 1527240 real_backward_count 64138   4.200%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.881313/  1.285975, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8642%\n",
      "layer   3  Sparsity: 76.8843%\n",
      "total_backward_count 1537030 real_backward_count 64157   4.174%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.878434/  1.296859, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7757%\n",
      "layer   3  Sparsity: 76.9548%\n",
      "total_backward_count 1546820 real_backward_count 64170   4.149%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.880345/  1.291608, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8796%\n",
      "layer   3  Sparsity: 76.9645%\n",
      "total_backward_count 1556610 real_backward_count 64196   4.124%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.879157/  1.288391, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.9037%\n",
      "layer   3  Sparsity: 77.0022%\n",
      "total_backward_count 1566400 real_backward_count 64230   4.100%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.880122/  1.290950, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7635%\n",
      "layer   3  Sparsity: 76.9144%\n",
      "total_backward_count 1576190 real_backward_count 64253   4.076%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.884443/  1.298241, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8719%\n",
      "layer   3  Sparsity: 76.9612%\n",
      "total_backward_count 1585980 real_backward_count 64294   4.054%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.878497/  1.287544, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6926%\n",
      "layer   3  Sparsity: 77.0938%\n",
      "total_backward_count 1595770 real_backward_count 64314   4.030%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.880123/  1.301227, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6342%\n",
      "layer   3  Sparsity: 77.1338%\n",
      "total_backward_count 1605560 real_backward_count 64345   4.008%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.877259/  1.307373, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6538%\n",
      "layer   3  Sparsity: 77.1454%\n",
      "total_backward_count 1615350 real_backward_count 64365   3.985%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.881409/  1.296715, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7392%\n",
      "layer   3  Sparsity: 77.1157%\n",
      "total_backward_count 1625140 real_backward_count 64388   3.962%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.876230/  1.298734, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7478%\n",
      "layer   3  Sparsity: 77.2412%\n",
      "total_backward_count 1634930 real_backward_count 64402   3.939%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.873445/  1.299126, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8579%\n",
      "layer   3  Sparsity: 77.2521%\n",
      "total_backward_count 1644720 real_backward_count 64415   3.916%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.873805/  1.296101, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8500%\n",
      "layer   3  Sparsity: 77.2656%\n",
      "total_backward_count 1654510 real_backward_count 64426   3.894%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.872247/  1.296558, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8153%\n",
      "layer   3  Sparsity: 77.2473%\n",
      "total_backward_count 1664300 real_backward_count 64435   3.872%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.872160/  1.307885, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8384%\n",
      "layer   3  Sparsity: 77.1848%\n",
      "total_backward_count 1674090 real_backward_count 64450   3.850%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.869750/  1.294391, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8949%\n",
      "layer   3  Sparsity: 77.1608%\n",
      "total_backward_count 1683880 real_backward_count 64465   3.828%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.875811/  1.306318, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8904%\n",
      "layer   3  Sparsity: 77.1088%\n",
      "total_backward_count 1693670 real_backward_count 64485   3.807%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.874409/  1.294450, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8974%\n",
      "layer   3  Sparsity: 76.9787%\n",
      "total_backward_count 1703460 real_backward_count 64508   3.787%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.873405/  1.285170, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8931%\n",
      "layer   3  Sparsity: 76.9304%\n",
      "total_backward_count 1713250 real_backward_count 64531   3.767%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.877486/  1.291777, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8587%\n",
      "layer   3  Sparsity: 76.9434%\n",
      "total_backward_count 1723040 real_backward_count 64544   3.746%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.869377/  1.296305, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7871%\n",
      "layer   3  Sparsity: 77.0861%\n",
      "total_backward_count 1732830 real_backward_count 64574   3.727%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.871995/  1.281321, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7453%\n",
      "layer   3  Sparsity: 76.9466%\n",
      "total_backward_count 1742620 real_backward_count 64594   3.707%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.868710/  1.283365, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8872%\n",
      "layer   3  Sparsity: 76.9194%\n",
      "total_backward_count 1752410 real_backward_count 64610   3.687%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.876210/  1.283861, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7400%\n",
      "layer   3  Sparsity: 76.7623%\n",
      "total_backward_count 1762200 real_backward_count 64625   3.667%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.874810/  1.284939, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6431%\n",
      "layer   3  Sparsity: 76.8655%\n",
      "total_backward_count 1771990 real_backward_count 64637   3.648%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.867483/  1.269635, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7167%\n",
      "layer   3  Sparsity: 76.9314%\n",
      "total_backward_count 1781780 real_backward_count 64656   3.629%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.862403/  1.269796, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7479%\n",
      "layer   3  Sparsity: 76.8904%\n",
      "total_backward_count 1791570 real_backward_count 64667   3.610%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.866381/  1.274886, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8150%\n",
      "layer   3  Sparsity: 76.8599%\n",
      "total_backward_count 1801360 real_backward_count 64684   3.591%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.867213/  1.280628, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.8043%\n",
      "layer   3  Sparsity: 76.9380%\n",
      "total_backward_count 1811150 real_backward_count 64698   3.572%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.867728/  1.281949, val:  78.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7885%\n",
      "layer   3  Sparsity: 76.7874%\n",
      "total_backward_count 1820940 real_backward_count 64713   3.554%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.864783/  1.289312, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7032%\n",
      "layer   3  Sparsity: 76.9474%\n",
      "total_backward_count 1830730 real_backward_count 64728   3.536%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.862675/  1.288540, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6576%\n",
      "layer   3  Sparsity: 77.0524%\n",
      "total_backward_count 1840520 real_backward_count 64744   3.518%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.861720/  1.293217, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6382%\n",
      "layer   3  Sparsity: 77.0661%\n",
      "total_backward_count 1850310 real_backward_count 64755   3.500%\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.858441/  1.289126, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6744%\n",
      "layer   3  Sparsity: 77.1662%\n",
      "total_backward_count 1860100 real_backward_count 64775   3.482%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.852963/  1.283987, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7074%\n",
      "layer   3  Sparsity: 77.2000%\n",
      "total_backward_count 1869890 real_backward_count 64784   3.465%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.854237/  1.287823, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7367%\n",
      "layer   3  Sparsity: 77.2716%\n",
      "total_backward_count 1879680 real_backward_count 64795   3.447%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.856220/  1.287262, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.7304%\n",
      "layer   3  Sparsity: 77.3553%\n",
      "total_backward_count 1889470 real_backward_count 64813   3.430%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.860785/  1.289320, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6661%\n",
      "layer   3  Sparsity: 77.1455%\n",
      "total_backward_count 1899260 real_backward_count 64831   3.413%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.863937/  1.282524, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6917%\n",
      "layer   3  Sparsity: 76.8962%\n",
      "total_backward_count 1909050 real_backward_count 64850   3.397%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.864810/  1.284382, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.6546%\n",
      "layer   3  Sparsity: 77.0737%\n",
      "total_backward_count 1918840 real_backward_count 64880   3.381%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.865556/  1.279559, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5819%\n",
      "layer   3  Sparsity: 77.1688%\n",
      "total_backward_count 1928630 real_backward_count 64890   3.365%\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.866574/  1.284009, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5263%\n",
      "layer   3  Sparsity: 77.1843%\n",
      "total_backward_count 1938420 real_backward_count 64897   3.348%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.861120/  1.280803, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5582%\n",
      "layer   3  Sparsity: 77.2295%\n",
      "total_backward_count 1948210 real_backward_count 64904   3.331%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.859875/  1.282194, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9740%\n",
      "layer   2  Sparsity: 76.5810%\n",
      "layer   3  Sparsity: 77.2519%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef0004c5b6f43919114086b31c43aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.85988</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.80833</td></tr><tr><td>val_loss</td><td>1.28219</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-163</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y1s9u6e9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y1s9u6e9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_100507-y1s9u6e9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gks8o3z3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_142310-gks8o3z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gks8o3z3' target=\"_blank\">fresh-sweep-168</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gks8o3z3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gks8o3z3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251118_142319_929', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 434.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 589.0\n",
      "lif layer 1 self.abs_max_v: 728.0\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 1001.0\n",
      "fc layer 1 self.abs_max_out: 697.0\n",
      "lif layer 1 self.abs_max_v: 1113.5\n",
      "lif layer 1 self.abs_max_v: 1240.0\n",
      "fc layer 1 self.abs_max_out: 750.0\n",
      "fc layer 1 self.abs_max_out: 763.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 942.0\n",
      "fc layer 2 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 317.5\n",
      "lif layer 1 self.abs_max_v: 1381.5\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "lif layer 2 self.abs_max_v: 403.0\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "lif layer 2 self.abs_max_v: 552.5\n",
      "fc layer 2 self.abs_max_out: 442.0\n",
      "fc layer 1 self.abs_max_out: 1328.0\n",
      "lif layer 1 self.abs_max_v: 1798.0\n",
      "fc layer 1 self.abs_max_out: 1453.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 603.5\n",
      "lif layer 1 self.abs_max_v: 1922.0\n",
      "lif layer 2 self.abs_max_v: 705.0\n",
      "lif layer 2 self.abs_max_v: 747.5\n",
      "lif layer 1 self.abs_max_v: 1936.5\n",
      "fc layer 2 self.abs_max_out: 615.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "lif layer 2 self.abs_max_v: 886.5\n",
      "lif layer 1 self.abs_max_v: 2009.0\n",
      "fc layer 2 self.abs_max_out: 679.0\n",
      "fc layer 1 self.abs_max_out: 1482.0\n",
      "lif layer 2 self.abs_max_v: 990.5\n",
      "fc layer 1 self.abs_max_out: 1533.0\n",
      "fc layer 2 self.abs_max_out: 697.0\n",
      "lif layer 2 self.abs_max_v: 1192.5\n",
      "fc layer 1 self.abs_max_out: 1569.0\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "lif layer 2 self.abs_max_v: 1218.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 1679.0\n",
      "fc layer 1 self.abs_max_out: 1760.0\n",
      "lif layer 1 self.abs_max_v: 2201.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "fc layer 1 self.abs_max_out: 1957.0\n",
      "lif layer 1 self.abs_max_v: 2419.0\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "fc layer 2 self.abs_max_out: 1046.0\n",
      "lif layer 2 self.abs_max_v: 1228.5\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "lif layer 2 self.abs_max_v: 1350.0\n",
      "fc layer 1 self.abs_max_out: 2031.0\n",
      "lif layer 1 self.abs_max_v: 2486.5\n",
      "fc layer 1 self.abs_max_out: 2322.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "lif layer 2 self.abs_max_v: 1369.0\n",
      "lif layer 2 self.abs_max_v: 1402.0\n",
      "lif layer 2 self.abs_max_v: 1409.5\n",
      "lif layer 1 self.abs_max_v: 2586.0\n",
      "fc layer 1 self.abs_max_out: 2435.0\n",
      "lif layer 2 self.abs_max_v: 1537.0\n",
      "fc layer 1 self.abs_max_out: 2571.0\n",
      "fc layer 2 self.abs_max_out: 1272.0\n",
      "lif layer 2 self.abs_max_v: 1768.5\n",
      "lif layer 2 self.abs_max_v: 1808.0\n",
      "fc layer 1 self.abs_max_out: 2766.0\n",
      "lif layer 1 self.abs_max_v: 2905.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 1876.5\n",
      "fc layer 1 self.abs_max_out: 3650.0\n",
      "lif layer 1 self.abs_max_v: 3650.0\n",
      "fc layer 2 self.abs_max_out: 1352.0\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "lif layer 2 self.abs_max_v: 1946.5\n",
      "fc layer 1 self.abs_max_out: 3687.0\n",
      "lif layer 1 self.abs_max_v: 3687.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "lif layer 2 self.abs_max_v: 2064.0\n",
      "lif layer 2 self.abs_max_v: 2283.5\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 2 self.abs_max_out: 1892.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "lif layer 2 self.abs_max_v: 2377.5\n",
      "lif layer 2 self.abs_max_v: 2521.5\n",
      "lif layer 2 self.abs_max_v: 2687.5\n",
      "fc layer 2 self.abs_max_out: 1993.0\n",
      "lif layer 2 self.abs_max_v: 2876.0\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "lif layer 2 self.abs_max_v: 2923.5\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "fc layer 2 self.abs_max_out: 2563.0\n",
      "fc layer 1 self.abs_max_out: 4119.0\n",
      "lif layer 1 self.abs_max_v: 4119.0\n",
      "fc layer 1 self.abs_max_out: 4285.0\n",
      "lif layer 1 self.abs_max_v: 4285.0\n",
      "lif layer 2 self.abs_max_v: 3020.5\n",
      "lif layer 2 self.abs_max_v: 3103.0\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "lif layer 2 self.abs_max_v: 3219.5\n",
      "fc layer 1 self.abs_max_out: 4986.0\n",
      "lif layer 1 self.abs_max_v: 4986.0\n",
      "fc layer 2 self.abs_max_out: 2569.0\n",
      "lif layer 2 self.abs_max_v: 3468.5\n",
      "fc layer 1 self.abs_max_out: 5311.0\n",
      "lif layer 1 self.abs_max_v: 5311.0\n",
      "fc layer 2 self.abs_max_out: 2691.0\n",
      "fc layer 2 self.abs_max_out: 2828.0\n",
      "fc layer 2 self.abs_max_out: 2835.0\n",
      "fc layer 2 self.abs_max_out: 2870.0\n",
      "fc layer 2 self.abs_max_out: 2973.0\n",
      "fc layer 2 self.abs_max_out: 3016.0\n",
      "fc layer 2 self.abs_max_out: 3024.0\n",
      "fc layer 2 self.abs_max_out: 3026.0\n",
      "fc layer 2 self.abs_max_out: 3223.0\n",
      "fc layer 2 self.abs_max_out: 3301.0\n",
      "fc layer 1 self.abs_max_out: 5340.0\n",
      "lif layer 1 self.abs_max_v: 5340.0\n",
      "fc layer 1 self.abs_max_out: 5539.0\n",
      "lif layer 1 self.abs_max_v: 5951.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "fc layer 1 self.abs_max_out: 5709.0\n",
      "fc layer 1 self.abs_max_out: 5843.0\n",
      "fc layer 1 self.abs_max_out: 6138.0\n",
      "lif layer 1 self.abs_max_v: 6138.0\n",
      "fc layer 1 self.abs_max_out: 6234.0\n",
      "lif layer 1 self.abs_max_v: 6234.0\n",
      "fc layer 2 self.abs_max_out: 3355.0\n",
      "fc layer 2 self.abs_max_out: 3381.0\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "lif layer 2 self.abs_max_v: 3825.0\n",
      "lif layer 2 self.abs_max_v: 4274.0\n",
      "fc layer 2 self.abs_max_out: 3419.0\n",
      "fc layer 1 self.abs_max_out: 6530.0\n",
      "lif layer 1 self.abs_max_v: 6530.0\n",
      "fc layer 1 self.abs_max_out: 7133.0\n",
      "lif layer 1 self.abs_max_v: 7133.0\n",
      "fc layer 3 self.abs_max_out: 620.0\n",
      "fc layer 1 self.abs_max_out: 7342.0\n",
      "lif layer 1 self.abs_max_v: 7342.0\n",
      "fc layer 1 self.abs_max_out: 7630.0\n",
      "lif layer 1 self.abs_max_v: 7630.0\n",
      "lif layer 2 self.abs_max_v: 4325.0\n",
      "fc layer 1 self.abs_max_out: 8077.0\n",
      "lif layer 1 self.abs_max_v: 8077.0\n",
      "fc layer 2 self.abs_max_out: 3522.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.229924/  2.213741, val:  47.92%, val_best:  47.92%, tr:  49.74%, tr_best:  49.74%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 89.0780%\n",
      "layer   3  Sparsity: 95.2170%\n",
      "total_backward_count 9790 real_backward_count 6320  64.556%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3553.0\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 2 self.abs_max_out: 3566.0\n",
      "fc layer 2 self.abs_max_out: 3703.0\n",
      "fc layer 1 self.abs_max_out: 8328.0\n",
      "lif layer 1 self.abs_max_v: 8328.0\n",
      "fc layer 2 self.abs_max_out: 4044.0\n",
      "fc layer 1 self.abs_max_out: 8394.0\n",
      "lif layer 1 self.abs_max_v: 8394.0\n",
      "fc layer 2 self.abs_max_out: 4186.0\n",
      "fc layer 1 self.abs_max_out: 8451.0\n",
      "lif layer 1 self.abs_max_v: 8451.0\n",
      "fc layer 1 self.abs_max_out: 9047.0\n",
      "lif layer 1 self.abs_max_v: 9047.0\n",
      "fc layer 2 self.abs_max_out: 4240.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.183011/  2.201667, val:  46.67%, val_best:  47.92%, tr:  77.73%, tr_best:  77.73%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0042%\n",
      "layer   2  Sparsity: 86.7461%\n",
      "layer   3  Sparsity: 92.1775%\n",
      "total_backward_count 19580 real_backward_count 10653  54.408%\n",
      "fc layer 1 self.abs_max_out: 9454.0\n",
      "lif layer 1 self.abs_max_v: 9454.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.180181/  2.218635, val:  46.67%, val_best:  47.92%, tr:  84.07%, tr_best:  84.07%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0101%\n",
      "layer   2  Sparsity: 85.8749%\n",
      "layer   3  Sparsity: 91.0854%\n",
      "total_backward_count 29370 real_backward_count 14353  48.870%\n",
      "fc layer 1 self.abs_max_out: 9528.0\n",
      "lif layer 1 self.abs_max_v: 9528.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.177732/  2.204830, val:  47.08%, val_best:  47.92%, tr:  88.05%, tr_best:  88.05%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0035%\n",
      "layer   2  Sparsity: 85.1791%\n",
      "layer   3  Sparsity: 89.9865%\n",
      "total_backward_count 39160 real_backward_count 17779  45.401%\n",
      "fc layer 1 self.abs_max_out: 9988.0\n",
      "lif layer 1 self.abs_max_v: 9988.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.180597/  2.210603, val:  42.50%, val_best:  47.92%, tr:  91.11%, tr_best:  91.11%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0066%\n",
      "layer   2  Sparsity: 85.0269%\n",
      "layer   3  Sparsity: 89.6511%\n",
      "total_backward_count 48950 real_backward_count 20950  42.799%\n",
      "fc layer 2 self.abs_max_out: 4397.0\n",
      "lif layer 2 self.abs_max_v: 4397.0\n",
      "fc layer 1 self.abs_max_out: 10916.0\n",
      "lif layer 1 self.abs_max_v: 10916.0\n",
      "lif layer 2 self.abs_max_v: 4527.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.184313/  2.218425, val:  51.25%, val_best:  51.25%, tr:  92.75%, tr_best:  92.75%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0000%\n",
      "layer   2  Sparsity: 84.6989%\n",
      "layer   3  Sparsity: 89.1082%\n",
      "total_backward_count 58740 real_backward_count 23870  40.637%\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.188425/  2.219777, val:  58.33%, val_best:  58.33%, tr:  93.77%, tr_best:  93.77%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0203%\n",
      "layer   2  Sparsity: 84.6479%\n",
      "layer   3  Sparsity: 88.7962%\n",
      "total_backward_count 68530 real_backward_count 26692  38.949%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.185002/  2.214687, val:  51.67%, val_best:  58.33%, tr:  94.38%, tr_best:  94.38%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0016%\n",
      "layer   2  Sparsity: 84.2329%\n",
      "layer   3  Sparsity: 88.4259%\n",
      "total_backward_count 78320 real_backward_count 29308  37.421%\n",
      "fc layer 2 self.abs_max_out: 4437.0\n",
      "fc layer 2 self.abs_max_out: 4439.0\n",
      "fc layer 2 self.abs_max_out: 4709.0\n",
      "lif layer 2 self.abs_max_v: 4709.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.190266/  2.215013, val:  57.92%, val_best:  58.33%, tr:  94.79%, tr_best:  94.79%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0119%\n",
      "layer   2  Sparsity: 84.1784%\n",
      "layer   3  Sparsity: 88.2752%\n",
      "total_backward_count 88110 real_backward_count 31968  36.282%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.190742/  2.223540, val:  51.67%, val_best:  58.33%, tr:  96.12%, tr_best:  96.12%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9921%\n",
      "layer   2  Sparsity: 84.0792%\n",
      "layer   3  Sparsity: 88.2681%\n",
      "total_backward_count 97900 real_backward_count 34428  35.166%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.192734/  2.228639, val:  49.58%, val_best:  58.33%, tr:  97.04%, tr_best:  97.04%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9956%\n",
      "layer   2  Sparsity: 83.8753%\n",
      "layer   3  Sparsity: 88.2822%\n",
      "total_backward_count 107690 real_backward_count 36909  34.273%\n",
      "fc layer 2 self.abs_max_out: 4832.0\n",
      "lif layer 2 self.abs_max_v: 4832.0\n",
      "fc layer 2 self.abs_max_out: 4918.0\n",
      "lif layer 2 self.abs_max_v: 4918.0\n",
      "fc layer 2 self.abs_max_out: 4920.0\n",
      "lif layer 2 self.abs_max_v: 4920.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.197712/  2.216260, val:  63.75%, val_best:  63.75%, tr:  96.73%, tr_best:  97.04%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 83.5148%\n",
      "layer   3  Sparsity: 87.9368%\n",
      "total_backward_count 117480 real_backward_count 39360  33.504%\n",
      "fc layer 2 self.abs_max_out: 4999.0\n",
      "lif layer 2 self.abs_max_v: 4999.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.197921/  2.224242, val:  45.00%, val_best:  63.75%, tr:  97.24%, tr_best:  97.24%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0169%\n",
      "layer   2  Sparsity: 83.5463%\n",
      "layer   3  Sparsity: 88.1521%\n",
      "total_backward_count 127270 real_backward_count 41691  32.758%\n",
      "fc layer 1 self.abs_max_out: 11315.0\n",
      "lif layer 1 self.abs_max_v: 11315.0\n",
      "fc layer 1 self.abs_max_out: 13263.0\n",
      "lif layer 1 self.abs_max_v: 13263.0\n",
      "fc layer 2 self.abs_max_out: 5038.0\n",
      "lif layer 2 self.abs_max_v: 5038.0\n",
      "fc layer 2 self.abs_max_out: 5125.0\n",
      "lif layer 2 self.abs_max_v: 5125.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.200288/  2.224695, val:  44.58%, val_best:  63.75%, tr:  97.24%, tr_best:  97.24%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0152%\n",
      "layer   2  Sparsity: 83.4716%\n",
      "layer   3  Sparsity: 88.0213%\n",
      "total_backward_count 137060 real_backward_count 44035  32.128%\n",
      "fc layer 1 self.abs_max_out: 13283.0\n",
      "lif layer 1 self.abs_max_v: 13283.0\n",
      "fc layer 2 self.abs_max_out: 5179.0\n",
      "lif layer 2 self.abs_max_v: 5179.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.196592/  2.226718, val:  43.75%, val_best:  63.75%, tr:  97.34%, tr_best:  97.34%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0167%\n",
      "layer   2  Sparsity: 83.4057%\n",
      "layer   3  Sparsity: 87.9539%\n",
      "total_backward_count 146850 real_backward_count 46306  31.533%\n",
      "fc layer 2 self.abs_max_out: 5216.0\n",
      "lif layer 2 self.abs_max_v: 5216.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.201176/  2.232126, val:  51.67%, val_best:  63.75%, tr:  97.65%, tr_best:  97.65%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9920%\n",
      "layer   2  Sparsity: 83.3620%\n",
      "layer   3  Sparsity: 87.8746%\n",
      "total_backward_count 156640 real_backward_count 48574  31.010%\n",
      "fc layer 1 self.abs_max_out: 13516.0\n",
      "lif layer 1 self.abs_max_v: 13516.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.203801/  2.228352, val:  61.67%, val_best:  63.75%, tr:  97.55%, tr_best:  97.65%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0048%\n",
      "layer   2  Sparsity: 83.3217%\n",
      "layer   3  Sparsity: 88.2010%\n",
      "total_backward_count 166430 real_backward_count 50827  30.540%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.205637/  2.227108, val:  71.25%, val_best:  71.25%, tr:  97.75%, tr_best:  97.75%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0057%\n",
      "layer   2  Sparsity: 83.3593%\n",
      "layer   3  Sparsity: 88.3713%\n",
      "total_backward_count 176220 real_backward_count 53146  30.159%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.203959/  2.226991, val:  61.25%, val_best:  71.25%, tr:  97.85%, tr_best:  97.85%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9975%\n",
      "layer   2  Sparsity: 83.3577%\n",
      "layer   3  Sparsity: 88.1492%\n",
      "total_backward_count 186010 real_backward_count 55383  29.774%\n",
      "fc layer 1 self.abs_max_out: 13717.0\n",
      "lif layer 1 self.abs_max_v: 13717.0\n",
      "fc layer 2 self.abs_max_out: 5287.0\n",
      "lif layer 2 self.abs_max_v: 5287.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.201468/  2.223068, val:  43.33%, val_best:  71.25%, tr:  98.37%, tr_best:  98.37%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9997%\n",
      "layer   2  Sparsity: 83.0975%\n",
      "layer   3  Sparsity: 87.6990%\n",
      "total_backward_count 195800 real_backward_count 57455  29.344%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.200601/  2.235161, val:  51.25%, val_best:  71.25%, tr:  98.16%, tr_best:  98.37%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9951%\n",
      "layer   2  Sparsity: 82.9962%\n",
      "layer   3  Sparsity: 88.0373%\n",
      "total_backward_count 205590 real_backward_count 59611  28.995%\n",
      "fc layer 2 self.abs_max_out: 5335.0\n",
      "lif layer 2 self.abs_max_v: 5335.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.205757/  2.229215, val:  57.50%, val_best:  71.25%, tr:  97.96%, tr_best:  98.37%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9890%\n",
      "layer   2  Sparsity: 82.9260%\n",
      "layer   3  Sparsity: 88.0325%\n",
      "total_backward_count 215380 real_backward_count 61820  28.703%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.207913/  2.230268, val:  57.08%, val_best:  71.25%, tr:  97.96%, tr_best:  98.37%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9960%\n",
      "layer   2  Sparsity: 82.9906%\n",
      "layer   3  Sparsity: 88.4236%\n",
      "total_backward_count 225170 real_backward_count 64013  28.429%\n",
      "fc layer 1 self.abs_max_out: 13957.0\n",
      "lif layer 1 self.abs_max_v: 13957.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.209151/  2.228359, val:  70.00%, val_best:  71.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0115%\n",
      "layer   2  Sparsity: 83.0404%\n",
      "layer   3  Sparsity: 88.3500%\n",
      "total_backward_count 234960 real_backward_count 66174  28.164%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.204853/  2.226354, val:  61.67%, val_best:  71.25%, tr:  98.06%, tr_best:  98.77%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9852%\n",
      "layer   2  Sparsity: 82.8123%\n",
      "layer   3  Sparsity: 88.1764%\n",
      "total_backward_count 244750 real_backward_count 68251  27.886%\n",
      "fc layer 1 self.abs_max_out: 13999.0\n",
      "lif layer 1 self.abs_max_v: 13999.0\n",
      "fc layer 2 self.abs_max_out: 5355.0\n",
      "lif layer 2 self.abs_max_v: 5355.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.204873/  2.223200, val:  68.75%, val_best:  71.25%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9926%\n",
      "layer   2  Sparsity: 82.7616%\n",
      "layer   3  Sparsity: 87.9751%\n",
      "total_backward_count 254540 real_backward_count 70459  27.681%\n",
      "fc layer 1 self.abs_max_out: 14114.0\n",
      "lif layer 1 self.abs_max_v: 14114.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.205136/  2.231233, val:  75.00%, val_best:  75.00%, tr:  97.65%, tr_best:  98.77%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9977%\n",
      "layer   2  Sparsity: 82.7207%\n",
      "layer   3  Sparsity: 88.0737%\n",
      "total_backward_count 264330 real_backward_count 72549  27.446%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.208673/  2.230017, val:  55.42%, val_best:  75.00%, tr:  98.57%, tr_best:  98.77%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 82.7006%\n",
      "layer   3  Sparsity: 88.0285%\n",
      "total_backward_count 274120 real_backward_count 74686  27.246%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.208411/  2.234875, val:  52.08%, val_best:  75.00%, tr:  98.57%, tr_best:  98.77%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0136%\n",
      "layer   2  Sparsity: 82.7120%\n",
      "layer   3  Sparsity: 88.1208%\n",
      "total_backward_count 283910 real_backward_count 76684  27.010%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.209358/  2.230430, val:  54.58%, val_best:  75.00%, tr:  97.96%, tr_best:  98.77%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0134%\n",
      "layer   2  Sparsity: 82.6676%\n",
      "layer   3  Sparsity: 87.9208%\n",
      "total_backward_count 293700 real_backward_count 78696  26.795%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.210205/  2.230850, val:  70.42%, val_best:  75.00%, tr:  98.67%, tr_best:  98.77%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 82.5448%\n",
      "layer   3  Sparsity: 87.9235%\n",
      "total_backward_count 303490 real_backward_count 80770  26.614%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.209615/  2.233315, val:  46.67%, val_best:  75.00%, tr:  98.57%, tr_best:  98.77%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0110%\n",
      "layer   2  Sparsity: 82.5169%\n",
      "layer   3  Sparsity: 87.9634%\n",
      "total_backward_count 313280 real_backward_count 82818  26.436%\n",
      "fc layer 1 self.abs_max_out: 14290.0\n",
      "lif layer 1 self.abs_max_v: 14290.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.207762/  2.234077, val:  49.58%, val_best:  75.00%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9918%\n",
      "layer   2  Sparsity: 82.5500%\n",
      "layer   3  Sparsity: 87.8161%\n",
      "total_backward_count 323070 real_backward_count 84857  26.266%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.210071/  2.230262, val:  60.42%, val_best:  75.00%, tr:  98.47%, tr_best:  98.77%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0060%\n",
      "layer   2  Sparsity: 82.5173%\n",
      "layer   3  Sparsity: 87.9158%\n",
      "total_backward_count 332860 real_backward_count 86844  26.090%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.204296/  2.228687, val:  62.50%, val_best:  75.00%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0124%\n",
      "layer   2  Sparsity: 82.2954%\n",
      "layer   3  Sparsity: 87.6213%\n",
      "total_backward_count 342650 real_backward_count 88830  25.924%\n",
      "fc layer 1 self.abs_max_out: 14421.0\n",
      "lif layer 1 self.abs_max_v: 14421.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.206766/  2.228182, val:  56.67%, val_best:  75.00%, tr:  98.37%, tr_best:  98.88%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0161%\n",
      "layer   2  Sparsity: 82.3522%\n",
      "layer   3  Sparsity: 88.0444%\n",
      "total_backward_count 352440 real_backward_count 90805  25.765%\n",
      "fc layer 1 self.abs_max_out: 14463.0\n",
      "lif layer 1 self.abs_max_v: 14463.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.205986/  2.227349, val:  68.33%, val_best:  75.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0046%\n",
      "layer   2  Sparsity: 82.3858%\n",
      "layer   3  Sparsity: 88.2025%\n",
      "total_backward_count 362230 real_backward_count 92734  25.601%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.204845/  2.228584, val:  61.67%, val_best:  75.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0011%\n",
      "layer   2  Sparsity: 82.2868%\n",
      "layer   3  Sparsity: 88.4726%\n",
      "total_backward_count 372020 real_backward_count 94590  25.426%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.204505/  2.231050, val:  61.67%, val_best:  75.00%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 82.3429%\n",
      "layer   3  Sparsity: 88.2943%\n",
      "total_backward_count 381810 real_backward_count 96511  25.277%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.206589/  2.225263, val:  64.58%, val_best:  75.00%, tr:  99.18%, tr_best:  99.39%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0199%\n",
      "layer   2  Sparsity: 82.2865%\n",
      "layer   3  Sparsity: 88.1334%\n",
      "total_backward_count 391600 real_backward_count 98403  25.128%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.205601/  2.227764, val:  67.08%, val_best:  75.00%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9917%\n",
      "layer   2  Sparsity: 82.2753%\n",
      "layer   3  Sparsity: 87.9539%\n",
      "total_backward_count 401390 real_backward_count 100336  24.997%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.208709/  2.233129, val:  67.08%, val_best:  75.00%, tr:  98.57%, tr_best:  99.39%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 82.2463%\n",
      "layer   3  Sparsity: 88.0813%\n",
      "total_backward_count 411180 real_backward_count 102199  24.855%\n",
      "fc layer 1 self.abs_max_out: 14730.0\n",
      "lif layer 1 self.abs_max_v: 14730.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.207350/  2.230427, val:  57.50%, val_best:  75.00%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0092%\n",
      "layer   2  Sparsity: 82.2427%\n",
      "layer   3  Sparsity: 88.0952%\n",
      "total_backward_count 420970 real_backward_count 104081  24.724%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.208891/  2.232861, val:  72.08%, val_best:  75.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0120%\n",
      "layer   2  Sparsity: 82.2007%\n",
      "layer   3  Sparsity: 88.0722%\n",
      "total_backward_count 430760 real_backward_count 105985  24.604%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.208738/  2.232579, val:  70.42%, val_best:  75.00%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0221%\n",
      "layer   2  Sparsity: 82.1393%\n",
      "layer   3  Sparsity: 87.9622%\n",
      "total_backward_count 440550 real_backward_count 107823  24.475%\n",
      "fc layer 1 self.abs_max_out: 14791.0\n",
      "lif layer 1 self.abs_max_v: 14791.0\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.209360/  2.229727, val:  67.92%, val_best:  75.00%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 82.0911%\n",
      "layer   3  Sparsity: 88.0017%\n",
      "total_backward_count 450340 real_backward_count 109634  24.345%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.211786/  2.233706, val:  63.75%, val_best:  75.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9894%\n",
      "layer   2  Sparsity: 82.1887%\n",
      "layer   3  Sparsity: 88.0683%\n",
      "total_backward_count 460130 real_backward_count 111501  24.232%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.207484/  2.234511, val:  62.08%, val_best:  75.00%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0072%\n",
      "layer   2  Sparsity: 82.2475%\n",
      "layer   3  Sparsity: 88.0101%\n",
      "total_backward_count 469920 real_backward_count 113350  24.121%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.209406/  2.233527, val:  78.33%, val_best:  78.33%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9955%\n",
      "layer   2  Sparsity: 82.0497%\n",
      "layer   3  Sparsity: 88.3618%\n",
      "total_backward_count 479710 real_backward_count 115175  24.009%\n",
      "fc layer 1 self.abs_max_out: 14864.0\n",
      "lif layer 1 self.abs_max_v: 14864.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.209956/  2.230728, val:  76.67%, val_best:  78.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0036%\n",
      "layer   2  Sparsity: 82.1470%\n",
      "layer   3  Sparsity: 88.2128%\n",
      "total_backward_count 489500 real_backward_count 117030  23.908%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.207417/  2.230772, val:  70.83%, val_best:  78.33%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0045%\n",
      "layer   2  Sparsity: 82.1957%\n",
      "layer   3  Sparsity: 88.1733%\n",
      "total_backward_count 499290 real_backward_count 118853  23.804%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.208947/  2.232949, val:  66.25%, val_best:  78.33%, tr:  99.08%, tr_best:  99.39%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0112%\n",
      "layer   2  Sparsity: 82.0864%\n",
      "layer   3  Sparsity: 88.4272%\n",
      "total_backward_count 509080 real_backward_count 120611  23.692%\n",
      "fc layer 1 self.abs_max_out: 14913.0\n",
      "lif layer 1 self.abs_max_v: 14913.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.206606/  2.231679, val:  73.75%, val_best:  78.33%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9967%\n",
      "layer   2  Sparsity: 82.0340%\n",
      "layer   3  Sparsity: 88.2241%\n",
      "total_backward_count 518870 real_backward_count 122460  23.601%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.207079/  2.228233, val:  72.08%, val_best:  78.33%, tr:  98.77%, tr_best:  99.39%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 82.0301%\n",
      "layer   3  Sparsity: 88.0755%\n",
      "total_backward_count 528660 real_backward_count 124259  23.505%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.207037/  2.225131, val:  75.83%, val_best:  78.33%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0020%\n",
      "layer   2  Sparsity: 81.9354%\n",
      "layer   3  Sparsity: 87.9969%\n",
      "total_backward_count 538450 real_backward_count 126096  23.418%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.206562/  2.231011, val:  74.58%, val_best:  78.33%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9952%\n",
      "layer   2  Sparsity: 81.7608%\n",
      "layer   3  Sparsity: 87.7130%\n",
      "total_backward_count 548240 real_backward_count 127887  23.327%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.205095/  2.231270, val:  71.67%, val_best:  78.33%, tr:  98.67%, tr_best:  99.39%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0023%\n",
      "layer   2  Sparsity: 81.7681%\n",
      "layer   3  Sparsity: 87.5680%\n",
      "total_backward_count 558030 real_backward_count 129649  23.233%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.206790/  2.229416, val:  64.58%, val_best:  78.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0143%\n",
      "layer   2  Sparsity: 81.8694%\n",
      "layer   3  Sparsity: 87.7269%\n",
      "total_backward_count 567820 real_backward_count 131404  23.142%\n",
      "fc layer 1 self.abs_max_out: 14956.0\n",
      "lif layer 1 self.abs_max_v: 14956.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.204480/  2.226666, val:  72.50%, val_best:  78.33%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 81.8894%\n",
      "layer   3  Sparsity: 88.1760%\n",
      "total_backward_count 577610 real_backward_count 133103  23.044%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.204878/  2.227046, val:  61.67%, val_best:  78.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0128%\n",
      "layer   2  Sparsity: 81.8677%\n",
      "layer   3  Sparsity: 87.9099%\n",
      "total_backward_count 587400 real_backward_count 134871  22.961%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.205547/  2.227539, val:  58.33%, val_best:  78.33%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 81.9305%\n",
      "layer   3  Sparsity: 88.2261%\n",
      "total_backward_count 597190 real_backward_count 136612  22.876%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.206167/  2.227847, val:  65.42%, val_best:  78.33%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9995%\n",
      "layer   2  Sparsity: 81.8931%\n",
      "layer   3  Sparsity: 88.0553%\n",
      "total_backward_count 606980 real_backward_count 138399  22.801%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.203456/  2.233852, val:  54.58%, val_best:  78.33%, tr:  98.67%, tr_best:  99.49%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9998%\n",
      "layer   2  Sparsity: 81.8891%\n",
      "layer   3  Sparsity: 88.0604%\n",
      "total_backward_count 616770 real_backward_count 140141  22.722%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.206177/  2.228447, val:  72.92%, val_best:  78.33%, tr:  98.98%, tr_best:  99.49%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9953%\n",
      "layer   2  Sparsity: 81.8013%\n",
      "layer   3  Sparsity: 87.8427%\n",
      "total_backward_count 626560 real_backward_count 141847  22.639%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.204208/  2.226852, val:  74.58%, val_best:  78.33%, tr:  98.88%, tr_best:  99.49%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0188%\n",
      "layer   2  Sparsity: 81.8397%\n",
      "layer   3  Sparsity: 87.9132%\n",
      "total_backward_count 636350 real_backward_count 143593  22.565%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.204239/  2.230116, val:  70.83%, val_best:  78.33%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0034%\n",
      "layer   2  Sparsity: 81.7577%\n",
      "layer   3  Sparsity: 88.0778%\n",
      "total_backward_count 646140 real_backward_count 145345  22.494%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.204834/  2.226399, val:  75.00%, val_best:  78.33%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 81.7413%\n",
      "layer   3  Sparsity: 87.7127%\n",
      "total_backward_count 655930 real_backward_count 147092  22.425%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.201716/  2.227560, val:  69.17%, val_best:  78.33%, tr:  98.88%, tr_best:  99.49%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 81.8187%\n",
      "layer   3  Sparsity: 87.7374%\n",
      "total_backward_count 665720 real_backward_count 148836  22.357%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.203310/  2.229423, val:  68.33%, val_best:  78.33%, tr:  99.39%, tr_best:  99.49%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9912%\n",
      "layer   2  Sparsity: 81.6878%\n",
      "layer   3  Sparsity: 87.5982%\n",
      "total_backward_count 675510 real_backward_count 150558  22.288%\n",
      "fc layer 1 self.abs_max_out: 15098.0\n",
      "lif layer 1 self.abs_max_v: 15098.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.200005/  2.229254, val:  67.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.49%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0120%\n",
      "layer   2  Sparsity: 81.5294%\n",
      "layer   3  Sparsity: 87.5689%\n",
      "total_backward_count 685300 real_backward_count 152292  22.223%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.204605/  2.227963, val:  71.67%, val_best:  78.33%, tr:  98.57%, tr_best:  99.49%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0206%\n",
      "layer   2  Sparsity: 81.6140%\n",
      "layer   3  Sparsity: 87.7842%\n",
      "total_backward_count 695090 real_backward_count 153995  22.155%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.202850/  2.224313, val:  76.25%, val_best:  78.33%, tr:  98.88%, tr_best:  99.49%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9999%\n",
      "layer   2  Sparsity: 81.6160%\n",
      "layer   3  Sparsity: 87.8945%\n",
      "total_backward_count 704880 real_backward_count 155700  22.089%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.203001/  2.221397, val:  77.08%, val_best:  78.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0090%\n",
      "layer   2  Sparsity: 81.7678%\n",
      "layer   3  Sparsity: 87.6562%\n",
      "total_backward_count 714670 real_backward_count 157390  22.023%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.199067/  2.221334, val:  69.17%, val_best:  78.33%, tr:  99.28%, tr_best:  99.49%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 81.7176%\n",
      "layer   3  Sparsity: 87.5793%\n",
      "total_backward_count 724460 real_backward_count 159009  21.949%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.198264/  2.226700, val:  73.33%, val_best:  78.33%, tr:  98.77%, tr_best:  99.49%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0068%\n",
      "layer   2  Sparsity: 81.7761%\n",
      "layer   3  Sparsity: 87.4800%\n",
      "total_backward_count 734250 real_backward_count 160721  21.889%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.200059/  2.219714, val:  81.25%, val_best:  81.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 81.6633%\n",
      "layer   3  Sparsity: 87.7978%\n",
      "total_backward_count 744040 real_backward_count 162357  21.821%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.199943/  2.219118, val:  78.33%, val_best:  81.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0127%\n",
      "layer   2  Sparsity: 81.6218%\n",
      "layer   3  Sparsity: 87.5274%\n",
      "total_backward_count 753830 real_backward_count 164086  21.767%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.196958/  2.226918, val:  65.00%, val_best:  81.25%, tr:  99.08%, tr_best:  99.49%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9901%\n",
      "layer   2  Sparsity: 81.6179%\n",
      "layer   3  Sparsity: 87.4960%\n",
      "total_backward_count 763620 real_backward_count 165776  21.709%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.199736/  2.222026, val:  72.50%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9943%\n",
      "layer   2  Sparsity: 81.6125%\n",
      "layer   3  Sparsity: 87.3287%\n",
      "total_backward_count 773410 real_backward_count 167443  21.650%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.197878/  2.224295, val:  68.75%, val_best:  81.25%, tr:  99.39%, tr_best:  99.59%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0022%\n",
      "layer   2  Sparsity: 81.6017%\n",
      "layer   3  Sparsity: 87.5735%\n",
      "total_backward_count 783200 real_backward_count 169163  21.599%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.197424/  2.219282, val:  83.75%, val_best:  83.75%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0038%\n",
      "layer   2  Sparsity: 81.5969%\n",
      "layer   3  Sparsity: 87.8195%\n",
      "total_backward_count 792990 real_backward_count 170878  21.549%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.196175/  2.222885, val:  59.58%, val_best:  83.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9940%\n",
      "layer   2  Sparsity: 81.5984%\n",
      "layer   3  Sparsity: 87.6953%\n",
      "total_backward_count 802780 real_backward_count 172492  21.487%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.198264/  2.221813, val:  69.17%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0126%\n",
      "layer   2  Sparsity: 81.7244%\n",
      "layer   3  Sparsity: 87.5365%\n",
      "total_backward_count 812570 real_backward_count 174127  21.429%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.196762/  2.225317, val:  68.33%, val_best:  83.75%, tr:  98.98%, tr_best:  99.59%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9955%\n",
      "layer   2  Sparsity: 81.7047%\n",
      "layer   3  Sparsity: 87.4619%\n",
      "total_backward_count 822360 real_backward_count 175784  21.376%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.202004/  2.221632, val:  69.58%, val_best:  83.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0139%\n",
      "layer   2  Sparsity: 81.6427%\n",
      "layer   3  Sparsity: 87.6828%\n",
      "total_backward_count 832150 real_backward_count 177413  21.320%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.201056/  2.224970, val:  73.33%, val_best:  83.75%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0189%\n",
      "layer   2  Sparsity: 81.6888%\n",
      "layer   3  Sparsity: 87.5533%\n",
      "total_backward_count 841940 real_backward_count 178989  21.259%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.196115/  2.223720, val:  67.50%, val_best:  83.75%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0089%\n",
      "layer   2  Sparsity: 81.5547%\n",
      "layer   3  Sparsity: 87.3605%\n",
      "total_backward_count 851730 real_backward_count 180591  21.203%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.196671/  2.221141, val:  82.50%, val_best:  83.75%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0047%\n",
      "layer   2  Sparsity: 81.5709%\n",
      "layer   3  Sparsity: 87.7246%\n",
      "total_backward_count 861520 real_backward_count 182182  21.147%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.198024/  2.221480, val:  74.17%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0068%\n",
      "layer   2  Sparsity: 81.5900%\n",
      "layer   3  Sparsity: 87.4710%\n",
      "total_backward_count 871310 real_backward_count 183805  21.095%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.194065/  2.215567, val:  80.00%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9883%\n",
      "layer   2  Sparsity: 81.5783%\n",
      "layer   3  Sparsity: 87.1003%\n",
      "total_backward_count 881100 real_backward_count 185447  21.047%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.195567/  2.223994, val:  68.75%, val_best:  83.75%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9873%\n",
      "layer   2  Sparsity: 81.4514%\n",
      "layer   3  Sparsity: 87.5115%\n",
      "total_backward_count 890890 real_backward_count 187020  20.992%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.193768/  2.217064, val:  77.92%, val_best:  83.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0023%\n",
      "layer   2  Sparsity: 81.3552%\n",
      "layer   3  Sparsity: 87.1991%\n",
      "total_backward_count 900680 real_backward_count 188607  20.941%\n",
      "lif layer 2 self.abs_max_v: 5369.0\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.197395/  2.218416, val:  74.17%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9895%\n",
      "layer   2  Sparsity: 81.4335%\n",
      "layer   3  Sparsity: 87.4628%\n",
      "total_backward_count 910470 real_backward_count 190237  20.894%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.199857/  2.225624, val:  73.75%, val_best:  83.75%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0218%\n",
      "layer   2  Sparsity: 81.5484%\n",
      "layer   3  Sparsity: 87.5900%\n",
      "total_backward_count 920260 real_backward_count 191830  20.845%\n",
      "lif layer 2 self.abs_max_v: 5419.0\n",
      "lif layer 2 self.abs_max_v: 5511.0\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.199813/  2.223001, val:  72.50%, val_best:  83.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 95.0031%\n",
      "layer   2  Sparsity: 81.5909%\n",
      "layer   3  Sparsity: 87.6772%\n",
      "total_backward_count 930050 real_backward_count 193471  20.802%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.198988/  2.219877, val:  76.67%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9968%\n",
      "layer   2  Sparsity: 81.5536%\n",
      "layer   3  Sparsity: 87.5785%\n",
      "total_backward_count 939840 real_backward_count 195092  20.758%\n",
      "lif layer 2 self.abs_max_v: 5646.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.196349/  2.224004, val:  76.67%, val_best:  83.75%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0109%\n",
      "layer   2  Sparsity: 81.4432%\n",
      "layer   3  Sparsity: 87.3357%\n",
      "total_backward_count 949630 real_backward_count 196653  20.708%\n",
      "lif layer 2 self.abs_max_v: 5841.0\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.197761/  2.220475, val:  82.50%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0045%\n",
      "layer   2  Sparsity: 81.3444%\n",
      "layer   3  Sparsity: 87.5078%\n",
      "total_backward_count 959420 real_backward_count 198254  20.664%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.196952/  2.225455, val:  66.25%, val_best:  83.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9992%\n",
      "layer   2  Sparsity: 81.4821%\n",
      "layer   3  Sparsity: 87.9253%\n",
      "total_backward_count 969210 real_backward_count 199835  20.618%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.199247/  2.219526, val:  77.92%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0047%\n",
      "layer   2  Sparsity: 81.4581%\n",
      "layer   3  Sparsity: 87.7902%\n",
      "total_backward_count 979000 real_backward_count 201378  20.570%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.197346/  2.218012, val:  76.67%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0185%\n",
      "layer   2  Sparsity: 81.5208%\n",
      "layer   3  Sparsity: 87.5084%\n",
      "total_backward_count 988790 real_backward_count 202898  20.520%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.199267/  2.223880, val:  78.33%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9982%\n",
      "layer   2  Sparsity: 81.5589%\n",
      "layer   3  Sparsity: 87.7839%\n",
      "total_backward_count 998580 real_backward_count 204440  20.473%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.202895/  2.227220, val:  67.50%, val_best:  83.75%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0070%\n",
      "layer   2  Sparsity: 81.4536%\n",
      "layer   3  Sparsity: 87.5744%\n",
      "total_backward_count 1008370 real_backward_count 205969  20.426%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.203382/  2.226532, val:  79.58%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9896%\n",
      "layer   2  Sparsity: 81.3432%\n",
      "layer   3  Sparsity: 87.5205%\n",
      "total_backward_count 1018160 real_backward_count 207502  20.380%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.200414/  2.221764, val:  80.00%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0007%\n",
      "layer   2  Sparsity: 81.3279%\n",
      "layer   3  Sparsity: 87.5445%\n",
      "total_backward_count 1027950 real_backward_count 209092  20.341%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.197895/  2.223642, val:  79.58%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9985%\n",
      "layer   2  Sparsity: 81.3037%\n",
      "layer   3  Sparsity: 87.4674%\n",
      "total_backward_count 1037740 real_backward_count 210652  20.299%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.197872/  2.222843, val:  76.25%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0040%\n",
      "layer   2  Sparsity: 81.4188%\n",
      "layer   3  Sparsity: 87.2969%\n",
      "total_backward_count 1047530 real_backward_count 212177  20.255%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.197759/  2.218791, val:  83.75%, val_best:  83.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9978%\n",
      "layer   2  Sparsity: 81.3584%\n",
      "layer   3  Sparsity: 87.4598%\n",
      "total_backward_count 1057320 real_backward_count 213780  20.219%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.195936/  2.220909, val:  79.17%, val_best:  83.75%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9842%\n",
      "layer   2  Sparsity: 81.2395%\n",
      "layer   3  Sparsity: 87.4090%\n",
      "total_backward_count 1067110 real_backward_count 215295  20.176%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.196456/  2.213724, val:  81.25%, val_best:  83.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0102%\n",
      "layer   2  Sparsity: 81.3948%\n",
      "layer   3  Sparsity: 87.5236%\n",
      "total_backward_count 1076900 real_backward_count 216837  20.135%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.193831/  2.217036, val:  75.00%, val_best:  83.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0078%\n",
      "layer   2  Sparsity: 81.3014%\n",
      "layer   3  Sparsity: 87.0642%\n",
      "total_backward_count 1086690 real_backward_count 218356  20.094%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.195144/  2.220314, val:  79.17%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9959%\n",
      "layer   2  Sparsity: 81.2758%\n",
      "layer   3  Sparsity: 87.2103%\n",
      "total_backward_count 1096480 real_backward_count 219876  20.053%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.193914/  2.222376, val:  81.67%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9900%\n",
      "layer   2  Sparsity: 81.3792%\n",
      "layer   3  Sparsity: 87.1735%\n",
      "total_backward_count 1106270 real_backward_count 221410  20.014%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.193028/  2.217931, val:  78.33%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0038%\n",
      "layer   2  Sparsity: 81.4585%\n",
      "layer   3  Sparsity: 87.3804%\n",
      "total_backward_count 1116060 real_backward_count 222884  19.971%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.195507/  2.219319, val:  75.00%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0104%\n",
      "layer   2  Sparsity: 81.5140%\n",
      "layer   3  Sparsity: 87.4570%\n",
      "total_backward_count 1125850 real_backward_count 224361  19.928%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.196246/  2.217306, val:  84.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0156%\n",
      "layer   2  Sparsity: 81.4508%\n",
      "layer   3  Sparsity: 87.4864%\n",
      "total_backward_count 1135640 real_backward_count 225909  19.893%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.194823/  2.219480, val:  73.75%, val_best:  84.58%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0163%\n",
      "layer   2  Sparsity: 81.4622%\n",
      "layer   3  Sparsity: 87.4195%\n",
      "total_backward_count 1145430 real_backward_count 227416  19.854%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.195920/  2.217537, val:  84.17%, val_best:  84.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0000%\n",
      "layer   2  Sparsity: 81.4310%\n",
      "layer   3  Sparsity: 87.4417%\n",
      "total_backward_count 1155220 real_backward_count 228920  19.816%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.196820/  2.217054, val:  81.67%, val_best:  84.58%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 81.3615%\n",
      "layer   3  Sparsity: 87.3380%\n",
      "total_backward_count 1165010 real_backward_count 230491  19.784%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.196347/  2.220718, val:  79.17%, val_best:  84.58%, tr:  98.88%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0073%\n",
      "layer   2  Sparsity: 81.4465%\n",
      "layer   3  Sparsity: 87.6575%\n",
      "total_backward_count 1174800 real_backward_count 231994  19.748%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.198300/  2.219075, val:  79.58%, val_best:  84.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0042%\n",
      "layer   2  Sparsity: 81.4563%\n",
      "layer   3  Sparsity: 87.4928%\n",
      "total_backward_count 1184590 real_backward_count 233482  19.710%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.195824/  2.223310, val:  74.17%, val_best:  84.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 94.9971%\n",
      "layer   2  Sparsity: 81.5012%\n",
      "layer   3  Sparsity: 87.5355%\n",
      "total_backward_count 1194380 real_backward_count 235052  19.680%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.196268/  2.218713, val:  79.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 68.61 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 95.0114%\n",
      "layer   2  Sparsity: 81.4592%\n",
      "layer   3  Sparsity: 87.6037%\n",
      "total_backward_count 1204170 real_backward_count 236601  19.648%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.198801/  2.218565, val:  82.50%, val_best:  84.58%, tr:  98.98%, tr_best:  99.59%, epoch time: 68.86 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 95.0221%\n",
      "layer   2  Sparsity: 81.4313%\n",
      "layer   3  Sparsity: 87.7012%\n",
      "total_backward_count 1213960 real_backward_count 238111  19.614%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.193828/  2.217416, val:  77.08%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 68.92 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 95.0009%\n",
      "layer   2  Sparsity: 81.4052%\n",
      "layer   3  Sparsity: 87.5998%\n",
      "total_backward_count 1223750 real_backward_count 239587  19.578%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.194476/  2.220630, val:  76.67%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 94.9957%\n",
      "layer   2  Sparsity: 81.3177%\n",
      "layer   3  Sparsity: 87.3005%\n",
      "total_backward_count 1233540 real_backward_count 241024  19.539%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.195676/  2.220023, val:  81.25%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 69.52 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 95.0025%\n",
      "layer   2  Sparsity: 81.4821%\n",
      "layer   3  Sparsity: 87.4407%\n",
      "total_backward_count 1243330 real_backward_count 242509  19.505%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.194159/  2.222911, val:  77.08%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 94.9997%\n",
      "layer   2  Sparsity: 81.5353%\n",
      "layer   3  Sparsity: 87.6039%\n",
      "total_backward_count 1253120 real_backward_count 243957  19.468%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.195193/  2.218436, val:  80.00%, val_best:  84.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 95.0015%\n",
      "layer   2  Sparsity: 81.4365%\n",
      "layer   3  Sparsity: 87.3772%\n",
      "total_backward_count 1262910 real_backward_count 245377  19.429%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.195220/  2.222386, val:  80.00%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 81.3760%\n",
      "layer   3  Sparsity: 87.2856%\n",
      "total_backward_count 1272700 real_backward_count 246856  19.396%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.196939/  2.218643, val:  62.50%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0067%\n",
      "layer   2  Sparsity: 81.3834%\n",
      "layer   3  Sparsity: 87.1913%\n",
      "total_backward_count 1282490 real_backward_count 248259  19.358%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.196041/  2.221205, val:  75.42%, val_best:  84.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0122%\n",
      "layer   2  Sparsity: 81.4355%\n",
      "layer   3  Sparsity: 87.3055%\n",
      "total_backward_count 1292280 real_backward_count 249718  19.324%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.195884/  2.218029, val:  74.17%, val_best:  84.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0125%\n",
      "layer   2  Sparsity: 81.4515%\n",
      "layer   3  Sparsity: 87.1992%\n",
      "total_backward_count 1302070 real_backward_count 251190  19.292%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.193054/  2.216719, val:  79.17%, val_best:  84.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0041%\n",
      "layer   2  Sparsity: 81.4312%\n",
      "layer   3  Sparsity: 87.6041%\n",
      "total_backward_count 1311860 real_backward_count 252703  19.263%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.192030/  2.218315, val:  74.58%, val_best:  84.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0122%\n",
      "layer   2  Sparsity: 81.5075%\n",
      "layer   3  Sparsity: 87.5498%\n",
      "total_backward_count 1321650 real_backward_count 254174  19.232%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.193948/  2.217137, val:  80.83%, val_best:  84.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9959%\n",
      "layer   2  Sparsity: 81.3625%\n",
      "layer   3  Sparsity: 87.4801%\n",
      "total_backward_count 1331440 real_backward_count 255648  19.201%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.190346/  2.215072, val:  75.42%, val_best:  84.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0077%\n",
      "layer   2  Sparsity: 81.4208%\n",
      "layer   3  Sparsity: 87.1532%\n",
      "total_backward_count 1341230 real_backward_count 257065  19.166%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.193740/  2.222826, val:  84.17%, val_best:  84.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9879%\n",
      "layer   2  Sparsity: 81.4818%\n",
      "layer   3  Sparsity: 87.4833%\n",
      "total_backward_count 1351020 real_backward_count 258545  19.137%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.197703/  2.217748, val:  82.50%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9909%\n",
      "layer   2  Sparsity: 81.4534%\n",
      "layer   3  Sparsity: 87.4596%\n",
      "total_backward_count 1360810 real_backward_count 260051  19.110%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.194810/  2.218508, val:  79.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0127%\n",
      "layer   2  Sparsity: 81.4222%\n",
      "layer   3  Sparsity: 87.2660%\n",
      "total_backward_count 1370600 real_backward_count 261563  19.084%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.193707/  2.217305, val:  75.83%, val_best:  84.58%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0134%\n",
      "layer   2  Sparsity: 81.4237%\n",
      "layer   3  Sparsity: 87.4241%\n",
      "total_backward_count 1380390 real_backward_count 262985  19.051%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.193913/  2.218366, val:  78.33%, val_best:  84.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9976%\n",
      "layer   2  Sparsity: 81.4796%\n",
      "layer   3  Sparsity: 87.4795%\n",
      "total_backward_count 1390180 real_backward_count 264409  19.020%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.191313/  2.214108, val:  74.58%, val_best:  84.58%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0090%\n",
      "layer   2  Sparsity: 81.4345%\n",
      "layer   3  Sparsity: 87.6050%\n",
      "total_backward_count 1399970 real_backward_count 265848  18.990%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.192960/  2.215650, val:  80.42%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0244%\n",
      "layer   2  Sparsity: 81.4424%\n",
      "layer   3  Sparsity: 87.5902%\n",
      "total_backward_count 1409760 real_backward_count 267310  18.961%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.194375/  2.217094, val:  82.50%, val_best:  84.58%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0158%\n",
      "layer   2  Sparsity: 81.3192%\n",
      "layer   3  Sparsity: 87.8468%\n",
      "total_backward_count 1419550 real_backward_count 268731  18.931%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.196307/  2.216348, val:  75.83%, val_best:  84.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9974%\n",
      "layer   2  Sparsity: 81.3904%\n",
      "layer   3  Sparsity: 87.8859%\n",
      "total_backward_count 1429340 real_backward_count 270173  18.902%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.198226/  2.220646, val:  81.67%, val_best:  84.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 81.3370%\n",
      "layer   3  Sparsity: 87.7877%\n",
      "total_backward_count 1439130 real_backward_count 271600  18.873%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.196437/  2.219976, val:  77.92%, val_best:  84.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0190%\n",
      "layer   2  Sparsity: 81.3235%\n",
      "layer   3  Sparsity: 87.4985%\n",
      "total_backward_count 1448920 real_backward_count 273010  18.842%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.194422/  2.219038, val:  80.00%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0169%\n",
      "layer   2  Sparsity: 81.3388%\n",
      "layer   3  Sparsity: 87.3335%\n",
      "total_backward_count 1458710 real_backward_count 274413  18.812%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.194141/  2.216297, val:  75.42%, val_best:  84.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 81.4078%\n",
      "layer   3  Sparsity: 87.3260%\n",
      "total_backward_count 1468500 real_backward_count 275841  18.784%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.194077/  2.219353, val:  80.42%, val_best:  84.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0057%\n",
      "layer   2  Sparsity: 81.3663%\n",
      "layer   3  Sparsity: 87.2989%\n",
      "total_backward_count 1478290 real_backward_count 277235  18.754%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.193253/  2.215635, val:  86.67%, val_best:  86.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9886%\n",
      "layer   2  Sparsity: 81.2758%\n",
      "layer   3  Sparsity: 87.5990%\n",
      "total_backward_count 1488080 real_backward_count 278650  18.725%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.193599/  2.220051, val:  73.33%, val_best:  86.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0111%\n",
      "layer   2  Sparsity: 81.2630%\n",
      "layer   3  Sparsity: 87.4846%\n",
      "total_backward_count 1497870 real_backward_count 280038  18.696%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.195891/  2.217278, val:  86.25%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9732%\n",
      "layer   2  Sparsity: 81.3196%\n",
      "layer   3  Sparsity: 87.3132%\n",
      "total_backward_count 1507660 real_backward_count 281435  18.667%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.195716/  2.219029, val:  82.08%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9915%\n",
      "layer   2  Sparsity: 81.2208%\n",
      "layer   3  Sparsity: 87.3986%\n",
      "total_backward_count 1517450 real_backward_count 282903  18.643%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.196035/  2.217879, val:  71.67%, val_best:  86.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0053%\n",
      "layer   2  Sparsity: 81.3473%\n",
      "layer   3  Sparsity: 87.5682%\n",
      "total_backward_count 1527240 real_backward_count 284318  18.616%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.193953/  2.215761, val:  81.25%, val_best:  86.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9867%\n",
      "layer   2  Sparsity: 81.2742%\n",
      "layer   3  Sparsity: 87.5169%\n",
      "total_backward_count 1537030 real_backward_count 285690  18.587%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.190967/  2.218969, val:  77.50%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0083%\n",
      "layer   2  Sparsity: 81.2685%\n",
      "layer   3  Sparsity: 87.2285%\n",
      "total_backward_count 1546820 real_backward_count 287081  18.559%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.193409/  2.217907, val:  79.17%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9985%\n",
      "layer   2  Sparsity: 81.2138%\n",
      "layer   3  Sparsity: 87.3447%\n",
      "total_backward_count 1556610 real_backward_count 288458  18.531%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.193370/  2.211162, val:  81.25%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0004%\n",
      "layer   2  Sparsity: 81.1702%\n",
      "layer   3  Sparsity: 87.3117%\n",
      "total_backward_count 1566400 real_backward_count 289839  18.504%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.188918/  2.215557, val:  74.17%, val_best:  86.67%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0054%\n",
      "layer   2  Sparsity: 81.2270%\n",
      "layer   3  Sparsity: 87.2298%\n",
      "total_backward_count 1576190 real_backward_count 291205  18.475%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.191303/  2.210863, val:  80.83%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9983%\n",
      "layer   2  Sparsity: 81.2397%\n",
      "layer   3  Sparsity: 87.0385%\n",
      "total_backward_count 1585980 real_backward_count 292617  18.450%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.189139/  2.211592, val:  79.17%, val_best:  86.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9948%\n",
      "layer   2  Sparsity: 81.2262%\n",
      "layer   3  Sparsity: 87.1802%\n",
      "total_backward_count 1595770 real_backward_count 293984  18.423%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.188781/  2.218803, val:  81.25%, val_best:  86.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9878%\n",
      "layer   2  Sparsity: 81.1949%\n",
      "layer   3  Sparsity: 87.4030%\n",
      "total_backward_count 1605560 real_backward_count 295354  18.396%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.189815/  2.215213, val:  75.00%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9946%\n",
      "layer   2  Sparsity: 81.1844%\n",
      "layer   3  Sparsity: 87.3465%\n",
      "total_backward_count 1615350 real_backward_count 296720  18.369%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.190903/  2.215877, val:  77.50%, val_best:  86.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9989%\n",
      "layer   2  Sparsity: 81.2655%\n",
      "layer   3  Sparsity: 87.3655%\n",
      "total_backward_count 1625140 real_backward_count 298177  18.348%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.191748/  2.216640, val:  75.42%, val_best:  86.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9998%\n",
      "layer   2  Sparsity: 81.3034%\n",
      "layer   3  Sparsity: 87.5484%\n",
      "total_backward_count 1634930 real_backward_count 299550  18.322%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.190976/  2.216011, val:  77.92%, val_best:  86.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0173%\n",
      "layer   2  Sparsity: 81.2453%\n",
      "layer   3  Sparsity: 87.2116%\n",
      "total_backward_count 1644720 real_backward_count 300922  18.296%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.187961/  2.212109, val:  81.25%, val_best:  86.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9902%\n",
      "layer   2  Sparsity: 81.1941%\n",
      "layer   3  Sparsity: 87.0631%\n",
      "total_backward_count 1654510 real_backward_count 302276  18.270%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.189070/  2.211378, val:  83.33%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0158%\n",
      "layer   2  Sparsity: 81.2814%\n",
      "layer   3  Sparsity: 87.1877%\n",
      "total_backward_count 1664300 real_backward_count 303686  18.247%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.187699/  2.214602, val:  62.50%, val_best:  86.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9892%\n",
      "layer   2  Sparsity: 81.3164%\n",
      "layer   3  Sparsity: 87.1338%\n",
      "total_backward_count 1674090 real_backward_count 304992  18.218%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.191062/  2.221766, val:  76.25%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9933%\n",
      "layer   2  Sparsity: 81.3400%\n",
      "layer   3  Sparsity: 87.2154%\n",
      "total_backward_count 1683880 real_backward_count 306386  18.195%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.190795/  2.211110, val:  82.50%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 81.4327%\n",
      "layer   3  Sparsity: 87.4650%\n",
      "total_backward_count 1693670 real_backward_count 307780  18.172%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.193128/  2.213846, val:  77.92%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0080%\n",
      "layer   2  Sparsity: 81.3327%\n",
      "layer   3  Sparsity: 87.3600%\n",
      "total_backward_count 1703460 real_backward_count 309213  18.152%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.190171/  2.211876, val:  82.08%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0131%\n",
      "layer   2  Sparsity: 81.2900%\n",
      "layer   3  Sparsity: 86.9724%\n",
      "total_backward_count 1713250 real_backward_count 310622  18.131%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.187018/  2.215417, val:  77.08%, val_best:  86.67%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9779%\n",
      "layer   2  Sparsity: 81.1643%\n",
      "layer   3  Sparsity: 87.1858%\n",
      "total_backward_count 1723040 real_backward_count 312043  18.110%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.185562/  2.205517, val:  77.92%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0107%\n",
      "layer   2  Sparsity: 81.3021%\n",
      "layer   3  Sparsity: 87.4729%\n",
      "total_backward_count 1732830 real_backward_count 313416  18.087%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.184391/  2.214803, val:  73.75%, val_best:  86.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0235%\n",
      "layer   2  Sparsity: 81.3885%\n",
      "layer   3  Sparsity: 87.1718%\n",
      "total_backward_count 1742620 real_backward_count 314815  18.066%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.184829/  2.208944, val:  70.42%, val_best:  86.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9957%\n",
      "layer   2  Sparsity: 81.2578%\n",
      "layer   3  Sparsity: 86.8217%\n",
      "total_backward_count 1752410 real_backward_count 316214  18.045%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.180120/  2.206587, val:  84.17%, val_best:  86.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9935%\n",
      "layer   2  Sparsity: 81.2977%\n",
      "layer   3  Sparsity: 87.1633%\n",
      "total_backward_count 1762200 real_backward_count 317613  18.024%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.182209/  2.211000, val:  85.42%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0212%\n",
      "layer   2  Sparsity: 81.2990%\n",
      "layer   3  Sparsity: 87.3776%\n",
      "total_backward_count 1771990 real_backward_count 318992  18.002%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.185380/  2.210740, val:  76.67%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0096%\n",
      "layer   2  Sparsity: 81.3163%\n",
      "layer   3  Sparsity: 87.2984%\n",
      "total_backward_count 1781780 real_backward_count 320346  17.979%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.185930/  2.210825, val:  85.83%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9910%\n",
      "layer   2  Sparsity: 81.2309%\n",
      "layer   3  Sparsity: 87.3031%\n",
      "total_backward_count 1791570 real_backward_count 321763  17.960%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.185656/  2.212934, val:  73.75%, val_best:  86.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9908%\n",
      "layer   2  Sparsity: 81.3129%\n",
      "layer   3  Sparsity: 87.4795%\n",
      "total_backward_count 1801360 real_backward_count 323162  17.940%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.189047/  2.212965, val:  77.50%, val_best:  86.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0030%\n",
      "layer   2  Sparsity: 81.2358%\n",
      "layer   3  Sparsity: 87.4197%\n",
      "total_backward_count 1811150 real_backward_count 324509  17.917%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.186222/  2.206898, val:  81.25%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 95.0077%\n",
      "layer   2  Sparsity: 81.2744%\n",
      "layer   3  Sparsity: 87.3380%\n",
      "total_backward_count 1820940 real_backward_count 325867  17.896%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.185560/  2.213054, val:  76.67%, val_best:  86.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0011%\n",
      "layer   2  Sparsity: 81.2204%\n",
      "layer   3  Sparsity: 87.2687%\n",
      "total_backward_count 1830730 real_backward_count 327169  17.871%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.187723/  2.214566, val:  80.42%, val_best:  86.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0129%\n",
      "layer   2  Sparsity: 81.1982%\n",
      "layer   3  Sparsity: 87.0992%\n",
      "total_backward_count 1840520 real_backward_count 328555  17.851%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.185234/  2.208710, val:  81.25%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0031%\n",
      "layer   2  Sparsity: 81.1525%\n",
      "layer   3  Sparsity: 87.0981%\n",
      "total_backward_count 1850310 real_backward_count 329921  17.831%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.187828/  2.208810, val:  81.67%, val_best:  86.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0153%\n",
      "layer   2  Sparsity: 81.2263%\n",
      "layer   3  Sparsity: 87.4000%\n",
      "total_backward_count 1860100 real_backward_count 331271  17.809%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.188984/  2.216050, val:  77.92%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0138%\n",
      "layer   2  Sparsity: 81.1888%\n",
      "layer   3  Sparsity: 87.1925%\n",
      "total_backward_count 1869890 real_backward_count 332588  17.787%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.188123/  2.210396, val:  67.50%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9956%\n",
      "layer   2  Sparsity: 81.1474%\n",
      "layer   3  Sparsity: 87.0784%\n",
      "total_backward_count 1879680 real_backward_count 333860  17.762%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.186959/  2.209374, val:  81.25%, val_best:  86.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0081%\n",
      "layer   2  Sparsity: 81.0920%\n",
      "layer   3  Sparsity: 87.3002%\n",
      "total_backward_count 1889470 real_backward_count 335192  17.740%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.185404/  2.205739, val:  78.75%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0104%\n",
      "layer   2  Sparsity: 81.1499%\n",
      "layer   3  Sparsity: 87.1131%\n",
      "total_backward_count 1899260 real_backward_count 336540  17.720%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.183696/  2.209221, val:  74.58%, val_best:  86.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0070%\n",
      "layer   2  Sparsity: 81.1626%\n",
      "layer   3  Sparsity: 87.0657%\n",
      "total_backward_count 1909050 real_backward_count 337889  17.699%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.188367/  2.208987, val:  79.58%, val_best:  86.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0074%\n",
      "layer   2  Sparsity: 81.1692%\n",
      "layer   3  Sparsity: 87.2015%\n",
      "total_backward_count 1918840 real_backward_count 339240  17.679%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.186869/  2.208457, val:  82.50%, val_best:  86.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0071%\n",
      "layer   2  Sparsity: 81.1765%\n",
      "layer   3  Sparsity: 87.0223%\n",
      "total_backward_count 1928630 real_backward_count 340590  17.660%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.187136/  2.209373, val:  70.42%, val_best:  86.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9880%\n",
      "layer   2  Sparsity: 81.1412%\n",
      "layer   3  Sparsity: 87.2002%\n",
      "total_backward_count 1938420 real_backward_count 341937  17.640%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.187278/  2.217116, val:  82.50%, val_best:  86.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 95.0133%\n",
      "layer   2  Sparsity: 81.1085%\n",
      "layer   3  Sparsity: 87.5855%\n",
      "total_backward_count 1948210 real_backward_count 343230  17.618%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.188951/  2.212843, val:  74.58%, val_best:  86.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0103%\n",
      "layer   2  Sparsity: 81.0021%\n",
      "layer   3  Sparsity: 87.2042%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc5ae04a94644d8b0a55ddd4bff4f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99591</td></tr><tr><td>tr_epoch_loss</td><td>2.18895</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.74583</td></tr><tr><td>val_loss</td><td>2.21284</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-168</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gks8o3z3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gks8o3z3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_142310-gks8o3z3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i1dc7g3k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_184123-i1dc7g3k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i1dc7g3k' target=\"_blank\">dark-sweep-171</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i1dc7g3k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i1dc7g3k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': True, 'unique_name': '20251118_184132_480', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 111.0\n",
      "lif layer 1 self.abs_max_v: 111.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 148.0\n",
      "lif layer 1 self.abs_max_v: 184.0\n",
      "fc layer 1 self.abs_max_out: 162.0\n",
      "lif layer 1 self.abs_max_v: 252.0\n",
      "fc layer 1 self.abs_max_out: 174.0\n",
      "fc layer 1 self.abs_max_out: 209.0\n",
      "lif layer 1 self.abs_max_v: 316.5\n",
      "fc layer 2 self.abs_max_out: 110.0\n",
      "lif layer 2 self.abs_max_v: 110.0\n",
      "fc layer 1 self.abs_max_out: 225.0\n",
      "lif layer 1 self.abs_max_v: 328.5\n",
      "fc layer 1 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "lif layer 1 self.abs_max_v: 367.0\n",
      "fc layer 2 self.abs_max_out: 116.0\n",
      "lif layer 2 self.abs_max_v: 119.0\n",
      "fc layer 1 self.abs_max_out: 433.0\n",
      "lif layer 1 self.abs_max_v: 553.0\n",
      "fc layer 2 self.abs_max_out: 175.0\n",
      "lif layer 2 self.abs_max_v: 186.0\n",
      "fc layer 1 self.abs_max_out: 798.0\n",
      "lif layer 1 self.abs_max_v: 798.0\n",
      "fc layer 2 self.abs_max_out: 213.0\n",
      "lif layer 2 self.abs_max_v: 279.5\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "lif layer 2 self.abs_max_v: 326.5\n",
      "lif layer 2 self.abs_max_v: 328.5\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 2 self.abs_max_out: 229.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 2 self.abs_max_out: 301.0\n",
      "fc layer 1 self.abs_max_out: 1056.0\n",
      "lif layer 1 self.abs_max_v: 1056.0\n",
      "fc layer 2 self.abs_max_out: 335.0\n",
      "lif layer 2 self.abs_max_v: 335.0\n",
      "lif layer 2 self.abs_max_v: 375.0\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 3 self.abs_max_out: 55.0\n",
      "lif layer 2 self.abs_max_v: 418.0\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "lif layer 2 self.abs_max_v: 488.5\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "fc layer 2 self.abs_max_out: 470.0\n",
      "fc layer 3 self.abs_max_out: 68.0\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "lif layer 2 self.abs_max_v: 610.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 644.0\n",
      "lif layer 2 self.abs_max_v: 644.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 690.5\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "fc layer 1 self.abs_max_out: 1165.0\n",
      "lif layer 1 self.abs_max_v: 1165.0\n",
      "fc layer 1 self.abs_max_out: 1508.0\n",
      "lif layer 1 self.abs_max_v: 1508.0\n",
      "fc layer 1 self.abs_max_out: 1625.0\n",
      "lif layer 1 self.abs_max_v: 1625.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "lif layer 2 self.abs_max_v: 747.5\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 751.0\n",
      "lif layer 2 self.abs_max_v: 751.0\n",
      "fc layer 2 self.abs_max_out: 759.0\n",
      "lif layer 2 self.abs_max_v: 759.0\n",
      "lif layer 2 self.abs_max_v: 786.5\n",
      "fc layer 2 self.abs_max_out: 789.0\n",
      "lif layer 2 self.abs_max_v: 789.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "lif layer 2 self.abs_max_v: 807.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "lif layer 2 self.abs_max_v: 809.0\n",
      "fc layer 2 self.abs_max_out: 868.0\n",
      "lif layer 2 self.abs_max_v: 868.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "lif layer 2 self.abs_max_v: 909.5\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "lif layer 2 self.abs_max_v: 958.0\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "lif layer 2 self.abs_max_v: 961.0\n",
      "fc layer 2 self.abs_max_out: 1010.0\n",
      "lif layer 2 self.abs_max_v: 1010.0\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 1051.5\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 1 self.abs_max_out: 1675.0\n",
      "lif layer 1 self.abs_max_v: 1675.0\n",
      "fc layer 2 self.abs_max_out: 1047.0\n",
      "lif layer 2 self.abs_max_v: 1075.0\n",
      "lif layer 2 self.abs_max_v: 1076.5\n",
      "lif layer 2 self.abs_max_v: 1090.5\n",
      "lif layer 2 self.abs_max_v: 1097.5\n",
      "fc layer 2 self.abs_max_out: 1057.0\n",
      "fc layer 1 self.abs_max_out: 1708.0\n",
      "lif layer 1 self.abs_max_v: 1708.0\n",
      "fc layer 1 self.abs_max_out: 1823.0\n",
      "lif layer 1 self.abs_max_v: 1823.0\n",
      "fc layer 2 self.abs_max_out: 1091.0\n",
      "fc layer 1 self.abs_max_out: 1862.0\n",
      "lif layer 1 self.abs_max_v: 1862.0\n",
      "fc layer 1 self.abs_max_out: 1896.0\n",
      "lif layer 1 self.abs_max_v: 1896.0\n",
      "fc layer 2 self.abs_max_out: 1095.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "lif layer 2 self.abs_max_v: 1106.0\n",
      "fc layer 1 self.abs_max_out: 2031.0\n",
      "lif layer 1 self.abs_max_v: 2031.0\n",
      "fc layer 1 self.abs_max_out: 2477.0\n",
      "lif layer 1 self.abs_max_v: 2477.0\n",
      "fc layer 2 self.abs_max_out: 1119.0\n",
      "lif layer 2 self.abs_max_v: 1119.0\n",
      "lif layer 2 self.abs_max_v: 1121.0\n",
      "lif layer 2 self.abs_max_v: 1206.5\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "lif layer 2 self.abs_max_v: 1293.0\n",
      "fc layer 2 self.abs_max_out: 1367.0\n",
      "lif layer 2 self.abs_max_v: 1367.0\n",
      "lif layer 2 self.abs_max_v: 1404.5\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "lif layer 2 self.abs_max_v: 1427.0\n",
      "lif layer 2 self.abs_max_v: 1444.5\n",
      "fc layer 3 self.abs_max_out: 232.0\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 1447.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "lif layer 2 self.abs_max_v: 1453.0\n",
      "fc layer 1 self.abs_max_out: 2488.0\n",
      "lif layer 1 self.abs_max_v: 2488.0\n",
      "fc layer 1 self.abs_max_out: 2699.0\n",
      "lif layer 1 self.abs_max_v: 2699.0\n",
      "lif layer 2 self.abs_max_v: 1673.5\n",
      "fc layer 2 self.abs_max_out: 1383.0\n",
      "fc layer 2 self.abs_max_out: 1416.0\n",
      "fc layer 2 self.abs_max_out: 1462.0\n",
      "fc layer 1 self.abs_max_out: 2875.0\n",
      "lif layer 1 self.abs_max_v: 2875.0\n",
      "fc layer 2 self.abs_max_out: 1501.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  2.033495/  2.117491, val:  29.17%, val_best:  29.17%, tr:  93.46%, tr_best:  93.46%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 85.0253%\n",
      "layer   3  Sparsity: 91.3777%\n",
      "total_backward_count 9790 real_backward_count 2764  28.233%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2930.0\n",
      "lif layer 1 self.abs_max_v: 2930.0\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 1 self.abs_max_out: 2966.0\n",
      "lif layer 1 self.abs_max_v: 2966.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1587.0\n",
      "lif layer 1 self.abs_max_v: 3069.5\n",
      "lif layer 1 self.abs_max_v: 3325.0\n",
      "fc layer 1 self.abs_max_out: 3047.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 1 self.abs_max_out: 3073.0\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "fc layer 3 self.abs_max_out: 315.0\n",
      "fc layer 2 self.abs_max_out: 1724.0\n",
      "lif layer 2 self.abs_max_v: 1724.0\n",
      "lif layer 1 self.abs_max_v: 3328.5\n",
      "lif layer 1 self.abs_max_v: 3539.5\n",
      "lif layer 1 self.abs_max_v: 3622.5\n",
      "lif layer 1 self.abs_max_v: 3843.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.886065/  2.051775, val:  36.67%, val_best:  36.67%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4487%\n",
      "layer   3  Sparsity: 88.4038%\n",
      "total_backward_count 19580 real_backward_count 4698  23.994%\n",
      "lif layer 2 self.abs_max_v: 1748.5\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 1754.0\n",
      "lif layer 2 self.abs_max_v: 1769.0\n",
      "fc layer 1 self.abs_max_out: 3284.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "lif layer 2 self.abs_max_v: 1927.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.840528/  2.058373, val:  39.58%, val_best:  39.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6183%\n",
      "layer   3  Sparsity: 87.7230%\n",
      "total_backward_count 29370 real_backward_count 6568  22.363%\n",
      "fc layer 2 self.abs_max_out: 1732.0\n",
      "fc layer 1 self.abs_max_out: 3377.0\n",
      "fc layer 1 self.abs_max_out: 3410.0\n",
      "lif layer 2 self.abs_max_v: 1943.0\n",
      "lif layer 2 self.abs_max_v: 1957.0\n",
      "fc layer 2 self.abs_max_out: 1777.0\n",
      "lif layer 2 self.abs_max_v: 2041.5\n",
      "fc layer 2 self.abs_max_out: 1819.0\n",
      "lif layer 2 self.abs_max_v: 2109.0\n",
      "fc layer 1 self.abs_max_out: 3435.0\n",
      "lif layer 1 self.abs_max_v: 3850.5\n",
      "fc layer 1 self.abs_max_out: 3443.0\n",
      "lif layer 1 self.abs_max_v: 3872.5\n",
      "lif layer 2 self.abs_max_v: 2121.5\n",
      "lif layer 2 self.abs_max_v: 2140.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.818696/  1.996475, val:  47.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4865%\n",
      "layer   3  Sparsity: 87.6353%\n",
      "total_backward_count 39160 real_backward_count 8348  21.318%\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "lif layer 1 self.abs_max_v: 4163.0\n",
      "fc layer 1 self.abs_max_out: 3512.0\n",
      "fc layer 2 self.abs_max_out: 1885.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.773443/  1.970071, val:  37.92%, val_best:  47.50%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3016%\n",
      "layer   3  Sparsity: 87.3743%\n",
      "total_backward_count 48950 real_backward_count 10021  20.472%\n",
      "lif layer 1 self.abs_max_v: 4469.0\n",
      "lif layer 2 self.abs_max_v: 2205.0\n",
      "lif layer 2 self.abs_max_v: 2324.5\n",
      "fc layer 1 self.abs_max_out: 3545.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.737748/  1.960682, val:  54.17%, val_best:  54.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4167%\n",
      "layer   3  Sparsity: 87.0128%\n",
      "total_backward_count 58740 real_backward_count 11639  19.814%\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 3587.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "lif layer 1 self.abs_max_v: 4727.5\n",
      "fc layer 1 self.abs_max_out: 3689.0\n",
      "lif layer 1 self.abs_max_v: 4795.0\n",
      "lif layer 1 self.abs_max_v: 5009.0\n",
      "lif layer 1 self.abs_max_v: 5413.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.708528/  1.923409, val:  50.42%, val_best:  54.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.1654%\n",
      "layer   3  Sparsity: 86.3767%\n",
      "total_backward_count 68530 real_backward_count 13212  19.279%\n",
      "fc layer 2 self.abs_max_out: 1894.0\n",
      "fc layer 1 self.abs_max_out: 3693.0\n",
      "lif layer 2 self.abs_max_v: 2458.5\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 1 self.abs_max_out: 3704.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.670387/  1.878917, val:  48.33%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.1483%\n",
      "layer   3  Sparsity: 86.2283%\n",
      "total_backward_count 78320 real_backward_count 14738  18.818%\n",
      "fc layer 1 self.abs_max_out: 3706.0\n",
      "fc layer 1 self.abs_max_out: 3709.0\n",
      "lif layer 2 self.abs_max_v: 2524.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.646773/  1.836360, val:  60.83%, val_best:  60.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4603%\n",
      "layer   3  Sparsity: 86.4570%\n",
      "total_backward_count 88110 real_backward_count 16306  18.506%\n",
      "fc layer 1 self.abs_max_out: 3766.0\n",
      "fc layer 2 self.abs_max_out: 1919.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "lif layer 1 self.abs_max_v: 5676.5\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.610996/  1.897571, val:  45.00%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6468%\n",
      "layer   3  Sparsity: 86.2975%\n",
      "total_backward_count 97900 real_backward_count 17764  18.145%\n",
      "fc layer 2 self.abs_max_out: 1939.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "fc layer 1 self.abs_max_out: 3794.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "fc layer 2 self.abs_max_out: 1940.0\n",
      "lif layer 2 self.abs_max_v: 2585.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.595911/  1.850017, val:  48.33%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3537%\n",
      "layer   3  Sparsity: 85.9525%\n",
      "total_backward_count 107690 real_backward_count 19180  17.810%\n",
      "lif layer 1 self.abs_max_v: 5723.0\n",
      "fc layer 1 self.abs_max_out: 3844.0\n",
      "lif layer 2 self.abs_max_v: 2721.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.592691/  1.810834, val:  63.33%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.1509%\n",
      "layer   3  Sparsity: 86.0349%\n",
      "total_backward_count 117480 real_backward_count 20565  17.505%\n",
      "fc layer 1 self.abs_max_out: 3854.0\n",
      "lif layer 1 self.abs_max_v: 5839.5\n",
      "lif layer 1 self.abs_max_v: 5915.5\n",
      "lif layer 1 self.abs_max_v: 6010.0\n",
      "lif layer 1 self.abs_max_v: 6543.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.529463/  1.773203, val:  54.17%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3550%\n",
      "layer   3  Sparsity: 85.9210%\n",
      "total_backward_count 127270 real_backward_count 21846  17.165%\n",
      "fc layer 1 self.abs_max_out: 3867.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.529253/  1.767044, val:  57.50%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6099%\n",
      "layer   3  Sparsity: 86.0774%\n",
      "total_backward_count 137060 real_backward_count 23124  16.871%\n",
      "fc layer 1 self.abs_max_out: 3928.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.524820/  1.726867, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3885%\n",
      "layer   3  Sparsity: 86.5089%\n",
      "total_backward_count 146850 real_backward_count 24353  16.584%\n",
      "fc layer 1 self.abs_max_out: 3983.0\n",
      "lif layer 2 self.abs_max_v: 2732.0\n",
      "lif layer 2 self.abs_max_v: 2762.0\n",
      "fc layer 2 self.abs_max_out: 1982.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.523419/  1.736155, val:  60.83%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3364%\n",
      "layer   3  Sparsity: 86.5740%\n",
      "total_backward_count 156640 real_backward_count 25546  16.309%\n",
      "fc layer 2 self.abs_max_out: 2053.0\n",
      "fc layer 1 self.abs_max_out: 4009.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.486131/  1.729240, val:  71.67%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6248%\n",
      "layer   3  Sparsity: 86.5862%\n",
      "total_backward_count 166430 real_backward_count 26655  16.016%\n",
      "lif layer 2 self.abs_max_v: 2947.5\n",
      "lif layer 2 self.abs_max_v: 2950.0\n",
      "fc layer 1 self.abs_max_out: 4014.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.496130/  1.733676, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5018%\n",
      "layer   3  Sparsity: 86.8806%\n",
      "total_backward_count 176220 real_backward_count 27674  15.704%\n",
      "fc layer 2 self.abs_max_out: 2070.0\n",
      "lif layer 2 self.abs_max_v: 3099.0\n",
      "lif layer 2 self.abs_max_v: 3159.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.507880/  1.723150, val:  70.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.1989%\n",
      "layer   3  Sparsity: 87.0124%\n",
      "total_backward_count 186010 real_backward_count 28700  15.429%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.493745/  1.736798, val:  66.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.2429%\n",
      "layer   3  Sparsity: 86.8377%\n",
      "total_backward_count 195800 real_backward_count 29686  15.161%\n",
      "fc layer 1 self.abs_max_out: 4017.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.467898/  1.729535, val:  67.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4242%\n",
      "layer   3  Sparsity: 86.6653%\n",
      "total_backward_count 205590 real_backward_count 30576  14.872%\n",
      "fc layer 1 self.abs_max_out: 4036.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.462231/  1.691981, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3630%\n",
      "layer   3  Sparsity: 86.7081%\n",
      "total_backward_count 215380 real_backward_count 31518  14.634%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.437069/  1.661602, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5400%\n",
      "layer   3  Sparsity: 86.8231%\n",
      "total_backward_count 225170 real_backward_count 32446  14.410%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.435817/  1.685014, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6223%\n",
      "layer   3  Sparsity: 86.9520%\n",
      "total_backward_count 234960 real_backward_count 33290  14.168%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.427550/  1.679990, val:  71.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6083%\n",
      "layer   3  Sparsity: 87.1338%\n",
      "total_backward_count 244750 real_backward_count 34108  13.936%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.445223/  1.651423, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6517%\n",
      "layer   3  Sparsity: 87.4476%\n",
      "total_backward_count 254540 real_backward_count 34945  13.729%\n",
      "lif layer 2 self.abs_max_v: 3163.5\n",
      "fc layer 1 self.abs_max_out: 4047.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.426997/  1.661067, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6157%\n",
      "layer   3  Sparsity: 87.4824%\n",
      "total_backward_count 264330 real_backward_count 35702  13.507%\n",
      "fc layer 1 self.abs_max_out: 4053.0\n",
      "lif layer 2 self.abs_max_v: 3279.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.440351/  1.665307, val:  80.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5423%\n",
      "layer   3  Sparsity: 87.2862%\n",
      "total_backward_count 274120 real_backward_count 36456  13.299%\n",
      "fc layer 1 self.abs_max_out: 4078.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.421242/  1.670346, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4445%\n",
      "layer   3  Sparsity: 87.6095%\n",
      "total_backward_count 283910 real_backward_count 37149  13.085%\n",
      "fc layer 1 self.abs_max_out: 4097.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.431877/  1.648176, val:  75.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3542%\n",
      "layer   3  Sparsity: 87.5898%\n",
      "total_backward_count 293700 real_backward_count 37870  12.894%\n",
      "fc layer 1 self.abs_max_out: 4103.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.439597/  1.673614, val:  76.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3673%\n",
      "layer   3  Sparsity: 87.6157%\n",
      "total_backward_count 303490 real_backward_count 38546  12.701%\n",
      "fc layer 1 self.abs_max_out: 4115.0\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.399979/  1.628162, val:  76.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5635%\n",
      "layer   3  Sparsity: 87.3076%\n",
      "total_backward_count 313280 real_backward_count 39179  12.506%\n",
      "fc layer 1 self.abs_max_out: 4161.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.393594/  1.631530, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.3872%\n",
      "layer   3  Sparsity: 87.3932%\n",
      "total_backward_count 323070 real_backward_count 39791  12.317%\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "lif layer 1 self.abs_max_v: 6831.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.388750/  1.617096, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.2931%\n",
      "layer   3  Sparsity: 87.4215%\n",
      "total_backward_count 332860 real_backward_count 40347  12.121%\n",
      "fc layer 1 self.abs_max_out: 4168.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.357089/  1.589583, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4922%\n",
      "layer   3  Sparsity: 87.3672%\n",
      "total_backward_count 342650 real_backward_count 40937  11.947%\n",
      "fc layer 1 self.abs_max_out: 4175.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.346988/  1.603327, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4587%\n",
      "layer   3  Sparsity: 87.2721%\n",
      "total_backward_count 352440 real_backward_count 41532  11.784%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.343485/  1.591279, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5676%\n",
      "layer   3  Sparsity: 87.5014%\n",
      "total_backward_count 362230 real_backward_count 42047  11.608%\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.336749/  1.604748, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5437%\n",
      "layer   3  Sparsity: 87.7490%\n",
      "total_backward_count 372020 real_backward_count 42521  11.430%\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "fc layer 1 self.abs_max_out: 4197.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.341735/  1.609026, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5763%\n",
      "layer   3  Sparsity: 87.9806%\n",
      "total_backward_count 381810 real_backward_count 43020  11.267%\n",
      "fc layer 1 self.abs_max_out: 4203.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.317051/  1.564857, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5976%\n",
      "layer   3  Sparsity: 87.3858%\n",
      "total_backward_count 391600 real_backward_count 43541  11.119%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.301614/  1.572580, val:  82.92%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4874%\n",
      "layer   3  Sparsity: 87.1824%\n",
      "total_backward_count 401390 real_backward_count 44021  10.967%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 1 self.abs_max_out: 4226.0\n",
      "lif layer 1 self.abs_max_v: 6890.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.306760/  1.550009, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4306%\n",
      "layer   3  Sparsity: 87.6337%\n",
      "total_backward_count 411180 real_backward_count 44502  10.823%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.315143/  1.595981, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.4680%\n",
      "layer   3  Sparsity: 87.6781%\n",
      "total_backward_count 420970 real_backward_count 44988  10.687%\n",
      "fc layer 1 self.abs_max_out: 4228.0\n",
      "fc layer 2 self.abs_max_out: 2195.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.319453/  1.582252, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5329%\n",
      "layer   3  Sparsity: 87.5739%\n",
      "total_backward_count 430760 real_backward_count 45478  10.558%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.294009/  1.564602, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6322%\n",
      "layer   3  Sparsity: 87.9377%\n",
      "total_backward_count 440550 real_backward_count 45925  10.424%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.300771/  1.552137, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5997%\n",
      "layer   3  Sparsity: 87.9960%\n",
      "total_backward_count 450340 real_backward_count 46361  10.295%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 1 self.abs_max_out: 4237.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.274531/  1.553442, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5973%\n",
      "layer   3  Sparsity: 87.6947%\n",
      "total_backward_count 460130 real_backward_count 46777  10.166%\n",
      "lif layer 2 self.abs_max_v: 3282.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.278806/  1.586873, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5030%\n",
      "layer   3  Sparsity: 87.9048%\n",
      "total_backward_count 469920 real_backward_count 47161  10.036%\n",
      "lif layer 2 self.abs_max_v: 3402.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.274062/  1.554233, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6672%\n",
      "layer   3  Sparsity: 88.0711%\n",
      "total_backward_count 479710 real_backward_count 47530   9.908%\n",
      "fc layer 1 self.abs_max_out: 4238.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.268893/  1.539450, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5973%\n",
      "layer   3  Sparsity: 88.2041%\n",
      "total_backward_count 489500 real_backward_count 47888   9.783%\n",
      "fc layer 1 self.abs_max_out: 4240.0\n",
      "lif layer 1 self.abs_max_v: 7033.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.269116/  1.530789, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.5397%\n",
      "layer   3  Sparsity: 88.1007%\n",
      "total_backward_count 499290 real_backward_count 48258   9.665%\n",
      "fc layer 1 self.abs_max_out: 4251.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.253903/  1.550545, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6367%\n",
      "layer   3  Sparsity: 88.1489%\n",
      "total_backward_count 509080 real_backward_count 48619   9.550%\n",
      "lif layer 1 self.abs_max_v: 7054.5\n",
      "fc layer 1 self.abs_max_out: 4252.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.279289/  1.529940, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6196%\n",
      "layer   3  Sparsity: 88.1009%\n",
      "total_backward_count 518870 real_backward_count 48973   9.438%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.258790/  1.522622, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.7318%\n",
      "layer   3  Sparsity: 87.9844%\n",
      "total_backward_count 528660 real_backward_count 49322   9.330%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.225713/  1.525256, val:  80.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.7591%\n",
      "layer   3  Sparsity: 87.8206%\n",
      "total_backward_count 538450 real_backward_count 49670   9.225%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.223499/  1.509308, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.7243%\n",
      "layer   3  Sparsity: 87.8136%\n",
      "total_backward_count 548240 real_backward_count 50022   9.124%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.220404/  1.510386, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.7206%\n",
      "layer   3  Sparsity: 88.0034%\n",
      "total_backward_count 558030 real_backward_count 50359   9.024%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.238092/  1.543399, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6489%\n",
      "layer   3  Sparsity: 88.0379%\n",
      "total_backward_count 567820 real_backward_count 50676   8.925%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.231080/  1.500875, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.7083%\n",
      "layer   2  Sparsity: 83.6303%\n",
      "layer   3  Sparsity: 87.6429%\n",
      "total_backward_count 577610 real_backward_count 50937   8.819%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [10,15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \n",
    "        \"random_select_ratio\": {\"values\": [1,2,3,4,5]},\n",
    "        \"leaky_temporal_filter\": {\"values\": [0.0, 0.25, 0.5, 0.75, 1.0]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"3\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "        random_select_ratio  =  wandb.config.random_select_ratio,\n",
    "        leaky_temporal_filter  =  wandb.config.leaky_temporal_filter,\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'hcapkd0n'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
