{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 08oo273s\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/08oo273s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j4c2kzla with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.2971018297378253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007136389167210705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 8.38532088956278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_143835-j4c2kzla</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/j4c2kzla' target=\"_blank\">lively-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/08oo273s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/08oo273s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/08oo273s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/08oo273s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/j4c2kzla' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/j4c2kzla</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "\n",
      "dvsgestrue 10th exclude class indices exist\n",
      "\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '1', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': which_data_hyper,\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
