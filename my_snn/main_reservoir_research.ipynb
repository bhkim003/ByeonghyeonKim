{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA76ElEQVR4nO3de1yUZf7/8feIAh4AjyAmIh22SCoMOnjqawfZXDXbDrpWHlJbDQ8pbilrm6Ulaa2xm0mZp8xD5KppZRabW1rqSuSh41ppgiWRZqKmIDP37w9XfjuCJuPMdTszr+fjcT8ecXPPdX+YSD+9r+u+xmFZliUAAAD4XC27CwAAAAgWNF4AAACG0HgBAAAYQuMFAABgCI0XAACAITReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XoAH5s2bJ4fDUXnUrl1bsbGx+sMf/qCvvvrKtroeffRRORwO2+5/soKCAg0bNkyXXXaZIiIiFBMTo5tuuklr1qypcu2AAQPc3tP69eurdevWuuWWWzR37lyVlZXV+P4ZGRlyOBzq3r27N34cADhrNF7AWZg7d642bNigf/7znxo+fLhWrlypjh07av/+/XaXdk5YvHixNm3apIEDB2rFihWaNWuWwsLCdOONN2r+/PlVrq9bt642bNigDRs26I033tDEiRNVv3593XfffUpJSdHu3bvP+N7Hjh3TggULJEmrV6/Wd99957WfCwA8ZgGosblz51qSrPz8fLfzjz32mCXJmjNnji11TZgwwTqX/rP+4YcfqpyrqKiwLr/8cuuCCy5wO9+/f3+rfv361Y7z9ttvW3Xq1LGuueaaM773kiVLLElWt27dLEnWE088cUavKy8vt44dO1bt9w4fPnzG9weA6pB4AV6UmpoqSfrhhx8qzx09elRjxoxRcnKyoqKi1LhxY7Vr104rVqyo8nqHw6Hhw4fr5ZdfVmJiourVq6crrrhCb7zxRpVr33zzTSUnJyssLEwJCQl6+umnq63p6NGjyszMVEJCgkJDQ3Xeeedp2LBh+vnnn92ua926tbp376433nhDbdu2Vd26dZWYmFh573nz5ikxMVH169fX1VdfrY8++uhX34/o6Ogq50JCQpSSkqKioqJfff0JaWlpuu+++/Tvf/9ba9euPaPXzJ49W6GhoZo7d67i4uI0d+5cWZblds17770nh8Ohl19+WWPGjNF5552nsLAwff311xowYIAaNGigTz75RGlpaYqIiNCNN94oScrLy1PPnj3VsmVLhYeH68ILL9SQIUO0d+/eyrHXrVsnh8OhxYsXV6lt/vz5cjgcys/PP+P3AEBgoPECvGjnzp2SpN/85jeV58rKyvTTTz/pT3/6k1577TUtXrxYHTt21G233VbtdNubb76p6dOna+LEiVq6dKkaN26s3//+99qxY0flNe+++6569uypiIgIvfLKK3rqqaf06quvau7cuW5jWZalW2+9VU8//bT69u2rN998UxkZGXrppZd0ww03VFk3tXXrVmVmZmrs2LFatmyZoqKidNttt2nChAmaNWuWJk+erIULF+rAgQPq3r27jhw5UuP3qKKiQuvWrVObNm1q9LpbbrlFks6o8dq9e7feeecd9ezZU82aNVP//v319ddfn/K1mZmZKiws1PPPP6/XX3+9smEsLy/XLbfcohtuuEErVqzQY489Jkn65ptv1K5dO+Xk5Oidd97RI488on//+9/q2LGjjh07Jknq1KmT2rZtq+eee67K/aZPn66rrrpKV111VY3eAwABwO7IDfBHJ6YaN27caB07dsw6ePCgtXr1aqt58+bWddddd8qpKss6PtV27Ngxa9CgQVbbtm3dvifJiomJsUpLSyvPFRcXW7Vq1bKysrIqz11zzTVWixYtrCNHjlSeKy0ttRo3buw21bh69WpLkjV16lS3++Tm5lqSrJkzZ1aei4+Pt+rWrWvt3r278tyWLVssSVZsbKzbNNtrr71mSbJWrlx5Jm+Xm/Hjx1uSrNdee83t/OmmGi3Lsr744gtLknX//ff/6j0mTpxoSbJWr15tWZZl7dixw3I4HFbfvn3drvvXv/5lSbKuu+66KmP079//jKaNXS6XdezYMWvXrl2WJGvFihWV3zvxe7J58+bKc5s2bbIkWS+99NKv/hwAAg+JF3AWrr32WtWpU0cRERG6+eab1ahRI61YsUK1a9d2u27JkiXq0KGDGjRooNq1a6tOnTqaPXu2vvjiiypjXn/99YqIiKj8OiYmRtHR0dq1a5ck6fDhw8rPz9dtt92m8PDwyusiIiLUo0cPt7FOPD04YMAAt/N33nmn6tevr3fffdftfHJyss4777zKrxMTEyVJnTt3Vr169aqcP1HTmZo1a5aeeOIJjRkzRj179qzRa62TpglPd92J6cUuXbpIkhISEtS5c2ctXbpUpaWlVV5z++23n3K86r5XUlKioUOHKi4urvLfZ3x8vCS5/Tvt06ePoqOj3VKvZ599Vs2aNVPv3r3P6OcBEFhovICzMH/+fOXn52vNmjUaMmSIvvjiC/Xp08ftmmXLlqlXr14677zztGDBAm3YsEH5+fkaOHCgjh49WmXMJk2aVDkXFhZWOa23f/9+uVwuNW/evMp1J5/bt2+fateurWbNmrmddzgcat68ufbt2+d2vnHjxm5fh4aGnvZ8dfWfyty5czVkyBD98Y9/1FNPPXXGrzvhRJPXokWL0163Zs0a7dy5U3feeadKS0v1888/6+eff1avXr30yy+/VLvmKjY2ttqx6tWrp8jISLdzLpdLaWlpWrZsmR566CG9++672rRpkzZu3ChJbtOvYWFhGjJkiBYtWqSff/5ZP/74o1599VUNHjxYYWFhNfr5AQSG2r9+CYBTSUxMrFxQf/3118vpdGrWrFn6xz/+oTvuuEOStGDBAiUkJCg3N9dtjy1P9qWSpEaNGsnhcKi4uLjK904+16RJE1VUVOjHH390a74sy1JxcbGxNUZz587V4MGD1b9/fz3//PMe7TW2cuVKScfTt9OZPXu2JGnatGmaNm1atd8fMmSI27lT1VPd+U8//VRbt27VvHnz1L9//8rzX3/9dbVj3H///XryySc1Z84cHT16VBUVFRo6dOhpfwYAgYvEC/CiqVOnqlGjRnrkkUfkcrkkHf/LOzQ01O0v8eLi4mqfajwTJ54qXLZsmVvidPDgQb3++utu1554Cu/EflYnLF26VIcPH678vi/NmzdPgwcP1j333KNZs2Z51HTl5eVp1qxZat++vTp27HjK6/bv36/ly5erQ4cO+te//lXluPvuu5Wfn69PP/3U45/nRP0nJ1YvvPBCtdfHxsbqzjvv1IwZM/T888+rR48eatWqlcf3B+DfSLwAL2rUqJEyMzP10EMPadGiRbrnnnvUvXt3LVu2TOnp6brjjjtUVFSkSZMmKTY21uNd7idNmqSbb75ZXbp00ZgxY+R0OjVlyhTVr19fP/30U+V1Xbp00W9/+1uNHTtWpaWl6tChg7Zt26YJEyaobdu26tu3r7d+9GotWbJEgwYNUnJysoYMGaJNmza5fb9t27ZuDYzL5aqcsisrK1NhYaHeeustvfrqq0pMTNSrr7562vstXLhQR48e1ciRI6tNxpo0aaKFCxdq9uzZeuaZZzz6mS655BJdcMEFGjdunCzLUuPGjfX6668rLy/vlK954IEHdM0110hSlSdPAQQZe9f2A/7pVBuoWpZlHTlyxGrVqpV10UUXWRUVFZZlWdaTTz5ptW7d2goLC7MSExOtF198sdrNTiVZw4YNqzJmfHy81b9/f7dzK1eutC6//HIrNDTUatWqlfXkk09WO+aRI0essWPHWvHx8VadOnWs2NhY6/7777f2799f5R7dunWrcu/qatq5c6clyXrqqadO+R5Z1v9/MvBUx86dO095bd26da1WrVpZPXr0sObMmWOVlZWd9l6WZVnJyclWdHT0aa+99tprraZNm1plZWWVTzUuWbKk2tpP9ZTl559/bnXp0sWKiIiwGjVqZN15551WYWGhJcmaMGFCta9p3bq1lZiY+Ks/A4DA5rCsM3xUCADgkW3btumKK67Qc889p/T0dLvLAWAjGi8A8JFvvvlGu3bt0p///GcVFhbq66+/dtuWA0DwYXE9APjIpEmT1KVLFx06dEhLliyh6QJA4gUAAGAKiRcAAIAhNF4AAACG0HgBAAAY4tcbqLpcLn3//feKiIjwaDdsAACCiWVZOnjwoFq0aKFatcxnL0ePHlV5eblPxg4NDVV4eLhPxvYmv268vv/+e8XFxdldBgAAfqWoqEgtW7Y0es+jR48qIb6BikucPhm/efPm2rlz5znffPl14xURESFJSuz7F4WEnttv9Mkc/vos6c0//fo156jQEN/8x+5rDabWt7sEj7TP/sjuEjz2zymn/jzIc1m9Ys8+eN1u+y+ua3cJHhsw4k27S6iRo4cq9OfOmyr//jSpvLxcxSVO7SporcgI76ZtpQddik/5VuXl5TRevnRiejEkNJzGy5R6Yb9+zTkqpLZ/Nl61a/vX7/YJ4Q3q2F2Cx2rX8c/3vHZt/1xy4W9/fv+vug38869RO5fnNIhwqEGEd+/vkv/87vvnbwwAAPBLTsslp5fDB6fl8u6APsRTjQAAAIaQeAEAAGNcsuSSdyMvb4/nSyReAAAAhpB4AQAAY1xyydsrsrw/ou+QeAEAABhC4gUAAIxxWpaclnfXZHl7PF8i8QIAADCExAsAABgT7E810ngBAABjXLLkDOLGi6lGAAAAQ0i8AACAMcE+1UjiBQAAYAiJFwAAMIbtJAAAAGAEiRcAADDG9d/D22P6C9sTrxkzZighIUHh4eFKSUnRunXr7C4JAADAJ2xtvHJzczVq1CiNHz9emzdvVqdOndS1a1cVFhbaWRYAAPAR53/38fL24S9sbbymTZumQYMGafDgwUpMTFR2drbi4uKUk5NjZ1kAAMBHnJZvDn9hW+NVXl6ugoICpaWluZ1PS0vT+vXrq31NWVmZSktL3Q4AAAB/YVvjtXfvXjmdTsXExLidj4mJUXFxcbWvycrKUlRUVOURFxdnolQAAOAlLh8d/sL2xfUOh8Pta8uyqpw7ITMzUwcOHKg8ioqKTJQIAADgFbZtJ9G0aVOFhIRUSbdKSkqqpGAnhIWFKSwszER5AADAB1xyyKnqA5azGdNf2JZ4hYaGKiUlRXl5eW7n8/Ly1L59e5uqAgAA8B1bN1DNyMhQ3759lZqaqnbt2mnmzJkqLCzU0KFD7SwLAAD4iMs6fnh7TH9ha+PVu3dv7du3TxMnTtSePXuUlJSkVatWKT4+3s6yAAAAfML2jwxKT09Xenq63WUAAAADnD5Y4+Xt8XzJ9sYLAAAEj2BvvGzfTgIAACBYkHgBAABjXJZDLsvL20l4eTxfIvECAAAwhMQLAAAYwxovAAAAGEHiBQAAjHGqlpxezn2cXh3Nt0i8AAAADCHxAgAAxlg+eKrR8qOnGmm8AACAMSyuBwAAgBEkXgAAwBinVUtOy8uL6y2vDudTJF4AAACGkHgBAABjXHLI5eXcxyX/ibxIvAAAAAwJiMQrake5atf2rx5yzfzZdpfgkfeO+Nf7/L8eHzzA7hI8UtbEP/8zXfOnjnaX4LFjMf75e35h9pd2l+CRfk0+tLsEjz367S12l1AjFYfLJK23tQaeagQAAIAR/vm/0gAAwC/55qlG/1njReMFAACMOb643rtTg94ez5eYagQAADCExAsAABjjUi052U4CAAAAvkbiBQAAjAn2xfUkXgAAAIaQeAEAAGNcqsVHBgEAAMD3SLwAAIAxTsshp+Xljwzy8ni+ROMFAACMcfpgOwknU40AAAA4GYkXAAAwxmXVksvL20m42E4CAAAAJyPxAgAAxrDGCwAAAEaQeAEAAGNc8v72Dy6vjuZbJF4AAACGkHgBAABjfPORQf6TI9F4AQAAY5xWLTm9vJ2Et8fzJf+pFAAAwM+ReAEAAGNccsglby+u95/PaiTxAgAAMITECwAAGMMaLwAAABhB4gUAAIzxzUcG+U+O5D+VAgAA+DkSLwAAYIzLcsjl7Y8M8vJ4vkTiBQAAYAiJFwAAMMblgzVefGQQAABANVxWLbm8vP2Dt8fzJf+pFAAAwM+ReAEAAGOccsjp5Y/48fZ4vkTiBQAAYAiJFwAAMIY1XgAAADCCxAsAABjjlPfXZDm9OppvkXgBAAAYQuIFAACMCfY1XjReAADAGKdVS04vN0reHs+X/KdSAAAAP0fjBQAAjLHkkMvLh+XhYv0ZM2YoISFB4eHhSklJ0bp16057/cKFC3XFFVeoXr16io2N1b333qt9+/bV6J40XgAAIOjk5uZq1KhRGj9+vDZv3qxOnTqpa9euKiwsrPb6Dz74QP369dOgQYP02WefacmSJcrPz9fgwYNrdF8aLwAAYMyJNV7ePmpq2rRpGjRokAYPHqzExERlZ2crLi5OOTk51V6/ceNGtW7dWiNHjlRCQoI6duyoIUOG6KOPPqrRfWm8AABAQCgtLXU7ysrKqr2uvLxcBQUFSktLczuflpam9evXV/ua9u3ba/fu3Vq1apUsy9IPP/ygf/zjH+rWrVuNagyIpxof/ttLqh/hXz3k7y693u4SPPLIx2vsLsFjv8SE2l2CR/zoYR03E555ye4SPJbdJtnuEjyyKfRKu0vwyK5VDe0uwWPf9Y23u4QacZYdtbsEuSyHXJZ3N1A9MV5cXJzb+QkTJujRRx+tcv3evXvldDoVExPjdj4mJkbFxcXV3qN9+/ZauHChevfuraNHj6qiokK33HKLnn322RrV6qd/pAMAALgrKirSgQMHKo/MzMzTXu9wuDeAlmVVOXfC559/rpEjR+qRRx5RQUGBVq9erZ07d2ro0KE1qjEgEi8AAOAfnKolp5dznxPjRUZGKjIy8levb9q0qUJCQqqkWyUlJVVSsBOysrLUoUMHPfjgg5Kkyy+/XPXr11enTp30+OOPKzY29oxqJfECAADGnJhq9PZRE6GhoUpJSVFeXp7b+by8PLVv377a1/zyyy+qVcu9bQoJCZF0PCk7UzReAAAg6GRkZGjWrFmaM2eOvvjiC40ePVqFhYWVU4eZmZnq169f5fU9evTQsmXLlJOTox07dujDDz/UyJEjdfXVV6tFixZnfF+mGgEAgDEu1ZLLy7mPJ+P17t1b+/bt08SJE7Vnzx4lJSVp1apVio8//sDEnj173Pb0GjBggA4ePKjp06drzJgxatiwoW644QZNmTKlRvel8QIAAEEpPT1d6enp1X5v3rx5Vc6NGDFCI0aMOKt70ngBAABjnJZDTi9vJ+Ht8XyJNV4AAACGkHgBAABjfLmBqj8g8QIAADCExAsAABhjWbXk8vJnoVl+9NlqNF4AAMAYpxxyysuL6708ni/5T4sIAADg50i8AACAMS7L+4vhXWf+iT22I/ECAAAwhMQLAAAY4/LB4npvj+dL/lMpAACAnyPxAgAAxrjkkMvLTyF6ezxfsjXxysrK0lVXXaWIiAhFR0fr1ltv1X/+8x87SwIAAPAZWxuv999/X8OGDdPGjRuVl5eniooKpaWl6fDhw3aWBQAAfOTEh2R7+/AXtk41rl692u3ruXPnKjo6WgUFBbruuutsqgoAAPhKsC+uP6fWeB04cECS1Lhx42q/X1ZWprKyssqvS0tLjdQFAADgDedMi2hZljIyMtSxY0clJSVVe01WVpaioqIqj7i4OMNVAgCAs+GSQy7LyweL62tu+PDh2rZtmxYvXnzKazIzM3XgwIHKo6ioyGCFAAAAZ+ecmGocMWKEVq5cqbVr16ply5anvC4sLExhYWEGKwMAAN5k+WA7CcuPEi9bGy/LsjRixAgtX75c7733nhISEuwsBwAAwKdsbbyGDRumRYsWacWKFYqIiFBxcbEkKSoqSnXr1rWzNAAA4AMn1mV5e0x/Yesar5ycHB04cECdO3dWbGxs5ZGbm2tnWQAAAD5h+1QjAAAIHuzjBQAAYAhTjQAAADCCxAsAABjj8sF2EmygCgAAgCpIvAAAgDGs8QIAAIARJF4AAMAYEi8AAAAYQeIFAACMCfbEi8YLAAAYE+yNF1ONAAAAhpB4AQAAYyx5f8NTf/rkZxIvAAAAQ0i8AACAMazxAgAAgBEkXgAAwJhgT7wCovG6PLRCkaH+Fd7949M8u0vwSNsPB9tdgscqupfZXYJHrP2hdpfgkevrHrK7BI+NyUixuwSPHI1x2V2CR5rMLra7BI8dSW5udwk14vrlqN0lBL2AaLwAAIB/IPECAAAwJNgbL/+anwMAAPBjJF4AAMAYy3LI8nJC5e3xfInECwAAwBASLwAAYIxLDq9/ZJC3x/MlEi8AAABDSLwAAIAxPNUIAAAAI0i8AACAMTzVCAAAACNIvAAAgDHBvsaLxgsAABjDVCMAAACMIPECAADGWD6YaiTxAgAAQBUkXgAAwBhLkmV5f0x/QeIFAABgCIkXAAAwxiWHHHxINgAAAHyNxAsAABgT7Pt40XgBAABjXJZDjiDeuZ6pRgAAAENIvAAAgDGW5YPtJPxoPwkSLwAAAENIvAAAgDHBvriexAsAAMAQEi8AAGAMiRcAAACMIPECAADGBPs+XjReAADAGLaTAAAAgBEkXgAAwJjjiZe3F9d7dTifIvECAAAwhMQLAAAYw3YSAAAAMILECwAAGGP99/D2mP6CxAsAAMAQEi8AAGBMsK/xovECAADmBPlcI1ONAAAAhpB4AQAAc3ww1Sg/mmok8QIAADCExgsAABhz4kOyvX14YsaMGUpISFB4eLhSUlK0bt26015fVlam8ePHKz4+XmFhYbrgggs0Z86cGt2TqUYAABB0cnNzNWrUKM2YMUMdOnTQCy+8oK5du+rzzz9Xq1atqn1Nr1699MMPP2j27Nm68MILVVJSooqKihrdNyAar47r+6tWvXC7y6iRFe1y7C7BI3XqOO0uwWMh2xrYXYJHWqw/ancJHvlHWnO7S/BYyi2f2l2CRza+18buEjyy48l2dpfgsTp1DtldQo04z4E/w8+V7SSmTZumQYMGafDgwZKk7Oxsvf3228rJyVFWVlaV61evXq33339fO3bsUOPGjSVJrVu3rvF9mWoEAAABobS01O0oKyur9rry8nIVFBQoLS3N7XxaWprWr19f7WtWrlyp1NRUTZ06Veedd55+85vf6E9/+pOOHDlSoxoDIvECAAB+wnJ4/ynE/44XFxfndnrChAl69NFHq1y+d+9eOZ1OxcTEuJ2PiYlRcXFxtbfYsWOHPvjgA4WHh2v58uXau3ev0tPT9dNPP9VonReNFwAAMOZsFsOfbkxJKioqUmRkZOX5sLCw077O4XBvAC3LqnLuBJfLJYfDoYULFyoqKkrS8enKO+64Q88995zq1q17RrUy1QgAAAJCZGSk23Gqxqtp06YKCQmpkm6VlJRUScFOiI2N1XnnnVfZdElSYmKiLMvS7t27z7hGGi8AAGCO5aOjBkJDQ5WSkqK8vDy383l5eWrfvn21r+nQoYO+//57HTr0/x+o2L59u2rVqqWWLVue8b1pvAAAQNDJyMjQrFmzNGfOHH3xxRcaPXq0CgsLNXToUElSZmam+vXrV3n9XXfdpSZNmujee+/V559/rrVr1+rBBx/UwIEDz3iaUWKNFwAAMOhc2U6id+/e2rdvnyZOnKg9e/YoKSlJq1atUnx8vCRpz549KiwsrLy+QYMGysvL04gRI5SamqomTZqoV69eevzxx2t0XxovAAAQlNLT05Wenl7t9+bNm1fl3CWXXFJlerKmaLwAAIBZXn6q0Z+wxgsAAMAQEi8AAGDMubLGyy40XgAAwBwPtn84ozH9BFONAAAAhpB4AQAAgxz/Pbw9pn8g8QIAADCExAsAAJjDGi8AAACYQOIFAADMIfECAACACedM45WVlSWHw6FRo0bZXQoAAPAVy+Gbw0+cE1ON+fn5mjlzpi6//HK7SwEAAD5kWccPb4/pL2xPvA4dOqS7775bL774oho1amR3OQAAAD5je+M1bNgwdevWTTfddNOvXltWVqbS0lK3AwAA+BHLR4efsHWq8ZVXXtHHH3+s/Pz8M7o+KytLjz32mI+rAgAA8A3bEq+ioiI98MADWrBggcLDw8/oNZmZmTpw4EDlUVRU5OMqAQCAV7G43h4FBQUqKSlRSkpK5Tmn06m1a9dq+vTpKisrU0hIiNtrwsLCFBYWZrpUAAAAr7Ct8brxxhv1ySefuJ279957dckll2js2LFVmi4AAOD/HNbxw9tj+gvbGq+IiAglJSW5natfv76aNGlS5TwAAEAgqPEar5deeklvvvlm5dcPPfSQGjZsqPbt22vXrl1eLQ4AAASYIH+qscaN1+TJk1W3bl1J0oYNGzR9+nRNnTpVTZs21ejRo8+qmPfee0/Z2dlnNQYAADiHsbi+ZoqKinThhRdKkl577TXdcccd+uMf/6gOHTqoc+fO3q4PAAAgYNQ48WrQoIH27dsnSXrnnXcqNz4NDw/XkSNHvFsdAAAILEE+1VjjxKtLly4aPHiw2rZtq+3bt6tbt26SpM8++0ytW7f2dn0AAAABo8aJ13PPPad27drpxx9/1NKlS9WkSRNJx/fl6tOnj9cLBAAAAYTEq2YaNmyo6dOnVznPR/kAAACc3hk1Xtu2bVNSUpJq1aqlbdu2nfbayy+/3CuFAQCAAOSLhCrQEq/k5GQVFxcrOjpaycnJcjgcsqz//1Oe+NrhcMjpdPqsWAAAAH92Ro3Xzp071axZs8p/BgAA8Igv9t0KtH284uPjq/3nk/1vCgYAAAB3NX6qsW/fvjp06FCV899++62uu+46rxQFAAAC04kPyfb24S9q3Hh9/vnnuuyyy/Thhx9WnnvppZd0xRVXKCYmxqvFAQCAAMN2EjXz73//Ww8//LBuuOEGjRkzRl999ZVWr16tv/3tbxo4cKAvagQAAAgINW68ateurSeffFJhYWGaNGmSateurffff1/t2rXzRX0AAAABo8ZTjceOHdOYMWM0ZcoUZWZmql27dvr973+vVatW+aI+AACAgFHjxCs1NVW//PKL3nvvPV177bWyLEtTp07VbbfdpoEDB2rGjBm+qBMAAAQAh7y/GN5/NpPwsPH6+9//rvr160s6vnnq2LFj9dvf/lb33HOP1ws8Ew8nv6l6ESG23NtTx2oeNp4T+l60ye4SPPbbtp/aXYJHbosZaXcJHll4TZLdJXguuqndFXhk+qpZdpfgkaf73213CR6r/fg+u0uokYrDZfrG7iKCXI0br9mzZ1d7Pjk5WQUFBWddEAAACGBsoOq5I0eO6NixY27nwsLCzqogAACAQFXj+a7Dhw9r+PDhio6OVoMGDdSoUSO3AwAA4JSCfB+vGjdeDz30kNasWaMZM2YoLCxMs2bN0mOPPaYWLVpo/vz5vqgRAAAEiiBvvGo81fj6669r/vz56ty5swYOHKhOnTrpwgsvVHx8vBYuXKi77/bfRZIAAAC+VOPE66efflJCQoIkKTIyUj/99JMkqWPHjlq7dq13qwMAAAGFz2qsofPPP1/ffvutJOnSSy/Vq6++Kul4EtawYUNv1gYAABBQatx43Xvvvdq6daskKTMzs3Kt1+jRo/Xggw96vUAAABBAWONVM6NHj6785+uvv15ffvmlPvroI11wwQW64oorvFocAABAIDmrfbwkqVWrVmrVqpU3agEAAIHOFwmVHyVe/vm5NQAAAH7orBMvAACAM+WLpxAD8qnG3bt3+7IOAAAQDE58VqO3Dz9xxo1XUlKSXn75ZV/WAgAAENDOuPGaPHmyhg0bpttvv1379u3zZU0AACBQBfl2EmfceKWnp2vr1q3av3+/2rRpo5UrV/qyLgAAgIBTo8X1CQkJWrNmjaZPn67bb79diYmJql3bfYiPP/7YqwUCAIDAEeyL62v8VOOuXbu0dOlSNW7cWD179qzSeAEAAKB6NeqaXnzxRY0ZM0Y33XSTPv30UzVr1sxXdQEAgEAU5BuonnHjdfPNN2vTpk2aPn26+vXr58uaAAAAAtIZN15Op1Pbtm1Ty5YtfVkPAAAIZD5Y4xWQiVdeXp4v6wAAAMEgyKca+axGAAAAQ3gkEQAAmEPiBQAAABNIvAAAgDHBvoEqiRcAAIAhNF4AAACG0HgBAAAYwhovAABgTpA/1UjjBQAAjGFxPQAAAIwg8QIAAGb5UULlbSReAAAAhpB4AQAAc4J8cT2JFwAAgCEkXgAAwBieagQAAIARJF4AAMCcIF/jReMFAACMYaoRAAAARpB4AQAAc4J8qpHECwAAwBASLwAAYA6JFwAAAEwg8QIAAMYE+1ONAdF4Lepzg2qHhNldRo0cyq6wuwSP7N7T2O4SPLbg8y52l+CRjSOesrsEj3zfw3//ePnD/NF2l+CR7J632V2CR4qfKLO7BI8d2RNtdwk14vrlqN0lnFNmzJihp556Snv27FGbNm2UnZ2tTp06/errPvzwQ/3f//2fkpKStGXLlhrdk6lGAABgjuWjo4Zyc3M1atQojR8/Xps3b1anTp3UtWtXFRYWnvZ1Bw4cUL9+/XTjjTfW/Kai8QIAACadI43XtGnTNGjQIA0ePFiJiYnKzs5WXFyccnJyTvu6IUOG6K677lK7du1qflPReAEAgABRWlrqdpSVVT+NXV5eroKCAqWlpbmdT0tL0/r16085/ty5c/XNN99owoQJHtdI4wUAAIw5sbje24ckxcXFKSoqqvLIysqqtoa9e/fK6XQqJibG7XxMTIyKi4urfc1XX32lcePGaeHChapd2/M1rP67+hUAAOB/FBUVKTIysvLrsLDTP3jncDjcvrYsq8o5SXI6nbrrrrv02GOP6Te/+c1Z1UjjBQAAzPHhBqqRkZFujdepNG3aVCEhIVXSrZKSkiopmCQdPHhQH330kTZv3qzhw4dLklwulyzLUu3atfXOO+/ohhtuOKNSmWoEAABBJTQ0VCkpKcrLy3M7n5eXp/bt21e5PjIyUp988om2bNlSeQwdOlQXX3yxtmzZomuuueaM703iBQAAjDlXNlDNyMhQ3759lZqaqnbt2mnmzJkqLCzU0KFDJUmZmZn67rvvNH/+fNWqVUtJSUlur4+OjlZ4eHiV87+GxgsAAASd3r17a9++fZo4caL27NmjpKQkrVq1SvHx8ZKkPXv2/OqeXp6g8QIAAOacQx+SnZ6ervT09Gq/N2/evNO+9tFHH9Wjjz5a43vSeAEAAHPOocbLDiyuBwAAMITECwAAGOP47+HtMf0FiRcAAIAhJF4AAMAc1ngBAADABBIvAABgzLmygapdSLwAAAAMsb3x+u6773TPPfeoSZMmqlevnpKTk1VQUGB3WQAAwBcsHx1+wtapxv3796tDhw66/vrr9dZbbyk6OlrffPONGjZsaGdZAADAl/yoUfI2WxuvKVOmKC4uTnPnzq0817p1a/sKAgAA8CFbpxpXrlyp1NRU3XnnnYqOjlbbtm314osvnvL6srIylZaWuh0AAMB/nFhc7+3DX9jaeO3YsUM5OTm66KKL9Pbbb2vo0KEaOXKk5s+fX+31WVlZioqKqjzi4uIMVwwAAOA5Wxsvl8ulK6+8UpMnT1bbtm01ZMgQ3XfffcrJyan2+szMTB04cKDyKCoqMlwxAAA4K0G+uN7Wxis2NlaXXnqp27nExEQVFhZWe31YWJgiIyPdDgAAAH9h6+L6Dh066D//+Y/bue3btys+Pt6migAAgC+xgaqNRo8erY0bN2ry5Mn6+uuvtWjRIs2cOVPDhg2zsywAAACfsLXxuuqqq7R8+XItXrxYSUlJmjRpkrKzs3X33XfbWRYAAPCVIF/jZftnNXbv3l3du3e3uwwAAACfs73xAgAAwSPY13jReAEAAHN8MTXoR42X7R+SDQAAECxIvAAAgDkkXgAAADCBxAsAABgT7IvrSbwAAAAMIfECAADmsMYLAAAAJpB4AQAAYxyWJYfl3YjK2+P5Eo0XAAAwh6lGAAAAmEDiBQAAjGE7CQAAABhB4gUAAMxhjRcAAABMCIjE69vxYQqpF253GTVS+58RdpfgkW0PTLO7BI89dnk7u0vwyO0PZNhdgkcefWqW3SV47IKcb+wuwSNfjTrf7hI8UmeTw+4SPGY18qOoRZJ11P73mjVeAAAAMCIgEi8AAOAngnyNF40XAAAwhqlGAAAAGEHiBQAAzAnyqUYSLwAAAENIvAAAgFH+tCbL20i8AAAADCHxAgAA5ljW8cPbY/oJEi8AAABDSLwAAIAxwb6PF40XAAAwh+0kAAAAYAKJFwAAMMbhOn54e0x/QeIFAABgCIkXAAAwhzVeAAAAMIHECwAAGBPs20mQeAEAABhC4gUAAMwJ8o8MovECAADGMNUIAAAAI0i8AACAOWwnAQAAABNIvAAAgDGs8QIAAIARJF4AAMCcIN9OgsQLAADAEBIvAABgTLCv8aLxAgAA5rCdBAAAAEwg8QIAAMYE+1QjiRcAAIAhJF4AAMAcl3X88PaYfoLECwAAwBASLwAAYA5PNQIAAMAEEi8AAGCMQz54qtG7w/kUjRcAADCHz2oEAACACSReAADAGDZQBQAAgBEkXgAAwBy2kwAAAIAJJF4AAMAYh2XJ4eWnEL09ni8FRON1XatvFNog1O4yamRDaGu7S/DI74aPtLsEjxVfHWJ3CR5Z/7en7S7BI9c/+6DdJXisbJT//CH+v1x1/LPuYw3srsBzYfv9aQcpyVnmX/UGooBovAAAgJ9w/ffw9ph+gjVeAADAmBNTjd4+PDFjxgwlJCQoPDxcKSkpWrdu3SmvXbZsmbp06aJmzZopMjJS7dq109tvv13je9J4AQCAoJObm6tRo0Zp/Pjx2rx5szp16qSuXbuqsLCw2uvXrl2rLl26aNWqVSooKND111+vHj16aPPmzTW6L1ONAADAnHNkO4lp06Zp0KBBGjx4sCQpOztbb7/9tnJycpSVlVXl+uzsbLevJ0+erBUrVuj1119X27Ztz/i+JF4AACAglJaWuh1lZWXVXldeXq6CggKlpaW5nU9LS9P69evP6F4ul0sHDx5U48aNa1QjjRcAADDnxIdke/uQFBcXp6ioqMqjuuRKkvbu3Sun06mYmBi38zExMSouLj6jH+Ovf/2rDh8+rF69etXox2eqEQAABISioiJFRkZWfh0WFnba6x0O9+01LMuqcq46ixcv1qOPPqoVK1YoOjq6RjXSeAEAAGN8+SHZkZGRbo3XqTRt2lQhISFV0q2SkpIqKdjJcnNzNWjQIC1ZskQ33XRTjWtlqhEAAASV0NBQpaSkKC8vz+18Xl6e2rdvf8rXLV68WAMGDNCiRYvUrVs3j+5N4gUAAMz5nzVZXh2zhjIyMtS3b1+lpqaqXbt2mjlzpgoLCzV06FBJUmZmpr777jvNnz9f0vGmq1+/fvrb3/6ma6+9tjItq1u3rqKios74vjReAAAg6PTu3Vv79u3TxIkTtWfPHiUlJWnVqlWKj4+XJO3Zs8dtT68XXnhBFRUVGjZsmIYNG1Z5vn///po3b94Z35fGCwAAGONwHT+8PaYn0tPTlZ6eXu33Tm6m3nvvPc9uchIaLwAAYM45MtVoFxbXAwAAGELiBQAAzDlHPjLILiReAAAAhpB4AQAAYxyWJYeX12R5ezxfIvECAAAwhMQLAACYw1ON9qmoqNDDDz+shIQE1a1bV+eff74mTpwol8vLG3wAAACcA2xNvKZMmaLnn39eL730ktq0aaOPPvpI9957r6KiovTAAw/YWRoAAPAFS5K38xX/Cbzsbbw2bNignj17Vn7QZOvWrbV48WJ99NFH1V5fVlamsrKyyq9LS0uN1AkAALyDxfU26tixo959911t375dkrR161Z98MEH+t3vflft9VlZWYqKiqo84uLiTJYLAABwVmxNvMaOHasDBw7okksuUUhIiJxOp5544gn16dOn2uszMzOVkZFR+XVpaSnNFwAA/sSSDxbXe3c4X7K18crNzdWCBQu0aNEitWnTRlu2bNGoUaPUokUL9e/fv8r1YWFhCgsLs6FSAACAs2dr4/Xggw9q3Lhx+sMf/iBJuuyyy7Rr1y5lZWVV23gBAAA/x3YS9vnll19Uq5Z7CSEhIWwnAQAAApKtiVePHj30xBNPqFWrVmrTpo02b96sadOmaeDAgXaWBQAAfMUlyeGDMf2ErY3Xs88+q7/85S9KT09XSUmJWrRooSFDhuiRRx6xsywAAACfsLXxioiIUHZ2trKzs+0sAwAAGBLs+3jxWY0AAMAcFtcDAADABBIvAABgDokXAAAATCDxAgAA5pB4AQAAwAQSLwAAYE6Qb6BK4gUAAGAIiRcAADCGDVQBAABMYXE9AAAATCDxAgAA5rgsyeHlhMpF4gUAAICTkHgBAABzWOMFAAAAE0i8AACAQT5IvOQ/iVdANF7rX0tWSFi43WXUyFvDp9pdgkfSNj1kdwkee+tu/3zPx353s90leOS8NaV2l+Cxo83q2l2CR+p99K3dJXikzdt77S7BY//4OMXuEmrEdaTc7hKCXkA0XgAAwE8E+RovGi8AAGCOy5LXpwbZTgIAAAAnI/ECAADmWK7jh7fH9BMkXgAAAIaQeAEAAHOCfHE9iRcAAIAhJF4AAMAcnmoEAACACSReAADAnCBf40XjBQAAzLHkg8bLu8P5ElONAAAAhpB4AQAAc4J8qpHECwAAwBASLwAAYI7LJcnLH/Hj4iODAAAAcBISLwAAYA5rvAAAAGACiRcAADAnyBMvGi8AAGAOn9UIAAAAE0i8AACAMZblkmV5d/sHb4/nSyReAAAAhpB4AQAAcyzL+2uy/GhxPYkXAACAISReAADAHMsHTzWSeAEAAOBkJF4AAMAcl0tyePkpRD96qpHGCwAAmMNUIwAAAEwg8QIAAMZYLpcsL081soEqAAAAqiDxAgAA5rDGCwAAACaQeAEAAHNcluQg8QIAAICPkXgBAABzLEuStzdQJfECAADASUi8AACAMZbLkuXlNV6WHyVeNF4AAMAcyyXvTzWygSoAAABOQuIFAACMCfapRhIvAAAAQ0i8AACAOUG+xsuvG68T0aKz7KjNldTcwYP+80vyv/zxvT7hkJ++5+WHyu0uwSMVTv/9Xak45rC7BI9UuPzzd6Xs0DG7S/CY64h//Z6fqNfOqbkKHfP6RzVWyH9+hxyWP02MnmT37t2Ki4uzuwwAAPxKUVGRWrZsafSeR48eVUJCgoqLi30yfvPmzbVz506Fh4f7ZHxv8evGy+Vy6fvvv1dERIQcDu/+H2ppaani4uJUVFSkyMhIr46N6vGem8X7bRbvt3m851VZlqWDBw+qRYsWqlXL/DLvo0ePqrzcN8lsaGjoOd90SX4+1VirVi2fd+yRkZH8B2sY77lZvN9m8X6bx3vuLioqyrZ7h4eH+0Vz5Es81QgAAGAIjRcAAIAhNF6nEBYWpgkTJigsLMzuUoIG77lZvN9m8X6bx3uOc5FfL64HAADwJyReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XqcwY8YMJSQkKDw8XCkpKVq3bp3dJQWkrKwsXXXVVYqIiFB0dLRuvfVW/ec//7G7rKCRlZUlh8OhUaNG2V1KQPvuu+90zz33qEmTJqpXr56Sk5NVUFBgd1kBqaKiQg8//LASEhJUt25dnX/++Zo4caJcLv/8rFYEHhqvauTm5mrUqFEaP368Nm/erE6dOqlr164qLCy0u7SA8/7772vYsGHauHGj8vLyVFFRobS0NB0+fNju0gJefn6+Zs6cqcsvv9zuUgLa/v371aFDB9WpU0dvvfWWPv/8c/31r39Vw4YN7S4tIE2ZMkXPP/+8pk+fri+++EJTp07VU089pWeffdbu0gBJbCdRrWuuuUZXXnmlcnJyKs8lJibq1ltvVVZWlo2VBb4ff/xR0dHRev/993XdddfZXU7AOnTokK688krNmDFDjz/+uJKTk5WdnW13WQFp3Lhx+vDDD0nNDenevbtiYmI0e/bsynO333676tWrp5dfftnGyoDjSLxOUl5eroKCAqWlpbmdT0tL0/r1622qKngcOHBAktS4cWObKwlsw4YNU7du3XTTTTfZXUrAW7lypVJTU3XnnXcqOjpabdu21Ysvvmh3WQGrY8eOevfdd7V9+3ZJ0tatW/XBBx/od7/7nc2VAcf59Ydk+8LevXvldDoVExPjdj4mJkbFxcU2VRUcLMtSRkaGOnbsqKSkJLvLCVivvPKKPv74Y+Xn59tdSlDYsWOHcnJylJGRoT//+c/atGmTRo4cqbCwMPXr18/u8gLO2LFjdeDAAV1yySUKCQmR0+nUE088oT59+thdGiCJxuuUHA6H29eWZVU5B+8aPny4tm3bpg8++MDuUgJWUVGRHnjgAb3zzjsKDw+3u5yg4HK5lJqaqsmTJ0uS2rZtq88++0w5OTk0Xj6Qm5urBQsWaNGiRWrTpo22bNmiUaNGqUWLFurfv7/d5QE0Xidr2rSpQkJCqqRbJSUlVVIweM+IESO0cuVKrV27Vi1btrS7nIBVUFCgkpISpaSkVJ5zOp1au3atpk+frrKyMoWEhNhYYeCJjY3VpZde6nYuMTFRS5cutamiwPbggw9q3Lhx+sMf/iBJuuyyy7Rr1y5lZWXReOGcwBqvk4SGhiolJUV5eXlu5/Py8tS+fXubqgpclmVp+PDhWrZsmdasWaOEhAS7SwpoN954oz755BNt2bKl8khNTdXdd9+tLVu20HT5QIcOHapskbJ9+3bFx8fbVFFg++WXX1SrlvtfbSEhIWwngXMGiVc1MjIy1LdvX6Wmpqpdu3aaOXOmCgsLNXToULtLCzjDhg3TokWLtGLFCkVERFQmjVFRUapbt67N1QWeiIiIKuvn6tevryZNmrCuzkdGjx6t9u3ba/LkyerVq5c2bdqkmTNnaubMmXaXFpB69OihJ554Qq1atVKbNm20efNmTZs2TQMHDrS7NEAS20mc0owZMzR16lTt2bNHSUlJeuaZZ9jewAdOtW5u7ty5GjBggNliglTnzp3ZTsLH3njjDWVmZuqrr75SQkKCMjIydN9999ldVkA6ePCg/vKXv2j58uUqKSlRixYt1KdPHz3yyCMKDQ21uzyAxgsAAMAU1ngBAAAYQuMFAABgCI0XAACAITReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XgAAAIbQeAGwncPh0GuvvWZ3GQDgczReAOR0OtW+fXvdfvvtbucPHDiguLg4Pfzwwz69/549e9S1a1ef3gMAzgV8ZBAASdJXX32l5ORkzZw5U3fffbckqV+/ftq6davy8/P5nDsA8AISLwCSpIsuukhZWVkaMWKEvv/+e61YsUKvvPKKXnrppdM2XQsWLFBqaqoiIiLUvHlz3XXXXSopKan8/sSJE9WiRQvt27ev8twtt9yi6667Ti6XS5L7VGN5ebmGDx+u2NhYhYeHq3Xr1srKyvLNDw0AhpF4AahkWZZuuOEGhYSE6JNPPtGIESN+dZpxzpw5io2N1cUXX6ySkhKNHj1ajRo10qpVqyQdn8bs1KmTYmJitHz5cj3//PMaN26ctm7dqvj4eEnHG6/ly5fr1ltv1dNPP62///3vWrhwoVq1aqWioiIVFRWpT58+Pv/5AcDXaLwAuPnyyy+VmJioyy67TB9//LFq165do9fn5+fr6quv1sGDB9WgQQNJ0o4dO5ScnKz09HQ9++yzbtOZknvjNXLkSH322Wf65z//KYfD4dWfDQDsxlQjADdz5sxRvXr1tHPnTu3evftXr9+8ebN69uyp+Ph4RUREqHPnzpKkwsLCymvOP/98Pf3005oyZYp69Ojh1nSdbMCAAdqyZYsuvvhijRw5Uu+8885Z/0wAcK6g8QJQacOGDXrmmWe0YsUKtWvXToMGDdLpQvHDhw8rLS1NDRo00IIFC5Sfn6/ly5dLOr5W63+tXbtWISEh+vbbb1VRUXHKMa+88krt3LlTkyZN0pEjR9SrVy/dcccd3vkBAcBmNF4AJElHjhxR//79NWTIEN10002aNWuW8vPz9cILL5zyNV9++aX27t2rJ598Up06ddIll1zitrD+hNzcXC1btkzvvfeeioqKNGnSpNPWEhkZqd69e+vFF19Ubm6uli5dqp9++umsf0YAsBuNFwBJ0rhx4+RyuTRlyhRJUqtWrfTXv/5VDz74oL799ttqX9OqVSuFhobq2Wef1Y4dO7Ry5coqTdXu3bt1//33a8qUKerYsaPmzZunrKwsbdy4sdoxn3nmGb3yyiv68ssvtX37di1ZskTNmzdXw4YNvfnjAoAtaLwA6P3339dzzz2nefPmqX79+pXn77vvPrVv3/6UU47NmjXTvHnztGTJEl166aV68skn9fTTT1d+37IsDRgwQFdffbWGDx8uSerSpYuGDx+ue+65R4cOHaoyZoMGDTRlyhSlpqbqqquu0rfffqtVq1apVi3+uALg/3iqEQAAwBD+FxIAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovAAAAQ2i8AAAADKHxAgAAMITGCwAAwBAaLwAAAENovAAAAAz5fwpwLlhog9zZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: n876ay1i\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n876ay1i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bth8g3fg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.4838886781354044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.021579208442625755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 8.914949471352589\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_225157-bth8g3fg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bth8g3fg' target=\"_blank\">peach-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n876ay1i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n876ay1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n876ay1i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n876ay1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bth8g3fg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bth8g3fg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "\n",
      "dvsgestrue 10 class indices exist. we want to exclude the 11th class\n",
      "\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 13.64%\n",
      "Test loss: 2.215, Val Accuracy: 20.83%\n",
      "Epoch 2\n",
      "Train Accuracy: 28.85%\n",
      "Test loss: 2.048, Val Accuracy: 43.18%\n",
      "Epoch 3\n",
      "Train Accuracy: 47.40%\n",
      "Test loss: 1.785, Val Accuracy: 44.70%\n",
      "Epoch 4\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '1', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.0001, \"max\": 0.05},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
