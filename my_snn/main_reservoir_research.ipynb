{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        self.conv_depthwise = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                        kernel_size=self.receptive_size, \n",
    "                                        stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.conv_depthwise.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv_depthwise.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "            recurrent_current = self.conv_depthwise(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*self.v_reset + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        if rate_coding == True:\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else:\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            if rate_coding == True:\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else:\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240724_104456-b1k0u8kr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b1k0u8kr' target=\"_blank\">eternal-firebrand-1857</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b1k0u8kr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b1k0u8kr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (conv_depthwise): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)\n",
      "  )\n",
      "  (linear): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 29.44%\n",
      "Test loss: 1.942, Accuracy: 30.91%\n",
      "Epoch 2\n",
      "Train Accuracy: 34.20%\n",
      "Test loss: 1.879, Accuracy: 34.29%\n",
      "Epoch 3\n",
      "Train Accuracy: 35.11%\n",
      "Test loss: 1.878, Accuracy: 33.62%\n",
      "Epoch 4\n",
      "Train Accuracy: 35.52%\n",
      "Test loss: 1.833, Accuracy: 36.74%\n",
      "Epoch 5\n",
      "Train Accuracy: 35.83%\n",
      "Test loss: 1.889, Accuracy: 32.30%\n",
      "Epoch 6\n",
      "Train Accuracy: 36.21%\n",
      "Test loss: 1.801, Accuracy: 37.51%\n",
      "Epoch 7\n",
      "Train Accuracy: 36.66%\n",
      "Test loss: 1.861, Accuracy: 35.17%\n",
      "Epoch 8\n",
      "Train Accuracy: 37.37%\n",
      "Test loss: 1.808, Accuracy: 36.25%\n",
      "Epoch 9\n",
      "Train Accuracy: 36.77%\n",
      "Test loss: 1.814, Accuracy: 35.53%\n",
      "Epoch 10\n",
      "Train Accuracy: 37.43%\n",
      "Test loss: 1.851, Accuracy: 34.56%\n",
      "Epoch 11\n",
      "Train Accuracy: 37.28%\n",
      "Test loss: 1.835, Accuracy: 36.14%\n",
      "Epoch 12\n",
      "Train Accuracy: 37.29%\n",
      "Test loss: 1.792, Accuracy: 37.89%\n",
      "Epoch 13\n",
      "Train Accuracy: 37.67%\n",
      "Test loss: 1.781, Accuracy: 37.79%\n",
      "Epoch 14\n",
      "Train Accuracy: 37.76%\n",
      "Test loss: 1.847, Accuracy: 36.01%\n",
      "Epoch 15\n",
      "Train Accuracy: 38.02%\n",
      "Test loss: 1.839, Accuracy: 35.58%\n",
      "Epoch 16\n",
      "Train Accuracy: 37.71%\n",
      "Test loss: 1.793, Accuracy: 37.66%\n",
      "Epoch 17\n",
      "Train Accuracy: 37.94%\n",
      "Test loss: 1.833, Accuracy: 36.74%\n",
      "Epoch 18\n",
      "Train Accuracy: 38.33%\n",
      "Test loss: 1.816, Accuracy: 36.80%\n",
      "Epoch 19\n",
      "Train Accuracy: 38.09%\n",
      "Test loss: 1.774, Accuracy: 37.87%\n",
      "Epoch 20\n",
      "Train Accuracy: 38.38%\n",
      "Test loss: 1.777, Accuracy: 38.78%\n",
      "Epoch 21\n",
      "Train Accuracy: 38.46%\n",
      "Test loss: 1.841, Accuracy: 35.01%\n",
      "Epoch 22\n",
      "Train Accuracy: 38.51%\n",
      "Test loss: 1.795, Accuracy: 36.57%\n",
      "Epoch 23\n",
      "Train Accuracy: 37.77%\n",
      "Test loss: 1.798, Accuracy: 36.55%\n",
      "Epoch 24\n",
      "Train Accuracy: 37.81%\n",
      "Test loss: 1.806, Accuracy: 37.11%\n",
      "Epoch 25\n",
      "Train Accuracy: 38.16%\n",
      "Test loss: 1.865, Accuracy: 35.19%\n",
      "Epoch 26\n",
      "Train Accuracy: 38.08%\n",
      "Test loss: 1.813, Accuracy: 36.31%\n",
      "Epoch 27\n",
      "Train Accuracy: 38.07%\n",
      "Test loss: 1.835, Accuracy: 35.59%\n",
      "Epoch 28\n",
      "Train Accuracy: 37.99%\n",
      "Test loss: 1.785, Accuracy: 37.67%\n",
      "Epoch 29\n",
      "Train Accuracy: 38.10%\n",
      "Test loss: 1.801, Accuracy: 36.72%\n",
      "Epoch 30\n",
      "Train Accuracy: 37.97%\n",
      "Test loss: 1.823, Accuracy: 35.70%\n",
      "Epoch 31\n",
      "Train Accuracy: 38.43%\n",
      "Test loss: 1.824, Accuracy: 36.20%\n",
      "Epoch 32\n",
      "Train Accuracy: 38.29%\n",
      "Test loss: 1.819, Accuracy: 36.49%\n",
      "Epoch 33\n",
      "Train Accuracy: 38.53%\n",
      "Test loss: 1.805, Accuracy: 36.56%\n",
      "Epoch 34\n",
      "Train Accuracy: 38.36%\n",
      "Test loss: 1.756, Accuracy: 39.32%\n",
      "Epoch 35\n",
      "Train Accuracy: 38.48%\n",
      "Test loss: 1.835, Accuracy: 35.08%\n",
      "Epoch 36\n",
      "Train Accuracy: 38.37%\n",
      "Test loss: 1.811, Accuracy: 37.37%\n",
      "Epoch 37\n",
      "Train Accuracy: 38.43%\n",
      "Test loss: 1.797, Accuracy: 36.97%\n",
      "Epoch 38\n",
      "Train Accuracy: 38.26%\n",
      "Test loss: 1.847, Accuracy: 35.02%\n",
      "Epoch 39\n",
      "Train Accuracy: 38.34%\n",
      "Test loss: 1.820, Accuracy: 37.22%\n",
      "Epoch 40\n",
      "Train Accuracy: 38.14%\n",
      "Test loss: 1.857, Accuracy: 35.47%\n",
      "Epoch 41\n",
      "Train Accuracy: 38.59%\n",
      "Test loss: 1.823, Accuracy: 37.77%\n",
      "Epoch 42\n",
      "Train Accuracy: 38.63%\n",
      "Test loss: 1.795, Accuracy: 37.92%\n",
      "Epoch 43\n",
      "Train Accuracy: 38.63%\n",
      "Test loss: 1.796, Accuracy: 37.13%\n",
      "Epoch 44\n",
      "Train Accuracy: 38.09%\n",
      "Test loss: 1.807, Accuracy: 36.88%\n",
      "Epoch 45\n",
      "Train Accuracy: 38.42%\n",
      "Test loss: 1.782, Accuracy: 37.52%\n",
      "Epoch 46\n",
      "Train Accuracy: 38.86%\n",
      "Test loss: 1.768, Accuracy: 38.84%\n",
      "Epoch 47\n",
      "Train Accuracy: 39.18%\n",
      "Test loss: 1.871, Accuracy: 34.04%\n",
      "Epoch 48\n",
      "Train Accuracy: 38.52%\n",
      "Test loss: 1.797, Accuracy: 37.35%\n",
      "Epoch 49\n",
      "Train Accuracy: 38.22%\n",
      "Test loss: 1.828, Accuracy: 36.75%\n",
      "Epoch 50\n",
      "Train Accuracy: 38.27%\n",
      "Test loss: 1.821, Accuracy: 36.63%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하기 싫을 때\n",
    "wandb.init(project=f'reservoir')\n",
    "main(data_path='/data2', which_data='CIFAR10',learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep하고싶을 때\n",
    "# def sweep_cover(data_path='/data2', which_data='CIFAR10', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "    \n",
    "#     wandb.init()\n",
    "\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     BATCH  =  wandb.config.batch_size\n",
    "#     TIME_STEP  =  wandb.config.time_step\n",
    "#     v_decay  =  wandb.config.decay\n",
    "#     pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "#     which_data  =  wandb.config.which_data\n",
    "#     data_path  =  wandb.config.data_path\n",
    "#     rate_coding  =  wandb.config.rate_coding\n",
    "#     EPOCH  =  wandb.config.EPOCH\n",
    "#     IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "#     main(data_path=data_path, which_data=which_data, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "# v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "\n",
    "\n",
    "\n",
    "# which_data_hyper = 'CIFAR10' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST'\n",
    "# data_path_hyper = '/data2'\n",
    "\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes',\n",
    "#     'name': which_data_hyper,\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "#         \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "#         \"time_step\": {\"values\": [4,5,6,7,8,9,10,11,12,13,14,15,16]},\n",
    "#         \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "#         \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "#         \"which_data\": {\"values\": [which_data_hyper]},\n",
    "#         \"data_path\": {\"values\": [data_path_hyper]},\n",
    "#         \"rate_coding\": {\"values\": [True, False]},\n",
    "#         \"EPOCH\": {\"values\": [20]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [32]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "# wandb.agent(sweep_id, function=sweep_cover, count=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
