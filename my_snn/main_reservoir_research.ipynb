{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        # class mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        print(f'train samples = {train_data.__len__()}, test samples = {test_data.__len__()}')\n",
    "        print(f'total samples = {train_data.__len__() + test_data.__len__()}')\n",
    "\n",
    "        ## 'Other' 클래스 배제 ########################################################################\n",
    "        # gesture_mapping = { 0 :'Hand Clapping' , 1 :'Right Hand Wave', 2:'Other',  3 :'Left Hand Wave' ,4 :'Right Arm CW'  , 5 :'Right Arm CCW' , 6 :'Left Arm CW' ,   7 :'Left Arm CCW' ,  8 :'Arm Roll'   ,    9 :'Air Drums'  ,    10 :'Air Guitar'}\n",
    "        \n",
    "        exclude_class = 2\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'/data2/gesture/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'/data2/gesture/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 classes\\' indices exist. we exclude the \\'other\\' class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 classes\\' indices doesn\\'t exist. we exclude the \\'other\\' class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "        # test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "\n",
    "        \n",
    "\n",
    "        # # # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        \n",
    "        # original\n",
    "        # train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, collate_fn=pad_sequence_collate, shuffle = True)\n",
    "        # test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, collate_fn=pad_sequence_collate, shuffle = False)\n",
    "        \n",
    "\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        \n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "        \n",
    "        ## DVS gesture에서 other label자리 매꾸기 ###############\n",
    "        if (which_data == 'DVS_GESTURE'):\n",
    "            labels[labels>2] -= 1\n",
    "        #######################################################\n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            ## DVS gesture에서 other label자리 매꾸기 ###############\n",
    "            if (which_data == 'DVS_GESTURE'):\n",
    "                labels[labels>2] -= 1\n",
    "            #######################################################\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: zxgwg8mn\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zxgwg8mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wg9sptu7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.9241953200299672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.012597818206628177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 4.209094551166574\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240730_233827-wg9sptu7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/wg9sptu7' target=\"_blank\">distinctive-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zxgwg8mn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zxgwg8mn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zxgwg8mn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zxgwg8mn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/wg9sptu7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/wg9sptu7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "train samples = 1176, test samples = 288\n",
      "total samples = 1464\n",
      "\n",
      "dvsgestrue 10 classes' indices exist. we exclude the 'other' class\n",
      "\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=32768, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 56.86%\n",
      "Test loss: 1.408, Val Accuracy: 61.36%\n",
      "Epoch 2\n",
      "Train Accuracy: 73.10%\n",
      "Test loss: 2.028, Val Accuracy: 54.55%\n",
      "Epoch 3\n",
      "Train Accuracy: 89.05%\n",
      "Test loss: 1.754, Val Accuracy: 63.64%\n",
      "Epoch 4\n",
      "Train Accuracy: 87.57%\n",
      "Test loss: 1.814, Val Accuracy: 58.71%\n",
      "Epoch 5\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '5', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.BATCH\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.0001, \"max\": 0.05},\n",
    "        \"BATCH\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [10_000]}, #100_000, 1_000_000, 10_000 , 1_000\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
