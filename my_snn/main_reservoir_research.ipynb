{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        self.conv_depthwise = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                        kernel_size=self.receptive_size, \n",
    "                                        stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.conv_depthwise.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv_depthwise.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "            recurrent_current = self.conv_depthwise(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NO_RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(NO_RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        \n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        exclude_class = 10\n",
    "        train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "        test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "    \n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if no_reservoir == True:\n",
    "        net = NO_RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    else:\n",
    "        net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 5uppwdo1\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f8uuflhy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.9455516917846862\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008805532307325892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 5.149551820475385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240725_213848-f8uuflhy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/f8uuflhy' target=\"_blank\">grateful-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/f8uuflhy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/f8uuflhy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (conv_depthwise): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  )\n",
      "  (linear): Linear(in_features=4608, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 27.27%\n",
      "Test loss: 1.581, Accuracy: 40.15%\n",
      "Epoch 2\n",
      "Train Accuracy: 53.53%\n",
      "Test loss: 1.308, Accuracy: 47.73%\n",
      "Epoch 3\n",
      "Train Accuracy: 59.74%\n",
      "Test loss: 1.578, Accuracy: 50.76%\n",
      "Epoch 4\n",
      "Train Accuracy: 66.42%\n",
      "Test loss: 1.313, Accuracy: 58.71%\n",
      "Epoch 5\n",
      "Train Accuracy: 71.99%\n",
      "Test loss: 1.176, Accuracy: 63.26%\n",
      "Epoch 6\n",
      "Train Accuracy: 84.32%\n",
      "Test loss: 1.332, Accuracy: 57.95%\n",
      "Epoch 7\n",
      "Train Accuracy: 80.06%\n",
      "Test loss: 1.451, Accuracy: 62.50%\n",
      "Epoch 8\n",
      "Train Accuracy: 84.14%\n",
      "Test loss: 1.177, Accuracy: 64.02%\n",
      "Epoch 9\n",
      "Train Accuracy: 90.72%\n",
      "Test loss: 1.232, Accuracy: 65.53%\n",
      "Epoch 10\n",
      "Train Accuracy: 91.19%\n",
      "Test loss: 1.113, Accuracy: 67.80%\n",
      "Epoch 11\n",
      "Train Accuracy: 89.33%\n",
      "Test loss: 1.413, Accuracy: 65.91%\n",
      "Epoch 12\n",
      "Train Accuracy: 92.21%\n",
      "Test loss: 1.619, Accuracy: 69.70%\n",
      "Epoch 13\n",
      "Train Accuracy: 94.34%\n",
      "Test loss: 1.248, Accuracy: 71.21%\n",
      "Epoch 14\n",
      "Train Accuracy: 96.66%\n",
      "Test loss: 1.234, Accuracy: 67.80%\n",
      "Epoch 15\n",
      "Train Accuracy: 97.40%\n",
      "Test loss: 1.271, Accuracy: 68.94%\n",
      "Epoch 16\n",
      "Train Accuracy: 98.89%\n",
      "Test loss: 1.346, Accuracy: 68.56%\n",
      "Epoch 17\n",
      "Train Accuracy: 98.61%\n",
      "Test loss: 1.368, Accuracy: 69.70%\n",
      "Epoch 18\n",
      "Train Accuracy: 98.98%\n",
      "Test loss: 1.254, Accuracy: 71.59%\n",
      "Epoch 19\n",
      "Train Accuracy: 98.89%\n",
      "Test loss: 1.273, Accuracy: 66.29%\n",
      "Epoch 20\n",
      "Train Accuracy: 99.26%\n",
      "Test loss: 1.494, Accuracy: 65.53%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c474f34c45f745648a0ef30cc3f56509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='20.841 MB of 20.841 MB uploaded (20.318 MB deduped)\\r'), FloatProgress(value=1.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 97.2%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter_accuracy</td><td>▁▃▄▄▄▅▄▅▅▆▆▇▆▇▆▆█▇▇▇▇▇▇█▇███████████████</td></tr><tr><td>tr_accuracy</td><td>▁▄▄▅▅▇▆▇▇▇▇▇████████</td></tr><tr><td>val_accuracy</td><td>▁▃▃▅▆▅▆▆▇▇▇██▇▇▇██▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter_accuracy</td><td>100.0</td></tr><tr><td>tr_accuracy</td><td>99.25788</td></tr><tr><td>val_accuracy</td><td>65.5303</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/f8uuflhy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/f8uuflhy</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 27 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240725_213848-f8uuflhy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cqeb896q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7109833806158179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.022035292410752725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 8.895036907164805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc01c280a4c464d9ac38a4a4410181e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113692566545473, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240725_214136-cqeb896q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/cqeb896q' target=\"_blank\">astral-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/5uppwdo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/cqeb896q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/cqeb896q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "The directory [/data2/gesture/duration_1000000] already exists.\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (conv_depthwise): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  )\n",
      "  (linear): Linear(in_features=32768, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': which_data_hyper,\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [50]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
