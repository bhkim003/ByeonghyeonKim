{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11463/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                # if epoch > 0:\n",
    "                #     assert val_acc_best > 0.2\n",
    "                # elif epoch > 10:\n",
    "                #     assert val_acc_best > 0.4\n",
    "                # elif epoch > 30:\n",
    "                #     assert val_acc_best > 0.5\n",
    "                # elif epoch > 100:\n",
    "                #     assert val_acc_best > 0.6\n",
    "                    \n",
    "                \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"3\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: djij695v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 36968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251119_144653-djij695v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/djij695v' target=\"_blank\">tough-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/djij695v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/djij695v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251119_144701_854', 'my_seed': 36968, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [16, 16, 16], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 16\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 16 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 16\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 16 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 16\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 16 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[16, 16, 16], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[16, 16, 16], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[16, 16, 16], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 86.0\n",
      "lif layer 1 self.abs_max_v: 86.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 144.0\n",
      "lif layer 1 self.abs_max_v: 168.0\n",
      "fc layer 2 self.abs_max_out: 96.0\n",
      "lif layer 2 self.abs_max_v: 96.0\n",
      "lif layer 1 self.abs_max_v: 191.0\n",
      "fc layer 2 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 141.5\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 2 self.abs_max_out: 146.0\n",
      "lif layer 2 self.abs_max_v: 185.5\n",
      "fc layer 1 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 206.5\n",
      "fc layer 2 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 262.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 185.0\n",
      "lif layer 1 self.abs_max_v: 233.0\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "lif layer 1 self.abs_max_v: 309.5\n",
      "lif layer 2 self.abs_max_v: 309.0\n",
      "lif layer 1 self.abs_max_v: 329.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "lif layer 2 self.abs_max_v: 335.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 1 self.abs_max_out: 250.0\n",
      "fc layer 1 self.abs_max_out: 284.0\n",
      "fc layer 1 self.abs_max_out: 500.0\n",
      "lif layer 1 self.abs_max_v: 500.0\n",
      "fc layer 2 self.abs_max_out: 298.0\n",
      "fc layer 2 self.abs_max_out: 372.0\n",
      "lif layer 2 self.abs_max_v: 418.5\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "lif layer 1 self.abs_max_v: 550.0\n",
      "lif layer 2 self.abs_max_v: 524.5\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 563.5\n",
      "lif layer 2 self.abs_max_v: 580.0\n",
      "fc layer 2 self.abs_max_out: 511.0\n",
      "lif layer 2 self.abs_max_v: 746.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "lif layer 1 self.abs_max_v: 586.0\n",
      "fc layer 1 self.abs_max_out: 713.0\n",
      "lif layer 1 self.abs_max_v: 713.0\n",
      "lif layer 2 self.abs_max_v: 757.0\n",
      "fc layer 2 self.abs_max_out: 515.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 2 self.abs_max_out: 556.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "lif layer 2 self.abs_max_v: 757.5\n",
      "lif layer 2 self.abs_max_v: 789.0\n",
      "lif layer 1 self.abs_max_v: 725.0\n",
      "lif layer 1 self.abs_max_v: 807.5\n",
      "lif layer 1 self.abs_max_v: 842.0\n",
      "lif layer 2 self.abs_max_v: 794.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "lif layer 1 self.abs_max_v: 851.5\n",
      "lif layer 2 self.abs_max_v: 839.0\n",
      "lif layer 2 self.abs_max_v: 858.0\n",
      "lif layer 2 self.abs_max_v: 868.0\n",
      "lif layer 2 self.abs_max_v: 1034.0\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 903.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 3 self.abs_max_out: 219.0\n",
      "lif layer 2 self.abs_max_v: 1058.0\n",
      "lif layer 2 self.abs_max_v: 1119.0\n",
      "fc layer 2 self.abs_max_out: 683.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 1129.0\n",
      "lif layer 2 self.abs_max_v: 1168.0\n",
      "lif layer 1 self.abs_max_v: 905.5\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "fc layer 1 self.abs_max_out: 1028.0\n",
      "lif layer 1 self.abs_max_v: 1028.0\n",
      "lif layer 2 self.abs_max_v: 1195.0\n",
      "lif layer 2 self.abs_max_v: 1222.5\n",
      "lif layer 2 self.abs_max_v: 1260.5\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "lif layer 2 self.abs_max_v: 1341.0\n",
      "fc layer 1 self.abs_max_out: 1111.0\n",
      "lif layer 1 self.abs_max_v: 1111.0\n",
      "fc layer 2 self.abs_max_out: 757.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "lif layer 1 self.abs_max_v: 1114.5\n",
      "lif layer 1 self.abs_max_v: 1281.5\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "lif layer 2 self.abs_max_v: 1364.0\n",
      "lif layer 2 self.abs_max_v: 1387.5\n",
      "lif layer 1 self.abs_max_v: 1415.0\n",
      "lif layer 1 self.abs_max_v: 1415.5\n",
      "fc layer 2 self.abs_max_out: 901.0\n",
      "lif layer 2 self.abs_max_v: 1410.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "lif layer 1 self.abs_max_v: 1421.5\n",
      "lif layer 1 self.abs_max_v: 1429.5\n",
      "lif layer 1 self.abs_max_v: 1499.0\n",
      "lif layer 1 self.abs_max_v: 1500.5\n",
      "lif layer 1 self.abs_max_v: 1522.0\n",
      "fc layer 2 self.abs_max_out: 942.0\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 1412.0\n",
      "lif layer 2 self.abs_max_v: 1415.0\n",
      "lif layer 2 self.abs_max_v: 1474.5\n",
      "lif layer 1 self.abs_max_v: 1569.5\n",
      "lif layer 1 self.abs_max_v: 1637.0\n",
      "fc layer 2 self.abs_max_out: 945.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 1 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 1068.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 1 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1682.0\n",
      "lif layer 1 self.abs_max_v: 1780.0\n",
      "lif layer 1 self.abs_max_v: 1882.0\n",
      "fc layer 1 self.abs_max_out: 1172.0\n",
      "fc layer 2 self.abs_max_out: 1084.0\n",
      "fc layer 1 self.abs_max_out: 1211.0\n",
      "lif layer 2 self.abs_max_v: 1489.5\n",
      "lif layer 2 self.abs_max_v: 1518.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 2 self.abs_max_out: 1148.0\n",
      "lif layer 1 self.abs_max_v: 1931.5\n",
      "lif layer 2 self.abs_max_v: 1522.0\n",
      "lif layer 2 self.abs_max_v: 1596.0\n",
      "fc layer 1 self.abs_max_out: 1243.0\n",
      "fc layer 1 self.abs_max_out: 1397.0\n",
      "lif layer 2 self.abs_max_v: 1616.0\n",
      "lif layer 1 self.abs_max_v: 1945.5\n",
      "lif layer 1 self.abs_max_v: 2045.5\n",
      "lif layer 1 self.abs_max_v: 2077.0\n",
      "lif layer 1 self.abs_max_v: 2248.5\n",
      "fc layer 3 self.abs_max_out: 365.0\n",
      "fc layer 1 self.abs_max_out: 1434.0\n",
      "fc layer 1 self.abs_max_out: 1450.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "fc layer 1 self.abs_max_out: 1632.0\n",
      "fc layer 2 self.abs_max_out: 1173.0\n",
      "lif layer 1 self.abs_max_v: 2341.5\n",
      "lif layer 1 self.abs_max_v: 2523.0\n",
      "lif layer 1 self.abs_max_v: 2574.5\n",
      "fc layer 1 self.abs_max_out: 1692.0\n",
      "fc layer 1 self.abs_max_out: 1700.0\n",
      "lif layer 2 self.abs_max_v: 1616.5\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.804614/  2.002697, val:  28.33%, val_best:  28.33%, tr:  96.32%, tr_best:  96.32%, epoch time: 89.32 seconds, 1.49 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 76.0961%\n",
      "layer   3  Sparsity: 72.5304%\n",
      "total_backward_count 9790 real_backward_count 2289  23.381%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 2741.0\n",
      "lif layer 1 self.abs_max_v: 2781.5\n",
      "fc layer 1 self.abs_max_out: 1809.0\n",
      "fc layer 2 self.abs_max_out: 1260.0\n",
      "fc layer 3 self.abs_max_out: 387.0\n",
      "lif layer 1 self.abs_max_v: 2813.5\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 1 self.abs_max_out: 1896.0\n",
      "lif layer 1 self.abs_max_v: 2828.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 1650.0\n",
      "lif layer 2 self.abs_max_v: 1709.0\n",
      "fc layer 2 self.abs_max_out: 1284.0\n",
      "fc layer 2 self.abs_max_out: 1438.0\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "lif layer 2 self.abs_max_v: 1711.5\n",
      "lif layer 2 self.abs_max_v: 1761.5\n",
      "lif layer 2 self.abs_max_v: 1771.5\n",
      "fc layer 2 self.abs_max_out: 1555.0\n",
      "fc layer 2 self.abs_max_out: 1602.0\n",
      "lif layer 2 self.abs_max_v: 1816.0\n",
      "lif layer 2 self.abs_max_v: 1819.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.690340/  1.903046, val:  47.92%, val_best:  47.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 86.47 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 74.7452%\n",
      "layer   3  Sparsity: 69.9209%\n",
      "total_backward_count 19580 real_backward_count 3799  19.402%\n",
      "lif layer 2 self.abs_max_v: 1857.5\n",
      "lif layer 2 self.abs_max_v: 1858.0\n",
      "fc layer 1 self.abs_max_out: 1994.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "lif layer 1 self.abs_max_v: 2866.0\n",
      "fc layer 1 self.abs_max_out: 2009.0\n",
      "lif layer 2 self.abs_max_v: 1894.5\n",
      "lif layer 2 self.abs_max_v: 1908.0\n",
      "lif layer 2 self.abs_max_v: 2028.5\n",
      "lif layer 2 self.abs_max_v: 2034.0\n",
      "lif layer 2 self.abs_max_v: 2041.5\n",
      "lif layer 2 self.abs_max_v: 2064.0\n",
      "lif layer 2 self.abs_max_v: 2068.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "lif layer 1 self.abs_max_v: 3015.5\n",
      "lif layer 1 self.abs_max_v: 3277.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.645771/  1.884696, val:  44.58%, val_best:  47.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 73.9518%\n",
      "layer   3  Sparsity: 69.7586%\n",
      "total_backward_count 29370 real_backward_count 5247  17.865%\n",
      "lif layer 2 self.abs_max_v: 2106.5\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "lif layer 1 self.abs_max_v: 3581.5\n",
      "lif layer 1 self.abs_max_v: 3657.5\n",
      "fc layer 2 self.abs_max_out: 1668.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.642007/  1.884059, val:  51.67%, val_best:  51.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 85.99 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 73.7870%\n",
      "layer   3  Sparsity: 68.9074%\n",
      "total_backward_count 39160 real_backward_count 6616  16.895%\n",
      "lif layer 2 self.abs_max_v: 2120.5\n",
      "lif layer 2 self.abs_max_v: 2187.5\n",
      "fc layer 1 self.abs_max_out: 2192.0\n",
      "fc layer 1 self.abs_max_out: 2279.0\n",
      "lif layer 1 self.abs_max_v: 3726.0\n",
      "lif layer 1 self.abs_max_v: 3940.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.618952/  1.890922, val:  36.25%, val_best:  51.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   2  Sparsity: 74.1901%\n",
      "layer   3  Sparsity: 69.7453%\n",
      "total_backward_count 48950 real_backward_count 7946  16.233%\n",
      "fc layer 1 self.abs_max_out: 2281.0\n",
      "lif layer 1 self.abs_max_v: 3979.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.590634/  1.839304, val:  47.08%, val_best:  51.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 74.0024%\n",
      "layer   3  Sparsity: 69.1921%\n",
      "total_backward_count 58740 real_backward_count 9227  15.708%\n",
      "lif layer 2 self.abs_max_v: 2190.5\n",
      "lif layer 2 self.abs_max_v: 2212.0\n",
      "fc layer 1 self.abs_max_out: 2429.0\n",
      "fc layer 1 self.abs_max_out: 2529.0\n",
      "lif layer 1 self.abs_max_v: 4225.5\n",
      "lif layer 1 self.abs_max_v: 4355.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.585418/  1.832599, val:  50.42%, val_best:  51.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 73.3037%\n",
      "layer   3  Sparsity: 68.3148%\n",
      "total_backward_count 68530 real_backward_count 10537  15.376%\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "lif layer 1 self.abs_max_v: 4435.5\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 2663.0\n",
      "fc layer 1 self.abs_max_out: 2802.0\n",
      "lif layer 1 self.abs_max_v: 4676.0\n",
      "lif layer 1 self.abs_max_v: 4847.0\n",
      "lif layer 1 self.abs_max_v: 5076.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.538780/  1.789261, val:  44.17%, val_best:  51.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 86.70 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0284%\n",
      "layer   2  Sparsity: 73.0353%\n",
      "layer   3  Sparsity: 68.5448%\n",
      "total_backward_count 78320 real_backward_count 11793  15.057%\n",
      "fc layer 2 self.abs_max_out: 1727.0\n",
      "lif layer 2 self.abs_max_v: 2258.5\n",
      "fc layer 1 self.abs_max_out: 2949.0\n",
      "lif layer 1 self.abs_max_v: 5257.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.533701/  1.863374, val:  42.08%, val_best:  51.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 72.6662%\n",
      "layer   3  Sparsity: 68.5547%\n",
      "total_backward_count 88110 real_backward_count 13011  14.767%\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "fc layer 3 self.abs_max_out: 524.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.539376/  1.763068, val:  48.75%, val_best:  51.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 73.1156%\n",
      "layer   3  Sparsity: 68.2137%\n",
      "total_backward_count 97900 real_backward_count 14232  14.537%\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "lif layer 2 self.abs_max_v: 2425.5\n",
      "lif layer 2 self.abs_max_v: 2508.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.517243/  1.782101, val:  59.17%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 73.5848%\n",
      "layer   3  Sparsity: 68.9180%\n",
      "total_backward_count 107690 real_backward_count 15439  14.337%\n",
      "fc layer 1 self.abs_max_out: 3100.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.535815/  1.816965, val:  41.25%, val_best:  59.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 72.9995%\n",
      "layer   3  Sparsity: 69.0508%\n",
      "total_backward_count 117480 real_backward_count 16633  14.158%\n",
      "fc layer 1 self.abs_max_out: 3148.0\n",
      "lif layer 1 self.abs_max_v: 5336.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.510949/  1.744962, val:  57.92%, val_best:  59.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   2  Sparsity: 72.5810%\n",
      "layer   3  Sparsity: 69.1912%\n",
      "total_backward_count 127270 real_backward_count 17806  13.991%\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "lif layer 1 self.abs_max_v: 5355.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.497921/  1.748957, val:  55.00%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 72.1836%\n",
      "layer   3  Sparsity: 68.8743%\n",
      "total_backward_count 137060 real_backward_count 18993  13.857%\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "fc layer 1 self.abs_max_out: 3326.0\n",
      "lif layer 1 self.abs_max_v: 5380.0\n",
      "lif layer 1 self.abs_max_v: 5561.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.496863/  1.730400, val:  48.33%, val_best:  59.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 72.6696%\n",
      "layer   3  Sparsity: 69.1692%\n",
      "total_backward_count 146850 real_backward_count 20144  13.717%\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 2 self.abs_max_out: 1759.0\n",
      "fc layer 1 self.abs_max_out: 3372.0\n",
      "lif layer 1 self.abs_max_v: 5619.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.477499/  1.742455, val:  55.83%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   2  Sparsity: 72.2646%\n",
      "layer   3  Sparsity: 69.3137%\n",
      "total_backward_count 156640 real_backward_count 21259  13.572%\n",
      "fc layer 2 self.abs_max_out: 1791.0\n",
      "fc layer 2 self.abs_max_out: 1866.0\n",
      "fc layer 2 self.abs_max_out: 1894.0\n",
      "fc layer 2 self.abs_max_out: 1909.0\n",
      "fc layer 1 self.abs_max_out: 3444.0\n",
      "lif layer 1 self.abs_max_v: 5655.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.475499/  1.746112, val:  54.58%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 72.1430%\n",
      "layer   3  Sparsity: 69.6252%\n",
      "total_backward_count 166430 real_backward_count 22403  13.461%\n",
      "lif layer 1 self.abs_max_v: 5713.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.475197/  1.719251, val:  55.00%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   2  Sparsity: 72.0501%\n",
      "layer   3  Sparsity: 69.6546%\n",
      "total_backward_count 176220 real_backward_count 23607  13.396%\n",
      "lif layer 2 self.abs_max_v: 2514.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.469155/  1.697918, val:  56.25%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1093%\n",
      "layer   2  Sparsity: 71.9260%\n",
      "layer   3  Sparsity: 70.0292%\n",
      "total_backward_count 186010 real_backward_count 24697  13.277%\n",
      "lif layer 2 self.abs_max_v: 2577.5\n",
      "fc layer 1 self.abs_max_out: 3544.0\n",
      "lif layer 1 self.abs_max_v: 5782.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.429180/  1.691376, val:  66.25%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 71.9537%\n",
      "layer   3  Sparsity: 68.6434%\n",
      "total_backward_count 195800 real_backward_count 25772  13.162%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "lif layer 2 self.abs_max_v: 2592.0\n",
      "lif layer 2 self.abs_max_v: 2595.5\n",
      "lif layer 2 self.abs_max_v: 2703.5\n",
      "fc layer 1 self.abs_max_out: 3574.0\n",
      "lif layer 1 self.abs_max_v: 5909.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.436787/  1.654717, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.06 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 72.0012%\n",
      "layer   3  Sparsity: 68.3744%\n",
      "total_backward_count 205590 real_backward_count 26841  13.056%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.408764/  1.649447, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   2  Sparsity: 71.8627%\n",
      "layer   3  Sparsity: 68.7373%\n",
      "total_backward_count 215380 real_backward_count 27837  12.925%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.423449/  1.664663, val:  62.50%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 71.4341%\n",
      "layer   3  Sparsity: 68.6388%\n",
      "total_backward_count 225170 real_backward_count 28911  12.840%\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "lif layer 2 self.abs_max_v: 2705.0\n",
      "lif layer 2 self.abs_max_v: 2719.5\n",
      "lif layer 2 self.abs_max_v: 2754.0\n",
      "lif layer 2 self.abs_max_v: 3110.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.393929/  1.657868, val:  58.33%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.11 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 71.7458%\n",
      "layer   3  Sparsity: 68.9629%\n",
      "total_backward_count 234960 real_backward_count 29939  12.742%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.375124/  1.628138, val:  62.50%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.94 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 71.8596%\n",
      "layer   3  Sparsity: 68.4398%\n",
      "total_backward_count 244750 real_backward_count 30983  12.659%\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "fc layer 2 self.abs_max_out: 1945.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.368577/  1.638626, val:  58.33%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 71.6910%\n",
      "layer   3  Sparsity: 68.5609%\n",
      "total_backward_count 254540 real_backward_count 31965  12.558%\n",
      "fc layer 1 self.abs_max_out: 3683.0\n",
      "lif layer 1 self.abs_max_v: 6020.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.345987/  1.608572, val:  65.83%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 71.1566%\n",
      "layer   3  Sparsity: 68.9435%\n",
      "total_backward_count 264330 real_backward_count 32932  12.459%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.363930/  1.619168, val:  63.33%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   2  Sparsity: 71.1628%\n",
      "layer   3  Sparsity: 69.0480%\n",
      "total_backward_count 274120 real_backward_count 33850  12.349%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.391293/  1.617505, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.04 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   2  Sparsity: 71.3711%\n",
      "layer   3  Sparsity: 69.6784%\n",
      "total_backward_count 283910 real_backward_count 34809  12.261%\n",
      "fc layer 2 self.abs_max_out: 1957.0\n",
      "fc layer 2 self.abs_max_out: 1971.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.329423/  1.562503, val:  74.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 70.9840%\n",
      "layer   3  Sparsity: 69.4928%\n",
      "total_backward_count 293700 real_backward_count 35776  12.181%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.323445/  1.581418, val:  70.00%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   2  Sparsity: 71.1592%\n",
      "layer   3  Sparsity: 69.2801%\n",
      "total_backward_count 303490 real_backward_count 36666  12.081%\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.322935/  1.567618, val:  63.75%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 70.8790%\n",
      "layer   3  Sparsity: 68.4423%\n",
      "total_backward_count 313280 real_backward_count 37547  11.985%\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "lif layer 2 self.abs_max_v: 3111.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.317474/  1.552107, val:  70.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.95 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   2  Sparsity: 70.7794%\n",
      "layer   3  Sparsity: 69.4228%\n",
      "total_backward_count 323070 real_backward_count 38390  11.883%\n",
      "fc layer 2 self.abs_max_out: 2084.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.295434/  1.545796, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 70.9747%\n",
      "layer   3  Sparsity: 69.2759%\n",
      "total_backward_count 332860 real_backward_count 39271  11.798%\n",
      "fc layer 2 self.abs_max_out: 2137.0\n",
      "lif layer 1 self.abs_max_v: 6034.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.290168/  1.541747, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   2  Sparsity: 70.8977%\n",
      "layer   3  Sparsity: 69.0726%\n",
      "total_backward_count 342650 real_backward_count 40089  11.700%\n",
      "lif layer 2 self.abs_max_v: 3224.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.287490/  1.529344, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.05 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   2  Sparsity: 71.0629%\n",
      "layer   3  Sparsity: 69.0876%\n",
      "total_backward_count 352440 real_backward_count 40961  11.622%\n",
      "lif layer 1 self.abs_max_v: 6102.5\n",
      "lif layer 1 self.abs_max_v: 6389.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.275356/  1.519374, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   2  Sparsity: 70.6667%\n",
      "layer   3  Sparsity: 69.1686%\n",
      "total_backward_count 362230 real_backward_count 41788  11.536%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.271115/  1.473334, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 70.9920%\n",
      "layer   3  Sparsity: 69.3635%\n",
      "total_backward_count 372020 real_backward_count 42583  11.446%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.269999/  1.512294, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   2  Sparsity: 71.4702%\n",
      "layer   3  Sparsity: 69.3925%\n",
      "total_backward_count 381810 real_backward_count 43420  11.372%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.274181/  1.485108, val:  73.33%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0220%\n",
      "layer   2  Sparsity: 71.0257%\n",
      "layer   3  Sparsity: 68.9753%\n",
      "total_backward_count 391600 real_backward_count 44235  11.296%\n",
      "fc layer 2 self.abs_max_out: 2180.0\n",
      "fc layer 2 self.abs_max_out: 2277.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.235453/  1.507961, val:  73.33%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 70.8934%\n",
      "layer   3  Sparsity: 69.3373%\n",
      "total_backward_count 401390 real_backward_count 44999  11.211%\n",
      "fc layer 1 self.abs_max_out: 3715.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.240667/  1.488100, val:  67.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1293%\n",
      "layer   2  Sparsity: 71.0178%\n",
      "layer   3  Sparsity: 69.3961%\n",
      "total_backward_count 411180 real_backward_count 45791  11.136%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.241649/  1.489084, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0541%\n",
      "layer   2  Sparsity: 71.2008%\n",
      "layer   3  Sparsity: 69.4531%\n",
      "total_backward_count 420970 real_backward_count 46560  11.060%\n",
      "fc layer 1 self.abs_max_out: 3915.0\n",
      "lif layer 1 self.abs_max_v: 6449.5\n",
      "lif layer 1 self.abs_max_v: 6742.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.242769/  1.506466, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.37 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 71.0654%\n",
      "layer   3  Sparsity: 69.3831%\n",
      "total_backward_count 430760 real_backward_count 47343  10.991%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.249140/  1.526098, val:  70.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1028%\n",
      "layer   2  Sparsity: 70.7550%\n",
      "layer   3  Sparsity: 69.2628%\n",
      "total_backward_count 440550 real_backward_count 48107  10.920%\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "lif layer 2 self.abs_max_v: 3404.5\n",
      "lif layer 2 self.abs_max_v: 3442.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.233066/  1.476515, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 70.5913%\n",
      "layer   3  Sparsity: 69.0100%\n",
      "total_backward_count 450340 real_backward_count 48840  10.845%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.220551/  1.482209, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 70.6851%\n",
      "layer   3  Sparsity: 69.1015%\n",
      "total_backward_count 460130 real_backward_count 49585  10.776%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.236980/  1.508637, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.88 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 70.8323%\n",
      "layer   3  Sparsity: 69.2142%\n",
      "total_backward_count 469920 real_backward_count 50327  10.710%\n",
      "fc layer 1 self.abs_max_out: 3928.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.227795/  1.499192, val:  75.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   2  Sparsity: 70.3621%\n",
      "layer   3  Sparsity: 69.2979%\n",
      "total_backward_count 479710 real_backward_count 51072  10.646%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.229515/  1.504645, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.20 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 70.4982%\n",
      "layer   3  Sparsity: 69.2060%\n",
      "total_backward_count 489500 real_backward_count 51810  10.584%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.232592/  1.497996, val:  67.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.54 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 70.0676%\n",
      "layer   3  Sparsity: 69.8724%\n",
      "total_backward_count 499290 real_backward_count 52535  10.522%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "fc layer 3 self.abs_max_out: 696.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.224848/  1.465335, val:  72.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.46 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 70.1762%\n",
      "layer   3  Sparsity: 69.7203%\n",
      "total_backward_count 509080 real_backward_count 53227  10.456%\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.209755/  1.439747, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.70 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   2  Sparsity: 70.3354%\n",
      "layer   3  Sparsity: 69.7178%\n",
      "total_backward_count 518870 real_backward_count 53917  10.391%\n",
      "lif layer 2 self.abs_max_v: 3519.0\n",
      "fc layer 1 self.abs_max_out: 3977.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.237280/  1.462966, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.85 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 70.4033%\n",
      "layer   3  Sparsity: 70.0917%\n",
      "total_backward_count 528660 real_backward_count 54579  10.324%\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.206031/  1.452582, val:  80.83%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 83.27 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0490%\n",
      "layer   2  Sparsity: 70.4197%\n",
      "layer   3  Sparsity: 70.0586%\n",
      "total_backward_count 538450 real_backward_count 55216  10.255%\n",
      "lif layer 2 self.abs_max_v: 3622.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.198957/  1.449603, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   2  Sparsity: 70.3386%\n",
      "layer   3  Sparsity: 70.4801%\n",
      "total_backward_count 548240 real_backward_count 55890  10.194%\n",
      "fc layer 1 self.abs_max_out: 4075.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.187134/  1.433182, val:  74.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.12 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 70.7037%\n",
      "layer   3  Sparsity: 70.6212%\n",
      "total_backward_count 558030 real_backward_count 56535  10.131%\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.161669/  1.398484, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.20 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1129%\n",
      "layer   2  Sparsity: 70.8541%\n",
      "layer   3  Sparsity: 70.3267%\n",
      "total_backward_count 567820 real_backward_count 57130  10.061%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.154350/  1.406350, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 70.5840%\n",
      "layer   3  Sparsity: 70.0967%\n",
      "total_backward_count 577610 real_backward_count 57762  10.000%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.155544/  1.382969, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0331%\n",
      "layer   2  Sparsity: 70.5333%\n",
      "layer   3  Sparsity: 69.5149%\n",
      "total_backward_count 587400 real_backward_count 58410   9.944%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.151199/  1.390677, val:  79.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   2  Sparsity: 70.0616%\n",
      "layer   3  Sparsity: 69.0197%\n",
      "total_backward_count 597190 real_backward_count 59068   9.891%\n",
      "fc layer 2 self.abs_max_out: 2315.0\n",
      "fc layer 2 self.abs_max_out: 2379.0\n",
      "lif layer 1 self.abs_max_v: 6794.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.156964/  1.420143, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 69.9845%\n",
      "layer   3  Sparsity: 69.2254%\n",
      "total_backward_count 606980 real_backward_count 59665   9.830%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.155334/  1.401651, val:  78.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.36 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 70.2134%\n",
      "layer   3  Sparsity: 69.0605%\n",
      "total_backward_count 616770 real_backward_count 60277   9.773%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.161510/  1.415252, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   2  Sparsity: 70.6807%\n",
      "layer   3  Sparsity: 69.5299%\n",
      "total_backward_count 626560 real_backward_count 60902   9.720%\n",
      "fc layer 1 self.abs_max_out: 4235.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.157813/  1.424017, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   2  Sparsity: 70.4216%\n",
      "layer   3  Sparsity: 69.1125%\n",
      "total_backward_count 636350 real_backward_count 61480   9.661%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.162452/  1.388819, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.28 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   2  Sparsity: 70.4791%\n",
      "layer   3  Sparsity: 68.7311%\n",
      "total_backward_count 646140 real_backward_count 62088   9.609%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.149572/  1.373498, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   2  Sparsity: 70.3882%\n",
      "layer   3  Sparsity: 68.4162%\n",
      "total_backward_count 655930 real_backward_count 62684   9.557%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.137245/  1.385685, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.42 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 70.2485%\n",
      "layer   3  Sparsity: 69.0349%\n",
      "total_backward_count 665720 real_backward_count 63229   9.498%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.127673/  1.377539, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.82 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   2  Sparsity: 70.2900%\n",
      "layer   3  Sparsity: 69.2535%\n",
      "total_backward_count 675510 real_backward_count 63798   9.444%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.122464/  1.350269, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.67 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.1040%\n",
      "layer   2  Sparsity: 70.5945%\n",
      "layer   3  Sparsity: 69.4921%\n",
      "total_backward_count 685300 real_backward_count 64317   9.385%\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.124821/  1.379697, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 70.3708%\n",
      "layer   3  Sparsity: 69.5604%\n",
      "total_backward_count 695090 real_backward_count 64867   9.332%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.119143/  1.394718, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   2  Sparsity: 70.4161%\n",
      "layer   3  Sparsity: 69.4713%\n",
      "total_backward_count 704880 real_backward_count 65369   9.274%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.137227/  1.421601, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 70.0996%\n",
      "layer   3  Sparsity: 69.3016%\n",
      "total_backward_count 714670 real_backward_count 65885   9.219%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.136704/  1.339478, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.05 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 70.2283%\n",
      "layer   3  Sparsity: 68.8095%\n",
      "total_backward_count 724460 real_backward_count 66420   9.168%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.087979/  1.370899, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.67 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   2  Sparsity: 70.3984%\n",
      "layer   3  Sparsity: 69.4140%\n",
      "total_backward_count 734250 real_backward_count 66903   9.112%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.099443/  1.358067, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0295%\n",
      "layer   2  Sparsity: 70.5246%\n",
      "layer   3  Sparsity: 69.2105%\n",
      "total_backward_count 744040 real_backward_count 67393   9.058%\n",
      "fc layer 1 self.abs_max_out: 4432.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.116605/  1.381386, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   2  Sparsity: 70.2661%\n",
      "layer   3  Sparsity: 69.0122%\n",
      "total_backward_count 753830 real_backward_count 67919   9.010%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.102684/  1.380348, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.27 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   2  Sparsity: 70.1643%\n",
      "layer   3  Sparsity: 69.5125%\n",
      "total_backward_count 763620 real_backward_count 68451   8.964%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.117976/  1.367347, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.82 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 70.1045%\n",
      "layer   3  Sparsity: 69.5737%\n",
      "total_backward_count 773410 real_backward_count 68964   8.917%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.136015/  1.359956, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.30 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 69.9269%\n",
      "layer   3  Sparsity: 69.8812%\n",
      "total_backward_count 783200 real_backward_count 69471   8.870%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.125286/  1.355778, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 69.9093%\n",
      "layer   3  Sparsity: 69.9696%\n",
      "total_backward_count 792990 real_backward_count 69979   8.825%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.106323/  1.358968, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.88 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 69.6785%\n",
      "layer   3  Sparsity: 70.0561%\n",
      "total_backward_count 802780 real_backward_count 70494   8.781%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.081289/  1.321038, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 69.9884%\n",
      "layer   3  Sparsity: 69.6853%\n",
      "total_backward_count 812570 real_backward_count 70982   8.735%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.075135/  1.325233, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.36 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   2  Sparsity: 70.2838%\n",
      "layer   3  Sparsity: 69.2657%\n",
      "total_backward_count 822360 real_backward_count 71427   8.686%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.081680/  1.346093, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.73 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   2  Sparsity: 70.3663%\n",
      "layer   3  Sparsity: 68.9662%\n",
      "total_backward_count 832150 real_backward_count 71894   8.640%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.077030/  1.325212, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.24 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   2  Sparsity: 70.3906%\n",
      "layer   3  Sparsity: 68.4996%\n",
      "total_backward_count 841940 real_backward_count 72358   8.594%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.068921/  1.345507, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.17 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 70.3035%\n",
      "layer   3  Sparsity: 68.6187%\n",
      "total_backward_count 851730 real_backward_count 72807   8.548%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.074258/  1.322998, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.17 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 70.3603%\n",
      "layer   3  Sparsity: 68.3671%\n",
      "total_backward_count 861520 real_backward_count 73292   8.507%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.060811/  1.306123, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   2  Sparsity: 70.2133%\n",
      "layer   3  Sparsity: 69.0676%\n",
      "total_backward_count 871310 real_backward_count 73743   8.463%\n",
      "fc layer 3 self.abs_max_out: 746.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.056302/  1.309868, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.42 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 69.8639%\n",
      "layer   3  Sparsity: 69.1411%\n",
      "total_backward_count 881100 real_backward_count 74209   8.422%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.044786/  1.308025, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.41 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   2  Sparsity: 69.7455%\n",
      "layer   3  Sparsity: 69.6361%\n",
      "total_backward_count 890890 real_backward_count 74670   8.382%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.036294/  1.305097, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 69.7949%\n",
      "layer   3  Sparsity: 69.7310%\n",
      "total_backward_count 900680 real_backward_count 75106   8.339%\n",
      "fc layer 2 self.abs_max_out: 2450.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.046194/  1.326565, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.22 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 70.3580%\n",
      "layer   3  Sparsity: 69.5021%\n",
      "total_backward_count 910470 real_backward_count 75559   8.299%\n",
      "lif layer 1 self.abs_max_v: 6893.5\n",
      "lif layer 1 self.abs_max_v: 6929.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.042681/  1.284024, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.92 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 70.2532%\n",
      "layer   3  Sparsity: 69.5318%\n",
      "total_backward_count 920260 real_backward_count 76015   8.260%\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "lif layer 1 self.abs_max_v: 6932.5\n",
      "lif layer 1 self.abs_max_v: 6967.5\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.038947/  1.301694, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   2  Sparsity: 69.9415%\n",
      "layer   3  Sparsity: 70.1333%\n",
      "total_backward_count 930050 real_backward_count 76421   8.217%\n",
      "lif layer 2 self.abs_max_v: 3637.5\n",
      "fc layer 3 self.abs_max_out: 759.0\n",
      "fc layer 3 self.abs_max_out: 766.0\n",
      "fc layer 3 self.abs_max_out: 782.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.030687/  1.313490, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 69.8651%\n",
      "layer   3  Sparsity: 69.9930%\n",
      "total_backward_count 939840 real_backward_count 76814   8.173%\n",
      "lif layer 1 self.abs_max_v: 7021.5\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.033373/  1.310829, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.75 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 69.7646%\n",
      "layer   3  Sparsity: 69.7311%\n",
      "total_backward_count 949630 real_backward_count 77238   8.133%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.034783/  1.304555, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0440%\n",
      "layer   2  Sparsity: 69.9210%\n",
      "layer   3  Sparsity: 69.7484%\n",
      "total_backward_count 959420 real_backward_count 77648   8.093%\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.031944/  1.303978, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.23 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 70.1991%\n",
      "layer   3  Sparsity: 69.1927%\n",
      "total_backward_count 969210 real_backward_count 78032   8.051%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.037587/  1.315257, val:  75.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.92 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 69.8643%\n",
      "layer   3  Sparsity: 68.7660%\n",
      "total_backward_count 979000 real_backward_count 78457   8.014%\n",
      "fc layer 1 self.abs_max_out: 4610.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.044748/  1.344311, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.04 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 70.0133%\n",
      "layer   3  Sparsity: 69.1945%\n",
      "total_backward_count 988790 real_backward_count 78820   7.971%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.053605/  1.303314, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.14 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   2  Sparsity: 70.0671%\n",
      "layer   3  Sparsity: 69.0663%\n",
      "total_backward_count 998580 real_backward_count 79258   7.937%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.039124/  1.296636, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.82 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   2  Sparsity: 70.1560%\n",
      "layer   3  Sparsity: 68.9284%\n",
      "total_backward_count 1008370 real_backward_count 79704   7.904%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.031087/  1.302840, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 70.0027%\n",
      "layer   3  Sparsity: 68.9308%\n",
      "total_backward_count 1018160 real_backward_count 80088   7.866%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.038927/  1.290531, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   2  Sparsity: 69.9071%\n",
      "layer   3  Sparsity: 69.0994%\n",
      "total_backward_count 1027950 real_backward_count 80472   7.828%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.024606/  1.276999, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.59 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   2  Sparsity: 70.1112%\n",
      "layer   3  Sparsity: 69.2676%\n",
      "total_backward_count 1037740 real_backward_count 80866   7.793%\n",
      "fc layer 1 self.abs_max_out: 4635.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.021644/  1.276387, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.02 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   2  Sparsity: 69.7126%\n",
      "layer   3  Sparsity: 69.1351%\n",
      "total_backward_count 1047530 real_backward_count 81254   7.757%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.011583/  1.285468, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 69.5868%\n",
      "layer   3  Sparsity: 68.7638%\n",
      "total_backward_count 1057320 real_backward_count 81655   7.723%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.008221/  1.298522, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   2  Sparsity: 69.6629%\n",
      "layer   3  Sparsity: 69.1413%\n",
      "total_backward_count 1067110 real_backward_count 82047   7.689%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.005996/  1.276520, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.36 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1438%\n",
      "layer   2  Sparsity: 69.8101%\n",
      "layer   3  Sparsity: 69.3105%\n",
      "total_backward_count 1076900 real_backward_count 82440   7.655%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.016167/  1.246715, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.23 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 69.6249%\n",
      "layer   3  Sparsity: 69.0453%\n",
      "total_backward_count 1086690 real_backward_count 82822   7.621%\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "fc layer 3 self.abs_max_out: 808.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.004514/  1.237632, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 69.5840%\n",
      "layer   3  Sparsity: 68.7309%\n",
      "total_backward_count 1096480 real_backward_count 83217   7.589%\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  0.975619/  1.256393, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0504%\n",
      "layer   2  Sparsity: 69.4326%\n",
      "layer   3  Sparsity: 69.5121%\n",
      "total_backward_count 1106270 real_backward_count 83570   7.554%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  0.983090/  1.257584, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.06 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 69.4580%\n",
      "layer   3  Sparsity: 69.5192%\n",
      "total_backward_count 1116060 real_backward_count 83955   7.522%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  0.958839/  1.241921, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.82 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   2  Sparsity: 69.6997%\n",
      "layer   3  Sparsity: 69.5552%\n",
      "total_backward_count 1125850 real_backward_count 84310   7.489%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  0.976221/  1.249607, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.89 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 69.9053%\n",
      "layer   3  Sparsity: 69.5263%\n",
      "total_backward_count 1135640 real_backward_count 84655   7.454%\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  0.965593/  1.241036, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.46 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.1266%\n",
      "layer   2  Sparsity: 69.6065%\n",
      "layer   3  Sparsity: 69.2635%\n",
      "total_backward_count 1145430 real_backward_count 85041   7.424%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  0.959665/  1.291046, val:  71.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 69.6707%\n",
      "layer   3  Sparsity: 69.3301%\n",
      "total_backward_count 1155220 real_backward_count 85406   7.393%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  0.962256/  1.237392, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 69.7518%\n",
      "layer   3  Sparsity: 69.7912%\n",
      "total_backward_count 1165010 real_backward_count 85718   7.358%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  0.962188/  1.255016, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.59 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1172%\n",
      "layer   2  Sparsity: 69.7769%\n",
      "layer   3  Sparsity: 70.3377%\n",
      "total_backward_count 1174800 real_backward_count 86077   7.327%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  0.962296/  1.236547, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.76 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   2  Sparsity: 69.7353%\n",
      "layer   3  Sparsity: 70.9260%\n",
      "total_backward_count 1184590 real_backward_count 86408   7.294%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  0.983685/  1.244128, val:  86.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 69.6575%\n",
      "layer   3  Sparsity: 70.9509%\n",
      "total_backward_count 1194380 real_backward_count 86752   7.263%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  0.964085/  1.247598, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   2  Sparsity: 69.7776%\n",
      "layer   3  Sparsity: 70.6675%\n",
      "total_backward_count 1204170 real_backward_count 87053   7.229%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  0.965386/  1.225811, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.11 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 69.5262%\n",
      "layer   3  Sparsity: 70.2974%\n",
      "total_backward_count 1213960 real_backward_count 87380   7.198%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  0.966297/  1.231560, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.02 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0284%\n",
      "layer   2  Sparsity: 69.6090%\n",
      "layer   3  Sparsity: 70.2479%\n",
      "total_backward_count 1223750 real_backward_count 87703   7.167%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  0.976633/  1.246943, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 69.8753%\n",
      "layer   3  Sparsity: 69.7639%\n",
      "total_backward_count 1233540 real_backward_count 88027   7.136%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  0.968394/  1.232032, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.80 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0450%\n",
      "layer   2  Sparsity: 69.7849%\n",
      "layer   3  Sparsity: 69.9005%\n",
      "total_backward_count 1243330 real_backward_count 88370   7.108%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  0.965712/  1.253361, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 69.9335%\n",
      "layer   3  Sparsity: 69.9615%\n",
      "total_backward_count 1253120 real_backward_count 88710   7.079%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  0.960646/  1.239390, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.99 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 69.4335%\n",
      "layer   3  Sparsity: 70.0332%\n",
      "total_backward_count 1262910 real_backward_count 89041   7.050%\n",
      "fc layer 3 self.abs_max_out: 831.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.959435/  1.203390, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.04 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 69.4822%\n",
      "layer   3  Sparsity: 69.9215%\n",
      "total_backward_count 1272700 real_backward_count 89343   7.020%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.943290/  1.231437, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.28 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   2  Sparsity: 69.9024%\n",
      "layer   3  Sparsity: 69.9027%\n",
      "total_backward_count 1282490 real_backward_count 89645   6.990%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.949476/  1.242051, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 70.0183%\n",
      "layer   3  Sparsity: 69.8958%\n",
      "total_backward_count 1292280 real_backward_count 89950   6.961%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.961158/  1.234174, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 69.8780%\n",
      "layer   3  Sparsity: 69.6222%\n",
      "total_backward_count 1302070 real_backward_count 90257   6.932%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.960527/  1.220513, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   2  Sparsity: 69.5946%\n",
      "layer   3  Sparsity: 69.5016%\n",
      "total_backward_count 1311860 real_backward_count 90577   6.904%\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "fc layer 1 self.abs_max_out: 4696.0\n",
      "fc layer 3 self.abs_max_out: 862.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.952699/  1.264996, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.17 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 69.4171%\n",
      "layer   3  Sparsity: 69.6119%\n",
      "total_backward_count 1321650 real_backward_count 90919   6.879%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.952046/  1.224502, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0523%\n",
      "layer   2  Sparsity: 69.3607%\n",
      "layer   3  Sparsity: 69.5610%\n",
      "total_backward_count 1331440 real_backward_count 91202   6.850%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.939086/  1.207959, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 69.3678%\n",
      "layer   3  Sparsity: 69.5605%\n",
      "total_backward_count 1341230 real_backward_count 91504   6.822%\n",
      "lif layer 2 self.abs_max_v: 3841.5\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.937211/  1.218908, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.94 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   2  Sparsity: 69.4473%\n",
      "layer   3  Sparsity: 69.7639%\n",
      "total_backward_count 1351020 real_backward_count 91803   6.795%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.938442/  1.213047, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.80 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 69.7654%\n",
      "layer   3  Sparsity: 69.9030%\n",
      "total_backward_count 1360810 real_backward_count 92130   6.770%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.926565/  1.200029, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 69.6802%\n",
      "layer   3  Sparsity: 70.0156%\n",
      "total_backward_count 1370600 real_backward_count 92394   6.741%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.926184/  1.207253, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.70 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 69.8234%\n",
      "layer   3  Sparsity: 69.7112%\n",
      "total_backward_count 1380390 real_backward_count 92706   6.716%\n",
      "fc layer 3 self.abs_max_out: 864.0\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.922153/  1.188745, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   2  Sparsity: 69.6879%\n",
      "layer   3  Sparsity: 69.5874%\n",
      "total_backward_count 1390180 real_backward_count 93002   6.690%\n",
      "lif layer 2 self.abs_max_v: 3904.5\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.930471/  1.178861, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.25 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   2  Sparsity: 69.5326%\n",
      "layer   3  Sparsity: 69.5430%\n",
      "total_backward_count 1399970 real_backward_count 93291   6.664%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.921813/  1.197420, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   2  Sparsity: 69.6459%\n",
      "layer   3  Sparsity: 70.2097%\n",
      "total_backward_count 1409760 real_backward_count 93596   6.639%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.902772/  1.183291, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.74 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 69.5027%\n",
      "layer   3  Sparsity: 70.6869%\n",
      "total_backward_count 1419550 real_backward_count 93889   6.614%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.907378/  1.185218, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.44 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0330%\n",
      "layer   2  Sparsity: 69.6405%\n",
      "layer   3  Sparsity: 70.9362%\n",
      "total_backward_count 1429340 real_backward_count 94160   6.588%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.912249/  1.190840, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.82 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 69.5985%\n",
      "layer   3  Sparsity: 71.3030%\n",
      "total_backward_count 1439130 real_backward_count 94430   6.562%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.926278/  1.179614, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.18 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 69.5364%\n",
      "layer   3  Sparsity: 70.9021%\n",
      "total_backward_count 1448920 real_backward_count 94706   6.536%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.919044/  1.192388, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   2  Sparsity: 69.6174%\n",
      "layer   3  Sparsity: 70.7884%\n",
      "total_backward_count 1458710 real_backward_count 94977   6.511%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.901433/  1.167714, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.06 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 69.5859%\n",
      "layer   3  Sparsity: 70.8091%\n",
      "total_backward_count 1468500 real_backward_count 95231   6.485%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.901718/  1.173351, val:  85.00%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   2  Sparsity: 69.7642%\n",
      "layer   3  Sparsity: 70.8804%\n",
      "total_backward_count 1478290 real_backward_count 95481   6.459%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.894582/  1.172579, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.26 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 69.7754%\n",
      "layer   3  Sparsity: 70.8795%\n",
      "total_backward_count 1488080 real_backward_count 95735   6.433%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.887450/  1.172173, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.26 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1109%\n",
      "layer   2  Sparsity: 69.7638%\n",
      "layer   3  Sparsity: 71.3814%\n",
      "total_backward_count 1497870 real_backward_count 96003   6.409%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.889814/  1.170233, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1407%\n",
      "layer   2  Sparsity: 70.0692%\n",
      "layer   3  Sparsity: 71.2076%\n",
      "total_backward_count 1507660 real_backward_count 96237   6.383%\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.893072/  1.171903, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.12 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 69.7205%\n",
      "layer   3  Sparsity: 71.4325%\n",
      "total_backward_count 1517450 real_backward_count 96510   6.360%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.901780/  1.179025, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.85 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   2  Sparsity: 69.5996%\n",
      "layer   3  Sparsity: 71.0374%\n",
      "total_backward_count 1527240 real_backward_count 96780   6.337%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.891702/  1.167300, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.68 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 69.9686%\n",
      "layer   3  Sparsity: 71.1406%\n",
      "total_backward_count 1537030 real_backward_count 97009   6.311%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.905702/  1.183028, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.73 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0386%\n",
      "layer   2  Sparsity: 69.9226%\n",
      "layer   3  Sparsity: 70.9836%\n",
      "total_backward_count 1546820 real_backward_count 97273   6.289%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.900326/  1.181888, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 69.7928%\n",
      "layer   3  Sparsity: 71.0740%\n",
      "total_backward_count 1556610 real_backward_count 97507   6.264%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.895210/  1.176901, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.93 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 70.0038%\n",
      "layer   3  Sparsity: 71.3138%\n",
      "total_backward_count 1566400 real_backward_count 97749   6.240%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.903289/  1.200689, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.68 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   2  Sparsity: 69.7910%\n",
      "layer   3  Sparsity: 71.2048%\n",
      "total_backward_count 1576190 real_backward_count 97997   6.217%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.890546/  1.159632, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.47 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 69.7564%\n",
      "layer   3  Sparsity: 71.2450%\n",
      "total_backward_count 1585980 real_backward_count 98225   6.193%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.885602/  1.186266, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.67 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   2  Sparsity: 69.6406%\n",
      "layer   3  Sparsity: 70.9228%\n",
      "total_backward_count 1595770 real_backward_count 98501   6.173%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.878054/  1.168204, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.96 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   2  Sparsity: 69.6468%\n",
      "layer   3  Sparsity: 71.0667%\n",
      "total_backward_count 1605560 real_backward_count 98742   6.150%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.885768/  1.162542, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.50 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   2  Sparsity: 69.8413%\n",
      "layer   3  Sparsity: 71.4096%\n",
      "total_backward_count 1615350 real_backward_count 98967   6.127%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.882107/  1.179151, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 69.8271%\n",
      "layer   3  Sparsity: 71.2344%\n",
      "total_backward_count 1625140 real_backward_count 99201   6.104%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.884007/  1.158560, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   2  Sparsity: 69.3516%\n",
      "layer   3  Sparsity: 70.7401%\n",
      "total_backward_count 1634930 real_backward_count 99449   6.083%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.876491/  1.174751, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.25 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 69.5512%\n",
      "layer   3  Sparsity: 71.2084%\n",
      "total_backward_count 1644720 real_backward_count 99686   6.061%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.892191/  1.160854, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.10 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   2  Sparsity: 69.5056%\n",
      "layer   3  Sparsity: 71.0178%\n",
      "total_backward_count 1654510 real_backward_count 99908   6.039%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.889283/  1.160991, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.65 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   2  Sparsity: 69.4767%\n",
      "layer   3  Sparsity: 71.0058%\n",
      "total_backward_count 1664300 real_backward_count 100147   6.017%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.877474/  1.169660, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.82 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 69.4948%\n",
      "layer   3  Sparsity: 71.2224%\n",
      "total_backward_count 1674090 real_backward_count 100382   5.996%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.886816/  1.153493, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.91 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1079%\n",
      "layer   2  Sparsity: 69.5584%\n",
      "layer   3  Sparsity: 70.9651%\n",
      "total_backward_count 1683880 real_backward_count 100617   5.975%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.882378/  1.133502, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   2  Sparsity: 69.4806%\n",
      "layer   3  Sparsity: 70.8943%\n",
      "total_backward_count 1693670 real_backward_count 100869   5.956%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.863956/  1.159327, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   2  Sparsity: 69.5750%\n",
      "layer   3  Sparsity: 71.5107%\n",
      "total_backward_count 1703460 real_backward_count 101104   5.935%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.854687/  1.148292, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.47 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 69.7532%\n",
      "layer   3  Sparsity: 71.4415%\n",
      "total_backward_count 1713250 real_backward_count 101309   5.913%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.857080/  1.148874, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.29 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 69.7140%\n",
      "layer   3  Sparsity: 71.4879%\n",
      "total_backward_count 1723040 real_backward_count 101562   5.894%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.850535/  1.162250, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.22 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 69.4921%\n",
      "layer   3  Sparsity: 71.5776%\n",
      "total_backward_count 1732830 real_backward_count 101765   5.873%\n",
      "fc layer 2 self.abs_max_out: 2458.0\n",
      "fc layer 2 self.abs_max_out: 2476.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.863409/  1.163555, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.62 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 69.2513%\n",
      "layer   3  Sparsity: 71.8696%\n",
      "total_backward_count 1742620 real_backward_count 102000   5.853%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.862359/  1.158210, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.98 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 69.1775%\n",
      "layer   3  Sparsity: 71.6855%\n",
      "total_backward_count 1752410 real_backward_count 102215   5.833%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.856490/  1.168824, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.65 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   2  Sparsity: 69.4680%\n",
      "layer   3  Sparsity: 71.6527%\n",
      "total_backward_count 1762200 real_backward_count 102427   5.812%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.856227/  1.127166, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.27 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 69.5055%\n",
      "layer   3  Sparsity: 71.5797%\n",
      "total_backward_count 1771990 real_backward_count 102628   5.792%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.843120/  1.131789, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.39 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 69.5635%\n",
      "layer   3  Sparsity: 71.5768%\n",
      "total_backward_count 1781780 real_backward_count 102839   5.772%\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "fc layer 3 self.abs_max_out: 906.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.845614/  1.145611, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.82 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1132%\n",
      "layer   2  Sparsity: 69.5726%\n",
      "layer   3  Sparsity: 71.7501%\n",
      "total_backward_count 1791570 real_backward_count 103032   5.751%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.842263/  1.161252, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.28 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 69.5483%\n",
      "layer   3  Sparsity: 71.2545%\n",
      "total_backward_count 1801360 real_backward_count 103212   5.730%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.850619/  1.148288, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.93 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 69.4056%\n",
      "layer   3  Sparsity: 71.1156%\n",
      "total_backward_count 1811150 real_backward_count 103423   5.710%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.850837/  1.138964, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.34 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   2  Sparsity: 69.1804%\n",
      "layer   3  Sparsity: 71.3546%\n",
      "total_backward_count 1820940 real_backward_count 103600   5.689%\n",
      "fc layer 3 self.abs_max_out: 919.0\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 950.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.846329/  1.160590, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 69.3120%\n",
      "layer   3  Sparsity: 71.1432%\n",
      "total_backward_count 1830730 real_backward_count 103820   5.671%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.847187/  1.151096, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.44 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 69.5712%\n",
      "layer   3  Sparsity: 71.3769%\n",
      "total_backward_count 1840520 real_backward_count 104023   5.652%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.852485/  1.154593, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.65 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 69.3228%\n",
      "layer   3  Sparsity: 71.8708%\n",
      "total_backward_count 1850310 real_backward_count 104219   5.633%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.849870/  1.145658, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.29 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 69.3399%\n",
      "layer   3  Sparsity: 71.5126%\n",
      "total_backward_count 1860100 real_backward_count 104398   5.612%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.857505/  1.149206, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 69.2377%\n",
      "layer   3  Sparsity: 71.2327%\n",
      "total_backward_count 1869890 real_backward_count 104596   5.594%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.854030/  1.129800, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.15 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0505%\n",
      "layer   2  Sparsity: 68.9813%\n",
      "layer   3  Sparsity: 71.3146%\n",
      "total_backward_count 1879680 real_backward_count 104780   5.574%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.858280/  1.118839, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.09 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   2  Sparsity: 69.3235%\n",
      "layer   3  Sparsity: 71.2786%\n",
      "total_backward_count 1889470 real_backward_count 104993   5.557%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.855014/  1.123047, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.40 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 69.4002%\n",
      "layer   3  Sparsity: 71.4484%\n",
      "total_backward_count 1899260 real_backward_count 105182   5.538%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.851440/  1.128741, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   2  Sparsity: 69.3882%\n",
      "layer   3  Sparsity: 71.6948%\n",
      "total_backward_count 1909050 real_backward_count 105381   5.520%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.858898/  1.133781, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.28 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0336%\n",
      "layer   2  Sparsity: 69.3257%\n",
      "layer   3  Sparsity: 71.8024%\n",
      "total_backward_count 1918840 real_backward_count 105560   5.501%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.850705/  1.138373, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.10 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 69.3279%\n",
      "layer   3  Sparsity: 71.8250%\n",
      "total_backward_count 1928630 real_backward_count 105771   5.484%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.859190/  1.150488, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.09 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 69.0183%\n",
      "layer   3  Sparsity: 71.6701%\n",
      "total_backward_count 1938420 real_backward_count 105969   5.467%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.864111/  1.130764, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 69.1703%\n",
      "layer   3  Sparsity: 71.2589%\n",
      "total_backward_count 1948210 real_backward_count 106176   5.450%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.849566/  1.135880, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.33 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   2  Sparsity: 69.2127%\n",
      "layer   3  Sparsity: 71.1347%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3363499302414ca7b3501a4a0cf3c273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñá‚ñÜ‚ñÅ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.84957</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>1.13588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/djij695v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/djij695v</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251119_144653-djij695v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i2ayigem with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 40235\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251119_192947-i2ayigem</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2ayigem' target=\"_blank\">fine-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2ayigem' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2ayigem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251119_192957_222', 'my_seed': 40235, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [10, 10, 10], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 10\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 10 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 10\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 10 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 10\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 10 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[10, 10, 10], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[10, 10, 10], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[10, 10, 10], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 236.0\n",
      "lif layer 1 self.abs_max_v: 236.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 264.0\n",
      "lif layer 2 self.abs_max_v: 264.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "lif layer 1 self.abs_max_v: 279.5\n",
      "lif layer 2 self.abs_max_v: 331.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 1 self.abs_max_out: 247.0\n",
      "lif layer 1 self.abs_max_v: 346.5\n",
      "fc layer 2 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 403.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 1 self.abs_max_out: 323.0\n",
      "lif layer 1 self.abs_max_v: 368.5\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 498.5\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "lif layer 1 self.abs_max_v: 397.5\n",
      "lif layer 2 self.abs_max_v: 543.0\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "lif layer 2 self.abs_max_v: 647.5\n",
      "fc layer 1 self.abs_max_out: 370.0\n",
      "lif layer 1 self.abs_max_v: 413.5\n",
      "fc layer 1 self.abs_max_out: 383.0\n",
      "fc layer 1 self.abs_max_out: 521.0\n",
      "lif layer 1 self.abs_max_v: 521.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 422.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 1 self.abs_max_v: 538.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "fc layer 1 self.abs_max_out: 531.0\n",
      "fc layer 1 self.abs_max_out: 539.0\n",
      "lif layer 1 self.abs_max_v: 574.0\n",
      "lif layer 1 self.abs_max_v: 657.0\n",
      "fc layer 1 self.abs_max_out: 622.0\n",
      "fc layer 1 self.abs_max_out: 658.0\n",
      "lif layer 1 self.abs_max_v: 658.0\n",
      "lif layer 1 self.abs_max_v: 661.5\n",
      "lif layer 1 self.abs_max_v: 678.0\n",
      "fc layer 1 self.abs_max_out: 702.0\n",
      "lif layer 1 self.abs_max_v: 732.5\n",
      "lif layer 1 self.abs_max_v: 803.5\n",
      "lif layer 2 self.abs_max_v: 675.0\n",
      "fc layer 1 self.abs_max_out: 855.0\n",
      "lif layer 1 self.abs_max_v: 855.0\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "lif layer 2 self.abs_max_v: 751.5\n",
      "lif layer 2 self.abs_max_v: 770.5\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "fc layer 2 self.abs_max_out: 633.0\n",
      "lif layer 2 self.abs_max_v: 950.5\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "fc layer 1 self.abs_max_out: 858.0\n",
      "lif layer 1 self.abs_max_v: 858.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "lif layer 2 self.abs_max_v: 1046.0\n",
      "lif layer 2 self.abs_max_v: 1058.0\n",
      "fc layer 2 self.abs_max_out: 697.0\n",
      "lif layer 1 self.abs_max_v: 901.5\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 903.0\n",
      "fc layer 2 self.abs_max_out: 778.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "lif layer 1 self.abs_max_v: 932.0\n",
      "lif layer 1 self.abs_max_v: 940.5\n",
      "lif layer 1 self.abs_max_v: 952.5\n",
      "lif layer 1 self.abs_max_v: 979.5\n",
      "lif layer 1 self.abs_max_v: 984.0\n",
      "fc layer 1 self.abs_max_out: 982.0\n",
      "fc layer 3 self.abs_max_out: 297.0\n",
      "lif layer 1 self.abs_max_v: 997.5\n",
      "lif layer 1 self.abs_max_v: 1025.0\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1110.5\n",
      "fc layer 2 self.abs_max_out: 803.0\n",
      "fc layer 2 self.abs_max_out: 877.0\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "fc layer 2 self.abs_max_out: 884.0\n",
      "lif layer 1 self.abs_max_v: 1154.5\n",
      "fc layer 1 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 1162.0\n",
      "lif layer 1 self.abs_max_v: 1162.5\n",
      "lif layer 1 self.abs_max_v: 1164.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "lif layer 1 self.abs_max_v: 1264.5\n",
      "lif layer 1 self.abs_max_v: 1361.5\n",
      "lif layer 1 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1109.5\n",
      "lif layer 2 self.abs_max_v: 1182.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "lif layer 1 self.abs_max_v: 1600.5\n",
      "lif layer 1 self.abs_max_v: 1652.0\n",
      "lif layer 2 self.abs_max_v: 1213.0\n",
      "lif layer 2 self.abs_max_v: 1215.5\n",
      "fc layer 1 self.abs_max_out: 1207.0\n",
      "fc layer 1 self.abs_max_out: 1223.0\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 1216.5\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "lif layer 1 self.abs_max_v: 1727.5\n",
      "fc layer 2 self.abs_max_out: 918.0\n",
      "fc layer 1 self.abs_max_out: 1314.0\n",
      "lif layer 1 self.abs_max_v: 1776.5\n",
      "lif layer 1 self.abs_max_v: 1786.5\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "fc layer 2 self.abs_max_out: 1041.0\n",
      "lif layer 2 self.abs_max_v: 1280.5\n",
      "fc layer 2 self.abs_max_out: 1056.0\n",
      "lif layer 2 self.abs_max_v: 1283.5\n",
      "lif layer 2 self.abs_max_v: 1401.0\n",
      "lif layer 2 self.abs_max_v: 1444.5\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "lif layer 1 self.abs_max_v: 1814.5\n",
      "lif layer 1 self.abs_max_v: 1866.5\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1571.5\n",
      "fc layer 1 self.abs_max_out: 1399.0\n",
      "fc layer 1 self.abs_max_out: 1401.0\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "lif layer 1 self.abs_max_v: 1909.5\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 1 self.abs_max_out: 1517.0\n",
      "fc layer 1 self.abs_max_out: 1566.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 1058.0\n",
      "fc layer 2 self.abs_max_out: 1069.0\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 1 self.abs_max_out: 1639.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "fc layer 2 self.abs_max_out: 1085.0\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "fc layer 1 self.abs_max_out: 1656.0\n",
      "fc layer 1 self.abs_max_out: 1672.0\n",
      "lif layer 1 self.abs_max_v: 2200.5\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "lif layer 1 self.abs_max_v: 2317.0\n",
      "fc layer 1 self.abs_max_out: 1680.0\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "lif layer 1 self.abs_max_v: 2352.0\n",
      "lif layer 1 self.abs_max_v: 2431.0\n",
      "lif layer 1 self.abs_max_v: 2588.5\n",
      "lif layer 1 self.abs_max_v: 2656.5\n",
      "lif layer 2 self.abs_max_v: 1595.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.760379/  1.942171, val:  39.17%, val_best:  39.17%, tr:  96.83%, tr_best:  96.83%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 76.6801%\n",
      "layer   3  Sparsity: 72.7085%\n",
      "total_backward_count 9790 real_backward_count 2223  22.707%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1684.5\n",
      "fc layer 1 self.abs_max_out: 1930.0\n",
      "lif layer 2 self.abs_max_v: 1701.0\n",
      "lif layer 2 self.abs_max_v: 1738.5\n",
      "lif layer 1 self.abs_max_v: 2773.5\n",
      "fc layer 2 self.abs_max_out: 1148.0\n",
      "lif layer 1 self.abs_max_v: 3074.0\n",
      "lif layer 1 self.abs_max_v: 3357.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.680450/  1.952709, val:  32.50%, val_best:  39.17%, tr:  99.28%, tr_best:  99.28%, epoch time: 84.10 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   2  Sparsity: 76.9379%\n",
      "layer   3  Sparsity: 70.6340%\n",
      "total_backward_count 19580 real_backward_count 3745  19.127%\n",
      "lif layer 2 self.abs_max_v: 1773.5\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 2 self.abs_max_out: 1158.0\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "lif layer 2 self.abs_max_v: 1871.5\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 1 self.abs_max_out: 1966.0\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.656921/  1.875554, val:  47.08%, val_best:  47.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   2  Sparsity: 77.1239%\n",
      "layer   3  Sparsity: 69.5404%\n",
      "total_backward_count 29370 real_backward_count 5173  17.613%\n",
      "fc layer 1 self.abs_max_out: 2082.0\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 1 self.abs_max_out: 2116.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "fc layer 2 self.abs_max_out: 1261.0\n",
      "fc layer 1 self.abs_max_out: 2282.0\n",
      "lif layer 1 self.abs_max_v: 3602.5\n",
      "lif layer 1 self.abs_max_v: 3653.5\n",
      "lif layer 1 self.abs_max_v: 3713.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.622392/  1.911826, val:  52.50%, val_best:  52.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 76.8627%\n",
      "layer   3  Sparsity: 69.0906%\n",
      "total_backward_count 39160 real_backward_count 6553  16.734%\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "fc layer 1 self.abs_max_out: 2296.0\n",
      "lif layer 1 self.abs_max_v: 3749.5\n",
      "lif layer 2 self.abs_max_v: 1964.0\n",
      "lif layer 2 self.abs_max_v: 2002.0\n",
      "lif layer 2 self.abs_max_v: 2038.5\n",
      "fc layer 1 self.abs_max_out: 2405.0\n",
      "lif layer 1 self.abs_max_v: 3853.0\n",
      "lif layer 1 self.abs_max_v: 3907.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.600346/  1.817988, val:  55.42%, val_best:  55.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.18 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1185%\n",
      "layer   2  Sparsity: 76.7105%\n",
      "layer   3  Sparsity: 68.4562%\n",
      "total_backward_count 48950 real_backward_count 7974  16.290%\n",
      "lif layer 2 self.abs_max_v: 2058.0\n",
      "lif layer 1 self.abs_max_v: 3917.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.567139/  1.806804, val:  39.58%, val_best:  55.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 83.92 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   2  Sparsity: 76.8871%\n",
      "layer   3  Sparsity: 68.8113%\n",
      "total_backward_count 58740 real_backward_count 9260  15.764%\n",
      "lif layer 2 self.abs_max_v: 2061.0\n",
      "lif layer 2 self.abs_max_v: 2149.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.570286/  1.814738, val:  60.42%, val_best:  60.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.03 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0414%\n",
      "layer   2  Sparsity: 77.2009%\n",
      "layer   3  Sparsity: 69.1578%\n",
      "total_backward_count 68530 real_backward_count 10596  15.462%\n",
      "lif layer 2 self.abs_max_v: 2198.0\n",
      "lif layer 2 self.abs_max_v: 2245.0\n",
      "fc layer 1 self.abs_max_out: 2460.0\n",
      "lif layer 1 self.abs_max_v: 4321.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.574046/  1.797068, val:  55.42%, val_best:  60.42%, tr:  99.39%, tr_best:  99.90%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   2  Sparsity: 77.1466%\n",
      "layer   3  Sparsity: 69.4998%\n",
      "total_backward_count 78320 real_backward_count 11836  15.112%\n",
      "fc layer 1 self.abs_max_out: 2513.0\n",
      "fc layer 2 self.abs_max_out: 1406.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.564133/  1.832123, val:  53.75%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 77.2820%\n",
      "layer   3  Sparsity: 69.4100%\n",
      "total_backward_count 88110 real_backward_count 13145  14.919%\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "fc layer 2 self.abs_max_out: 1448.0\n",
      "fc layer 1 self.abs_max_out: 2545.0\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "fc layer 2 self.abs_max_out: 1503.0\n",
      "fc layer 1 self.abs_max_out: 2568.0\n",
      "lif layer 1 self.abs_max_v: 4452.0\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.559615/  1.786307, val:  55.42%, val_best:  60.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1161%\n",
      "layer   2  Sparsity: 76.8856%\n",
      "layer   3  Sparsity: 68.9043%\n",
      "total_backward_count 97900 real_backward_count 14314  14.621%\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "lif layer 2 self.abs_max_v: 2255.5\n",
      "lif layer 2 self.abs_max_v: 2303.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.554440/  1.797876, val:  60.42%, val_best:  60.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 84.06 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   2  Sparsity: 76.7602%\n",
      "layer   3  Sparsity: 68.7006%\n",
      "total_backward_count 107690 real_backward_count 15541  14.431%\n",
      "fc layer 1 self.abs_max_out: 2583.0\n",
      "fc layer 1 self.abs_max_out: 2631.0\n",
      "fc layer 1 self.abs_max_out: 2663.0\n",
      "lif layer 1 self.abs_max_v: 4851.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.552218/  1.832161, val:  42.08%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   2  Sparsity: 77.2298%\n",
      "layer   3  Sparsity: 69.0130%\n",
      "total_backward_count 117480 real_backward_count 16823  14.320%\n",
      "lif layer 2 self.abs_max_v: 2360.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.532934/  1.774521, val:  55.00%, val_best:  60.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   2  Sparsity: 76.0949%\n",
      "layer   3  Sparsity: 69.3480%\n",
      "total_backward_count 127270 real_backward_count 18010  14.151%\n",
      "lif layer 2 self.abs_max_v: 2398.0\n",
      "fc layer 1 self.abs_max_out: 2670.0\n",
      "fc layer 1 self.abs_max_out: 2721.0\n",
      "fc layer 1 self.abs_max_out: 2897.0\n",
      "lif layer 1 self.abs_max_v: 5105.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.514960/  1.742530, val:  54.58%, val_best:  60.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 76.0344%\n",
      "layer   3  Sparsity: 69.1571%\n",
      "total_backward_count 137060 real_backward_count 19172  13.988%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.527130/  1.794192, val:  45.42%, val_best:  60.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0491%\n",
      "layer   2  Sparsity: 76.1884%\n",
      "layer   3  Sparsity: 70.4613%\n",
      "total_backward_count 146850 real_backward_count 20292  13.818%\n",
      "fc layer 2 self.abs_max_out: 1583.0\n",
      "fc layer 2 self.abs_max_out: 1611.0\n",
      "lif layer 2 self.abs_max_v: 2422.5\n",
      "lif layer 2 self.abs_max_v: 2516.5\n",
      "lif layer 2 self.abs_max_v: 2694.0\n",
      "fc layer 1 self.abs_max_out: 2967.0\n",
      "lif layer 1 self.abs_max_v: 5192.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.530310/  1.767726, val:  54.17%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 84.82 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 75.9557%\n",
      "layer   3  Sparsity: 70.3559%\n",
      "total_backward_count 156640 real_backward_count 21485  13.716%\n",
      "fc layer 2 self.abs_max_out: 1743.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.497787/  1.747300, val:  53.75%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 75.7654%\n",
      "layer   3  Sparsity: 71.0076%\n",
      "total_backward_count 166430 real_backward_count 22600  13.579%\n",
      "fc layer 1 self.abs_max_out: 3230.0\n",
      "lif layer 1 self.abs_max_v: 5636.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.487698/  1.757887, val:  52.50%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 83.92 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 76.0325%\n",
      "layer   3  Sparsity: 71.8679%\n",
      "total_backward_count 176220 real_backward_count 23731  13.467%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.488374/  1.771029, val:  48.75%, val_best:  60.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 83.42 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   2  Sparsity: 75.7075%\n",
      "layer   3  Sparsity: 71.7044%\n",
      "total_backward_count 186010 real_backward_count 24812  13.339%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.474258/  1.698779, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 83.79 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1174%\n",
      "layer   2  Sparsity: 75.2515%\n",
      "layer   3  Sparsity: 71.0082%\n",
      "total_backward_count 195800 real_backward_count 25866  13.210%\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.456981/  1.695306, val:  55.00%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   2  Sparsity: 74.8145%\n",
      "layer   3  Sparsity: 71.0022%\n",
      "total_backward_count 205590 real_backward_count 26862  13.066%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.441362/  1.680243, val:  53.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 74.7459%\n",
      "layer   3  Sparsity: 71.1908%\n",
      "total_backward_count 215380 real_backward_count 27925  12.965%\n",
      "fc layer 1 self.abs_max_out: 3239.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.427734/  1.676300, val:  59.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 74.7040%\n",
      "layer   3  Sparsity: 71.1180%\n",
      "total_backward_count 225170 real_backward_count 28940  12.853%\n",
      "fc layer 1 self.abs_max_out: 3298.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.442958/  1.675608, val:  62.50%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1029%\n",
      "layer   2  Sparsity: 74.4482%\n",
      "layer   3  Sparsity: 71.1599%\n",
      "total_backward_count 234960 real_backward_count 29931  12.739%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.433579/  1.672356, val:  63.33%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.70 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 74.5515%\n",
      "layer   3  Sparsity: 71.1010%\n",
      "total_backward_count 244750 real_backward_count 30885  12.619%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.416293/  1.657399, val:  64.17%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 74.3810%\n",
      "layer   3  Sparsity: 70.3024%\n",
      "total_backward_count 254540 real_backward_count 31804  12.495%\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "lif layer 2 self.abs_max_v: 2717.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.414674/  1.657149, val:  67.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.66 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   2  Sparsity: 74.2988%\n",
      "layer   3  Sparsity: 70.0102%\n",
      "total_backward_count 264330 real_backward_count 32762  12.394%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.408771/  1.605665, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1160%\n",
      "layer   2  Sparsity: 74.5520%\n",
      "layer   3  Sparsity: 70.2371%\n",
      "total_backward_count 274120 real_backward_count 33671  12.283%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.390940/  1.648354, val:  67.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 74.2902%\n",
      "layer   3  Sparsity: 70.9644%\n",
      "total_backward_count 283910 real_backward_count 34638  12.200%\n",
      "lif layer 1 self.abs_max_v: 5646.5\n",
      "lif layer 1 self.abs_max_v: 5756.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.385173/  1.588591, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 74.1471%\n",
      "layer   3  Sparsity: 70.3279%\n",
      "total_backward_count 293700 real_backward_count 35565  12.109%\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.356200/  1.622635, val:  64.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.38 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0330%\n",
      "layer   2  Sparsity: 74.0863%\n",
      "layer   3  Sparsity: 70.4130%\n",
      "total_backward_count 303490 real_backward_count 36443  12.008%\n",
      "lif layer 1 self.abs_max_v: 5800.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.351746/  1.593255, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   2  Sparsity: 74.4009%\n",
      "layer   3  Sparsity: 70.6906%\n",
      "total_backward_count 313280 real_backward_count 37341  11.919%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.362944/  1.588518, val:  75.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   2  Sparsity: 74.2372%\n",
      "layer   3  Sparsity: 70.5251%\n",
      "total_backward_count 323070 real_backward_count 38174  11.816%\n",
      "fc layer 1 self.abs_max_out: 3308.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.351317/  1.606391, val:  61.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 73.8993%\n",
      "layer   3  Sparsity: 70.1517%\n",
      "total_backward_count 332860 real_backward_count 39034  11.727%\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.340726/  1.569286, val:  73.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.44 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1172%\n",
      "layer   2  Sparsity: 73.7817%\n",
      "layer   3  Sparsity: 70.0306%\n",
      "total_backward_count 342650 real_backward_count 39903  11.645%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.321274/  1.546182, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.21 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0355%\n",
      "layer   2  Sparsity: 73.9557%\n",
      "layer   3  Sparsity: 70.3597%\n",
      "total_backward_count 352440 real_backward_count 40733  11.557%\n",
      "lif layer 1 self.abs_max_v: 6032.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.338581/  1.542526, val:  70.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   2  Sparsity: 73.9216%\n",
      "layer   3  Sparsity: 70.9726%\n",
      "total_backward_count 362230 real_backward_count 41573  11.477%\n",
      "fc layer 2 self.abs_max_out: 1792.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.327495/  1.542271, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.41 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1070%\n",
      "layer   2  Sparsity: 74.1336%\n",
      "layer   3  Sparsity: 70.8309%\n",
      "total_backward_count 372020 real_backward_count 42397  11.396%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.318299/  1.546629, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0284%\n",
      "layer   2  Sparsity: 73.9005%\n",
      "layer   3  Sparsity: 70.1233%\n",
      "total_backward_count 381810 real_backward_count 43233  11.323%\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 1 self.abs_max_out: 3463.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.312239/  1.557965, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.58 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   2  Sparsity: 73.9396%\n",
      "layer   3  Sparsity: 70.6285%\n",
      "total_backward_count 391600 real_backward_count 44093  11.260%\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.292525/  1.503313, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 73.9444%\n",
      "layer   3  Sparsity: 70.7769%\n",
      "total_backward_count 401390 real_backward_count 44825  11.167%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.298943/  1.520568, val:  74.17%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 73.4930%\n",
      "layer   3  Sparsity: 70.8875%\n",
      "total_backward_count 411180 real_backward_count 45601  11.090%\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.290709/  1.497962, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   2  Sparsity: 73.6488%\n",
      "layer   3  Sparsity: 70.8083%\n",
      "total_backward_count 420970 real_backward_count 46368  11.015%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.295978/  1.518651, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0337%\n",
      "layer   2  Sparsity: 73.5900%\n",
      "layer   3  Sparsity: 70.4561%\n",
      "total_backward_count 430760 real_backward_count 47167  10.950%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.274342/  1.508863, val:  75.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 74.0143%\n",
      "layer   3  Sparsity: 70.8527%\n",
      "total_backward_count 440550 real_backward_count 47897  10.872%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.275669/  1.517544, val:  62.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1024%\n",
      "layer   2  Sparsity: 73.5540%\n",
      "layer   3  Sparsity: 70.5233%\n",
      "total_backward_count 450340 real_backward_count 48654  10.804%\n",
      "fc layer 1 self.abs_max_out: 3619.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.259945/  1.499981, val:  72.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.55 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   2  Sparsity: 73.4785%\n",
      "layer   3  Sparsity: 71.0038%\n",
      "total_backward_count 460130 real_backward_count 49432  10.743%\n",
      "fc layer 2 self.abs_max_out: 1823.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.264266/  1.493281, val:  65.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.31 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   2  Sparsity: 73.3776%\n",
      "layer   3  Sparsity: 70.9925%\n",
      "total_backward_count 469920 real_backward_count 50156  10.673%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.258233/  1.522430, val:  66.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   2  Sparsity: 73.4255%\n",
      "layer   3  Sparsity: 70.5577%\n",
      "total_backward_count 479710 real_backward_count 50937  10.618%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.262158/  1.524549, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   2  Sparsity: 73.4668%\n",
      "layer   3  Sparsity: 70.3093%\n",
      "total_backward_count 489500 real_backward_count 51655  10.553%\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.262366/  1.514180, val:  70.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   2  Sparsity: 73.4062%\n",
      "layer   3  Sparsity: 70.1107%\n",
      "total_backward_count 499290 real_backward_count 52388  10.492%\n",
      "fc layer 2 self.abs_max_out: 1856.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.237431/  1.467482, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.82 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 73.3004%\n",
      "layer   3  Sparsity: 70.6756%\n",
      "total_backward_count 509080 real_backward_count 53078  10.426%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.230674/  1.499600, val:  75.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.55 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1309%\n",
      "layer   2  Sparsity: 73.4793%\n",
      "layer   3  Sparsity: 70.8189%\n",
      "total_backward_count 518870 real_backward_count 53722  10.354%\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "fc layer 1 self.abs_max_out: 3654.0\n",
      "lif layer 1 self.abs_max_v: 6054.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.228483/  1.492959, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.84 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   2  Sparsity: 73.3386%\n",
      "layer   3  Sparsity: 71.1039%\n",
      "total_backward_count 528660 real_backward_count 54386  10.288%\n",
      "fc layer 1 self.abs_max_out: 3753.0\n",
      "lif layer 1 self.abs_max_v: 6206.5\n",
      "lif layer 1 self.abs_max_v: 6228.0\n",
      "lif layer 1 self.abs_max_v: 6266.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.248332/  1.471382, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.35 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 73.6078%\n",
      "layer   3  Sparsity: 70.7212%\n",
      "total_backward_count 538450 real_backward_count 55121  10.237%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.242892/  1.495235, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   2  Sparsity: 73.7296%\n",
      "layer   3  Sparsity: 70.8916%\n",
      "total_backward_count 548240 real_backward_count 55748  10.169%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.220793/  1.422793, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.51 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   2  Sparsity: 73.2943%\n",
      "layer   3  Sparsity: 71.0738%\n",
      "total_backward_count 558030 real_backward_count 56423  10.111%\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.206555/  1.448145, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   2  Sparsity: 73.0804%\n",
      "layer   3  Sparsity: 70.5588%\n",
      "total_backward_count 567820 real_backward_count 57060  10.049%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.215358/  1.436516, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 72.8676%\n",
      "layer   3  Sparsity: 70.7559%\n",
      "total_backward_count 577610 real_backward_count 57706   9.990%\n",
      "fc layer 2 self.abs_max_out: 1989.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.210132/  1.464100, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.73 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1118%\n",
      "layer   2  Sparsity: 72.8626%\n",
      "layer   3  Sparsity: 70.3890%\n",
      "total_backward_count 587400 real_backward_count 58333   9.931%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.216496/  1.437994, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 73.1891%\n",
      "layer   3  Sparsity: 70.3658%\n",
      "total_backward_count 597190 real_backward_count 58948   9.871%\n",
      "lif layer 1 self.abs_max_v: 6323.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.220466/  1.456450, val:  73.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   2  Sparsity: 72.9938%\n",
      "layer   3  Sparsity: 71.1781%\n",
      "total_backward_count 606980 real_backward_count 59576   9.815%\n",
      "lif layer 2 self.abs_max_v: 2720.5\n",
      "lif layer 2 self.abs_max_v: 2874.5\n",
      "lif layer 1 self.abs_max_v: 6337.0\n",
      "lif layer 2 self.abs_max_v: 2888.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.201796/  1.437783, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   2  Sparsity: 73.3627%\n",
      "layer   3  Sparsity: 71.7343%\n",
      "total_backward_count 616770 real_backward_count 60184   9.758%\n",
      "lif layer 2 self.abs_max_v: 2919.5\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.220062/  1.470531, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   2  Sparsity: 73.2878%\n",
      "layer   3  Sparsity: 71.5100%\n",
      "total_backward_count 626560 real_backward_count 60794   9.703%\n",
      "fc layer 2 self.abs_max_out: 1991.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.222100/  1.466314, val:  82.50%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 73.5552%\n",
      "layer   3  Sparsity: 71.2216%\n",
      "total_backward_count 636350 real_backward_count 61402   9.649%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.214464/  1.453326, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.25 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0633%\n",
      "layer   2  Sparsity: 73.3731%\n",
      "layer   3  Sparsity: 71.1444%\n",
      "total_backward_count 646140 real_backward_count 61995   9.595%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.204858/  1.439602, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.43 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0381%\n",
      "layer   2  Sparsity: 73.1447%\n",
      "layer   3  Sparsity: 71.1153%\n",
      "total_backward_count 655930 real_backward_count 62555   9.537%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.198690/  1.453195, val:  72.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 73.2068%\n",
      "layer   3  Sparsity: 71.0204%\n",
      "total_backward_count 665720 real_backward_count 63094   9.478%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "lif layer 1 self.abs_max_v: 6386.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.194483/  1.451483, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.26 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   2  Sparsity: 72.9670%\n",
      "layer   3  Sparsity: 70.9456%\n",
      "total_backward_count 675510 real_backward_count 63674   9.426%\n",
      "fc layer 2 self.abs_max_out: 2006.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.191728/  1.377265, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0255%\n",
      "layer   2  Sparsity: 73.1849%\n",
      "layer   3  Sparsity: 70.9965%\n",
      "total_backward_count 685300 real_backward_count 64258   9.377%\n",
      "lif layer 1 self.abs_max_v: 6435.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.188325/  1.423371, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 73.0479%\n",
      "layer   3  Sparsity: 71.3110%\n",
      "total_backward_count 695090 real_backward_count 64835   9.328%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.188245/  1.453008, val:  74.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.24 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 73.3209%\n",
      "layer   3  Sparsity: 71.4518%\n",
      "total_backward_count 704880 real_backward_count 65419   9.281%\n",
      "lif layer 1 self.abs_max_v: 6474.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.202654/  1.435119, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.91 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   2  Sparsity: 72.9621%\n",
      "layer   3  Sparsity: 71.0869%\n",
      "total_backward_count 714670 real_backward_count 65979   9.232%\n",
      "fc layer 1 self.abs_max_out: 3795.0\n",
      "fc layer 2 self.abs_max_out: 2050.0\n",
      "fc layer 1 self.abs_max_out: 3856.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.196021/  1.414001, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.52 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0551%\n",
      "layer   2  Sparsity: 72.8441%\n",
      "layer   3  Sparsity: 70.2869%\n",
      "total_backward_count 724460 real_backward_count 66510   9.181%\n",
      "fc layer 1 self.abs_max_out: 3954.0\n",
      "lif layer 1 self.abs_max_v: 6522.5\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.170799/  1.412713, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.69 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   2  Sparsity: 72.6921%\n",
      "layer   3  Sparsity: 70.6681%\n",
      "total_backward_count 734250 real_backward_count 67072   9.135%\n",
      "fc layer 2 self.abs_max_out: 2109.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.179549/  1.402122, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 72.6153%\n",
      "layer   3  Sparsity: 70.6852%\n",
      "total_backward_count 744040 real_backward_count 67613   9.087%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.162677/  1.393476, val:  76.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.23 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 72.9626%\n",
      "layer   3  Sparsity: 70.5386%\n",
      "total_backward_count 753830 real_backward_count 68177   9.044%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.167958/  1.390442, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.01 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.1124%\n",
      "layer   2  Sparsity: 73.1320%\n",
      "layer   3  Sparsity: 70.7085%\n",
      "total_backward_count 763620 real_backward_count 68744   9.002%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.154260/  1.396819, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.08 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   2  Sparsity: 73.1535%\n",
      "layer   3  Sparsity: 71.0221%\n",
      "total_backward_count 773410 real_backward_count 69246   8.953%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "fc layer 1 self.abs_max_out: 4096.0\n",
      "lif layer 1 self.abs_max_v: 6740.5\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.139787/  1.382936, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   2  Sparsity: 73.1395%\n",
      "layer   3  Sparsity: 71.0479%\n",
      "total_backward_count 783200 real_backward_count 69706   8.900%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.127866/  1.367554, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.34 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 73.2442%\n",
      "layer   3  Sparsity: 71.2421%\n",
      "total_backward_count 792990 real_backward_count 70203   8.853%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.117595/  1.365656, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.47 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 73.2214%\n",
      "layer   3  Sparsity: 70.8915%\n",
      "total_backward_count 802780 real_backward_count 70714   8.809%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.121927/  1.399812, val:  64.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.89 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 73.0618%\n",
      "layer   3  Sparsity: 70.3458%\n",
      "total_backward_count 812570 real_backward_count 71212   8.764%\n",
      "lif layer 2 self.abs_max_v: 2984.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 2 self.abs_max_out: 2112.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.118336/  1.357652, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0457%\n",
      "layer   2  Sparsity: 72.8176%\n",
      "layer   3  Sparsity: 70.2224%\n",
      "total_backward_count 822360 real_backward_count 71706   8.720%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.118757/  1.374899, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.31 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 73.0138%\n",
      "layer   3  Sparsity: 70.6072%\n",
      "total_backward_count 832150 real_backward_count 72192   8.675%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.129104/  1.357494, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1074%\n",
      "layer   2  Sparsity: 72.7484%\n",
      "layer   3  Sparsity: 70.4915%\n",
      "total_backward_count 841940 real_backward_count 72696   8.634%\n",
      "fc layer 3 self.abs_max_out: 698.0\n",
      "lif layer 1 self.abs_max_v: 6792.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.117331/  1.384922, val:  79.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.48 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1098%\n",
      "layer   2  Sparsity: 73.2151%\n",
      "layer   3  Sparsity: 71.0249%\n",
      "total_backward_count 851730 real_backward_count 73224   8.597%\n",
      "lif layer 1 self.abs_max_v: 6804.5\n",
      "lif layer 1 self.abs_max_v: 6921.5\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.136380/  1.375559, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.34 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0035%\n",
      "layer   2  Sparsity: 73.0023%\n",
      "layer   3  Sparsity: 70.6505%\n",
      "total_backward_count 861520 real_backward_count 73719   8.557%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.118697/  1.387250, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.52 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   2  Sparsity: 73.0415%\n",
      "layer   3  Sparsity: 70.4033%\n",
      "total_backward_count 871310 real_backward_count 74200   8.516%\n",
      "fc layer 1 self.abs_max_out: 4169.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.106804/  1.355740, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.45 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 73.0179%\n",
      "layer   3  Sparsity: 70.8496%\n",
      "total_backward_count 881100 real_backward_count 74651   8.472%\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.093551/  1.346069, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1324%\n",
      "layer   2  Sparsity: 73.0881%\n",
      "layer   3  Sparsity: 70.7621%\n",
      "total_backward_count 890890 real_backward_count 75119   8.432%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.075231/  1.326011, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   2  Sparsity: 73.1033%\n",
      "layer   3  Sparsity: 70.7481%\n",
      "total_backward_count 900680 real_backward_count 75593   8.393%\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.067190/  1.318272, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 73.1828%\n",
      "layer   3  Sparsity: 70.8390%\n",
      "total_backward_count 910470 real_backward_count 76020   8.350%\n",
      "lif layer 2 self.abs_max_v: 3099.5\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.092400/  1.338758, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   2  Sparsity: 72.9930%\n",
      "layer   3  Sparsity: 70.5661%\n",
      "total_backward_count 920260 real_backward_count 76477   8.310%\n",
      "fc layer 1 self.abs_max_out: 4223.0\n",
      "fc layer 2 self.abs_max_out: 2131.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.099111/  1.327331, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.15 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 72.8178%\n",
      "layer   3  Sparsity: 70.8478%\n",
      "total_backward_count 930050 real_backward_count 76920   8.271%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.086506/  1.342862, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0456%\n",
      "layer   2  Sparsity: 72.9331%\n",
      "layer   3  Sparsity: 71.1307%\n",
      "total_backward_count 939840 real_backward_count 77352   8.230%\n",
      "lif layer 1 self.abs_max_v: 7039.0\n",
      "lif layer 1 self.abs_max_v: 7193.5\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.095208/  1.332361, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.43 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   2  Sparsity: 72.9093%\n",
      "layer   3  Sparsity: 71.3026%\n",
      "total_backward_count 949630 real_backward_count 77799   8.193%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.074963/  1.346525, val:  82.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0504%\n",
      "layer   2  Sparsity: 72.8239%\n",
      "layer   3  Sparsity: 71.2610%\n",
      "total_backward_count 959420 real_backward_count 78245   8.155%\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.062388/  1.315679, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.52 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 73.0755%\n",
      "layer   3  Sparsity: 71.1050%\n",
      "total_backward_count 969210 real_backward_count 78681   8.118%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.060528/  1.306290, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.46 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1181%\n",
      "layer   2  Sparsity: 73.0885%\n",
      "layer   3  Sparsity: 71.2555%\n",
      "total_backward_count 979000 real_backward_count 79089   8.079%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.061341/  1.304505, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0294%\n",
      "layer   2  Sparsity: 72.9632%\n",
      "layer   3  Sparsity: 71.5160%\n",
      "total_backward_count 988790 real_backward_count 79489   8.039%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.067100/  1.338943, val:  82.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.46 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   2  Sparsity: 72.8607%\n",
      "layer   3  Sparsity: 71.3705%\n",
      "total_backward_count 998580 real_backward_count 79838   7.995%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.066747/  1.301703, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.31 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   2  Sparsity: 73.0217%\n",
      "layer   3  Sparsity: 71.3829%\n",
      "total_backward_count 1008370 real_backward_count 80247   7.958%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.076234/  1.320219, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   2  Sparsity: 72.8285%\n",
      "layer   3  Sparsity: 71.1325%\n",
      "total_backward_count 1018160 real_backward_count 80649   7.921%\n",
      "fc layer 3 self.abs_max_out: 723.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.070529/  1.286305, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   2  Sparsity: 72.6696%\n",
      "layer   3  Sparsity: 71.1231%\n",
      "total_backward_count 1027950 real_backward_count 81007   7.880%\n",
      "fc layer 1 self.abs_max_out: 4255.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.068154/  1.325427, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.00 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   2  Sparsity: 72.7183%\n",
      "layer   3  Sparsity: 71.5660%\n",
      "total_backward_count 1037740 real_backward_count 81432   7.847%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.060342/  1.309984, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.59 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   2  Sparsity: 72.8599%\n",
      "layer   3  Sparsity: 71.3945%\n",
      "total_backward_count 1047530 real_backward_count 81818   7.811%\n",
      "fc layer 2 self.abs_max_out: 2161.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.062282/  1.325687, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1129%\n",
      "layer   2  Sparsity: 72.7954%\n",
      "layer   3  Sparsity: 71.6629%\n",
      "total_backward_count 1057320 real_backward_count 82212   7.776%\n",
      "lif layer 1 self.abs_max_v: 7383.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.076730/  1.299790, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 72.4353%\n",
      "layer   3  Sparsity: 71.5341%\n",
      "total_backward_count 1067110 real_backward_count 82657   7.746%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.076764/  1.318100, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   2  Sparsity: 72.6428%\n",
      "layer   3  Sparsity: 71.4702%\n",
      "total_backward_count 1076900 real_backward_count 83097   7.716%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.071630/  1.320027, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   2  Sparsity: 72.6242%\n",
      "layer   3  Sparsity: 71.0973%\n",
      "total_backward_count 1086690 real_backward_count 83518   7.686%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.071046/  1.306041, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   2  Sparsity: 72.8436%\n",
      "layer   3  Sparsity: 70.8260%\n",
      "total_backward_count 1096480 real_backward_count 83905   7.652%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.070370/  1.322517, val:  75.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   2  Sparsity: 72.8160%\n",
      "layer   3  Sparsity: 70.9894%\n",
      "total_backward_count 1106270 real_backward_count 84298   7.620%\n",
      "fc layer 1 self.abs_max_out: 4297.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.061689/  1.320087, val:  82.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 73.0904%\n",
      "layer   3  Sparsity: 71.3007%\n",
      "total_backward_count 1116060 real_backward_count 84665   7.586%\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.049902/  1.320488, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.38 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1228%\n",
      "layer   2  Sparsity: 72.8957%\n",
      "layer   3  Sparsity: 71.4809%\n",
      "total_backward_count 1125850 real_backward_count 85048   7.554%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.057171/  1.312949, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 72.6462%\n",
      "layer   3  Sparsity: 71.7300%\n",
      "total_backward_count 1135640 real_backward_count 85400   7.520%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.055327/  1.284224, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 72.6329%\n",
      "layer   3  Sparsity: 71.4608%\n",
      "total_backward_count 1145430 real_backward_count 85816   7.492%\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.046835/  1.304285, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 72.2948%\n",
      "layer   3  Sparsity: 71.4700%\n",
      "total_backward_count 1155220 real_backward_count 86185   7.460%\n",
      "lif layer 1 self.abs_max_v: 7405.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.054256/  1.305718, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0407%\n",
      "layer   2  Sparsity: 72.3819%\n",
      "layer   3  Sparsity: 71.3568%\n",
      "total_backward_count 1165010 real_backward_count 86537   7.428%\n",
      "lif layer 2 self.abs_max_v: 3121.5\n",
      "lif layer 2 self.abs_max_v: 3210.5\n",
      "fc layer 1 self.abs_max_out: 4302.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.046728/  1.290787, val:  80.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   2  Sparsity: 72.4324%\n",
      "layer   3  Sparsity: 71.4288%\n",
      "total_backward_count 1174800 real_backward_count 86884   7.396%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.046571/  1.285075, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.28 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   2  Sparsity: 72.4826%\n",
      "layer   3  Sparsity: 71.3863%\n",
      "total_backward_count 1184590 real_backward_count 87315   7.371%\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.043721/  1.295152, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 72.7171%\n",
      "layer   3  Sparsity: 71.4412%\n",
      "total_backward_count 1194380 real_backward_count 87700   7.343%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.044755/  1.288393, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 72.7348%\n",
      "layer   3  Sparsity: 71.8701%\n",
      "total_backward_count 1204170 real_backward_count 88007   7.309%\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.045127/  1.283763, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.95 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   2  Sparsity: 72.5772%\n",
      "layer   3  Sparsity: 71.6753%\n",
      "total_backward_count 1213960 real_backward_count 88363   7.279%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.022210/  1.277095, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   2  Sparsity: 72.4207%\n",
      "layer   3  Sparsity: 71.8976%\n",
      "total_backward_count 1223750 real_backward_count 88751   7.252%\n",
      "fc layer 1 self.abs_max_out: 4328.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.033852/  1.269772, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.54 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 72.3300%\n",
      "layer   3  Sparsity: 71.4604%\n",
      "total_backward_count 1233540 real_backward_count 89087   7.222%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.020690/  1.295817, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 72.5780%\n",
      "layer   3  Sparsity: 71.6708%\n",
      "total_backward_count 1243330 real_backward_count 89436   7.193%\n",
      "fc layer 2 self.abs_max_out: 2203.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.033401/  1.264019, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 72.5048%\n",
      "layer   3  Sparsity: 71.6205%\n",
      "total_backward_count 1253120 real_backward_count 89787   7.165%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.033390/  1.287199, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0346%\n",
      "layer   2  Sparsity: 72.2009%\n",
      "layer   3  Sparsity: 71.3442%\n",
      "total_backward_count 1262910 real_backward_count 90124   7.136%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.052548/  1.316531, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   2  Sparsity: 72.1638%\n",
      "layer   3  Sparsity: 71.6950%\n",
      "total_backward_count 1272700 real_backward_count 90497   7.111%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.033227/  1.293273, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.67 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 72.4022%\n",
      "layer   3  Sparsity: 71.7944%\n",
      "total_backward_count 1282490 real_backward_count 90824   7.082%\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.039307/  1.297120, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 72.5804%\n",
      "layer   3  Sparsity: 71.8051%\n",
      "total_backward_count 1292280 real_backward_count 91142   7.053%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.019930/  1.286005, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   2  Sparsity: 72.4458%\n",
      "layer   3  Sparsity: 71.9226%\n",
      "total_backward_count 1302070 real_backward_count 91456   7.024%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.017424/  1.267494, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.19 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   2  Sparsity: 72.4777%\n",
      "layer   3  Sparsity: 71.6494%\n",
      "total_backward_count 1311860 real_backward_count 91760   6.995%\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.007231/  1.255136, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 72.3614%\n",
      "layer   3  Sparsity: 71.9686%\n",
      "total_backward_count 1321650 real_backward_count 92060   6.966%\n",
      "fc layer 2 self.abs_max_out: 2224.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.011180/  1.267854, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1157%\n",
      "layer   2  Sparsity: 72.2998%\n",
      "layer   3  Sparsity: 71.7631%\n",
      "total_backward_count 1331440 real_backward_count 92367   6.937%\n",
      "lif layer 2 self.abs_max_v: 3229.0\n",
      "lif layer 2 self.abs_max_v: 3251.5\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.007676/  1.257409, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.73 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1257%\n",
      "layer   2  Sparsity: 72.4655%\n",
      "layer   3  Sparsity: 71.9438%\n",
      "total_backward_count 1341230 real_backward_count 92677   6.910%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.009923/  1.274218, val:  80.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 72.6456%\n",
      "layer   3  Sparsity: 71.7392%\n",
      "total_backward_count 1351020 real_backward_count 93021   6.885%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.018032/  1.274825, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.27 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   2  Sparsity: 72.3793%\n",
      "layer   3  Sparsity: 71.4012%\n",
      "total_backward_count 1360810 real_backward_count 93337   6.859%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.004067/  1.244195, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 72.2975%\n",
      "layer   3  Sparsity: 71.8078%\n",
      "total_backward_count 1370600 real_backward_count 93667   6.834%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.005724/  1.264498, val:  81.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   2  Sparsity: 72.4374%\n",
      "layer   3  Sparsity: 71.4687%\n",
      "total_backward_count 1380390 real_backward_count 93954   6.806%\n",
      "lif layer 2 self.abs_max_v: 3303.5\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.011521/  1.249587, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   2  Sparsity: 72.5123%\n",
      "layer   3  Sparsity: 71.4130%\n",
      "total_backward_count 1390180 real_backward_count 94270   6.781%\n",
      "fc layer 2 self.abs_max_out: 2284.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.998380/  1.260382, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 72.4145%\n",
      "layer   3  Sparsity: 71.2105%\n",
      "total_backward_count 1399970 real_backward_count 94595   6.757%\n",
      "lif layer 2 self.abs_max_v: 3342.0\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "lif layer 2 self.abs_max_v: 3378.0\n",
      "lif layer 2 self.abs_max_v: 3437.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.012410/  1.280745, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 72.0731%\n",
      "layer   3  Sparsity: 71.3277%\n",
      "total_backward_count 1409760 real_backward_count 94883   6.730%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.007524/  1.257200, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   2  Sparsity: 72.0149%\n",
      "layer   3  Sparsity: 71.3502%\n",
      "total_backward_count 1419550 real_backward_count 95225   6.708%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.008850/  1.288352, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.47 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 72.1907%\n",
      "layer   3  Sparsity: 71.2000%\n",
      "total_backward_count 1429340 real_backward_count 95545   6.685%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.008511/  1.279736, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 72.3481%\n",
      "layer   3  Sparsity: 71.1210%\n",
      "total_backward_count 1439130 real_backward_count 95863   6.661%\n",
      "fc layer 1 self.abs_max_out: 4332.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.015969/  1.284425, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 72.2736%\n",
      "layer   3  Sparsity: 71.1653%\n",
      "total_backward_count 1448920 real_backward_count 96153   6.636%\n",
      "fc layer 1 self.abs_max_out: 4405.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.015615/  1.262165, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   2  Sparsity: 72.3373%\n",
      "layer   3  Sparsity: 71.2155%\n",
      "total_backward_count 1458710 real_backward_count 96441   6.611%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.996167/  1.236416, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 72.3584%\n",
      "layer   3  Sparsity: 71.4931%\n",
      "total_backward_count 1468500 real_backward_count 96758   6.589%\n",
      "lif layer 2 self.abs_max_v: 3448.5\n",
      "lif layer 2 self.abs_max_v: 3590.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.981626/  1.250218, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0280%\n",
      "layer   2  Sparsity: 72.2932%\n",
      "layer   3  Sparsity: 71.6236%\n",
      "total_backward_count 1478290 real_backward_count 97031   6.564%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.989870/  1.258206, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   2  Sparsity: 72.3916%\n",
      "layer   3  Sparsity: 71.8254%\n",
      "total_backward_count 1488080 real_backward_count 97339   6.541%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.982095/  1.247017, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 72.3979%\n",
      "layer   3  Sparsity: 72.0038%\n",
      "total_backward_count 1497870 real_backward_count 97594   6.516%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.976964/  1.241541, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 72.2199%\n",
      "layer   3  Sparsity: 71.9545%\n",
      "total_backward_count 1507660 real_backward_count 97871   6.492%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.985789/  1.233972, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   2  Sparsity: 72.2620%\n",
      "layer   3  Sparsity: 71.2055%\n",
      "total_backward_count 1517450 real_backward_count 98174   6.470%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.969602/  1.236881, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 72.1411%\n",
      "layer   3  Sparsity: 71.7728%\n",
      "total_backward_count 1527240 real_backward_count 98469   6.448%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.970864/  1.205881, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.54 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 72.2268%\n",
      "layer   3  Sparsity: 71.5267%\n",
      "total_backward_count 1537030 real_backward_count 98720   6.423%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.969616/  1.237398, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   2  Sparsity: 72.3658%\n",
      "layer   3  Sparsity: 71.3178%\n",
      "total_backward_count 1546820 real_backward_count 98959   6.398%\n",
      "fc layer 1 self.abs_max_out: 4476.0\n",
      "fc layer 2 self.abs_max_out: 2321.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.969193/  1.237763, val:  88.75%, val_best:  91.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 72.1699%\n",
      "layer   3  Sparsity: 71.0904%\n",
      "total_backward_count 1556610 real_backward_count 99257   6.376%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.953395/  1.229488, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 72.1556%\n",
      "layer   3  Sparsity: 71.5282%\n",
      "total_backward_count 1566400 real_backward_count 99528   6.354%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.951937/  1.233008, val:  84.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.95 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   2  Sparsity: 72.3948%\n",
      "layer   3  Sparsity: 71.6643%\n",
      "total_backward_count 1576190 real_backward_count 99807   6.332%\n",
      "fc layer 1 self.abs_max_out: 4566.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.951584/  1.218083, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 72.3074%\n",
      "layer   3  Sparsity: 71.6800%\n",
      "total_backward_count 1585980 real_backward_count 100067   6.309%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.948697/  1.196373, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   2  Sparsity: 72.3527%\n",
      "layer   3  Sparsity: 71.5191%\n",
      "total_backward_count 1595770 real_backward_count 100322   6.287%\n",
      "lif layer 2 self.abs_max_v: 3654.5\n",
      "lif layer 2 self.abs_max_v: 3708.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.932790/  1.222024, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 72.4922%\n",
      "layer   3  Sparsity: 71.6093%\n",
      "total_backward_count 1605560 real_backward_count 100611   6.266%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.952718/  1.215349, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 72.3548%\n",
      "layer   3  Sparsity: 71.3577%\n",
      "total_backward_count 1615350 real_backward_count 100878   6.245%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.933711/  1.200559, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 72.1387%\n",
      "layer   3  Sparsity: 71.2251%\n",
      "total_backward_count 1625140 real_backward_count 101113   6.222%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.928652/  1.223788, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.51 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0267%\n",
      "layer   2  Sparsity: 71.9244%\n",
      "layer   3  Sparsity: 71.3268%\n",
      "total_backward_count 1634930 real_backward_count 101367   6.200%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.939004/  1.196318, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.58 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 72.1285%\n",
      "layer   3  Sparsity: 71.7794%\n",
      "total_backward_count 1644720 real_backward_count 101609   6.178%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.948900/  1.192057, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0709%\n",
      "layer   2  Sparsity: 72.1175%\n",
      "layer   3  Sparsity: 71.4791%\n",
      "total_backward_count 1654510 real_backward_count 101854   6.156%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.940086/  1.216252, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   2  Sparsity: 72.0465%\n",
      "layer   3  Sparsity: 70.7822%\n",
      "total_backward_count 1664300 real_backward_count 102127   6.136%\n",
      "lif layer 2 self.abs_max_v: 3731.0\n",
      "lif layer 2 self.abs_max_v: 3749.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.936016/  1.190426, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 72.2672%\n",
      "layer   3  Sparsity: 70.7828%\n",
      "total_backward_count 1674090 real_backward_count 102396   6.117%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.932976/  1.194272, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 72.1189%\n",
      "layer   3  Sparsity: 71.1387%\n",
      "total_backward_count 1683880 real_backward_count 102642   6.096%\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.925432/  1.188517, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   2  Sparsity: 72.1221%\n",
      "layer   3  Sparsity: 71.4762%\n",
      "total_backward_count 1693670 real_backward_count 102893   6.075%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.921967/  1.178678, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   2  Sparsity: 71.9270%\n",
      "layer   3  Sparsity: 71.6347%\n",
      "total_backward_count 1703460 real_backward_count 103121   6.054%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.912689/  1.176505, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 72.1173%\n",
      "layer   3  Sparsity: 71.7752%\n",
      "total_backward_count 1713250 real_backward_count 103354   6.033%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.920781/  1.197821, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.06 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   2  Sparsity: 71.8471%\n",
      "layer   3  Sparsity: 71.6995%\n",
      "total_backward_count 1723040 real_backward_count 103588   6.012%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.930721/  1.194293, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 71.8645%\n",
      "layer   3  Sparsity: 71.4701%\n",
      "total_backward_count 1732830 real_backward_count 103826   5.992%\n",
      "fc layer 3 self.abs_max_out: 794.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.935729/  1.193848, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.50 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 72.0831%\n",
      "layer   3  Sparsity: 71.2397%\n",
      "total_backward_count 1742620 real_backward_count 104094   5.973%\n",
      "fc layer 3 self.abs_max_out: 812.0\n",
      "fc layer 3 self.abs_max_out: 817.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.936547/  1.200888, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   2  Sparsity: 72.1550%\n",
      "layer   3  Sparsity: 71.3122%\n",
      "total_backward_count 1752410 real_backward_count 104367   5.956%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.925391/  1.191193, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.18 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   2  Sparsity: 71.9622%\n",
      "layer   3  Sparsity: 71.5959%\n",
      "total_backward_count 1762200 real_backward_count 104568   5.934%\n",
      "lif layer 2 self.abs_max_v: 3896.0\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.923431/  1.196609, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0313%\n",
      "layer   2  Sparsity: 72.0376%\n",
      "layer   3  Sparsity: 71.5740%\n",
      "total_backward_count 1771990 real_backward_count 104824   5.916%\n",
      "lif layer 1 self.abs_max_v: 7461.5\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.925225/  1.194952, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1165%\n",
      "layer   2  Sparsity: 72.1851%\n",
      "layer   3  Sparsity: 71.4059%\n",
      "total_backward_count 1781780 real_backward_count 105042   5.895%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.931036/  1.198451, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 72.1772%\n",
      "layer   3  Sparsity: 71.3786%\n",
      "total_backward_count 1791570 real_backward_count 105284   5.877%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.929067/  1.209151, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 72.4611%\n",
      "layer   3  Sparsity: 71.2110%\n",
      "total_backward_count 1801360 real_backward_count 105514   5.857%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.914504/  1.206798, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.44 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 72.4299%\n",
      "layer   3  Sparsity: 71.5416%\n",
      "total_backward_count 1811150 real_backward_count 105734   5.838%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.920563/  1.206011, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   2  Sparsity: 72.0624%\n",
      "layer   3  Sparsity: 71.5752%\n",
      "total_backward_count 1820940 real_backward_count 105934   5.818%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.916857/  1.181398, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0444%\n",
      "layer   2  Sparsity: 71.9478%\n",
      "layer   3  Sparsity: 71.5957%\n",
      "total_backward_count 1830730 real_backward_count 106133   5.797%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.924182/  1.200563, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0317%\n",
      "layer   2  Sparsity: 71.5547%\n",
      "layer   3  Sparsity: 71.2371%\n",
      "total_backward_count 1840520 real_backward_count 106372   5.779%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.919460/  1.187300, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.64 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 72.0152%\n",
      "layer   3  Sparsity: 71.6954%\n",
      "total_backward_count 1850310 real_backward_count 106592   5.761%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.920063/  1.180136, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 72.1311%\n",
      "layer   3  Sparsity: 71.4712%\n",
      "total_backward_count 1860100 real_backward_count 106810   5.742%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.916851/  1.166854, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   2  Sparsity: 72.1860%\n",
      "layer   3  Sparsity: 71.3566%\n",
      "total_backward_count 1869890 real_backward_count 107022   5.723%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.921653/  1.191025, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1193%\n",
      "layer   2  Sparsity: 71.8876%\n",
      "layer   3  Sparsity: 71.3251%\n",
      "total_backward_count 1879680 real_backward_count 107242   5.705%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.906874/  1.179837, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0238%\n",
      "layer   2  Sparsity: 71.9301%\n",
      "layer   3  Sparsity: 71.8323%\n",
      "total_backward_count 1889470 real_backward_count 107434   5.686%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.912204/  1.185601, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   2  Sparsity: 72.1696%\n",
      "layer   3  Sparsity: 72.2432%\n",
      "total_backward_count 1899260 real_backward_count 107643   5.668%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.907977/  1.172338, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   2  Sparsity: 71.9646%\n",
      "layer   3  Sparsity: 72.1543%\n",
      "total_backward_count 1909050 real_backward_count 107860   5.650%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.888682/  1.153010, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.91 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   2  Sparsity: 72.0173%\n",
      "layer   3  Sparsity: 72.5217%\n",
      "total_backward_count 1918840 real_backward_count 108053   5.631%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.898489/  1.158626, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.50 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   2  Sparsity: 72.4123%\n",
      "layer   3  Sparsity: 72.6652%\n",
      "total_backward_count 1928630 real_backward_count 108252   5.613%\n",
      "fc layer 3 self.abs_max_out: 824.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.887822/  1.160857, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 72.4768%\n",
      "layer   3  Sparsity: 72.8638%\n",
      "total_backward_count 1938420 real_backward_count 108436   5.594%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.891338/  1.153988, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.70 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   2  Sparsity: 72.5781%\n",
      "layer   3  Sparsity: 72.2366%\n",
      "total_backward_count 1948210 real_backward_count 108627   5.576%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.894173/  1.172615, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 72.1775%\n",
      "layer   3  Sparsity: 72.0003%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42e399cd4374091819aab83bc9b0d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.89417</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>1.17262</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2ayigem' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2ayigem</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251119_192947-i2ayigem/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p69wczms with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 29996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_001352-p69wczms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p69wczms' target=\"_blank\">cool-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p69wczms' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p69wczms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251120_001401_643', 'my_seed': 29996, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [10, 10, 10], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 10\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 10 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 10\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 10 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 10\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 10 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[10, 10, 10], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[10, 10, 10], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[10, 10, 10], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 132.0\n",
      "lif layer 1 self.abs_max_v: 132.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 135.0\n",
      "lif layer 1 self.abs_max_v: 164.0\n",
      "fc layer 2 self.abs_max_out: 116.0\n",
      "lif layer 2 self.abs_max_v: 124.0\n",
      "lif layer 1 self.abs_max_v: 182.5\n",
      "fc layer 2 self.abs_max_out: 182.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "fc layer 1 self.abs_max_out: 182.0\n",
      "lif layer 1 self.abs_max_v: 263.5\n",
      "lif layer 2 self.abs_max_v: 217.5\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 1 self.abs_max_out: 184.0\n",
      "fc layer 2 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 219.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 273.0\n",
      "fc layer 2 self.abs_max_out: 213.0\n",
      "lif layer 1 self.abs_max_v: 298.0\n",
      "fc layer 2 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 282.5\n",
      "lif layer 1 self.abs_max_v: 312.5\n",
      "lif layer 2 self.abs_max_v: 309.5\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 2 self.abs_max_out: 265.0\n",
      "lif layer 2 self.abs_max_v: 400.0\n",
      "lif layer 1 self.abs_max_v: 375.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "fc layer 1 self.abs_max_out: 284.0\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "fc layer 1 self.abs_max_out: 346.0\n",
      "fc layer 2 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 428.5\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 1 self.abs_max_v: 376.5\n",
      "lif layer 1 self.abs_max_v: 443.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "lif layer 1 self.abs_max_v: 501.5\n",
      "lif layer 2 self.abs_max_v: 556.5\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 1 self.abs_max_out: 444.0\n",
      "lif layer 2 self.abs_max_v: 615.5\n",
      "fc layer 1 self.abs_max_out: 497.0\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "lif layer 1 self.abs_max_v: 586.0\n",
      "lif layer 2 self.abs_max_v: 637.5\n",
      "fc layer 2 self.abs_max_out: 453.0\n",
      "fc layer 1 self.abs_max_out: 650.0\n",
      "lif layer 1 self.abs_max_v: 650.0\n",
      "fc layer 2 self.abs_max_out: 458.0\n",
      "fc layer 1 self.abs_max_out: 660.0\n",
      "lif layer 1 self.abs_max_v: 660.0\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 698.5\n",
      "fc layer 2 self.abs_max_out: 526.0\n",
      "lif layer 2 self.abs_max_v: 773.5\n",
      "fc layer 1 self.abs_max_out: 807.0\n",
      "lif layer 1 self.abs_max_v: 807.0\n",
      "fc layer 2 self.abs_max_out: 567.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "fc layer 2 self.abs_max_out: 621.0\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 1 self.abs_max_out: 824.0\n",
      "lif layer 1 self.abs_max_v: 824.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 1 self.abs_max_out: 907.0\n",
      "lif layer 1 self.abs_max_v: 907.0\n",
      "fc layer 1 self.abs_max_out: 984.0\n",
      "lif layer 1 self.abs_max_v: 984.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 800.5\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "lif layer 2 self.abs_max_v: 822.0\n",
      "lif layer 2 self.abs_max_v: 892.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 934.5\n",
      "lif layer 2 self.abs_max_v: 969.5\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 2 self.abs_max_out: 635.0\n",
      "lif layer 1 self.abs_max_v: 997.0\n",
      "lif layer 1 self.abs_max_v: 1011.0\n",
      "lif layer 1 self.abs_max_v: 1088.0\n",
      "lif layer 2 self.abs_max_v: 978.5\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "lif layer 2 self.abs_max_v: 1051.5\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "lif layer 2 self.abs_max_v: 1052.0\n",
      "lif layer 2 self.abs_max_v: 1154.0\n",
      "fc layer 2 self.abs_max_out: 729.0\n",
      "lif layer 2 self.abs_max_v: 1205.0\n",
      "fc layer 1 self.abs_max_out: 993.0\n",
      "lif layer 1 self.abs_max_v: 1089.5\n",
      "lif layer 1 self.abs_max_v: 1106.0\n",
      "fc layer 2 self.abs_max_out: 735.0\n",
      "lif layer 1 self.abs_max_v: 1114.0\n",
      "lif layer 1 self.abs_max_v: 1175.0\n",
      "lif layer 1 self.abs_max_v: 1214.5\n",
      "fc layer 1 self.abs_max_out: 1022.0\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "fc layer 1 self.abs_max_out: 1232.0\n",
      "lif layer 1 self.abs_max_v: 1232.0\n",
      "lif layer 2 self.abs_max_v: 1221.5\n",
      "lif layer 2 self.abs_max_v: 1309.0\n",
      "fc layer 2 self.abs_max_out: 821.0\n",
      "lif layer 2 self.abs_max_v: 1356.0\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "lif layer 1 self.abs_max_v: 1276.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "lif layer 1 self.abs_max_v: 1287.5\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 1 self.abs_max_out: 1266.0\n",
      "fc layer 2 self.abs_max_out: 967.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "lif layer 1 self.abs_max_v: 1381.0\n",
      "lif layer 1 self.abs_max_v: 1607.5\n",
      "lif layer 1 self.abs_max_v: 1626.0\n",
      "fc layer 2 self.abs_max_out: 971.0\n",
      "fc layer 2 self.abs_max_out: 987.0\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "lif layer 1 self.abs_max_v: 1684.5\n",
      "fc layer 1 self.abs_max_out: 1279.0\n",
      "lif layer 1 self.abs_max_v: 1954.5\n",
      "fc layer 1 self.abs_max_out: 1348.0\n",
      "fc layer 1 self.abs_max_out: 1349.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "fc layer 2 self.abs_max_out: 1098.0\n",
      "fc layer 2 self.abs_max_out: 1110.0\n",
      "lif layer 2 self.abs_max_v: 1383.5\n",
      "lif layer 2 self.abs_max_v: 1393.0\n",
      "fc layer 1 self.abs_max_out: 1400.0\n",
      "lif layer 1 self.abs_max_v: 2214.0\n",
      "fc layer 1 self.abs_max_out: 1517.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "lif layer 1 self.abs_max_v: 2302.0\n",
      "fc layer 1 self.abs_max_out: 1576.0\n",
      "fc layer 1 self.abs_max_out: 1585.0\n",
      "fc layer 1 self.abs_max_out: 1597.0\n",
      "lif layer 2 self.abs_max_v: 1457.0\n",
      "lif layer 2 self.abs_max_v: 1520.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "fc layer 1 self.abs_max_out: 1653.0\n",
      "lif layer 1 self.abs_max_v: 2723.0\n",
      "lif layer 1 self.abs_max_v: 2735.5\n",
      "lif layer 2 self.abs_max_v: 1540.0\n",
      "lif layer 2 self.abs_max_v: 1551.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.783461/  1.969079, val:  43.75%, val_best:  43.75%, tr:  95.61%, tr_best:  95.61%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   2  Sparsity: 75.1435%\n",
      "layer   3  Sparsity: 73.0815%\n",
      "total_backward_count 9790 real_backward_count 2189  22.360%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1591.0\n",
      "fc layer 1 self.abs_max_out: 1685.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "lif layer 2 self.abs_max_v: 1599.5\n",
      "lif layer 2 self.abs_max_v: 1605.5\n",
      "fc layer 1 self.abs_max_out: 1729.0\n",
      "lif layer 2 self.abs_max_v: 1647.5\n",
      "fc layer 1 self.abs_max_out: 1778.0\n",
      "fc layer 1 self.abs_max_out: 1785.0\n",
      "lif layer 1 self.abs_max_v: 2815.5\n",
      "lif layer 2 self.abs_max_v: 1652.0\n",
      "fc layer 1 self.abs_max_out: 1792.0\n",
      "lif layer 1 self.abs_max_v: 3058.5\n",
      "lif layer 2 self.abs_max_v: 1678.5\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "fc layer 2 self.abs_max_out: 1185.0\n",
      "fc layer 2 self.abs_max_out: 1214.0\n",
      "lif layer 2 self.abs_max_v: 1708.0\n",
      "fc layer 1 self.abs_max_out: 1809.0\n",
      "fc layer 1 self.abs_max_out: 1859.0\n",
      "lif layer 1 self.abs_max_v: 3068.5\n",
      "lif layer 2 self.abs_max_v: 1761.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.693437/  1.925187, val:  39.58%, val_best:  43.75%, tr:  98.98%, tr_best:  98.98%, epoch time: 86.62 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 75.2409%\n",
      "layer   3  Sparsity: 70.8466%\n",
      "total_backward_count 19580 real_backward_count 3828  19.551%\n",
      "lif layer 2 self.abs_max_v: 1842.5\n",
      "lif layer 2 self.abs_max_v: 1871.5\n",
      "fc layer 3 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 1874.0\n",
      "lif layer 2 self.abs_max_v: 1878.5\n",
      "lif layer 2 self.abs_max_v: 1886.5\n",
      "lif layer 2 self.abs_max_v: 1917.0\n",
      "lif layer 2 self.abs_max_v: 2051.0\n",
      "lif layer 2 self.abs_max_v: 2072.5\n",
      "fc layer 2 self.abs_max_out: 1235.0\n",
      "fc layer 1 self.abs_max_out: 1954.0\n",
      "fc layer 1 self.abs_max_out: 2028.0\n",
      "fc layer 1 self.abs_max_out: 2163.0\n",
      "lif layer 1 self.abs_max_v: 3528.0\n",
      "lif layer 1 self.abs_max_v: 3586.0\n",
      "fc layer 2 self.abs_max_out: 1251.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.652494/  1.907473, val:  39.17%, val_best:  43.75%, tr:  99.28%, tr_best:  99.28%, epoch time: 86.47 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   2  Sparsity: 75.6788%\n",
      "layer   3  Sparsity: 70.7198%\n",
      "total_backward_count 29370 real_backward_count 5314  18.093%\n",
      "fc layer 2 self.abs_max_out: 1269.0\n",
      "lif layer 1 self.abs_max_v: 3718.0\n",
      "lif layer 2 self.abs_max_v: 2079.5\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "fc layer 1 self.abs_max_out: 2173.0\n",
      "fc layer 1 self.abs_max_out: 2264.0\n",
      "lif layer 1 self.abs_max_v: 3773.5\n",
      "lif layer 1 self.abs_max_v: 3928.0\n",
      "lif layer 1 self.abs_max_v: 3945.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.649374/  1.889418, val:  48.75%, val_best:  48.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 86.52 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   2  Sparsity: 76.0391%\n",
      "layer   3  Sparsity: 69.9107%\n",
      "total_backward_count 39160 real_backward_count 6746  17.227%\n",
      "lif layer 2 self.abs_max_v: 2088.5\n",
      "lif layer 2 self.abs_max_v: 2151.5\n",
      "lif layer 2 self.abs_max_v: 2278.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 1 self.abs_max_out: 2339.0\n",
      "lif layer 1 self.abs_max_v: 3987.5\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 1409.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 1 self.abs_max_out: 2350.0\n",
      "lif layer 1 self.abs_max_v: 4011.5\n",
      "lif layer 1 self.abs_max_v: 4233.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.631416/  1.873271, val:  43.75%, val_best:  48.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 86.50 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1046%\n",
      "layer   2  Sparsity: 75.5136%\n",
      "layer   3  Sparsity: 69.5911%\n",
      "total_backward_count 48950 real_backward_count 8113  16.574%\n",
      "fc layer 1 self.abs_max_out: 2352.0\n",
      "fc layer 1 self.abs_max_out: 2493.0\n",
      "lif layer 1 self.abs_max_v: 4347.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.590302/  1.886916, val:  43.75%, val_best:  48.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 75.8278%\n",
      "layer   3  Sparsity: 69.4172%\n",
      "total_backward_count 58740 real_backward_count 9387  15.981%\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "fc layer 1 self.abs_max_out: 2554.0\n",
      "fc layer 1 self.abs_max_out: 2577.0\n",
      "lif layer 1 self.abs_max_v: 4364.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.596867/  1.824272, val:  47.50%, val_best:  48.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.61 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 75.6605%\n",
      "layer   3  Sparsity: 68.9840%\n",
      "total_backward_count 68530 real_backward_count 10674  15.576%\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 1 self.abs_max_out: 2672.0\n",
      "lif layer 1 self.abs_max_v: 4476.5\n",
      "lif layer 1 self.abs_max_v: 4866.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.573184/  1.808316, val:  56.67%, val_best:  56.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 76.4699%\n",
      "layer   3  Sparsity: 70.6252%\n",
      "total_backward_count 78320 real_backward_count 11896  15.189%\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 2 self.abs_max_out: 1412.0\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "fc layer 2 self.abs_max_out: 1447.0\n",
      "fc layer 1 self.abs_max_out: 2686.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.565990/  1.825272, val:  52.08%, val_best:  56.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 76.0751%\n",
      "layer   3  Sparsity: 71.3280%\n",
      "total_backward_count 88110 real_backward_count 13108  14.877%\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "fc layer 2 self.abs_max_out: 1486.0\n",
      "fc layer 1 self.abs_max_out: 2745.0\n",
      "fc layer 1 self.abs_max_out: 2925.0\n",
      "lif layer 1 self.abs_max_v: 4900.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.591324/  1.848908, val:  60.83%, val_best:  60.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   2  Sparsity: 76.1494%\n",
      "layer   3  Sparsity: 71.4260%\n",
      "total_backward_count 97900 real_backward_count 14383  14.692%\n",
      "lif layer 2 self.abs_max_v: 2300.5\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "lif layer 1 self.abs_max_v: 5166.5\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.572086/  1.813687, val:  53.75%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   2  Sparsity: 75.0164%\n",
      "layer   3  Sparsity: 70.9570%\n",
      "total_backward_count 107690 real_backward_count 15589  14.476%\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 2 self.abs_max_out: 1572.0\n",
      "lif layer 2 self.abs_max_v: 2358.5\n",
      "lif layer 2 self.abs_max_v: 2376.5\n",
      "fc layer 1 self.abs_max_out: 2961.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.560091/  1.803817, val:  55.00%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 74.2543%\n",
      "layer   3  Sparsity: 70.5295%\n",
      "total_backward_count 117480 real_backward_count 16785  14.288%\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 1 self.abs_max_out: 3097.0\n",
      "lif layer 1 self.abs_max_v: 5478.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.535627/  1.765696, val:  57.08%, val_best:  60.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 73.9639%\n",
      "layer   3  Sparsity: 70.2267%\n",
      "total_backward_count 127270 real_backward_count 17957  14.109%\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "fc layer 2 self.abs_max_out: 1668.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.516808/  1.750679, val:  55.83%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1377%\n",
      "layer   2  Sparsity: 73.7071%\n",
      "layer   3  Sparsity: 70.0558%\n",
      "total_backward_count 137060 real_backward_count 19081  13.922%\n",
      "lif layer 2 self.abs_max_v: 2453.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.509365/  1.777316, val:  53.33%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   2  Sparsity: 73.7217%\n",
      "layer   3  Sparsity: 70.5715%\n",
      "total_backward_count 146850 real_backward_count 20186  13.746%\n",
      "fc layer 2 self.abs_max_out: 1708.0\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.505891/  1.766207, val:  53.33%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 74.1976%\n",
      "layer   3  Sparsity: 70.5295%\n",
      "total_backward_count 156640 real_backward_count 21307  13.603%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.516431/  1.762897, val:  58.33%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   2  Sparsity: 73.2844%\n",
      "layer   3  Sparsity: 70.7281%\n",
      "total_backward_count 166430 real_backward_count 22475  13.504%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.505049/  1.727117, val:  56.25%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0895%\n",
      "layer   2  Sparsity: 73.0934%\n",
      "layer   3  Sparsity: 70.8902%\n",
      "total_backward_count 176220 real_backward_count 23616  13.401%\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "lif layer 2 self.abs_max_v: 2468.5\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.466751/  1.704737, val:  64.17%, val_best:  64.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.41 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 73.8908%\n",
      "layer   3  Sparsity: 70.9840%\n",
      "total_backward_count 186010 real_backward_count 24684  13.270%\n",
      "lif layer 2 self.abs_max_v: 2599.0\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "fc layer 2 self.abs_max_out: 1743.0\n",
      "fc layer 1 self.abs_max_out: 3194.0\n",
      "lif layer 1 self.abs_max_v: 5597.0\n",
      "lif layer 1 self.abs_max_v: 5778.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.475750/  1.701012, val:  60.83%, val_best:  64.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   2  Sparsity: 74.3149%\n",
      "layer   3  Sparsity: 71.4943%\n",
      "total_backward_count 195800 real_backward_count 25757  13.155%\n",
      "fc layer 2 self.abs_max_out: 1834.0\n",
      "fc layer 1 self.abs_max_out: 3221.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.454471/  1.705841, val:  59.58%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 74.0651%\n",
      "layer   3  Sparsity: 71.0416%\n",
      "total_backward_count 205590 real_backward_count 26832  13.051%\n",
      "fc layer 2 self.abs_max_out: 1868.0\n",
      "fc layer 2 self.abs_max_out: 1897.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.452136/  1.690799, val:  53.33%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 73.3108%\n",
      "layer   3  Sparsity: 71.0919%\n",
      "total_backward_count 215380 real_backward_count 27834  12.923%\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.414054/  1.666663, val:  65.83%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 72.9740%\n",
      "layer   3  Sparsity: 70.5297%\n",
      "total_backward_count 225170 real_backward_count 28814  12.797%\n",
      "fc layer 1 self.abs_max_out: 3296.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.424018/  1.646233, val:  64.58%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 72.9234%\n",
      "layer   3  Sparsity: 70.7833%\n",
      "total_backward_count 234960 real_backward_count 29746  12.660%\n",
      "fc layer 1 self.abs_max_out: 3327.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.434134/  1.680819, val:  60.00%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 72.8827%\n",
      "layer   3  Sparsity: 70.5224%\n",
      "total_backward_count 244750 real_backward_count 30680  12.535%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.422645/  1.644921, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.92 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 72.9047%\n",
      "layer   3  Sparsity: 70.7326%\n",
      "total_backward_count 254540 real_backward_count 31654  12.436%\n",
      "fc layer 1 self.abs_max_out: 3363.0\n",
      "lif layer 1 self.abs_max_v: 5915.0\n",
      "lif layer 1 self.abs_max_v: 6050.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.423297/  1.672462, val:  69.17%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   2  Sparsity: 72.7141%\n",
      "layer   3  Sparsity: 71.2658%\n",
      "total_backward_count 264330 real_backward_count 32596  12.332%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.432475/  1.659795, val:  67.92%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 72.6292%\n",
      "layer   3  Sparsity: 71.4315%\n",
      "total_backward_count 274120 real_backward_count 33595  12.256%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.412471/  1.646473, val:  55.00%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.95 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   2  Sparsity: 72.5736%\n",
      "layer   3  Sparsity: 71.7186%\n",
      "total_backward_count 283910 real_backward_count 34545  12.168%\n",
      "fc layer 2 self.abs_max_out: 1968.0\n",
      "fc layer 1 self.abs_max_out: 3401.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.397745/  1.628315, val:  62.08%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.60 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   2  Sparsity: 72.4091%\n",
      "layer   3  Sparsity: 71.2557%\n",
      "total_backward_count 293700 real_backward_count 35482  12.081%\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "fc layer 1 self.abs_max_out: 3425.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.381257/  1.588480, val:  69.17%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.16 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   2  Sparsity: 72.2127%\n",
      "layer   3  Sparsity: 70.6603%\n",
      "total_backward_count 303490 real_backward_count 36404  11.995%\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.365118/  1.585790, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 72.4987%\n",
      "layer   3  Sparsity: 71.2471%\n",
      "total_backward_count 313280 real_backward_count 37260  11.894%\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "fc layer 3 self.abs_max_out: 607.0\n",
      "fc layer 1 self.abs_max_out: 3460.0\n",
      "lif layer 1 self.abs_max_v: 6068.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.357730/  1.583240, val:  69.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.93 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   2  Sparsity: 72.4460%\n",
      "layer   3  Sparsity: 70.8443%\n",
      "total_backward_count 323070 real_backward_count 38148  11.808%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.343454/  1.627730, val:  68.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.06 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   2  Sparsity: 72.4562%\n",
      "layer   3  Sparsity: 71.0997%\n",
      "total_backward_count 332860 real_backward_count 38997  11.716%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.353187/  1.606694, val:  67.50%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0215%\n",
      "layer   2  Sparsity: 72.1477%\n",
      "layer   3  Sparsity: 70.9957%\n",
      "total_backward_count 342650 real_backward_count 39823  11.622%\n",
      "fc layer 1 self.abs_max_out: 3529.0\n",
      "lif layer 1 self.abs_max_v: 6199.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.334573/  1.581819, val:  64.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 72.2084%\n",
      "layer   3  Sparsity: 71.0536%\n",
      "total_backward_count 352440 real_backward_count 40662  11.537%\n",
      "lif layer 2 self.abs_max_v: 2625.5\n",
      "lif layer 2 self.abs_max_v: 2755.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.339271/  1.576415, val:  67.08%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0581%\n",
      "layer   2  Sparsity: 72.2385%\n",
      "layer   3  Sparsity: 71.6380%\n",
      "total_backward_count 362230 real_backward_count 41475  11.450%\n",
      "lif layer 2 self.abs_max_v: 2772.5\n",
      "lif layer 2 self.abs_max_v: 2786.5\n",
      "fc layer 1 self.abs_max_out: 3563.0\n",
      "lif layer 1 self.abs_max_v: 6228.5\n",
      "lif layer 1 self.abs_max_v: 6241.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.328305/  1.551396, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 71.7759%\n",
      "layer   3  Sparsity: 71.3722%\n",
      "total_backward_count 372020 real_backward_count 42304  11.371%\n",
      "lif layer 2 self.abs_max_v: 2837.5\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.342689/  1.547698, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 71.1928%\n",
      "layer   3  Sparsity: 71.0187%\n",
      "total_backward_count 381810 real_backward_count 43157  11.303%\n",
      "lif layer 2 self.abs_max_v: 2913.0\n",
      "lif layer 2 self.abs_max_v: 3205.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.329998/  1.543130, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   2  Sparsity: 71.4153%\n",
      "layer   3  Sparsity: 71.2490%\n",
      "total_backward_count 391600 real_backward_count 44006  11.237%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.342175/  1.559783, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 71.0105%\n",
      "layer   3  Sparsity: 71.2616%\n",
      "total_backward_count 401390 real_backward_count 44793  11.159%\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.337787/  1.532573, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 71.3441%\n",
      "layer   3  Sparsity: 71.2788%\n",
      "total_backward_count 411180 real_backward_count 45610  11.092%\n",
      "fc layer 1 self.abs_max_out: 3570.0\n",
      "fc layer 1 self.abs_max_out: 3676.0\n",
      "lif layer 1 self.abs_max_v: 6379.5\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.318298/  1.532495, val:  77.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.03 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 71.0978%\n",
      "layer   3  Sparsity: 71.7045%\n",
      "total_backward_count 420970 real_backward_count 46364  11.014%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.322954/  1.535224, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   2  Sparsity: 71.3531%\n",
      "layer   3  Sparsity: 71.5688%\n",
      "total_backward_count 430760 real_backward_count 47107  10.936%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.329745/  1.569768, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 71.5452%\n",
      "layer   3  Sparsity: 71.3164%\n",
      "total_backward_count 440550 real_backward_count 47880  10.868%\n",
      "fc layer 2 self.abs_max_out: 2034.0\n",
      "lif layer 1 self.abs_max_v: 6446.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.329500/  1.535335, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.93 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 71.6604%\n",
      "layer   3  Sparsity: 71.3011%\n",
      "total_backward_count 450340 real_backward_count 48601  10.792%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.333310/  1.515063, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 71.2282%\n",
      "layer   3  Sparsity: 70.9317%\n",
      "total_backward_count 460130 real_backward_count 49352  10.726%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.312106/  1.539176, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 71.2079%\n",
      "layer   3  Sparsity: 71.2315%\n",
      "total_backward_count 469920 real_backward_count 50125  10.667%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.290363/  1.531595, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.40 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 71.4012%\n",
      "layer   3  Sparsity: 71.1369%\n",
      "total_backward_count 479710 real_backward_count 50854  10.601%\n",
      "fc layer 2 self.abs_max_out: 2063.0\n",
      "fc layer 2 self.abs_max_out: 2080.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.277123/  1.519564, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   2  Sparsity: 71.2443%\n",
      "layer   3  Sparsity: 71.2994%\n",
      "total_backward_count 489500 real_backward_count 51594  10.540%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.289226/  1.513933, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 70.9900%\n",
      "layer   3  Sparsity: 71.0263%\n",
      "total_backward_count 499290 real_backward_count 52345  10.484%\n",
      "fc layer 1 self.abs_max_out: 3686.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.271192/  1.495851, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 71.1171%\n",
      "layer   3  Sparsity: 70.7836%\n",
      "total_backward_count 509080 real_backward_count 53055  10.422%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.259646/  1.532268, val:  73.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0410%\n",
      "layer   2  Sparsity: 71.2134%\n",
      "layer   3  Sparsity: 70.9008%\n",
      "total_backward_count 518870 real_backward_count 53772  10.363%\n",
      "lif layer 1 self.abs_max_v: 6470.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.255256/  1.511327, val:  65.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   2  Sparsity: 71.1638%\n",
      "layer   3  Sparsity: 70.9546%\n",
      "total_backward_count 528660 real_backward_count 54481  10.305%\n",
      "lif layer 2 self.abs_max_v: 3287.0\n",
      "lif layer 2 self.abs_max_v: 3470.5\n",
      "fc layer 1 self.abs_max_out: 3710.0\n",
      "lif layer 1 self.abs_max_v: 6754.0\n",
      "lif layer 1 self.abs_max_v: 6833.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.271025/  1.497327, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.57 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   2  Sparsity: 71.0163%\n",
      "layer   3  Sparsity: 71.6419%\n",
      "total_backward_count 538450 real_backward_count 55122  10.237%\n",
      "fc layer 1 self.abs_max_out: 3908.0\n",
      "lif layer 2 self.abs_max_v: 3539.0\n",
      "fc layer 2 self.abs_max_out: 2114.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.260936/  1.469155, val:  69.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.57 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   2  Sparsity: 71.1477%\n",
      "layer   3  Sparsity: 71.4413%\n",
      "total_backward_count 548240 real_backward_count 55779  10.174%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.229739/  1.484138, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   2  Sparsity: 71.4227%\n",
      "layer   3  Sparsity: 71.8290%\n",
      "total_backward_count 558030 real_backward_count 56436  10.113%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.246730/  1.483536, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 71.3294%\n",
      "layer   3  Sparsity: 71.9690%\n",
      "total_backward_count 567820 real_backward_count 57065  10.050%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.230511/  1.480041, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   2  Sparsity: 70.9658%\n",
      "layer   3  Sparsity: 71.8508%\n",
      "total_backward_count 577610 real_backward_count 57696   9.989%\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.222165/  1.467411, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.06 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 70.9329%\n",
      "layer   3  Sparsity: 72.4575%\n",
      "total_backward_count 587400 real_backward_count 58298   9.925%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.241326/  1.485126, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 70.7340%\n",
      "layer   3  Sparsity: 72.5126%\n",
      "total_backward_count 597190 real_backward_count 58894   9.862%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.256657/  1.507334, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   2  Sparsity: 70.7646%\n",
      "layer   3  Sparsity: 72.2255%\n",
      "total_backward_count 606980 real_backward_count 59542   9.810%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.244407/  1.478503, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.36 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   2  Sparsity: 70.9061%\n",
      "layer   3  Sparsity: 71.9434%\n",
      "total_backward_count 616770 real_backward_count 60159   9.754%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.240594/  1.500066, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   2  Sparsity: 71.0946%\n",
      "layer   3  Sparsity: 71.6991%\n",
      "total_backward_count 626560 real_backward_count 60725   9.692%\n",
      "fc layer 2 self.abs_max_out: 2208.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.244801/  1.510275, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 71.2619%\n",
      "layer   3  Sparsity: 72.4356%\n",
      "total_backward_count 636350 real_backward_count 61314   9.635%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.249753/  1.476228, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 71.3848%\n",
      "layer   3  Sparsity: 72.0374%\n",
      "total_backward_count 646140 real_backward_count 61916   9.582%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.225167/  1.439454, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   2  Sparsity: 71.3377%\n",
      "layer   3  Sparsity: 71.8842%\n",
      "total_backward_count 655930 real_backward_count 62451   9.521%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.203886/  1.450187, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.04 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1286%\n",
      "layer   2  Sparsity: 71.1946%\n",
      "layer   3  Sparsity: 71.6501%\n",
      "total_backward_count 665720 real_backward_count 63026   9.467%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.212916/  1.462406, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 71.3162%\n",
      "layer   3  Sparsity: 71.5311%\n",
      "total_backward_count 675510 real_backward_count 63584   9.413%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.213307/  1.463130, val:  70.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   2  Sparsity: 71.1925%\n",
      "layer   3  Sparsity: 71.1673%\n",
      "total_backward_count 685300 real_backward_count 64123   9.357%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.200060/  1.449206, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.71 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 71.4580%\n",
      "layer   3  Sparsity: 71.5532%\n",
      "total_backward_count 695090 real_backward_count 64705   9.309%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.192935/  1.443931, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1011%\n",
      "layer   2  Sparsity: 71.1862%\n",
      "layer   3  Sparsity: 71.9772%\n",
      "total_backward_count 704880 real_backward_count 65293   9.263%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.195796/  1.443139, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   2  Sparsity: 70.9647%\n",
      "layer   3  Sparsity: 72.2290%\n",
      "total_backward_count 714670 real_backward_count 65894   9.220%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.194435/  1.435376, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 70.9220%\n",
      "layer   3  Sparsity: 72.1500%\n",
      "total_backward_count 724460 real_backward_count 66443   9.171%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.198317/  1.442089, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 71.3409%\n",
      "layer   3  Sparsity: 71.8868%\n",
      "total_backward_count 734250 real_backward_count 66970   9.121%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.201161/  1.433619, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 70.8352%\n",
      "layer   3  Sparsity: 71.5747%\n",
      "total_backward_count 744040 real_backward_count 67499   9.072%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.179453/  1.417619, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.44 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   2  Sparsity: 70.8294%\n",
      "layer   3  Sparsity: 71.3263%\n",
      "total_backward_count 753830 real_backward_count 68029   9.024%\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "fc layer 2 self.abs_max_out: 2213.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.152669/  1.402777, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   2  Sparsity: 70.8178%\n",
      "layer   3  Sparsity: 71.1162%\n",
      "total_backward_count 763620 real_backward_count 68542   8.976%\n",
      "lif layer 1 self.abs_max_v: 6907.5\n",
      "lif layer 1 self.abs_max_v: 7006.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.152003/  1.421081, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   2  Sparsity: 70.8461%\n",
      "layer   3  Sparsity: 71.2447%\n",
      "total_backward_count 773410 real_backward_count 69066   8.930%\n",
      "lif layer 1 self.abs_max_v: 7133.0\n",
      "lif layer 1 self.abs_max_v: 7228.5\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.161510/  1.395076, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   2  Sparsity: 71.1495%\n",
      "layer   3  Sparsity: 71.0731%\n",
      "total_backward_count 783200 real_backward_count 69580   8.884%\n",
      "fc layer 1 self.abs_max_out: 3984.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.160414/  1.425336, val:  79.17%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 70.9226%\n",
      "layer   3  Sparsity: 71.2865%\n",
      "total_backward_count 792990 real_backward_count 70085   8.838%\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.160973/  1.382527, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0978%\n",
      "layer   2  Sparsity: 70.6588%\n",
      "layer   3  Sparsity: 71.2131%\n",
      "total_backward_count 802780 real_backward_count 70536   8.786%\n",
      "lif layer 1 self.abs_max_v: 7243.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.130143/  1.390186, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 70.7809%\n",
      "layer   3  Sparsity: 71.4331%\n",
      "total_backward_count 812570 real_backward_count 71040   8.743%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.137561/  1.371761, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.25 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 70.6575%\n",
      "layer   3  Sparsity: 71.7144%\n",
      "total_backward_count 822360 real_backward_count 71539   8.699%\n",
      "fc layer 2 self.abs_max_out: 2282.0\n",
      "fc layer 3 self.abs_max_out: 691.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.126286/  1.397863, val:  78.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   2  Sparsity: 70.4326%\n",
      "layer   3  Sparsity: 71.9866%\n",
      "total_backward_count 832150 real_backward_count 72047   8.658%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.104235/  1.367131, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0945%\n",
      "layer   2  Sparsity: 70.5038%\n",
      "layer   3  Sparsity: 72.1791%\n",
      "total_backward_count 841940 real_backward_count 72536   8.615%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.126651/  1.384497, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.42 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 70.5186%\n",
      "layer   3  Sparsity: 71.7646%\n",
      "total_backward_count 851730 real_backward_count 72983   8.569%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.125880/  1.369191, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.17 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 70.4797%\n",
      "layer   3  Sparsity: 71.1023%\n",
      "total_backward_count 861520 real_backward_count 73432   8.524%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.122079/  1.386829, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 70.5248%\n",
      "layer   3  Sparsity: 71.1944%\n",
      "total_backward_count 871310 real_backward_count 73909   8.483%\n",
      "fc layer 3 self.abs_max_out: 692.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.126940/  1.411988, val:  70.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 70.6284%\n",
      "layer   3  Sparsity: 70.9329%\n",
      "total_backward_count 881100 real_backward_count 74401   8.444%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.120060/  1.402313, val:  78.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 70.5941%\n",
      "layer   3  Sparsity: 71.0531%\n",
      "total_backward_count 890890 real_backward_count 74880   8.405%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.129766/  1.361989, val:  90.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   2  Sparsity: 70.8097%\n",
      "layer   3  Sparsity: 70.9702%\n",
      "total_backward_count 900680 real_backward_count 75320   8.363%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.117966/  1.400482, val:  80.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.27 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0505%\n",
      "layer   2  Sparsity: 70.5301%\n",
      "layer   3  Sparsity: 70.8193%\n",
      "total_backward_count 910470 real_backward_count 75795   8.325%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.119109/  1.350687, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   2  Sparsity: 70.3376%\n",
      "layer   3  Sparsity: 70.9321%\n",
      "total_backward_count 920260 real_backward_count 76225   8.283%\n",
      "fc layer 1 self.abs_max_out: 3997.0\n",
      "lif layer 1 self.abs_max_v: 7283.0\n",
      "fc layer 1 self.abs_max_out: 4010.0\n",
      "lif layer 1 self.abs_max_v: 7318.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.117715/  1.369141, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 70.4897%\n",
      "layer   3  Sparsity: 71.1399%\n",
      "total_backward_count 930050 real_backward_count 76673   8.244%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.104087/  1.350885, val:  86.67%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 70.3647%\n",
      "layer   3  Sparsity: 70.7123%\n",
      "total_backward_count 939840 real_backward_count 77108   8.204%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.123378/  1.356688, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 70.4865%\n",
      "layer   3  Sparsity: 70.6317%\n",
      "total_backward_count 949630 real_backward_count 77609   8.173%\n",
      "fc layer 1 self.abs_max_out: 4082.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.111895/  1.384967, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.50 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1218%\n",
      "layer   2  Sparsity: 70.4048%\n",
      "layer   3  Sparsity: 70.7274%\n",
      "total_backward_count 959420 real_backward_count 78070   8.137%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.108512/  1.356252, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 70.5862%\n",
      "layer   3  Sparsity: 70.6593%\n",
      "total_backward_count 969210 real_backward_count 78477   8.097%\n",
      "lif layer 2 self.abs_max_v: 3601.0\n",
      "fc layer 1 self.abs_max_out: 4099.0\n",
      "lif layer 1 self.abs_max_v: 7329.5\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.105542/  1.367013, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 70.4014%\n",
      "layer   3  Sparsity: 70.7146%\n",
      "total_backward_count 979000 real_backward_count 78946   8.064%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.098702/  1.345812, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 70.4430%\n",
      "layer   3  Sparsity: 70.7047%\n",
      "total_backward_count 988790 real_backward_count 79366   8.027%\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.076264/  1.354272, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.14 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0667%\n",
      "layer   2  Sparsity: 70.4430%\n",
      "layer   3  Sparsity: 70.5164%\n",
      "total_backward_count 998580 real_backward_count 79792   7.991%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.089426/  1.362730, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.54 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 70.5407%\n",
      "layer   3  Sparsity: 70.7543%\n",
      "total_backward_count 1008370 real_backward_count 80188   7.952%\n",
      "fc layer 1 self.abs_max_out: 4100.0\n",
      "lif layer 1 self.abs_max_v: 7367.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.085916/  1.346527, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   2  Sparsity: 70.5096%\n",
      "layer   3  Sparsity: 70.6449%\n",
      "total_backward_count 1018160 real_backward_count 80597   7.916%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.102880/  1.356612, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 70.6560%\n",
      "layer   3  Sparsity: 70.9137%\n",
      "total_backward_count 1027950 real_backward_count 81017   7.881%\n",
      "fc layer 1 self.abs_max_out: 4290.0\n",
      "lif layer 1 self.abs_max_v: 7468.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.102678/  1.355501, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 70.6791%\n",
      "layer   3  Sparsity: 71.1189%\n",
      "total_backward_count 1037740 real_backward_count 81420   7.846%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.081663/  1.340334, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   2  Sparsity: 70.5511%\n",
      "layer   3  Sparsity: 71.2466%\n",
      "total_backward_count 1047530 real_backward_count 81839   7.813%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.085523/  1.334572, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 70.5185%\n",
      "layer   3  Sparsity: 71.5799%\n",
      "total_backward_count 1057320 real_backward_count 82226   7.777%\n",
      "fc layer 2 self.abs_max_out: 2316.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.086610/  1.336095, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 70.7335%\n",
      "layer   3  Sparsity: 71.2043%\n",
      "total_backward_count 1067110 real_backward_count 82658   7.746%\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.068328/  1.343027, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 70.5562%\n",
      "layer   3  Sparsity: 71.7419%\n",
      "total_backward_count 1076900 real_backward_count 82991   7.706%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.064558/  1.325664, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 70.6699%\n",
      "layer   3  Sparsity: 71.5148%\n",
      "total_backward_count 1086690 real_backward_count 83357   7.671%\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.078641/  1.329017, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   2  Sparsity: 70.7310%\n",
      "layer   3  Sparsity: 71.2826%\n",
      "total_backward_count 1096480 real_backward_count 83787   7.641%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.084987/  1.357880, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   2  Sparsity: 70.6600%\n",
      "layer   3  Sparsity: 71.3153%\n",
      "total_backward_count 1106270 real_backward_count 84165   7.608%\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 1 self.abs_max_out: 4343.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.085157/  1.341234, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   2  Sparsity: 70.4829%\n",
      "layer   3  Sparsity: 71.0169%\n",
      "total_backward_count 1116060 real_backward_count 84572   7.578%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.083627/  1.336246, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   2  Sparsity: 70.5273%\n",
      "layer   3  Sparsity: 71.0503%\n",
      "total_backward_count 1125850 real_backward_count 84935   7.544%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.066969/  1.322424, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.52 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 70.3677%\n",
      "layer   3  Sparsity: 70.4640%\n",
      "total_backward_count 1135640 real_backward_count 85281   7.510%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.081174/  1.324008, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 69.9855%\n",
      "layer   3  Sparsity: 70.8672%\n",
      "total_backward_count 1145430 real_backward_count 85625   7.475%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.069941/  1.308642, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.17 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 70.2189%\n",
      "layer   3  Sparsity: 70.8944%\n",
      "total_backward_count 1155220 real_backward_count 85975   7.442%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.050366/  1.302827, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.04 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   2  Sparsity: 69.9782%\n",
      "layer   3  Sparsity: 70.9292%\n",
      "total_backward_count 1165010 real_backward_count 86330   7.410%\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.054217/  1.306328, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   2  Sparsity: 70.0391%\n",
      "layer   3  Sparsity: 70.7717%\n",
      "total_backward_count 1174800 real_backward_count 86721   7.382%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.062243/  1.318986, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   2  Sparsity: 70.2713%\n",
      "layer   3  Sparsity: 70.8415%\n",
      "total_backward_count 1184590 real_backward_count 87031   7.347%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.050050/  1.318375, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 70.3022%\n",
      "layer   3  Sparsity: 70.9114%\n",
      "total_backward_count 1194380 real_backward_count 87380   7.316%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.041643/  1.298935, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   2  Sparsity: 70.5000%\n",
      "layer   3  Sparsity: 70.8183%\n",
      "total_backward_count 1204170 real_backward_count 87723   7.285%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.049931/  1.304729, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.36 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 70.5727%\n",
      "layer   3  Sparsity: 70.5550%\n",
      "total_backward_count 1213960 real_backward_count 88051   7.253%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.060757/  1.319507, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   2  Sparsity: 70.3342%\n",
      "layer   3  Sparsity: 70.7798%\n",
      "total_backward_count 1223750 real_backward_count 88396   7.223%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.055081/  1.318103, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0344%\n",
      "layer   2  Sparsity: 70.3995%\n",
      "layer   3  Sparsity: 71.2063%\n",
      "total_backward_count 1233540 real_backward_count 88722   7.192%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.050777/  1.316104, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 70.5266%\n",
      "layer   3  Sparsity: 71.2262%\n",
      "total_backward_count 1243330 real_backward_count 89078   7.164%\n",
      "fc layer 3 self.abs_max_out: 752.0\n",
      "fc layer 3 self.abs_max_out: 759.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.030584/  1.295330, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.54 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 70.3002%\n",
      "layer   3  Sparsity: 71.3957%\n",
      "total_backward_count 1253120 real_backward_count 89391   7.133%\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.021005/  1.294162, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.18 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0263%\n",
      "layer   2  Sparsity: 70.0452%\n",
      "layer   3  Sparsity: 71.2767%\n",
      "total_backward_count 1262910 real_backward_count 89737   7.106%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.029646/  1.307112, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   2  Sparsity: 70.5047%\n",
      "layer   3  Sparsity: 70.9487%\n",
      "total_backward_count 1272700 real_backward_count 90063   7.077%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.023177/  1.301478, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 70.5865%\n",
      "layer   3  Sparsity: 71.0177%\n",
      "total_backward_count 1282490 real_backward_count 90378   7.047%\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.016919/  1.255323, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.76 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 70.3939%\n",
      "layer   3  Sparsity: 71.1860%\n",
      "total_backward_count 1292280 real_backward_count 90716   7.020%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.007507/  1.259950, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 70.7448%\n",
      "layer   3  Sparsity: 71.1276%\n",
      "total_backward_count 1302070 real_backward_count 91034   6.991%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.013384/  1.284820, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   2  Sparsity: 70.5856%\n",
      "layer   3  Sparsity: 71.6047%\n",
      "total_backward_count 1311860 real_backward_count 91373   6.965%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.009079/  1.296425, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 70.3514%\n",
      "layer   3  Sparsity: 71.6806%\n",
      "total_backward_count 1321650 real_backward_count 91655   6.935%\n",
      "fc layer 2 self.abs_max_out: 2339.0\n",
      "fc layer 2 self.abs_max_out: 2371.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.025122/  1.286831, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   2  Sparsity: 70.4473%\n",
      "layer   3  Sparsity: 71.4644%\n",
      "total_backward_count 1331440 real_backward_count 91977   6.908%\n",
      "fc layer 3 self.abs_max_out: 783.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.019964/  1.279805, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 70.4831%\n",
      "layer   3  Sparsity: 71.3492%\n",
      "total_backward_count 1341230 real_backward_count 92293   6.881%\n",
      "fc layer 2 self.abs_max_out: 2377.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.008437/  1.266313, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   2  Sparsity: 70.3041%\n",
      "layer   3  Sparsity: 71.6015%\n",
      "total_backward_count 1351020 real_backward_count 92593   6.854%\n",
      "fc layer 1 self.abs_max_out: 4348.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.008984/  1.292465, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 70.4932%\n",
      "layer   3  Sparsity: 71.6279%\n",
      "total_backward_count 1360810 real_backward_count 92880   6.825%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.023102/  1.281841, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.25 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0469%\n",
      "layer   2  Sparsity: 70.4791%\n",
      "layer   3  Sparsity: 71.6930%\n",
      "total_backward_count 1370600 real_backward_count 93215   6.801%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.028250/  1.287997, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   2  Sparsity: 70.2410%\n",
      "layer   3  Sparsity: 71.7926%\n",
      "total_backward_count 1380390 real_backward_count 93538   6.776%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.028685/  1.302856, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 70.2110%\n",
      "layer   3  Sparsity: 71.3982%\n",
      "total_backward_count 1390180 real_backward_count 93812   6.748%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.028238/  1.290840, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.28 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 70.3141%\n",
      "layer   3  Sparsity: 71.4832%\n",
      "total_backward_count 1399970 real_backward_count 94103   6.722%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.013353/  1.269832, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1251%\n",
      "layer   2  Sparsity: 70.2397%\n",
      "layer   3  Sparsity: 71.7362%\n",
      "total_backward_count 1409760 real_backward_count 94419   6.698%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.012740/  1.282911, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   2  Sparsity: 70.0775%\n",
      "layer   3  Sparsity: 71.5192%\n",
      "total_backward_count 1419550 real_backward_count 94701   6.671%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.016218/  1.304233, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0964%\n",
      "layer   2  Sparsity: 70.0877%\n",
      "layer   3  Sparsity: 71.2246%\n",
      "total_backward_count 1429340 real_backward_count 94992   6.646%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.015180/  1.284863, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.99 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 70.3636%\n",
      "layer   3  Sparsity: 71.3381%\n",
      "total_backward_count 1439130 real_backward_count 95284   6.621%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.021918/  1.278354, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 70.3732%\n",
      "layer   3  Sparsity: 71.1553%\n",
      "total_backward_count 1448920 real_backward_count 95555   6.595%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.005445/  1.269030, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.09 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   2  Sparsity: 70.4826%\n",
      "layer   3  Sparsity: 71.3190%\n",
      "total_backward_count 1458710 real_backward_count 95848   6.571%\n",
      "fc layer 2 self.abs_max_out: 2393.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.009090/  1.275777, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.70 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 70.4477%\n",
      "layer   3  Sparsity: 71.3147%\n",
      "total_backward_count 1468500 real_backward_count 96119   6.545%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.007692/  1.278872, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0395%\n",
      "layer   2  Sparsity: 70.4384%\n",
      "layer   3  Sparsity: 71.5219%\n",
      "total_backward_count 1478290 real_backward_count 96416   6.522%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.001758/  1.293519, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0393%\n",
      "layer   2  Sparsity: 70.4238%\n",
      "layer   3  Sparsity: 71.5179%\n",
      "total_backward_count 1488080 real_backward_count 96688   6.498%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.000284/  1.278376, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   2  Sparsity: 70.3749%\n",
      "layer   3  Sparsity: 71.4664%\n",
      "total_backward_count 1497870 real_backward_count 96957   6.473%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.987245/  1.258570, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 70.6238%\n",
      "layer   3  Sparsity: 71.8076%\n",
      "total_backward_count 1507660 real_backward_count 97201   6.447%\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.993632/  1.287125, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 70.4543%\n",
      "layer   3  Sparsity: 71.5079%\n",
      "total_backward_count 1517450 real_backward_count 97467   6.423%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.987693/  1.258548, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   2  Sparsity: 70.4257%\n",
      "layer   3  Sparsity: 71.7340%\n",
      "total_backward_count 1527240 real_backward_count 97720   6.398%\n",
      "fc layer 1 self.abs_max_out: 4493.0\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.979448/  1.256375, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   2  Sparsity: 70.3283%\n",
      "layer   3  Sparsity: 71.9672%\n",
      "total_backward_count 1537030 real_backward_count 97975   6.374%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.981628/  1.305378, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1293%\n",
      "layer   2  Sparsity: 70.1353%\n",
      "layer   3  Sparsity: 71.7155%\n",
      "total_backward_count 1546820 real_backward_count 98238   6.351%\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.987622/  1.250735, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.32 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0709%\n",
      "layer   2  Sparsity: 70.1103%\n",
      "layer   3  Sparsity: 71.3875%\n",
      "total_backward_count 1556610 real_backward_count 98518   6.329%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.992513/  1.258046, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 70.2491%\n",
      "layer   3  Sparsity: 71.3188%\n",
      "total_backward_count 1566400 real_backward_count 98757   6.305%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.009973/  1.263833, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 70.1896%\n",
      "layer   3  Sparsity: 71.5986%\n",
      "total_backward_count 1576190 real_backward_count 99026   6.283%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.993940/  1.247601, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 70.2341%\n",
      "layer   3  Sparsity: 71.8820%\n",
      "total_backward_count 1585980 real_backward_count 99298   6.261%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.985241/  1.229369, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   2  Sparsity: 70.4510%\n",
      "layer   3  Sparsity: 71.6995%\n",
      "total_backward_count 1595770 real_backward_count 99551   6.238%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.972944/  1.239669, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 70.3528%\n",
      "layer   3  Sparsity: 71.7015%\n",
      "total_backward_count 1605560 real_backward_count 99762   6.214%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.976107/  1.269683, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   2  Sparsity: 70.1819%\n",
      "layer   3  Sparsity: 72.2741%\n",
      "total_backward_count 1615350 real_backward_count 100026   6.192%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.962338/  1.255174, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.62 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 70.1437%\n",
      "layer   3  Sparsity: 72.4611%\n",
      "total_backward_count 1625140 real_backward_count 100282   6.171%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.974345/  1.244900, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1413%\n",
      "layer   2  Sparsity: 69.8618%\n",
      "layer   3  Sparsity: 72.1309%\n",
      "total_backward_count 1634930 real_backward_count 100567   6.151%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.974162/  1.252821, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.86 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   2  Sparsity: 69.9228%\n",
      "layer   3  Sparsity: 71.8749%\n",
      "total_backward_count 1644720 real_backward_count 100810   6.129%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.975443/  1.262673, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 70.0387%\n",
      "layer   3  Sparsity: 71.7814%\n",
      "total_backward_count 1654510 real_backward_count 101067   6.109%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.972481/  1.238446, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.31 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   2  Sparsity: 70.2953%\n",
      "layer   3  Sparsity: 72.2647%\n",
      "total_backward_count 1664300 real_backward_count 101278   6.085%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.978654/  1.284162, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.49 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0153%\n",
      "layer   2  Sparsity: 70.1114%\n",
      "layer   3  Sparsity: 71.9113%\n",
      "total_backward_count 1674090 real_backward_count 101545   6.066%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.977075/  1.231363, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.38 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   2  Sparsity: 70.4119%\n",
      "layer   3  Sparsity: 72.1953%\n",
      "total_backward_count 1683880 real_backward_count 101765   6.043%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.962036/  1.234141, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 70.2495%\n",
      "layer   3  Sparsity: 72.1845%\n",
      "total_backward_count 1693670 real_backward_count 101988   6.022%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.975872/  1.232773, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0428%\n",
      "layer   2  Sparsity: 70.2564%\n",
      "layer   3  Sparsity: 72.1408%\n",
      "total_backward_count 1703460 real_backward_count 102227   6.001%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.976430/  1.257830, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 70.1297%\n",
      "layer   3  Sparsity: 72.1437%\n",
      "total_backward_count 1713250 real_backward_count 102518   5.984%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.978037/  1.259187, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   2  Sparsity: 69.9922%\n",
      "layer   3  Sparsity: 71.9339%\n",
      "total_backward_count 1723040 real_backward_count 102796   5.966%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.964343/  1.211025, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 70.0675%\n",
      "layer   3  Sparsity: 71.7495%\n",
      "total_backward_count 1732830 real_backward_count 102988   5.943%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.947440/  1.218392, val:  88.75%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   2  Sparsity: 70.3088%\n",
      "layer   3  Sparsity: 71.7838%\n",
      "total_backward_count 1742620 real_backward_count 103190   5.922%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.951333/  1.224815, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.20 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   2  Sparsity: 70.2195%\n",
      "layer   3  Sparsity: 71.5522%\n",
      "total_backward_count 1752410 real_backward_count 103430   5.902%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.967629/  1.238894, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 69.8384%\n",
      "layer   3  Sparsity: 71.5203%\n",
      "total_backward_count 1762200 real_backward_count 103650   5.882%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.971049/  1.245178, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   2  Sparsity: 69.7576%\n",
      "layer   3  Sparsity: 71.6509%\n",
      "total_backward_count 1771990 real_backward_count 103873   5.862%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.960401/  1.249313, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.42 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   2  Sparsity: 70.1117%\n",
      "layer   3  Sparsity: 72.0127%\n",
      "total_backward_count 1781780 real_backward_count 104077   5.841%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.961244/  1.244308, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.21 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   2  Sparsity: 69.9246%\n",
      "layer   3  Sparsity: 71.6042%\n",
      "total_backward_count 1791570 real_backward_count 104316   5.823%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.961740/  1.226305, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   2  Sparsity: 69.9416%\n",
      "layer   3  Sparsity: 71.4163%\n",
      "total_backward_count 1801360 real_backward_count 104555   5.804%\n",
      "fc layer 1 self.abs_max_out: 4645.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.954747/  1.221217, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 70.2161%\n",
      "layer   3  Sparsity: 71.4006%\n",
      "total_backward_count 1811150 real_backward_count 104781   5.785%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.952653/  1.232573, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 70.2432%\n",
      "layer   3  Sparsity: 71.7992%\n",
      "total_backward_count 1820940 real_backward_count 104964   5.764%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.953568/  1.235302, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 70.4819%\n",
      "layer   3  Sparsity: 72.0555%\n",
      "total_backward_count 1830730 real_backward_count 105173   5.745%\n",
      "lif layer 1 self.abs_max_v: 7556.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.965821/  1.221324, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   2  Sparsity: 70.3222%\n",
      "layer   3  Sparsity: 71.7973%\n",
      "total_backward_count 1840520 real_backward_count 105389   5.726%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.967246/  1.242534, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.06 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1106%\n",
      "layer   2  Sparsity: 69.9989%\n",
      "layer   3  Sparsity: 71.6221%\n",
      "total_backward_count 1850310 real_backward_count 105620   5.708%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.978357/  1.238866, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.14 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   2  Sparsity: 69.9343%\n",
      "layer   3  Sparsity: 71.6063%\n",
      "total_backward_count 1860100 real_backward_count 105862   5.691%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.976591/  1.236356, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.99 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 70.1275%\n",
      "layer   3  Sparsity: 71.2163%\n",
      "total_backward_count 1869890 real_backward_count 106095   5.674%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.981248/  1.216472, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.85 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 70.0352%\n",
      "layer   3  Sparsity: 70.7305%\n",
      "total_backward_count 1879680 real_backward_count 106353   5.658%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.957434/  1.231389, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.92 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   2  Sparsity: 70.2815%\n",
      "layer   3  Sparsity: 71.2592%\n",
      "total_backward_count 1889470 real_backward_count 106590   5.641%\n",
      "fc layer 2 self.abs_max_out: 2437.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.963023/  1.231070, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.72 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   2  Sparsity: 70.1255%\n",
      "layer   3  Sparsity: 71.2500%\n",
      "total_backward_count 1899260 real_backward_count 106778   5.622%\n",
      "fc layer 2 self.abs_max_out: 2465.0\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.954160/  1.218822, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.38 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 70.2971%\n",
      "layer   3  Sparsity: 71.6967%\n",
      "total_backward_count 1909050 real_backward_count 107001   5.605%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.948463/  1.221395, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 70.4106%\n",
      "layer   3  Sparsity: 71.8037%\n",
      "total_backward_count 1918840 real_backward_count 107209   5.587%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.935983/  1.246308, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.98 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   2  Sparsity: 70.4607%\n",
      "layer   3  Sparsity: 71.9425%\n",
      "total_backward_count 1928630 real_backward_count 107384   5.568%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.947679/  1.229387, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   2  Sparsity: 70.2363%\n",
      "layer   3  Sparsity: 71.9730%\n",
      "total_backward_count 1938420 real_backward_count 107576   5.550%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.935687/  1.216673, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   2  Sparsity: 70.1063%\n",
      "layer   3  Sparsity: 71.8909%\n",
      "total_backward_count 1948210 real_backward_count 107795   5.533%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.929256/  1.218135, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.15 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 70.0760%\n",
      "layer   3  Sparsity: 72.0730%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c8a8b4f17c44bf9fe4d0beb72749db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.92926</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>1.21814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p69wczms' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p69wczms</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251120_001352-p69wczms/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7fc10w5a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 30374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_045950-7fc10w5a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fc10w5a' target=\"_blank\">major-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fc10w5a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fc10w5a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251120_045959_522', 'my_seed': 30374, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [6, 6, 6], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 6\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 6 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 6\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 6 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 6\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 6 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[6, 6, 6], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[6, 6, 6], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[6, 6, 6], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 287.0\n",
      "lif layer 1 self.abs_max_v: 287.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 177.0\n",
      "lif layer 2 self.abs_max_v: 177.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "lif layer 1 self.abs_max_v: 362.0\n",
      "fc layer 2 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 365.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "lif layer 1 self.abs_max_v: 379.5\n",
      "lif layer 2 self.abs_max_v: 407.0\n",
      "fc layer 1 self.abs_max_out: 421.0\n",
      "lif layer 1 self.abs_max_v: 421.0\n",
      "lif layer 2 self.abs_max_v: 458.5\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 1 self.abs_max_out: 426.0\n",
      "lif layer 1 self.abs_max_v: 426.0\n",
      "fc layer 2 self.abs_max_out: 327.0\n",
      "lif layer 2 self.abs_max_v: 521.5\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 1 self.abs_max_out: 460.0\n",
      "lif layer 1 self.abs_max_v: 460.0\n",
      "fc layer 2 self.abs_max_out: 397.0\n",
      "lif layer 2 self.abs_max_v: 658.0\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 570.0\n",
      "lif layer 1 self.abs_max_v: 679.0\n",
      "lif layer 2 self.abs_max_v: 689.5\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 2 self.abs_max_out: 408.0\n",
      "fc layer 1 self.abs_max_out: 486.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 1 self.abs_max_out: 518.0\n",
      "fc layer 2 self.abs_max_out: 413.0\n",
      "fc layer 1 self.abs_max_out: 535.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 2 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 610.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "fc layer 1 self.abs_max_out: 612.0\n",
      "fc layer 2 self.abs_max_out: 464.0\n",
      "lif layer 2 self.abs_max_v: 708.5\n",
      "lif layer 2 self.abs_max_v: 717.5\n",
      "fc layer 2 self.abs_max_out: 522.0\n",
      "lif layer 2 self.abs_max_v: 752.0\n",
      "lif layer 2 self.abs_max_v: 789.5\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 1 self.abs_max_out: 770.0\n",
      "lif layer 1 self.abs_max_v: 770.0\n",
      "fc layer 2 self.abs_max_out: 568.0\n",
      "lif layer 2 self.abs_max_v: 834.0\n",
      "lif layer 2 self.abs_max_v: 969.0\n",
      "lif layer 2 self.abs_max_v: 1026.5\n",
      "lif layer 2 self.abs_max_v: 1055.5\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "lif layer 1 self.abs_max_v: 859.0\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "lif layer 2 self.abs_max_v: 1168.0\n",
      "fc layer 2 self.abs_max_out: 660.0\n",
      "lif layer 2 self.abs_max_v: 1244.0\n",
      "lif layer 1 self.abs_max_v: 863.5\n",
      "lif layer 1 self.abs_max_v: 874.0\n",
      "lif layer 1 self.abs_max_v: 969.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 2 self.abs_max_out: 697.0\n",
      "fc layer 1 self.abs_max_out: 878.0\n",
      "fc layer 1 self.abs_max_out: 923.0\n",
      "lif layer 1 self.abs_max_v: 973.0\n",
      "lif layer 1 self.abs_max_v: 988.0\n",
      "lif layer 1 self.abs_max_v: 1111.0\n",
      "lif layer 1 self.abs_max_v: 1160.5\n",
      "lif layer 1 self.abs_max_v: 1182.5\n",
      "lif layer 1 self.abs_max_v: 1280.5\n",
      "fc layer 1 self.abs_max_out: 943.0\n",
      "fc layer 1 self.abs_max_out: 949.0\n",
      "fc layer 1 self.abs_max_out: 1023.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "fc layer 2 self.abs_max_out: 753.0\n",
      "fc layer 1 self.abs_max_out: 1040.0\n",
      "fc layer 2 self.abs_max_out: 840.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 1 self.abs_max_out: 1145.0\n",
      "fc layer 2 self.abs_max_out: 916.0\n",
      "lif layer 2 self.abs_max_v: 1257.0\n",
      "lif layer 2 self.abs_max_v: 1257.5\n",
      "lif layer 2 self.abs_max_v: 1325.0\n",
      "lif layer 1 self.abs_max_v: 1393.0\n",
      "lif layer 1 self.abs_max_v: 1544.5\n",
      "lif layer 1 self.abs_max_v: 1620.5\n",
      "lif layer 1 self.abs_max_v: 1631.5\n",
      "fc layer 1 self.abs_max_out: 1179.0\n",
      "lif layer 1 self.abs_max_v: 1683.0\n",
      "fc layer 1 self.abs_max_out: 1243.0\n",
      "fc layer 1 self.abs_max_out: 1322.0\n",
      "lif layer 2 self.abs_max_v: 1341.5\n",
      "lif layer 2 self.abs_max_v: 1374.0\n",
      "lif layer 2 self.abs_max_v: 1471.0\n",
      "fc layer 1 self.abs_max_out: 1408.0\n",
      "lif layer 1 self.abs_max_v: 1740.5\n",
      "lif layer 1 self.abs_max_v: 1742.0\n",
      "lif layer 1 self.abs_max_v: 1983.5\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "fc layer 1 self.abs_max_out: 1458.0\n",
      "fc layer 1 self.abs_max_out: 1462.0\n",
      "fc layer 2 self.abs_max_out: 965.0\n",
      "fc layer 2 self.abs_max_out: 992.0\n",
      "lif layer 1 self.abs_max_v: 2044.5\n",
      "lif layer 1 self.abs_max_v: 2129.5\n",
      "lif layer 1 self.abs_max_v: 2175.5\n",
      "fc layer 1 self.abs_max_out: 1529.0\n",
      "fc layer 1 self.abs_max_out: 1690.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 2 self.abs_max_out: 1034.0\n",
      "fc layer 1 self.abs_max_out: 1777.0\n",
      "lif layer 1 self.abs_max_v: 2190.5\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "lif layer 1 self.abs_max_v: 2281.5\n",
      "lif layer 1 self.abs_max_v: 2491.5\n",
      "fc layer 1 self.abs_max_out: 1785.0\n",
      "lif layer 1 self.abs_max_v: 2588.5\n",
      "lif layer 1 self.abs_max_v: 2602.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.774799/  1.968908, val:  38.33%, val_best:  38.33%, tr:  96.63%, tr_best:  96.63%, epoch time: 86.70 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 74.3393%\n",
      "layer   3  Sparsity: 73.1010%\n",
      "total_backward_count 9790 real_backward_count 2259  23.075%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1807.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "lif layer 2 self.abs_max_v: 1499.5\n",
      "lif layer 2 self.abs_max_v: 1501.0\n",
      "lif layer 2 self.abs_max_v: 1612.5\n",
      "lif layer 2 self.abs_max_v: 1650.5\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "lif layer 1 self.abs_max_v: 2674.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.683588/  1.903120, val:  46.25%, val_best:  46.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 85.44 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   2  Sparsity: 74.1901%\n",
      "layer   3  Sparsity: 70.4311%\n",
      "total_backward_count 19580 real_backward_count 3830  19.561%\n",
      "lif layer 1 self.abs_max_v: 2820.0\n",
      "lif layer 1 self.abs_max_v: 2892.0\n",
      "fc layer 1 self.abs_max_out: 1869.0\n",
      "fc layer 1 self.abs_max_out: 1950.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 413.0\n",
      "fc layer 1 self.abs_max_out: 2032.0\n",
      "lif layer 1 self.abs_max_v: 3253.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.644123/  1.871342, val:  41.67%, val_best:  46.25%, tr:  99.28%, tr_best:  99.39%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 74.5104%\n",
      "layer   3  Sparsity: 69.3589%\n",
      "total_backward_count 29370 real_backward_count 5250  17.875%\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "fc layer 1 self.abs_max_out: 2153.0\n",
      "lif layer 1 self.abs_max_v: 3391.0\n",
      "lif layer 1 self.abs_max_v: 3403.0\n",
      "fc layer 2 self.abs_max_out: 1151.0\n",
      "lif layer 2 self.abs_max_v: 1655.0\n",
      "lif layer 2 self.abs_max_v: 1656.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.629351/  1.867547, val:  50.83%, val_best:  50.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 84.46 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 74.9069%\n",
      "layer   3  Sparsity: 69.6753%\n",
      "total_backward_count 39160 real_backward_count 6688  17.079%\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "lif layer 2 self.abs_max_v: 1760.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "fc layer 1 self.abs_max_out: 2188.0\n",
      "fc layer 1 self.abs_max_out: 2203.0\n",
      "lif layer 1 self.abs_max_v: 3555.0\n",
      "lif layer 1 self.abs_max_v: 3754.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.607859/  1.855402, val:  47.08%, val_best:  50.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   2  Sparsity: 74.2654%\n",
      "layer   3  Sparsity: 68.7663%\n",
      "total_backward_count 48950 real_backward_count 8110  16.568%\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "fc layer 1 self.abs_max_out: 2620.0\n",
      "lif layer 1 self.abs_max_v: 4215.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.607773/  1.858368, val:  42.92%, val_best:  50.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0487%\n",
      "layer   2  Sparsity: 74.6899%\n",
      "layer   3  Sparsity: 69.2229%\n",
      "total_backward_count 58740 real_backward_count 9475  16.130%\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "fc layer 1 self.abs_max_out: 2660.0\n",
      "lif layer 1 self.abs_max_v: 4278.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.580673/  1.836680, val:  53.33%, val_best:  53.33%, tr:  99.49%, tr_best:  99.59%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   2  Sparsity: 75.1747%\n",
      "layer   3  Sparsity: 69.9142%\n",
      "total_backward_count 68530 real_backward_count 10895  15.898%\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.568529/  1.805235, val:  59.58%, val_best:  59.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1103%\n",
      "layer   2  Sparsity: 74.3347%\n",
      "layer   3  Sparsity: 69.1080%\n",
      "total_backward_count 78320 real_backward_count 12294  15.697%\n",
      "lif layer 2 self.abs_max_v: 1782.0\n",
      "fc layer 1 self.abs_max_out: 2745.0\n",
      "lif layer 1 self.abs_max_v: 4388.0\n",
      "lif layer 1 self.abs_max_v: 4414.0\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.565994/  1.843236, val:  46.25%, val_best:  59.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 74.1805%\n",
      "layer   3  Sparsity: 70.1901%\n",
      "total_backward_count 88110 real_backward_count 13621  15.459%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.599127/  1.867181, val:  51.67%, val_best:  59.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   2  Sparsity: 74.2221%\n",
      "layer   3  Sparsity: 70.8797%\n",
      "total_backward_count 97900 real_backward_count 14962  15.283%\n",
      "lif layer 2 self.abs_max_v: 1848.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.587882/  1.809659, val:  54.58%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   2  Sparsity: 74.0523%\n",
      "layer   3  Sparsity: 71.0274%\n",
      "total_backward_count 107690 real_backward_count 16290  15.127%\n",
      "lif layer 2 self.abs_max_v: 1851.5\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.579312/  1.801412, val:  49.58%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.07 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   2  Sparsity: 74.1906%\n",
      "layer   3  Sparsity: 71.6562%\n",
      "total_backward_count 117480 real_backward_count 17601  14.982%\n",
      "fc layer 2 self.abs_max_out: 1271.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "lif layer 1 self.abs_max_v: 4438.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.582180/  1.806470, val:  48.33%, val_best:  59.58%, tr:  99.18%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   2  Sparsity: 74.2397%\n",
      "layer   3  Sparsity: 71.0494%\n",
      "total_backward_count 127270 real_backward_count 18863  14.821%\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1858.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.604318/  1.782171, val:  55.83%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   2  Sparsity: 74.4682%\n",
      "layer   3  Sparsity: 72.1223%\n",
      "total_backward_count 137060 real_backward_count 20181  14.724%\n",
      "fc layer 2 self.abs_max_out: 1357.0\n",
      "fc layer 1 self.abs_max_out: 2757.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.578818/  1.854533, val:  52.50%, val_best:  59.58%, tr:  99.39%, tr_best: 100.00%, epoch time: 84.95 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 74.3870%\n",
      "layer   3  Sparsity: 73.3356%\n",
      "total_backward_count 146850 real_backward_count 21505  14.644%\n",
      "lif layer 1 self.abs_max_v: 4453.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.592690/  1.832339, val:  54.17%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 74.1692%\n",
      "layer   3  Sparsity: 72.8108%\n",
      "total_backward_count 156640 real_backward_count 22838  14.580%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.600281/  1.834965, val:  54.58%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.74 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   2  Sparsity: 73.1178%\n",
      "layer   3  Sparsity: 72.8442%\n",
      "total_backward_count 166430 real_backward_count 24130  14.499%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.604694/  1.830373, val:  61.67%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.10 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 73.5940%\n",
      "layer   3  Sparsity: 73.9019%\n",
      "total_backward_count 176220 real_backward_count 25415  14.422%\n",
      "lif layer 2 self.abs_max_v: 1880.0\n",
      "lif layer 2 self.abs_max_v: 1899.5\n",
      "lif layer 2 self.abs_max_v: 1959.0\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "lif layer 1 self.abs_max_v: 4703.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.614082/  1.862184, val:  53.75%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.74 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   2  Sparsity: 73.2465%\n",
      "layer   3  Sparsity: 74.7831%\n",
      "total_backward_count 186010 real_backward_count 26666  14.336%\n",
      "lif layer 2 self.abs_max_v: 1963.5\n",
      "lif layer 1 self.abs_max_v: 4753.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.650722/  1.831609, val:  54.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   2  Sparsity: 73.4298%\n",
      "layer   3  Sparsity: 76.0397%\n",
      "total_backward_count 195800 real_backward_count 27960  14.280%\n",
      "fc layer 1 self.abs_max_out: 2775.0\n",
      "lif layer 1 self.abs_max_v: 4857.5\n",
      "lif layer 1 self.abs_max_v: 4995.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.611665/  1.818408, val:  58.75%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.82 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 73.7548%\n",
      "layer   3  Sparsity: 75.1938%\n",
      "total_backward_count 205590 real_backward_count 29196  14.201%\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.617676/  1.848661, val:  51.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.99 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 73.6726%\n",
      "layer   3  Sparsity: 74.9551%\n",
      "total_backward_count 215380 real_backward_count 30429  14.128%\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.625436/  1.870002, val:  54.58%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.00 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0377%\n",
      "layer   2  Sparsity: 73.2565%\n",
      "layer   3  Sparsity: 75.3838%\n",
      "total_backward_count 225170 real_backward_count 31710  14.083%\n",
      "fc layer 2 self.abs_max_out: 1405.0\n",
      "fc layer 1 self.abs_max_out: 2806.0\n",
      "lif layer 1 self.abs_max_v: 4997.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.653060/  1.864503, val:  61.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 72.5854%\n",
      "layer   3  Sparsity: 76.0013%\n",
      "total_backward_count 234960 real_backward_count 32907  14.005%\n",
      "fc layer 1 self.abs_max_out: 2842.0\n",
      "lif layer 1 self.abs_max_v: 5098.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.637994/  1.874500, val:  61.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.80 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   2  Sparsity: 72.7562%\n",
      "layer   3  Sparsity: 75.4984%\n",
      "total_backward_count 244750 real_backward_count 34112  13.937%\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.659411/  1.873555, val:  52.08%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 73.3180%\n",
      "layer   3  Sparsity: 76.2672%\n",
      "total_backward_count 254540 real_backward_count 35320  13.876%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.662357/  1.884969, val:  50.00%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.79 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0313%\n",
      "layer   2  Sparsity: 72.6793%\n",
      "layer   3  Sparsity: 75.8495%\n",
      "total_backward_count 264330 real_backward_count 36515  13.814%\n",
      "fc layer 2 self.abs_max_out: 1463.0\n",
      "fc layer 1 self.abs_max_out: 2863.0\n",
      "fc layer 2 self.abs_max_out: 1469.0\n",
      "lif layer 1 self.abs_max_v: 5232.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.656947/  1.851573, val:  63.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.59 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 72.9562%\n",
      "layer   3  Sparsity: 76.0603%\n",
      "total_backward_count 274120 real_backward_count 37730  13.764%\n",
      "fc layer 1 self.abs_max_out: 2917.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.656556/  1.827089, val:  59.17%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 73.1626%\n",
      "layer   3  Sparsity: 76.6258%\n",
      "total_backward_count 283910 real_backward_count 38916  13.707%\n",
      "fc layer 1 self.abs_max_out: 2923.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.662606/  1.829616, val:  56.25%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   2  Sparsity: 72.8522%\n",
      "layer   3  Sparsity: 76.6077%\n",
      "total_backward_count 293700 real_backward_count 40102  13.654%\n",
      "fc layer 1 self.abs_max_out: 2962.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.656924/  1.830964, val:  67.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 72.4642%\n",
      "layer   3  Sparsity: 76.4348%\n",
      "total_backward_count 303490 real_backward_count 41276  13.600%\n",
      "fc layer 2 self.abs_max_out: 1514.0\n",
      "fc layer 1 self.abs_max_out: 2999.0\n",
      "fc layer 1 self.abs_max_out: 3098.0\n",
      "lif layer 1 self.abs_max_v: 5390.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.662862/  1.854252, val:  58.33%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0371%\n",
      "layer   2  Sparsity: 72.8543%\n",
      "layer   3  Sparsity: 76.9817%\n",
      "total_backward_count 313280 real_backward_count 42458  13.553%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.643961/  1.844194, val:  56.25%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0564%\n",
      "layer   2  Sparsity: 73.3797%\n",
      "layer   3  Sparsity: 76.7009%\n",
      "total_backward_count 323070 real_backward_count 43654  13.512%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.638835/  1.812036, val:  63.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   2  Sparsity: 72.7863%\n",
      "layer   3  Sparsity: 75.7011%\n",
      "total_backward_count 332860 real_backward_count 44872  13.481%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.634075/  1.833697, val:  68.75%, val_best:  68.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 73.1678%\n",
      "layer   3  Sparsity: 77.1180%\n",
      "total_backward_count 342650 real_backward_count 46014  13.429%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.661874/  1.859795, val:  56.25%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.85 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   2  Sparsity: 72.9459%\n",
      "layer   3  Sparsity: 77.6144%\n",
      "total_backward_count 352440 real_backward_count 47213  13.396%\n",
      "lif layer 1 self.abs_max_v: 5465.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.680122/  1.862042, val:  62.08%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.95 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   2  Sparsity: 72.8088%\n",
      "layer   3  Sparsity: 77.9132%\n",
      "total_backward_count 362230 real_backward_count 48417  13.366%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.654458/  1.862801, val:  60.42%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 73.1152%\n",
      "layer   3  Sparsity: 77.6937%\n",
      "total_backward_count 372020 real_backward_count 49576  13.326%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.667905/  1.825755, val:  60.00%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.09 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 73.0075%\n",
      "layer   3  Sparsity: 77.2965%\n",
      "total_backward_count 381810 real_backward_count 50719  13.284%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.639205/  1.813772, val:  67.50%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 73.2900%\n",
      "layer   3  Sparsity: 76.6104%\n",
      "total_backward_count 391600 real_backward_count 51833  13.236%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.642967/  1.844943, val:  63.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   2  Sparsity: 73.2371%\n",
      "layer   3  Sparsity: 77.5798%\n",
      "total_backward_count 401390 real_backward_count 52914  13.183%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.631389/  1.807626, val:  57.50%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 72.8228%\n",
      "layer   3  Sparsity: 77.0713%\n",
      "total_backward_count 411180 real_backward_count 53963  13.124%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.617712/  1.824669, val:  64.58%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   2  Sparsity: 72.8907%\n",
      "layer   3  Sparsity: 76.9026%\n",
      "total_backward_count 420970 real_backward_count 55100  13.089%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.622887/  1.840644, val:  58.33%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   2  Sparsity: 72.5686%\n",
      "layer   3  Sparsity: 76.7070%\n",
      "total_backward_count 430760 real_backward_count 56238  13.056%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.611811/  1.822867, val:  67.50%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.43 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   2  Sparsity: 72.5086%\n",
      "layer   3  Sparsity: 77.6449%\n",
      "total_backward_count 440550 real_backward_count 57377  13.024%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.651903/  1.833081, val:  59.58%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 72.8717%\n",
      "layer   3  Sparsity: 77.7456%\n",
      "total_backward_count 450340 real_backward_count 58525  12.996%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.641296/  1.826117, val:  67.92%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 72.6308%\n",
      "layer   3  Sparsity: 77.9348%\n",
      "total_backward_count 460130 real_backward_count 59642  12.962%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.653455/  1.836912, val:  57.92%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 72.3997%\n",
      "layer   3  Sparsity: 78.8084%\n",
      "total_backward_count 469920 real_backward_count 60860  12.951%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.674410/  1.866897, val:  62.92%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 72.5575%\n",
      "layer   3  Sparsity: 78.7946%\n",
      "total_backward_count 479710 real_backward_count 62033  12.931%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.675274/  1.867460, val:  60.83%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0397%\n",
      "layer   2  Sparsity: 72.6430%\n",
      "layer   3  Sparsity: 79.1752%\n",
      "total_backward_count 489500 real_backward_count 63125  12.896%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.687739/  1.863940, val:  62.50%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 72.4301%\n",
      "layer   3  Sparsity: 78.9223%\n",
      "total_backward_count 499290 real_backward_count 64273  12.873%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.668923/  1.849550, val:  70.42%, val_best:  70.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   2  Sparsity: 72.1909%\n",
      "layer   3  Sparsity: 79.0185%\n",
      "total_backward_count 509080 real_backward_count 65445  12.856%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.678876/  1.872202, val:  57.92%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   2  Sparsity: 72.7149%\n",
      "layer   3  Sparsity: 79.4541%\n",
      "total_backward_count 518870 real_backward_count 66527  12.822%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.680365/  1.835233, val:  67.08%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   2  Sparsity: 72.1584%\n",
      "layer   3  Sparsity: 78.5979%\n",
      "total_backward_count 528660 real_backward_count 67583  12.784%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.665535/  1.859927, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1164%\n",
      "layer   2  Sparsity: 72.6605%\n",
      "layer   3  Sparsity: 78.7497%\n",
      "total_backward_count 538450 real_backward_count 68679  12.755%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.648996/  1.805213, val:  53.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   2  Sparsity: 72.1692%\n",
      "layer   3  Sparsity: 77.4761%\n",
      "total_backward_count 548240 real_backward_count 69739  12.721%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.616007/  1.828683, val:  66.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.65 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   2  Sparsity: 71.9427%\n",
      "layer   3  Sparsity: 77.3313%\n",
      "total_backward_count 558030 real_backward_count 70820  12.691%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.639967/  1.833156, val:  67.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1071%\n",
      "layer   2  Sparsity: 71.9665%\n",
      "layer   3  Sparsity: 77.6809%\n",
      "total_backward_count 567820 real_backward_count 71922  12.666%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.633756/  1.806612, val:  73.75%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   2  Sparsity: 71.8310%\n",
      "layer   3  Sparsity: 77.8470%\n",
      "total_backward_count 577610 real_backward_count 73009  12.640%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.660174/  1.812896, val:  72.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0447%\n",
      "layer   2  Sparsity: 71.6018%\n",
      "layer   3  Sparsity: 78.4112%\n",
      "total_backward_count 587400 real_backward_count 74069  12.610%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.645020/  1.829842, val:  62.08%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   2  Sparsity: 72.1307%\n",
      "layer   3  Sparsity: 78.6820%\n",
      "total_backward_count 597190 real_backward_count 75196  12.592%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.634856/  1.809462, val:  66.25%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.06 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   2  Sparsity: 71.8741%\n",
      "layer   3  Sparsity: 77.6304%\n",
      "total_backward_count 606980 real_backward_count 76295  12.570%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.633811/  1.806622, val:  67.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.78 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0412%\n",
      "layer   2  Sparsity: 71.3154%\n",
      "layer   3  Sparsity: 77.0649%\n",
      "total_backward_count 616770 real_backward_count 77394  12.548%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.632684/  1.856559, val:  70.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.53 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0401%\n",
      "layer   2  Sparsity: 71.6443%\n",
      "layer   3  Sparsity: 78.6397%\n",
      "total_backward_count 626560 real_backward_count 78472  12.524%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.639205/  1.864016, val:  47.08%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   2  Sparsity: 71.8568%\n",
      "layer   3  Sparsity: 78.8577%\n",
      "total_backward_count 636350 real_backward_count 79584  12.506%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.673623/  1.836046, val:  70.00%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   2  Sparsity: 71.7008%\n",
      "layer   3  Sparsity: 79.2513%\n",
      "total_backward_count 646140 real_backward_count 80667  12.484%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.657512/  1.845549, val:  77.50%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 71.6027%\n",
      "layer   3  Sparsity: 79.6477%\n",
      "total_backward_count 655930 real_backward_count 81819  12.474%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.675538/  1.846485, val:  68.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 71.6799%\n",
      "layer   3  Sparsity: 79.2974%\n",
      "total_backward_count 665720 real_backward_count 82955  12.461%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.664080/  1.844209, val:  69.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 71.8681%\n",
      "layer   3  Sparsity: 79.0991%\n",
      "total_backward_count 675510 real_backward_count 83984  12.433%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.648239/  1.816793, val:  73.75%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   2  Sparsity: 71.7194%\n",
      "layer   3  Sparsity: 79.1975%\n",
      "total_backward_count 685300 real_backward_count 85056  12.411%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.654364/  1.841882, val:  60.00%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0315%\n",
      "layer   2  Sparsity: 72.1520%\n",
      "layer   3  Sparsity: 79.2871%\n",
      "total_backward_count 695090 real_backward_count 86097  12.386%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.666629/  1.805527, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.14 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 71.9426%\n",
      "layer   3  Sparsity: 79.0260%\n",
      "total_backward_count 704880 real_backward_count 87123  12.360%\n",
      "fc layer 1 self.abs_max_out: 3124.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.659011/  1.839533, val:  60.83%, val_best:  77.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0614%\n",
      "layer   2  Sparsity: 72.0452%\n",
      "layer   3  Sparsity: 79.6507%\n",
      "total_backward_count 714670 real_backward_count 88190  12.340%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.664583/  1.850625, val:  76.25%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0492%\n",
      "layer   2  Sparsity: 71.8923%\n",
      "layer   3  Sparsity: 79.0447%\n",
      "total_backward_count 724460 real_backward_count 89217  12.315%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.680275/  1.801288, val:  72.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.12 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 72.1612%\n",
      "layer   3  Sparsity: 79.5947%\n",
      "total_backward_count 734250 real_backward_count 90338  12.303%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.662548/  1.869321, val:  65.00%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   2  Sparsity: 72.2663%\n",
      "layer   3  Sparsity: 80.1901%\n",
      "total_backward_count 744040 real_backward_count 91432  12.289%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.645326/  1.852470, val:  54.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   2  Sparsity: 72.2002%\n",
      "layer   3  Sparsity: 78.9042%\n",
      "total_backward_count 753830 real_backward_count 92480  12.268%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.665837/  1.865805, val:  67.92%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 72.1743%\n",
      "layer   3  Sparsity: 79.2822%\n",
      "total_backward_count 763620 real_backward_count 93522  12.247%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.658452/  1.805946, val:  65.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   2  Sparsity: 71.8257%\n",
      "layer   3  Sparsity: 78.9087%\n",
      "total_backward_count 773410 real_backward_count 94591  12.230%\n",
      "fc layer 2 self.abs_max_out: 1537.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.638399/  1.804156, val:  73.33%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   2  Sparsity: 71.8697%\n",
      "layer   3  Sparsity: 78.3633%\n",
      "total_backward_count 783200 real_backward_count 95580  12.204%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.634162/  1.840867, val:  59.17%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 72.1810%\n",
      "layer   3  Sparsity: 78.6406%\n",
      "total_backward_count 792990 real_backward_count 96592  12.181%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.640841/  1.860623, val:  55.42%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 71.7626%\n",
      "layer   3  Sparsity: 78.3699%\n",
      "total_backward_count 802780 real_backward_count 97645  12.163%\n",
      "fc layer 2 self.abs_max_out: 1613.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.644219/  1.861804, val:  50.00%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.86 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   2  Sparsity: 71.7944%\n",
      "layer   3  Sparsity: 79.1277%\n",
      "total_backward_count 812570 real_backward_count 98722  12.149%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.640406/  1.868809, val:  61.67%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   2  Sparsity: 71.7169%\n",
      "layer   3  Sparsity: 79.4133%\n",
      "total_backward_count 822360 real_backward_count 99728  12.127%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.666773/  1.830575, val:  68.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.45 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0564%\n",
      "layer   2  Sparsity: 71.5442%\n",
      "layer   3  Sparsity: 78.9836%\n",
      "total_backward_count 832150 real_backward_count 100780  12.111%\n",
      "lif layer 2 self.abs_max_v: 2052.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.649132/  1.803878, val:  73.33%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.88 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 71.8142%\n",
      "layer   3  Sparsity: 79.0108%\n",
      "total_backward_count 841940 real_backward_count 101784  12.089%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.666155/  1.832543, val:  77.08%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 71.8091%\n",
      "layer   3  Sparsity: 79.7161%\n",
      "total_backward_count 851730 real_backward_count 102822  12.072%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.670481/  1.837261, val:  69.58%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   2  Sparsity: 71.1672%\n",
      "layer   3  Sparsity: 80.0971%\n",
      "total_backward_count 861520 real_backward_count 103846  12.054%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.699127/  1.871812, val:  64.17%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.65 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1163%\n",
      "layer   2  Sparsity: 71.9228%\n",
      "layer   3  Sparsity: 80.9349%\n",
      "total_backward_count 871310 real_backward_count 104838  12.032%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.675943/  1.847941, val:  60.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 71.7098%\n",
      "layer   3  Sparsity: 79.8022%\n",
      "total_backward_count 881100 real_backward_count 105800  12.008%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.651638/  1.860087, val:  67.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.09 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   2  Sparsity: 71.6599%\n",
      "layer   3  Sparsity: 80.1582%\n",
      "total_backward_count 890890 real_backward_count 106793  11.987%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.672906/  1.840062, val:  77.08%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 71.3710%\n",
      "layer   3  Sparsity: 80.3034%\n",
      "total_backward_count 900680 real_backward_count 107841  11.973%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.693614/  1.842066, val:  79.58%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.47 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 71.5742%\n",
      "layer   3  Sparsity: 80.0109%\n",
      "total_backward_count 910470 real_backward_count 108875  11.958%\n",
      "lif layer 1 self.abs_max_v: 5526.0\n",
      "fc layer 1 self.abs_max_out: 3160.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.662254/  1.822876, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.47 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   2  Sparsity: 71.7877%\n",
      "layer   3  Sparsity: 79.4343%\n",
      "total_backward_count 920260 real_backward_count 109868  11.939%\n",
      "lif layer 1 self.abs_max_v: 5595.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.679543/  1.852998, val:  65.42%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 71.9186%\n",
      "layer   3  Sparsity: 80.4224%\n",
      "total_backward_count 930050 real_backward_count 110939  11.928%\n",
      "fc layer 1 self.abs_max_out: 3186.0\n",
      "lif layer 1 self.abs_max_v: 5826.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.688278/  1.866157, val:  58.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 71.2357%\n",
      "layer   3  Sparsity: 79.9139%\n",
      "total_backward_count 939840 real_backward_count 111940  11.911%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.701964/  1.853621, val:  65.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   2  Sparsity: 72.0969%\n",
      "layer   3  Sparsity: 80.2142%\n",
      "total_backward_count 949630 real_backward_count 112963  11.895%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.687149/  1.844779, val:  70.00%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   2  Sparsity: 71.9639%\n",
      "layer   3  Sparsity: 80.3522%\n",
      "total_backward_count 959420 real_backward_count 113958  11.878%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.699835/  1.875613, val:  67.08%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.12 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   2  Sparsity: 71.6715%\n",
      "layer   3  Sparsity: 79.6765%\n",
      "total_backward_count 969210 real_backward_count 114997  11.865%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.711953/  1.883087, val:  71.67%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 71.5532%\n",
      "layer   3  Sparsity: 80.6409%\n",
      "total_backward_count 979000 real_backward_count 116065  11.855%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.707103/  1.857442, val:  65.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   2  Sparsity: 71.5580%\n",
      "layer   3  Sparsity: 80.0215%\n",
      "total_backward_count 988790 real_backward_count 117079  11.841%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.679698/  1.857885, val:  73.75%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 71.7476%\n",
      "layer   3  Sparsity: 80.7376%\n",
      "total_backward_count 998580 real_backward_count 118074  11.824%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.683834/  1.862908, val:  63.75%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   2  Sparsity: 71.6331%\n",
      "layer   3  Sparsity: 79.9338%\n",
      "total_backward_count 1008370 real_backward_count 119067  11.808%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.676811/  1.855115, val:  59.17%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 71.4094%\n",
      "layer   3  Sparsity: 79.2827%\n",
      "total_backward_count 1018160 real_backward_count 120083  11.794%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.676442/  1.844378, val:  67.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   2  Sparsity: 71.3525%\n",
      "layer   3  Sparsity: 79.8829%\n",
      "total_backward_count 1027950 real_backward_count 121105  11.781%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.685148/  1.871563, val:  57.08%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.94 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 71.6078%\n",
      "layer   3  Sparsity: 80.5233%\n",
      "total_backward_count 1037740 real_backward_count 122062  11.762%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.686112/  1.800789, val:  84.17%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   2  Sparsity: 71.6947%\n",
      "layer   3  Sparsity: 79.7407%\n",
      "total_backward_count 1047530 real_backward_count 123082  11.750%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.656134/  1.851424, val:  80.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.21 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 71.6746%\n",
      "layer   3  Sparsity: 80.3561%\n",
      "total_backward_count 1057320 real_backward_count 124055  11.733%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.699409/  1.934351, val:  61.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.85 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1071%\n",
      "layer   2  Sparsity: 71.5670%\n",
      "layer   3  Sparsity: 81.3259%\n",
      "total_backward_count 1067110 real_backward_count 125086  11.722%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.724134/  1.883906, val:  70.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   2  Sparsity: 71.5909%\n",
      "layer   3  Sparsity: 80.7846%\n",
      "total_backward_count 1076900 real_backward_count 126119  11.711%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.750855/  1.885130, val:  74.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.57 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0504%\n",
      "layer   2  Sparsity: 71.6893%\n",
      "layer   3  Sparsity: 81.4998%\n",
      "total_backward_count 1086690 real_backward_count 127160  11.702%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.730849/  1.876190, val:  79.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 71.6896%\n",
      "layer   3  Sparsity: 81.3330%\n",
      "total_backward_count 1096480 real_backward_count 128233  11.695%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.749250/  1.891015, val:  75.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   2  Sparsity: 71.9386%\n",
      "layer   3  Sparsity: 82.1727%\n",
      "total_backward_count 1106270 real_backward_count 129220  11.681%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.716650/  1.878887, val:  69.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 71.7230%\n",
      "layer   3  Sparsity: 80.8986%\n",
      "total_backward_count 1116060 real_backward_count 130246  11.670%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.708216/  1.889945, val:  66.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.56 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   2  Sparsity: 71.6600%\n",
      "layer   3  Sparsity: 80.4016%\n",
      "total_backward_count 1125850 real_backward_count 131225  11.656%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.720613/  1.866305, val:  51.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 71.1812%\n",
      "layer   3  Sparsity: 79.5408%\n",
      "total_backward_count 1135640 real_backward_count 132256  11.646%\n",
      "fc layer 1 self.abs_max_out: 3224.0\n",
      "fc layer 1 self.abs_max_out: 3357.0\n",
      "lif layer 1 self.abs_max_v: 6098.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.715398/  1.897632, val:  62.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.95 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 71.4422%\n",
      "layer   3  Sparsity: 81.2890%\n",
      "total_backward_count 1145430 real_backward_count 133261  11.634%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.740602/  1.852388, val:  73.75%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 71.7207%\n",
      "layer   3  Sparsity: 81.2000%\n",
      "total_backward_count 1155220 real_backward_count 134258  11.622%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.716446/  1.859298, val:  77.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.13 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 71.7575%\n",
      "layer   3  Sparsity: 80.8498%\n",
      "total_backward_count 1165010 real_backward_count 135250  11.609%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.743458/  1.897032, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.09 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0614%\n",
      "layer   2  Sparsity: 71.6611%\n",
      "layer   3  Sparsity: 81.4851%\n",
      "total_backward_count 1174800 real_backward_count 136267  11.599%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.736359/  1.879391, val:  72.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 71.7434%\n",
      "layer   3  Sparsity: 80.9235%\n",
      "total_backward_count 1184590 real_backward_count 137259  11.587%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.727703/  1.891464, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.24 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 71.8052%\n",
      "layer   3  Sparsity: 81.4764%\n",
      "total_backward_count 1194380 real_backward_count 138216  11.572%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.733303/  1.865906, val:  70.83%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 84.68 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 71.7422%\n",
      "layer   3  Sparsity: 80.7322%\n",
      "total_backward_count 1204170 real_backward_count 139211  11.561%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.737582/  1.862564, val:  70.83%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.31 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   2  Sparsity: 71.5678%\n",
      "layer   3  Sparsity: 81.0947%\n",
      "total_backward_count 1213960 real_backward_count 140242  11.552%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.716213/  1.887339, val:  64.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.67 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   2  Sparsity: 71.4134%\n",
      "layer   3  Sparsity: 80.9619%\n",
      "total_backward_count 1223750 real_backward_count 141225  11.540%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.703413/  1.897357, val:  70.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 71.7398%\n",
      "layer   3  Sparsity: 80.9910%\n",
      "total_backward_count 1233540 real_backward_count 142181  11.526%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.726285/  1.918758, val:  62.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   2  Sparsity: 71.7173%\n",
      "layer   3  Sparsity: 81.3432%\n",
      "total_backward_count 1243330 real_backward_count 143161  11.514%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.752486/  1.903173, val:  75.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0375%\n",
      "layer   2  Sparsity: 71.6627%\n",
      "layer   3  Sparsity: 81.4635%\n",
      "total_backward_count 1253120 real_backward_count 144180  11.506%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.720694/  1.841484, val:  68.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 71.9848%\n",
      "layer   3  Sparsity: 80.6851%\n",
      "total_backward_count 1262910 real_backward_count 145199  11.497%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.718134/  1.925035, val:  64.17%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 71.4427%\n",
      "layer   3  Sparsity: 80.5978%\n",
      "total_backward_count 1272700 real_backward_count 146179  11.486%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.721866/  1.869800, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   2  Sparsity: 71.9313%\n",
      "layer   3  Sparsity: 81.5618%\n",
      "total_backward_count 1282490 real_backward_count 147140  11.473%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.732397/  1.910576, val:  75.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 71.8776%\n",
      "layer   3  Sparsity: 81.8441%\n",
      "total_backward_count 1292280 real_backward_count 148085  11.459%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.738330/  1.906598, val:  54.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.98 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 71.6261%\n",
      "layer   3  Sparsity: 81.1883%\n",
      "total_backward_count 1302070 real_backward_count 149110  11.452%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.739816/  1.896104, val:  65.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 71.4870%\n",
      "layer   3  Sparsity: 81.3987%\n",
      "total_backward_count 1311860 real_backward_count 150071  11.440%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.733046/  1.916057, val:  58.33%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 85.49 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 71.8220%\n",
      "layer   3  Sparsity: 81.5212%\n",
      "total_backward_count 1321650 real_backward_count 151056  11.429%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.746575/  1.898049, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   2  Sparsity: 71.8901%\n",
      "layer   3  Sparsity: 81.7356%\n",
      "total_backward_count 1331440 real_backward_count 152033  11.419%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.744947/  1.870552, val:  80.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 71.8068%\n",
      "layer   3  Sparsity: 81.6953%\n",
      "total_backward_count 1341230 real_backward_count 153057  11.412%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.733128/  1.917118, val:  65.83%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.81 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 71.8778%\n",
      "layer   3  Sparsity: 81.4608%\n",
      "total_backward_count 1351020 real_backward_count 154057  11.403%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.743430/  1.923993, val:  75.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.67 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   2  Sparsity: 71.6078%\n",
      "layer   3  Sparsity: 80.3338%\n",
      "total_backward_count 1360810 real_backward_count 155044  11.394%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.766832/  1.902670, val:  74.58%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   2  Sparsity: 71.6469%\n",
      "layer   3  Sparsity: 81.4860%\n",
      "total_backward_count 1370600 real_backward_count 156107  11.390%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.753061/  1.920593, val:  66.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0340%\n",
      "layer   2  Sparsity: 71.6712%\n",
      "layer   3  Sparsity: 81.6576%\n",
      "total_backward_count 1380390 real_backward_count 157096  11.381%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.754980/  1.910036, val:  72.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.59 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 71.6758%\n",
      "layer   3  Sparsity: 81.8378%\n",
      "total_backward_count 1390180 real_backward_count 158082  11.371%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.752448/  1.902158, val:  76.67%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.68 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 71.5956%\n",
      "layer   3  Sparsity: 82.0780%\n",
      "total_backward_count 1399970 real_backward_count 159110  11.365%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.759167/  1.916687, val:  66.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.19 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 71.8034%\n",
      "layer   3  Sparsity: 82.5871%\n",
      "total_backward_count 1409760 real_backward_count 160119  11.358%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.784712/  1.960594, val:  67.92%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.33 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 71.6838%\n",
      "layer   3  Sparsity: 83.5132%\n",
      "total_backward_count 1419550 real_backward_count 161173  11.354%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.767215/  1.900655, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0487%\n",
      "layer   2  Sparsity: 71.2705%\n",
      "layer   3  Sparsity: 82.3391%\n",
      "total_backward_count 1429340 real_backward_count 162156  11.345%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.763358/  1.933372, val:  75.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   2  Sparsity: 71.4432%\n",
      "layer   3  Sparsity: 82.8425%\n",
      "total_backward_count 1439130 real_backward_count 163158  11.337%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.776162/  1.904824, val:  74.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 71.7501%\n",
      "layer   3  Sparsity: 82.5631%\n",
      "total_backward_count 1448920 real_backward_count 164141  11.329%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.766894/  1.900164, val:  81.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 71.4225%\n",
      "layer   3  Sparsity: 82.5168%\n",
      "total_backward_count 1458710 real_backward_count 165127  11.320%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.775851/  1.893090, val:  76.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1325%\n",
      "layer   2  Sparsity: 71.3209%\n",
      "layer   3  Sparsity: 82.7988%\n",
      "total_backward_count 1468500 real_backward_count 166103  11.311%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.779431/  1.940134, val:  64.58%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 71.2349%\n",
      "layer   3  Sparsity: 82.9927%\n",
      "total_backward_count 1478290 real_backward_count 167120  11.305%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.778401/  1.888416, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 71.4724%\n",
      "layer   3  Sparsity: 82.4234%\n",
      "total_backward_count 1488080 real_backward_count 168104  11.297%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.759939/  1.932337, val:  54.17%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.90 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   2  Sparsity: 71.3802%\n",
      "layer   3  Sparsity: 82.3675%\n",
      "total_backward_count 1497870 real_backward_count 169156  11.293%\n",
      "lif layer 2 self.abs_max_v: 2099.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.787756/  1.939241, val:  68.75%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0329%\n",
      "layer   2  Sparsity: 71.8964%\n",
      "layer   3  Sparsity: 83.3828%\n",
      "total_backward_count 1507660 real_backward_count 170163  11.287%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.789374/  1.907835, val:  73.33%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.88 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0387%\n",
      "layer   2  Sparsity: 71.8079%\n",
      "layer   3  Sparsity: 82.3630%\n",
      "total_backward_count 1517450 real_backward_count 171180  11.281%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.784992/  1.917749, val:  77.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.96 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   2  Sparsity: 71.4016%\n",
      "layer   3  Sparsity: 83.1801%\n",
      "total_backward_count 1527240 real_backward_count 172166  11.273%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.789691/  1.925830, val:  65.42%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   2  Sparsity: 71.7936%\n",
      "layer   3  Sparsity: 83.0154%\n",
      "total_backward_count 1537030 real_backward_count 173210  11.269%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.780711/  1.926467, val:  65.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.05 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   2  Sparsity: 71.4548%\n",
      "layer   3  Sparsity: 82.6595%\n",
      "total_backward_count 1546820 real_backward_count 174192  11.261%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.778679/  1.915349, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   2  Sparsity: 71.6107%\n",
      "layer   3  Sparsity: 82.6224%\n",
      "total_backward_count 1556610 real_backward_count 175156  11.252%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.771745/  1.931002, val:  82.08%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1207%\n",
      "layer   2  Sparsity: 71.6645%\n",
      "layer   3  Sparsity: 82.3008%\n",
      "total_backward_count 1566400 real_backward_count 176087  11.242%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.783069/  1.939481, val:  76.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   2  Sparsity: 71.0749%\n",
      "layer   3  Sparsity: 82.4293%\n",
      "total_backward_count 1576190 real_backward_count 177057  11.233%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.796708/  1.951680, val:  71.25%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 71.2126%\n",
      "layer   3  Sparsity: 82.8550%\n",
      "total_backward_count 1585980 real_backward_count 178097  11.229%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.793071/  1.889809, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 70.8767%\n",
      "layer   3  Sparsity: 82.1731%\n",
      "total_backward_count 1595770 real_backward_count 179066  11.221%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.771261/  1.912622, val:  74.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.90 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 71.1650%\n",
      "layer   3  Sparsity: 82.1567%\n",
      "total_backward_count 1605560 real_backward_count 180018  11.212%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.772020/  1.952319, val:  61.25%, val_best:  84.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   2  Sparsity: 71.1802%\n",
      "layer   3  Sparsity: 82.8581%\n",
      "total_backward_count 1615350 real_backward_count 181001  11.205%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.798258/  1.922905, val:  71.67%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.60 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   2  Sparsity: 71.3322%\n",
      "layer   3  Sparsity: 82.9951%\n",
      "total_backward_count 1625140 real_backward_count 182004  11.199%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.775141/  1.935451, val:  54.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 71.4407%\n",
      "layer   3  Sparsity: 82.6448%\n",
      "total_backward_count 1634930 real_backward_count 182976  11.192%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.780441/  1.941762, val:  71.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.37 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 71.8399%\n",
      "layer   3  Sparsity: 82.8149%\n",
      "total_backward_count 1644720 real_backward_count 183930  11.183%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.793662/  1.919578, val:  68.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.32 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 71.2151%\n",
      "layer   3  Sparsity: 82.7491%\n",
      "total_backward_count 1654510 real_backward_count 184871  11.174%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.775908/  1.898828, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 71.3322%\n",
      "layer   3  Sparsity: 82.8006%\n",
      "total_backward_count 1664300 real_backward_count 185853  11.167%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.791329/  1.934335, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.11 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1043%\n",
      "layer   2  Sparsity: 71.7858%\n",
      "layer   3  Sparsity: 83.4771%\n",
      "total_backward_count 1674090 real_backward_count 186837  11.161%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.771713/  1.907924, val:  69.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 71.5817%\n",
      "layer   3  Sparsity: 83.2344%\n",
      "total_backward_count 1683880 real_backward_count 187716  11.148%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.773519/  1.890511, val:  79.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 70.8735%\n",
      "layer   3  Sparsity: 82.1685%\n",
      "total_backward_count 1693670 real_backward_count 188698  11.141%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.769463/  1.931290, val:  59.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 71.4219%\n",
      "layer   3  Sparsity: 81.9527%\n",
      "total_backward_count 1703460 real_backward_count 189689  11.136%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.784013/  1.914779, val:  78.75%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.54 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0356%\n",
      "layer   2  Sparsity: 71.4192%\n",
      "layer   3  Sparsity: 82.6787%\n",
      "total_backward_count 1713250 real_backward_count 190692  11.130%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.779472/  1.915220, val:  78.75%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.50 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0770%\n",
      "layer   2  Sparsity: 71.5705%\n",
      "layer   3  Sparsity: 83.0349%\n",
      "total_backward_count 1723040 real_backward_count 191609  11.120%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.787417/  1.952172, val:  62.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.72 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1011%\n",
      "layer   2  Sparsity: 71.7893%\n",
      "layer   3  Sparsity: 83.1729%\n",
      "total_backward_count 1732830 real_backward_count 192570  11.113%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.801015/  1.929611, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1156%\n",
      "layer   2  Sparsity: 71.4369%\n",
      "layer   3  Sparsity: 84.0936%\n",
      "total_backward_count 1742620 real_backward_count 193552  11.107%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.800972/  1.963370, val:  69.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 71.6311%\n",
      "layer   3  Sparsity: 83.6100%\n",
      "total_backward_count 1752410 real_backward_count 194495  11.099%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.822861/  1.980426, val:  60.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.57 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   2  Sparsity: 71.4331%\n",
      "layer   3  Sparsity: 83.8473%\n",
      "total_backward_count 1762200 real_backward_count 195506  11.094%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.818035/  1.939016, val:  75.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 82.48 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.1144%\n",
      "layer   2  Sparsity: 71.4733%\n",
      "layer   3  Sparsity: 83.6433%\n",
      "total_backward_count 1771990 real_backward_count 196425  11.085%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.790147/  1.938678, val:  70.42%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.45 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   2  Sparsity: 71.5573%\n",
      "layer   3  Sparsity: 83.0480%\n",
      "total_backward_count 1781780 real_backward_count 197346  11.076%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.819124/  1.942582, val:  76.67%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.38 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   2  Sparsity: 71.4769%\n",
      "layer   3  Sparsity: 83.8258%\n",
      "total_backward_count 1791570 real_backward_count 198274  11.067%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.819604/  1.925394, val:  75.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.90 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 71.2994%\n",
      "layer   3  Sparsity: 83.1868%\n",
      "total_backward_count 1801360 real_backward_count 199228  11.060%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.809072/  1.939670, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 71.2456%\n",
      "layer   3  Sparsity: 83.7340%\n",
      "total_backward_count 1811150 real_backward_count 200219  11.055%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.828907/  1.968164, val:  65.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.50 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   2  Sparsity: 71.2530%\n",
      "layer   3  Sparsity: 83.6236%\n",
      "total_backward_count 1820940 real_backward_count 201214  11.050%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.815925/  1.966019, val:  54.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.69 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   2  Sparsity: 71.7177%\n",
      "layer   3  Sparsity: 83.7523%\n",
      "total_backward_count 1830730 real_backward_count 202147  11.042%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.816238/  1.942473, val:  55.83%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.08 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 71.4850%\n",
      "layer   3  Sparsity: 83.6357%\n",
      "total_backward_count 1840520 real_backward_count 203089  11.034%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.793921/  1.965605, val:  77.08%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   2  Sparsity: 71.5852%\n",
      "layer   3  Sparsity: 84.0867%\n",
      "total_backward_count 1850310 real_backward_count 203999  11.025%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.820573/  2.007975, val:  65.42%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.97 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 71.3877%\n",
      "layer   3  Sparsity: 85.0627%\n",
      "total_backward_count 1860100 real_backward_count 204994  11.021%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.825849/  1.935728, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.50 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0449%\n",
      "layer   2  Sparsity: 71.3091%\n",
      "layer   3  Sparsity: 83.8059%\n",
      "total_backward_count 1869890 real_backward_count 205933  11.013%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.814783/  1.955894, val:  63.75%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 84.87 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 71.2182%\n",
      "layer   3  Sparsity: 83.5042%\n",
      "total_backward_count 1879680 real_backward_count 206919  11.008%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.816646/  1.976149, val:  65.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 71.4906%\n",
      "layer   3  Sparsity: 83.9404%\n",
      "total_backward_count 1889470 real_backward_count 207865  11.001%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.811624/  1.915142, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.37 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0414%\n",
      "layer   2  Sparsity: 71.2305%\n",
      "layer   3  Sparsity: 83.0570%\n",
      "total_backward_count 1899260 real_backward_count 208807  10.994%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.812348/  1.957249, val:  73.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   2  Sparsity: 71.4595%\n",
      "layer   3  Sparsity: 83.4918%\n",
      "total_backward_count 1909050 real_backward_count 209778  10.989%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.822490/  1.970530, val:  70.83%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.14 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0289%\n",
      "layer   2  Sparsity: 71.5462%\n",
      "layer   3  Sparsity: 83.4365%\n",
      "total_backward_count 1918840 real_backward_count 210760  10.984%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.791502/  1.947448, val:  67.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.15 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   2  Sparsity: 71.8433%\n",
      "layer   3  Sparsity: 82.5862%\n",
      "total_backward_count 1928630 real_backward_count 211733  10.978%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.790278/  1.928634, val:  72.92%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1057%\n",
      "layer   2  Sparsity: 71.9877%\n",
      "layer   3  Sparsity: 82.8045%\n",
      "total_backward_count 1938420 real_backward_count 212690  10.972%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.806590/  1.949719, val:  56.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 71.3595%\n",
      "layer   3  Sparsity: 83.4591%\n",
      "total_backward_count 1948210 real_backward_count 213714  10.970%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.789239/  1.942876, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1062%\n",
      "layer   2  Sparsity: 71.8445%\n",
      "layer   3  Sparsity: 82.7236%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edf966e86c24319be2c58c4418aeda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÉ‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÖ‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÉ‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.78924</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.7625</td></tr><tr><td>val_loss</td><td>1.94288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fc10w5a' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fc10w5a</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251120_045950-7fc10w5a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ti944tyk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 9623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_094500-ti944tyk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ti944tyk' target=\"_blank\">solar-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ti944tyk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ti944tyk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251120_094510_639', 'my_seed': 9623, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [16, 16, 16], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 16\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 16 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 16\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 16 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 16\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 16 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[16, 16, 16], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[16, 16, 16], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[16, 16, 16], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 228.0\n",
      "lif layer 1 self.abs_max_v: 228.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 199.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 1 self.abs_max_out: 237.0\n",
      "lif layer 1 self.abs_max_v: 248.5\n",
      "fc layer 2 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 255.5\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "lif layer 1 self.abs_max_v: 285.5\n",
      "fc layer 2 self.abs_max_out: 274.0\n",
      "lif layer 2 self.abs_max_v: 336.5\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "lif layer 2 self.abs_max_v: 380.0\n",
      "lif layer 1 self.abs_max_v: 344.0\n",
      "fc layer 2 self.abs_max_out: 357.0\n",
      "fc layer 1 self.abs_max_out: 260.0\n",
      "fc layer 1 self.abs_max_out: 308.0\n",
      "lif layer 1 self.abs_max_v: 361.5\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "lif layer 2 self.abs_max_v: 455.5\n",
      "fc layer 1 self.abs_max_out: 335.0\n",
      "lif layer 1 self.abs_max_v: 413.5\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "lif layer 1 self.abs_max_v: 481.0\n",
      "fc layer 1 self.abs_max_out: 486.0\n",
      "lif layer 1 self.abs_max_v: 486.0\n",
      "lif layer 2 self.abs_max_v: 486.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "lif layer 2 self.abs_max_v: 487.5\n",
      "fc layer 1 self.abs_max_out: 709.0\n",
      "lif layer 1 self.abs_max_v: 709.0\n",
      "lif layer 2 self.abs_max_v: 499.0\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "lif layer 2 self.abs_max_v: 701.5\n",
      "lif layer 2 self.abs_max_v: 749.0\n",
      "fc layer 2 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "fc layer 2 self.abs_max_out: 523.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "lif layer 2 self.abs_max_v: 770.5\n",
      "lif layer 2 self.abs_max_v: 846.5\n",
      "fc layer 2 self.abs_max_out: 630.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "fc layer 1 self.abs_max_out: 750.0\n",
      "lif layer 1 self.abs_max_v: 750.0\n",
      "lif layer 2 self.abs_max_v: 891.5\n",
      "lif layer 2 self.abs_max_v: 895.0\n",
      "lif layer 1 self.abs_max_v: 786.0\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "fc layer 1 self.abs_max_out: 794.0\n",
      "lif layer 1 self.abs_max_v: 794.0\n",
      "fc layer 1 self.abs_max_out: 931.0\n",
      "lif layer 1 self.abs_max_v: 931.0\n",
      "lif layer 2 self.abs_max_v: 901.5\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "fc layer 2 self.abs_max_out: 678.0\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "fc layer 2 self.abs_max_out: 711.0\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "fc layer 2 self.abs_max_out: 745.0\n",
      "fc layer 2 self.abs_max_out: 800.0\n",
      "lif layer 2 self.abs_max_v: 1043.5\n",
      "lif layer 2 self.abs_max_v: 1051.0\n",
      "lif layer 2 self.abs_max_v: 1094.5\n",
      "lif layer 1 self.abs_max_v: 941.5\n",
      "lif layer 1 self.abs_max_v: 979.0\n",
      "fc layer 1 self.abs_max_out: 936.0\n",
      "lif layer 1 self.abs_max_v: 1028.0\n",
      "fc layer 1 self.abs_max_out: 943.0\n",
      "fc layer 1 self.abs_max_out: 989.0\n",
      "lif layer 2 self.abs_max_v: 1128.0\n",
      "lif layer 1 self.abs_max_v: 1031.5\n",
      "lif layer 1 self.abs_max_v: 1067.0\n",
      "lif layer 1 self.abs_max_v: 1115.5\n",
      "lif layer 1 self.abs_max_v: 1212.0\n",
      "lif layer 1 self.abs_max_v: 1290.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "lif layer 2 self.abs_max_v: 1129.0\n",
      "fc layer 2 self.abs_max_out: 847.0\n",
      "fc layer 1 self.abs_max_out: 1013.0\n",
      "fc layer 2 self.abs_max_out: 862.0\n",
      "fc layer 1 self.abs_max_out: 1030.0\n",
      "fc layer 1 self.abs_max_out: 1048.0\n",
      "lif layer 1 self.abs_max_v: 1307.0\n",
      "lif layer 1 self.abs_max_v: 1445.5\n",
      "fc layer 1 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "fc layer 2 self.abs_max_out: 890.0\n",
      "fc layer 2 self.abs_max_out: 891.0\n",
      "fc layer 2 self.abs_max_out: 922.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "lif layer 1 self.abs_max_v: 1592.5\n",
      "fc layer 1 self.abs_max_out: 1141.0\n",
      "lif layer 2 self.abs_max_v: 1156.0\n",
      "fc layer 2 self.abs_max_out: 993.0\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "lif layer 2 self.abs_max_v: 1192.5\n",
      "lif layer 2 self.abs_max_v: 1217.0\n",
      "fc layer 1 self.abs_max_out: 1231.0\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "fc layer 1 self.abs_max_out: 1238.0\n",
      "lif layer 2 self.abs_max_v: 1239.5\n",
      "fc layer 2 self.abs_max_out: 1075.0\n",
      "lif layer 2 self.abs_max_v: 1323.0\n",
      "fc layer 1 self.abs_max_out: 1241.0\n",
      "fc layer 1 self.abs_max_out: 1286.0\n",
      "fc layer 1 self.abs_max_out: 1304.0\n",
      "fc layer 1 self.abs_max_out: 1467.0\n",
      "lif layer 1 self.abs_max_v: 1656.0\n",
      "lif layer 1 self.abs_max_v: 1738.5\n",
      "lif layer 1 self.abs_max_v: 1829.5\n",
      "fc layer 2 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 1949.0\n",
      "lif layer 1 self.abs_max_v: 2088.5\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "fc layer 2 self.abs_max_out: 1167.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "lif layer 2 self.abs_max_v: 1423.0\n",
      "lif layer 1 self.abs_max_v: 2229.0\n",
      "lif layer 2 self.abs_max_v: 1447.0\n",
      "fc layer 2 self.abs_max_out: 1209.0\n",
      "fc layer 1 self.abs_max_out: 1572.0\n",
      "lif layer 1 self.abs_max_v: 2472.0\n",
      "lif layer 1 self.abs_max_v: 2629.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.774768/  1.997780, val:  38.33%, val_best:  38.33%, tr:  96.73%, tr_best:  96.73%, epoch time: 85.79 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   2  Sparsity: 77.2498%\n",
      "layer   3  Sparsity: 72.1103%\n",
      "total_backward_count 9790 real_backward_count 2287  23.361%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "fc layer 2 self.abs_max_out: 1365.0\n",
      "fc layer 1 self.abs_max_out: 1769.0\n",
      "lif layer 2 self.abs_max_v: 1451.0\n",
      "lif layer 2 self.abs_max_v: 1452.5\n",
      "fc layer 1 self.abs_max_out: 1956.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "lif layer 2 self.abs_max_v: 1579.5\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "lif layer 2 self.abs_max_v: 1580.5\n",
      "lif layer 2 self.abs_max_v: 1604.0\n",
      "lif layer 2 self.abs_max_v: 1605.5\n",
      "lif layer 2 self.abs_max_v: 1656.0\n",
      "lif layer 2 self.abs_max_v: 1734.0\n",
      "lif layer 2 self.abs_max_v: 1742.5\n",
      "lif layer 1 self.abs_max_v: 3062.5\n",
      "lif layer 1 self.abs_max_v: 3369.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.676383/  1.900136, val:  48.75%, val_best:  48.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0414%\n",
      "layer   2  Sparsity: 75.5903%\n",
      "layer   3  Sparsity: 68.8652%\n",
      "total_backward_count 19580 real_backward_count 3963  20.240%\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 1750.5\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "lif layer 2 self.abs_max_v: 1755.0\n",
      "lif layer 2 self.abs_max_v: 1839.5\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "lif layer 2 self.abs_max_v: 1887.0\n",
      "lif layer 2 self.abs_max_v: 2085.5\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.589012/  1.893481, val:  42.50%, val_best:  48.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 85.58 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 75.2816%\n",
      "layer   3  Sparsity: 65.5317%\n",
      "total_backward_count 29370 real_backward_count 5418  18.447%\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "fc layer 1 self.abs_max_out: 2116.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "lif layer 1 self.abs_max_v: 3454.0\n",
      "lif layer 1 self.abs_max_v: 3780.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.540109/  1.835398, val:  41.67%, val_best:  48.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 73.4505%\n",
      "layer   3  Sparsity: 63.7523%\n",
      "total_backward_count 39160 real_backward_count 6810  17.390%\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "fc layer 3 self.abs_max_out: 436.0\n",
      "lif layer 2 self.abs_max_v: 2110.0\n",
      "lif layer 2 self.abs_max_v: 2205.5\n",
      "fc layer 1 self.abs_max_out: 2320.0\n",
      "lif layer 1 self.abs_max_v: 4022.0\n",
      "lif layer 1 self.abs_max_v: 4211.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.514940/  1.832107, val:  43.75%, val_best:  48.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 85.03 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 73.9527%\n",
      "layer   3  Sparsity: 64.1498%\n",
      "total_backward_count 48950 real_backward_count 8128  16.605%\n",
      "lif layer 2 self.abs_max_v: 2228.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "lif layer 2 self.abs_max_v: 2275.5\n",
      "lif layer 2 self.abs_max_v: 2353.5\n",
      "lif layer 2 self.abs_max_v: 2361.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.505499/  1.756368, val:  54.17%, val_best:  54.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   2  Sparsity: 74.4825%\n",
      "layer   3  Sparsity: 63.9886%\n",
      "total_backward_count 58740 real_backward_count 9452  16.091%\n",
      "lif layer 2 self.abs_max_v: 2378.5\n",
      "fc layer 1 self.abs_max_out: 2408.0\n",
      "lif layer 2 self.abs_max_v: 2407.0\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.487332/  1.735583, val:  54.17%, val_best:  54.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.71 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   2  Sparsity: 75.0990%\n",
      "layer   3  Sparsity: 63.3015%\n",
      "total_backward_count 68530 real_backward_count 10662  15.558%\n",
      "lif layer 2 self.abs_max_v: 2498.5\n",
      "lif layer 2 self.abs_max_v: 2508.5\n",
      "lif layer 2 self.abs_max_v: 2509.5\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "fc layer 1 self.abs_max_out: 2494.0\n",
      "fc layer 1 self.abs_max_out: 2521.0\n",
      "lif layer 1 self.abs_max_v: 4228.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.471652/  1.728848, val:  57.08%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.39 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 74.3469%\n",
      "layer   3  Sparsity: 63.9696%\n",
      "total_backward_count 78320 real_backward_count 11879  15.167%\n",
      "lif layer 2 self.abs_max_v: 2630.5\n",
      "fc layer 1 self.abs_max_out: 2697.0\n",
      "fc layer 1 self.abs_max_out: 2699.0\n",
      "lif layer 1 self.abs_max_v: 4496.0\n",
      "lif layer 1 self.abs_max_v: 4598.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.465303/  1.744985, val:  48.75%, val_best:  57.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.66 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   2  Sparsity: 74.1804%\n",
      "layer   3  Sparsity: 64.6264%\n",
      "total_backward_count 88110 real_backward_count 13069  14.833%\n",
      "lif layer 1 self.abs_max_v: 4690.5\n",
      "lif layer 1 self.abs_max_v: 4737.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.449574/  1.738512, val:  48.75%, val_best:  57.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.81 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   2  Sparsity: 74.4291%\n",
      "layer   3  Sparsity: 64.6286%\n",
      "total_backward_count 97900 real_backward_count 14280  14.586%\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "fc layer 1 self.abs_max_out: 2756.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.412284/  1.712336, val:  50.00%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.42 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 74.5420%\n",
      "layer   3  Sparsity: 64.6105%\n",
      "total_backward_count 107690 real_backward_count 15474  14.369%\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "lif layer 1 self.abs_max_v: 4773.5\n",
      "fc layer 1 self.abs_max_out: 2843.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.420359/  1.691028, val:  52.92%, val_best:  57.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   2  Sparsity: 73.5758%\n",
      "layer   3  Sparsity: 64.8444%\n",
      "total_backward_count 117480 real_backward_count 16646  14.169%\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "fc layer 1 self.abs_max_out: 2946.0\n",
      "fc layer 1 self.abs_max_out: 3050.0\n",
      "lif layer 1 self.abs_max_v: 5032.5\n",
      "lif layer 1 self.abs_max_v: 5234.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.383431/  1.702207, val:  52.08%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.29 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 73.5571%\n",
      "layer   3  Sparsity: 65.4632%\n",
      "total_backward_count 127270 real_backward_count 17785  13.974%\n",
      "fc layer 2 self.abs_max_out: 1862.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.415094/  1.654593, val:  52.92%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 72.9260%\n",
      "layer   3  Sparsity: 64.2910%\n",
      "total_backward_count 137060 real_backward_count 18945  13.822%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.365286/  1.650764, val:  55.42%, val_best:  57.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 72.9034%\n",
      "layer   3  Sparsity: 65.0751%\n",
      "total_backward_count 146850 real_backward_count 20072  13.668%\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.379394/  1.667423, val:  57.08%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.03 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   2  Sparsity: 72.8585%\n",
      "layer   3  Sparsity: 65.4635%\n",
      "total_backward_count 156640 real_backward_count 21199  13.534%\n",
      "lif layer 2 self.abs_max_v: 2718.5\n",
      "lif layer 2 self.abs_max_v: 2724.5\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "lif layer 1 self.abs_max_v: 5249.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.373466/  1.629057, val:  58.33%, val_best:  58.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.88 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   2  Sparsity: 72.7070%\n",
      "layer   3  Sparsity: 65.8575%\n",
      "total_backward_count 166430 real_backward_count 22304  13.401%\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "lif layer 2 self.abs_max_v: 2745.0\n",
      "lif layer 2 self.abs_max_v: 2779.5\n",
      "lif layer 2 self.abs_max_v: 2781.0\n",
      "lif layer 2 self.abs_max_v: 2815.5\n",
      "lif layer 2 self.abs_max_v: 2832.5\n",
      "lif layer 2 self.abs_max_v: 2841.5\n",
      "lif layer 2 self.abs_max_v: 2848.5\n",
      "lif layer 2 self.abs_max_v: 2871.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.329046/  1.602172, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.78 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   2  Sparsity: 72.5248%\n",
      "layer   3  Sparsity: 65.5126%\n",
      "total_backward_count 176220 real_backward_count 23399  13.278%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.324740/  1.606714, val:  58.33%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   2  Sparsity: 72.2692%\n",
      "layer   3  Sparsity: 65.2086%\n",
      "total_backward_count 186010 real_backward_count 24511  13.177%\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "fc layer 1 self.abs_max_out: 3163.0\n",
      "lif layer 1 self.abs_max_v: 5296.0\n",
      "lif layer 1 self.abs_max_v: 5391.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.318184/  1.588240, val:  68.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   2  Sparsity: 72.6222%\n",
      "layer   3  Sparsity: 65.1357%\n",
      "total_backward_count 195800 real_backward_count 25555  13.052%\n",
      "fc layer 1 self.abs_max_out: 3291.0\n",
      "lif layer 1 self.abs_max_v: 5464.0\n",
      "lif layer 1 self.abs_max_v: 5606.0\n",
      "lif layer 2 self.abs_max_v: 2910.5\n",
      "lif layer 2 self.abs_max_v: 2947.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.336535/  1.586118, val:  59.58%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.04 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1086%\n",
      "layer   2  Sparsity: 72.5589%\n",
      "layer   3  Sparsity: 65.0554%\n",
      "total_backward_count 205590 real_backward_count 26612  12.944%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.283276/  1.552575, val:  68.33%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 84.61 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   2  Sparsity: 72.3998%\n",
      "layer   3  Sparsity: 64.5185%\n",
      "total_backward_count 215380 real_backward_count 27628  12.828%\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.282979/  1.606508, val:  45.00%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.10 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 72.1914%\n",
      "layer   3  Sparsity: 64.3877%\n",
      "total_backward_count 225170 real_backward_count 28634  12.717%\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.267321/  1.596162, val:  51.67%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.62 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0507%\n",
      "layer   2  Sparsity: 71.7492%\n",
      "layer   3  Sparsity: 64.8322%\n",
      "total_backward_count 234960 real_backward_count 29652  12.620%\n",
      "lif layer 2 self.abs_max_v: 2953.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.265427/  1.556158, val:  57.50%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.36 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 71.9563%\n",
      "layer   3  Sparsity: 65.2109%\n",
      "total_backward_count 244750 real_backward_count 30644  12.521%\n",
      "fc layer 2 self.abs_max_out: 1888.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.245568/  1.547544, val:  54.58%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.10 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 71.8575%\n",
      "layer   3  Sparsity: 65.2640%\n",
      "total_backward_count 254540 real_backward_count 31641  12.431%\n",
      "fc layer 3 self.abs_max_out: 632.0\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "fc layer 1 self.abs_max_out: 3315.0\n",
      "lif layer 1 self.abs_max_v: 5715.0\n",
      "lif layer 1 self.abs_max_v: 6037.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.256617/  1.529094, val:  70.00%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.56 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 71.5752%\n",
      "layer   3  Sparsity: 64.7398%\n",
      "total_backward_count 264330 real_backward_count 32631  12.345%\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 2 self.abs_max_out: 1929.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.234430/  1.572466, val:  51.25%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.80 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 71.2640%\n",
      "layer   3  Sparsity: 64.4994%\n",
      "total_backward_count 274120 real_backward_count 33626  12.267%\n",
      "lif layer 2 self.abs_max_v: 3002.0\n",
      "lif layer 2 self.abs_max_v: 3159.5\n",
      "lif layer 2 self.abs_max_v: 3178.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.231384/  1.548504, val:  66.67%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.63 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 71.6138%\n",
      "layer   3  Sparsity: 65.2388%\n",
      "total_backward_count 283910 real_backward_count 34588  12.183%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.220842/  1.462093, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.48 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   2  Sparsity: 71.5781%\n",
      "layer   3  Sparsity: 65.4514%\n",
      "total_backward_count 293700 real_backward_count 35521  12.094%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.208998/  1.520752, val:  61.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.16 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 71.7276%\n",
      "layer   3  Sparsity: 65.3010%\n",
      "total_backward_count 303490 real_backward_count 36464  12.015%\n",
      "fc layer 2 self.abs_max_out: 1959.0\n",
      "fc layer 1 self.abs_max_out: 3326.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.194313/  1.527037, val:  73.33%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 85.07 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   2  Sparsity: 71.2501%\n",
      "layer   3  Sparsity: 64.5329%\n",
      "total_backward_count 313280 real_backward_count 37320  11.913%\n",
      "fc layer 3 self.abs_max_out: 701.0\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "fc layer 1 self.abs_max_out: 3431.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.199398/  1.520071, val:  57.50%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0541%\n",
      "layer   2  Sparsity: 70.9554%\n",
      "layer   3  Sparsity: 64.2599%\n",
      "total_backward_count 323070 real_backward_count 38245  11.838%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.188700/  1.493888, val:  54.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.53 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 70.9135%\n",
      "layer   3  Sparsity: 63.9052%\n",
      "total_backward_count 332860 real_backward_count 39103  11.748%\n",
      "fc layer 3 self.abs_max_out: 727.0\n",
      "fc layer 1 self.abs_max_out: 3495.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.182025/  1.477720, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.30 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.1345%\n",
      "layer   2  Sparsity: 70.8804%\n",
      "layer   3  Sparsity: 64.7808%\n",
      "total_backward_count 342650 real_backward_count 39920  11.650%\n",
      "fc layer 1 self.abs_max_out: 3497.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.193875/  1.453500, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   2  Sparsity: 71.1332%\n",
      "layer   3  Sparsity: 65.4730%\n",
      "total_backward_count 352440 real_backward_count 40729  11.556%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.178874/  1.510675, val:  59.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.26 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   2  Sparsity: 70.9657%\n",
      "layer   3  Sparsity: 65.4787%\n",
      "total_backward_count 362230 real_backward_count 41573  11.477%\n",
      "fc layer 1 self.abs_max_out: 3499.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.177428/  1.474241, val:  67.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   2  Sparsity: 71.1186%\n",
      "layer   3  Sparsity: 64.9025%\n",
      "total_backward_count 372020 real_backward_count 42425  11.404%\n",
      "fc layer 1 self.abs_max_out: 3595.0\n",
      "lif layer 1 self.abs_max_v: 6135.5\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.179966/  1.464628, val:  67.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 71.0976%\n",
      "layer   3  Sparsity: 65.4989%\n",
      "total_backward_count 381810 real_backward_count 43202  11.315%\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 1 self.abs_max_out: 3653.0\n",
      "lif layer 1 self.abs_max_v: 6266.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.166572/  1.438313, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 71.0922%\n",
      "layer   3  Sparsity: 65.6055%\n",
      "total_backward_count 391600 real_backward_count 43977  11.230%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.133277/  1.396049, val:  75.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.47 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 70.6434%\n",
      "layer   3  Sparsity: 65.6256%\n",
      "total_backward_count 401390 real_backward_count 44747  11.148%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.131251/  1.455256, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.11 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   2  Sparsity: 71.2119%\n",
      "layer   3  Sparsity: 65.7480%\n",
      "total_backward_count 411180 real_backward_count 45558  11.080%\n",
      "fc layer 1 self.abs_max_out: 4124.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.156179/  1.423043, val:  72.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.91 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0406%\n",
      "layer   2  Sparsity: 70.9654%\n",
      "layer   3  Sparsity: 64.8952%\n",
      "total_backward_count 420970 real_backward_count 46339  11.008%\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.131179/  1.398712, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.17 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1188%\n",
      "layer   2  Sparsity: 70.7079%\n",
      "layer   3  Sparsity: 64.9282%\n",
      "total_backward_count 430760 real_backward_count 47112  10.937%\n",
      "fc layer 2 self.abs_max_out: 2148.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.124315/  1.395349, val:  73.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.84 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   2  Sparsity: 70.5894%\n",
      "layer   3  Sparsity: 65.0325%\n",
      "total_backward_count 440550 real_backward_count 47871  10.866%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.113050/  1.446974, val:  67.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.09 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 70.7629%\n",
      "layer   3  Sparsity: 65.2826%\n",
      "total_backward_count 450340 real_backward_count 48622  10.797%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "fc layer 3 self.abs_max_out: 757.0\n",
      "fc layer 3 self.abs_max_out: 759.0\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.119546/  1.363214, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.33 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 70.7649%\n",
      "layer   3  Sparsity: 65.1465%\n",
      "total_backward_count 460130 real_backward_count 49399  10.736%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.104946/  1.381982, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.98 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0382%\n",
      "layer   2  Sparsity: 70.2963%\n",
      "layer   3  Sparsity: 64.9254%\n",
      "total_backward_count 469920 real_backward_count 50123  10.666%\n",
      "lif layer 1 self.abs_max_v: 6285.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.097209/  1.375341, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 70.4150%\n",
      "layer   3  Sparsity: 64.7415%\n",
      "total_backward_count 479710 real_backward_count 50843  10.599%\n",
      "lif layer 1 self.abs_max_v: 6474.5\n",
      "fc layer 2 self.abs_max_out: 2177.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.107457/  1.356999, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.34 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   2  Sparsity: 70.7688%\n",
      "layer   3  Sparsity: 64.7311%\n",
      "total_backward_count 489500 real_backward_count 51556  10.532%\n",
      "fc layer 2 self.abs_max_out: 2181.0\n",
      "fc layer 2 self.abs_max_out: 2214.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.094934/  1.381292, val:  70.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.92 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 70.8404%\n",
      "layer   3  Sparsity: 64.3824%\n",
      "total_backward_count 499290 real_backward_count 52204  10.456%\n",
      "fc layer 2 self.abs_max_out: 2247.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.086029/  1.378476, val:  75.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 70.9620%\n",
      "layer   3  Sparsity: 64.2676%\n",
      "total_backward_count 509080 real_backward_count 52850  10.381%\n",
      "fc layer 2 self.abs_max_out: 2256.0\n",
      "fc layer 2 self.abs_max_out: 2296.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.084344/  1.328513, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.71 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1177%\n",
      "layer   2  Sparsity: 70.5242%\n",
      "layer   3  Sparsity: 64.6027%\n",
      "total_backward_count 518870 real_backward_count 53521  10.315%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.059351/  1.398074, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.63 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 70.4953%\n",
      "layer   3  Sparsity: 64.8420%\n",
      "total_backward_count 528660 real_backward_count 54150  10.243%\n",
      "fc layer 1 self.abs_max_out: 4179.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.056822/  1.361071, val:  75.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.84 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0491%\n",
      "layer   2  Sparsity: 70.6226%\n",
      "layer   3  Sparsity: 65.5948%\n",
      "total_backward_count 538450 real_backward_count 54822  10.181%\n",
      "lif layer 2 self.abs_max_v: 3222.5\n",
      "lif layer 2 self.abs_max_v: 3229.5\n",
      "lif layer 2 self.abs_max_v: 3245.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.048770/  1.358187, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0328%\n",
      "layer   2  Sparsity: 70.3817%\n",
      "layer   3  Sparsity: 65.0743%\n",
      "total_backward_count 548240 real_backward_count 55507  10.125%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.058182/  1.344398, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 70.2189%\n",
      "layer   3  Sparsity: 65.5550%\n",
      "total_backward_count 558030 real_backward_count 56193  10.070%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.052941/  1.322729, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 70.2445%\n",
      "layer   3  Sparsity: 65.0539%\n",
      "total_backward_count 567820 real_backward_count 56835  10.009%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.058845/  1.364822, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.08 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0176%\n",
      "layer   2  Sparsity: 70.0499%\n",
      "layer   3  Sparsity: 64.8030%\n",
      "total_backward_count 577610 real_backward_count 57476   9.951%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.074926/  1.351021, val:  78.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 70.4412%\n",
      "layer   3  Sparsity: 65.1231%\n",
      "total_backward_count 587400 real_backward_count 58119   9.894%\n",
      "lif layer 2 self.abs_max_v: 3310.5\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.050733/  1.338025, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 70.4774%\n",
      "layer   3  Sparsity: 64.9922%\n",
      "total_backward_count 597190 real_backward_count 58771   9.841%\n",
      "lif layer 1 self.abs_max_v: 6481.5\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.039066/  1.321730, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.42 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0483%\n",
      "layer   2  Sparsity: 70.1007%\n",
      "layer   3  Sparsity: 64.7842%\n",
      "total_backward_count 606980 real_backward_count 59406   9.787%\n",
      "lif layer 2 self.abs_max_v: 3472.5\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.037794/  1.351636, val:  67.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.93 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0212%\n",
      "layer   2  Sparsity: 69.6606%\n",
      "layer   3  Sparsity: 64.9976%\n",
      "total_backward_count 616770 real_backward_count 60022   9.732%\n",
      "fc layer 2 self.abs_max_out: 2325.0\n",
      "fc layer 2 self.abs_max_out: 2340.0\n",
      "fc layer 3 self.abs_max_out: 765.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.039093/  1.310533, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   2  Sparsity: 69.7928%\n",
      "layer   3  Sparsity: 64.7405%\n",
      "total_backward_count 626560 real_backward_count 60618   9.675%\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.028896/  1.306937, val:  78.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   2  Sparsity: 69.8280%\n",
      "layer   3  Sparsity: 65.1299%\n",
      "total_backward_count 636350 real_backward_count 61199   9.617%\n",
      "fc layer 1 self.abs_max_out: 4291.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.037324/  1.306637, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 69.6528%\n",
      "layer   3  Sparsity: 65.1305%\n",
      "total_backward_count 646140 real_backward_count 61815   9.567%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.032784/  1.351822, val:  69.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.55 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 69.7269%\n",
      "layer   3  Sparsity: 65.2466%\n",
      "total_backward_count 655930 real_backward_count 62377   9.510%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.023415/  1.301818, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.81 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 69.4954%\n",
      "layer   3  Sparsity: 65.0262%\n",
      "total_backward_count 665720 real_backward_count 62931   9.453%\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.011952/  1.277250, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.60 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 69.5157%\n",
      "layer   3  Sparsity: 64.9047%\n",
      "total_backward_count 675510 real_backward_count 63534   9.405%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  0.988845/  1.255150, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.73 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 70.0064%\n",
      "layer   3  Sparsity: 65.3941%\n",
      "total_backward_count 685300 real_backward_count 64087   9.352%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.007555/  1.285242, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.83 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 69.8176%\n",
      "layer   3  Sparsity: 65.0144%\n",
      "total_backward_count 695090 real_backward_count 64690   9.307%\n",
      "lif layer 2 self.abs_max_v: 3536.0\n",
      "lif layer 2 self.abs_max_v: 3681.0\n",
      "fc layer 2 self.abs_max_out: 2422.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  0.986699/  1.312529, val:  69.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 69.5931%\n",
      "layer   3  Sparsity: 64.8899%\n",
      "total_backward_count 704880 real_backward_count 65222   9.253%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  0.991384/  1.241590, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.13 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1401%\n",
      "layer   2  Sparsity: 69.8378%\n",
      "layer   3  Sparsity: 64.3591%\n",
      "total_backward_count 714670 real_backward_count 65779   9.204%\n",
      "lif layer 2 self.abs_max_v: 3742.5\n",
      "fc layer 2 self.abs_max_out: 2434.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  0.983918/  1.236802, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.26 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   2  Sparsity: 70.0765%\n",
      "layer   3  Sparsity: 64.1966%\n",
      "total_backward_count 724460 real_backward_count 66324   9.155%\n",
      "lif layer 2 self.abs_max_v: 3778.5\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  0.978284/  1.269070, val:  72.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.04 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 69.8678%\n",
      "layer   3  Sparsity: 64.7129%\n",
      "total_backward_count 734250 real_backward_count 66855   9.105%\n",
      "lif layer 1 self.abs_max_v: 6574.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  0.968741/  1.253261, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.74 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 69.9779%\n",
      "layer   3  Sparsity: 64.7511%\n",
      "total_backward_count 744040 real_backward_count 67373   9.055%\n",
      "lif layer 1 self.abs_max_v: 6591.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  0.952145/  1.249158, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.45 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 70.1651%\n",
      "layer   3  Sparsity: 65.0027%\n",
      "total_backward_count 753830 real_backward_count 67894   9.007%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  0.957602/  1.242738, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 70.2456%\n",
      "layer   3  Sparsity: 65.4110%\n",
      "total_backward_count 763620 real_backward_count 68382   8.955%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  0.968832/  1.225644, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.81 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   2  Sparsity: 69.8564%\n",
      "layer   3  Sparsity: 65.4345%\n",
      "total_backward_count 773410 real_backward_count 68874   8.905%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  0.953103/  1.243557, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1140%\n",
      "layer   2  Sparsity: 69.9622%\n",
      "layer   3  Sparsity: 65.5486%\n",
      "total_backward_count 783200 real_backward_count 69409   8.862%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  0.967549/  1.300533, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.93 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 70.2032%\n",
      "layer   3  Sparsity: 65.3870%\n",
      "total_backward_count 792990 real_backward_count 69950   8.821%\n",
      "lif layer 2 self.abs_max_v: 3907.5\n",
      "lif layer 1 self.abs_max_v: 6631.5\n",
      "fc layer 2 self.abs_max_out: 2439.0\n",
      "fc layer 2 self.abs_max_out: 2450.0\n",
      "fc layer 2 self.abs_max_out: 2462.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  0.978654/  1.277345, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.94 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 70.1443%\n",
      "layer   3  Sparsity: 65.3776%\n",
      "total_backward_count 802780 real_backward_count 70473   8.779%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  0.981020/  1.281239, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0481%\n",
      "layer   2  Sparsity: 69.8272%\n",
      "layer   3  Sparsity: 65.8542%\n",
      "total_backward_count 812570 real_backward_count 70961   8.733%\n",
      "lif layer 1 self.abs_max_v: 6691.0\n",
      "lif layer 1 self.abs_max_v: 6733.5\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  0.977635/  1.267565, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.65 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 70.0343%\n",
      "layer   3  Sparsity: 65.8867%\n",
      "total_backward_count 822360 real_backward_count 71467   8.690%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  0.982229/  1.242891, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   2  Sparsity: 70.0227%\n",
      "layer   3  Sparsity: 66.4044%\n",
      "total_backward_count 832150 real_backward_count 71914   8.642%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  0.978120/  1.250048, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0369%\n",
      "layer   2  Sparsity: 69.8911%\n",
      "layer   3  Sparsity: 66.5115%\n",
      "total_backward_count 841940 real_backward_count 72387   8.598%\n",
      "fc layer 1 self.abs_max_out: 4311.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  0.973293/  1.274102, val:  82.50%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   2  Sparsity: 69.8810%\n",
      "layer   3  Sparsity: 66.2744%\n",
      "total_backward_count 851730 real_backward_count 72896   8.559%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  0.966089/  1.233356, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.10 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 70.1593%\n",
      "layer   3  Sparsity: 66.2719%\n",
      "total_backward_count 861520 real_backward_count 73362   8.515%\n",
      "lif layer 1 self.abs_max_v: 6776.5\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  0.953254/  1.251044, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 69.9646%\n",
      "layer   3  Sparsity: 66.4261%\n",
      "total_backward_count 871310 real_backward_count 73823   8.473%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  0.958680/  1.240029, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.19 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1361%\n",
      "layer   2  Sparsity: 69.9846%\n",
      "layer   3  Sparsity: 66.3531%\n",
      "total_backward_count 881100 real_backward_count 74266   8.429%\n",
      "lif layer 1 self.abs_max_v: 6803.5\n",
      "lif layer 1 self.abs_max_v: 6933.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  0.953909/  1.227718, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 69.8570%\n",
      "layer   3  Sparsity: 66.1886%\n",
      "total_backward_count 890890 real_backward_count 74658   8.380%\n",
      "lif layer 1 self.abs_max_v: 7102.5\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  0.944889/  1.234418, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 69.7787%\n",
      "layer   3  Sparsity: 66.3580%\n",
      "total_backward_count 900680 real_backward_count 75127   8.341%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  0.948802/  1.249115, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 69.8095%\n",
      "layer   3  Sparsity: 65.9649%\n",
      "total_backward_count 910470 real_backward_count 75587   8.302%\n",
      "fc layer 1 self.abs_max_out: 4381.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  0.960248/  1.242869, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 70.1804%\n",
      "layer   3  Sparsity: 66.2234%\n",
      "total_backward_count 920260 real_backward_count 76021   8.261%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  0.962172/  1.258737, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.69 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 70.2280%\n",
      "layer   3  Sparsity: 66.0663%\n",
      "total_backward_count 930050 real_backward_count 76465   8.222%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  0.962408/  1.230235, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.75 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 70.0834%\n",
      "layer   3  Sparsity: 66.5274%\n",
      "total_backward_count 939840 real_backward_count 76931   8.186%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  0.958766/  1.246025, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 69.8203%\n",
      "layer   3  Sparsity: 66.4263%\n",
      "total_backward_count 949630 real_backward_count 77366   8.147%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  0.947485/  1.257982, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.10 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 69.8182%\n",
      "layer   3  Sparsity: 66.2790%\n",
      "total_backward_count 959420 real_backward_count 77795   8.109%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  0.939196/  1.244488, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.41 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 69.6769%\n",
      "layer   3  Sparsity: 66.4237%\n",
      "total_backward_count 969210 real_backward_count 78232   8.072%\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "fc layer 2 self.abs_max_out: 2483.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  0.939408/  1.224681, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1102%\n",
      "layer   2  Sparsity: 69.8203%\n",
      "layer   3  Sparsity: 66.3330%\n",
      "total_backward_count 979000 real_backward_count 78680   8.037%\n",
      "fc layer 1 self.abs_max_out: 4426.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  0.947885/  1.223096, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.23 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   2  Sparsity: 69.6601%\n",
      "layer   3  Sparsity: 66.3258%\n",
      "total_backward_count 988790 real_backward_count 79085   7.998%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  0.933805/  1.245502, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.84 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1137%\n",
      "layer   2  Sparsity: 69.6727%\n",
      "layer   3  Sparsity: 66.1608%\n",
      "total_backward_count 998580 real_backward_count 79488   7.960%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  0.928368/  1.218808, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 69.9737%\n",
      "layer   3  Sparsity: 65.9160%\n",
      "total_backward_count 1008370 real_backward_count 79907   7.924%\n",
      "fc layer 1 self.abs_max_out: 4473.0\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  0.936369/  1.218768, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 69.7197%\n",
      "layer   3  Sparsity: 65.9710%\n",
      "total_backward_count 1018160 real_backward_count 80345   7.891%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  0.945962/  1.229316, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.81 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   2  Sparsity: 69.5096%\n",
      "layer   3  Sparsity: 65.4105%\n",
      "total_backward_count 1027950 real_backward_count 80729   7.853%\n",
      "lif layer 2 self.abs_max_v: 4064.5\n",
      "lif layer 2 self.abs_max_v: 4071.5\n",
      "lif layer 2 self.abs_max_v: 4106.0\n",
      "lif layer 1 self.abs_max_v: 7124.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  0.941901/  1.233566, val:  79.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0420%\n",
      "layer   2  Sparsity: 69.4574%\n",
      "layer   3  Sparsity: 65.9889%\n",
      "total_backward_count 1037740 real_backward_count 81119   7.817%\n",
      "lif layer 1 self.abs_max_v: 7266.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  0.945356/  1.237487, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.91 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 69.6949%\n",
      "layer   3  Sparsity: 65.9212%\n",
      "total_backward_count 1047530 real_backward_count 81531   7.783%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  0.945566/  1.227161, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 69.8842%\n",
      "layer   3  Sparsity: 66.1651%\n",
      "total_backward_count 1057320 real_backward_count 81923   7.748%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  0.942013/  1.225926, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.33 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   2  Sparsity: 69.8158%\n",
      "layer   3  Sparsity: 66.0330%\n",
      "total_backward_count 1067110 real_backward_count 82336   7.716%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  0.934958/  1.242094, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.77 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   2  Sparsity: 69.6632%\n",
      "layer   3  Sparsity: 66.2673%\n",
      "total_backward_count 1076900 real_backward_count 82710   7.680%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  0.911479/  1.215874, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 69.4081%\n",
      "layer   3  Sparsity: 66.3623%\n",
      "total_backward_count 1086690 real_backward_count 83103   7.647%\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 870.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  0.915617/  1.203353, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.18 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 69.5923%\n",
      "layer   3  Sparsity: 66.0467%\n",
      "total_backward_count 1096480 real_backward_count 83486   7.614%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  0.905320/  1.228580, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.03 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   2  Sparsity: 69.5645%\n",
      "layer   3  Sparsity: 65.7982%\n",
      "total_backward_count 1106270 real_backward_count 83839   7.579%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  0.910401/  1.180765, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.0523%\n",
      "layer   2  Sparsity: 69.2918%\n",
      "layer   3  Sparsity: 65.9727%\n",
      "total_backward_count 1116060 real_backward_count 84247   7.549%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  0.899461/  1.197605, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 69.5477%\n",
      "layer   3  Sparsity: 66.3118%\n",
      "total_backward_count 1125850 real_backward_count 84653   7.519%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  0.888970/  1.192196, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.04 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0261%\n",
      "layer   2  Sparsity: 69.6309%\n",
      "layer   3  Sparsity: 65.9843%\n",
      "total_backward_count 1135640 real_backward_count 85006   7.485%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  0.890479/  1.190338, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.92 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   2  Sparsity: 69.6512%\n",
      "layer   3  Sparsity: 66.1592%\n",
      "total_backward_count 1145430 real_backward_count 85391   7.455%\n",
      "lif layer 2 self.abs_max_v: 4194.5\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  0.893699/  1.187200, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 69.5211%\n",
      "layer   3  Sparsity: 65.8416%\n",
      "total_backward_count 1155220 real_backward_count 85777   7.425%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  0.901262/  1.165583, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 69.5139%\n",
      "layer   3  Sparsity: 66.1713%\n",
      "total_backward_count 1165010 real_backward_count 86188   7.398%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  0.883528/  1.224799, val:  79.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.81 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.1145%\n",
      "layer   2  Sparsity: 69.5740%\n",
      "layer   3  Sparsity: 65.9977%\n",
      "total_backward_count 1174800 real_backward_count 86576   7.369%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  0.882926/  1.184881, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 69.5929%\n",
      "layer   3  Sparsity: 66.1470%\n",
      "total_backward_count 1184590 real_backward_count 86942   7.339%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  0.886993/  1.203898, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.18 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 69.3144%\n",
      "layer   3  Sparsity: 65.8885%\n",
      "total_backward_count 1194380 real_backward_count 87307   7.310%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  0.899043/  1.186189, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.70 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   2  Sparsity: 69.4190%\n",
      "layer   3  Sparsity: 66.0712%\n",
      "total_backward_count 1204170 real_backward_count 87679   7.281%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  0.895209/  1.197364, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.25 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   2  Sparsity: 69.4594%\n",
      "layer   3  Sparsity: 66.5633%\n",
      "total_backward_count 1213960 real_backward_count 88022   7.251%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  0.901110/  1.189525, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.48 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0359%\n",
      "layer   2  Sparsity: 69.5237%\n",
      "layer   3  Sparsity: 66.7281%\n",
      "total_backward_count 1223750 real_backward_count 88349   7.220%\n",
      "lif layer 1 self.abs_max_v: 7270.5\n",
      "fc layer 1 self.abs_max_out: 4488.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  0.901884/  1.188739, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.34 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 69.9664%\n",
      "layer   3  Sparsity: 66.8516%\n",
      "total_backward_count 1233540 real_backward_count 88699   7.191%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  0.888863/  1.171739, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 69.8634%\n",
      "layer   3  Sparsity: 66.3685%\n",
      "total_backward_count 1243330 real_backward_count 89023   7.160%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  0.871406/  1.168493, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.02 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 69.5561%\n",
      "layer   3  Sparsity: 65.9621%\n",
      "total_backward_count 1253120 real_backward_count 89377   7.132%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  0.870660/  1.162007, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 69.6298%\n",
      "layer   3  Sparsity: 65.9046%\n",
      "total_backward_count 1262910 real_backward_count 89703   7.103%\n",
      "fc layer 2 self.abs_max_out: 2507.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.872244/  1.169129, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   2  Sparsity: 69.6014%\n",
      "layer   3  Sparsity: 66.2509%\n",
      "total_backward_count 1272700 real_backward_count 90039   7.075%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.853979/  1.154069, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.28 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 69.6032%\n",
      "layer   3  Sparsity: 66.5311%\n",
      "total_backward_count 1282490 real_backward_count 90372   7.047%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.858937/  1.158228, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   2  Sparsity: 69.7391%\n",
      "layer   3  Sparsity: 66.5838%\n",
      "total_backward_count 1292280 real_backward_count 90684   7.017%\n",
      "fc layer 2 self.abs_max_out: 2546.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.858947/  1.142300, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.93 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 69.7549%\n",
      "layer   3  Sparsity: 66.6556%\n",
      "total_backward_count 1302070 real_backward_count 91023   6.991%\n",
      "fc layer 1 self.abs_max_out: 4675.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.840482/  1.217193, val:  77.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.28 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 69.7068%\n",
      "layer   3  Sparsity: 66.8738%\n",
      "total_backward_count 1311860 real_backward_count 91324   6.961%\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "lif layer 1 self.abs_max_v: 7316.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.856533/  1.167041, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 69.9225%\n",
      "layer   3  Sparsity: 66.9371%\n",
      "total_backward_count 1321650 real_backward_count 91636   6.933%\n",
      "lif layer 2 self.abs_max_v: 4323.5\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.873447/  1.160969, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.15 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   2  Sparsity: 69.8657%\n",
      "layer   3  Sparsity: 66.6515%\n",
      "total_backward_count 1331440 real_backward_count 91927   6.904%\n",
      "fc layer 2 self.abs_max_out: 2598.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.867150/  1.161916, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0317%\n",
      "layer   2  Sparsity: 69.5516%\n",
      "layer   3  Sparsity: 66.4941%\n",
      "total_backward_count 1341230 real_backward_count 92274   6.880%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.859699/  1.176524, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.12 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1212%\n",
      "layer   2  Sparsity: 69.6635%\n",
      "layer   3  Sparsity: 66.6913%\n",
      "total_backward_count 1351020 real_backward_count 92574   6.852%\n",
      "lif layer 1 self.abs_max_v: 7374.5\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.848611/  1.139216, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 69.8873%\n",
      "layer   3  Sparsity: 66.5350%\n",
      "total_backward_count 1360810 real_backward_count 92915   6.828%\n",
      "fc layer 2 self.abs_max_out: 2607.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.847311/  1.132634, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   2  Sparsity: 69.7241%\n",
      "layer   3  Sparsity: 66.9657%\n",
      "total_backward_count 1370600 real_backward_count 93201   6.800%\n",
      "lif layer 2 self.abs_max_v: 4384.0\n",
      "lif layer 2 self.abs_max_v: 4418.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.850057/  1.184744, val:  78.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 69.7379%\n",
      "layer   3  Sparsity: 67.2682%\n",
      "total_backward_count 1380390 real_backward_count 93511   6.774%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.858278/  1.174129, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   2  Sparsity: 69.6083%\n",
      "layer   3  Sparsity: 67.0957%\n",
      "total_backward_count 1390180 real_backward_count 93806   6.748%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.850293/  1.133284, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.81 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 69.8868%\n",
      "layer   3  Sparsity: 66.8879%\n",
      "total_backward_count 1399970 real_backward_count 94102   6.722%\n",
      "lif layer 1 self.abs_max_v: 7437.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.837207/  1.141924, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.58 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   2  Sparsity: 69.8635%\n",
      "layer   3  Sparsity: 67.2706%\n",
      "total_backward_count 1409760 real_backward_count 94411   6.697%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.834923/  1.179460, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.85 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   2  Sparsity: 69.8809%\n",
      "layer   3  Sparsity: 67.0461%\n",
      "total_backward_count 1419550 real_backward_count 94710   6.672%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.835401/  1.144581, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.77 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 69.5396%\n",
      "layer   3  Sparsity: 67.0863%\n",
      "total_backward_count 1429340 real_backward_count 94961   6.644%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.833897/  1.154064, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   2  Sparsity: 69.8080%\n",
      "layer   3  Sparsity: 67.2473%\n",
      "total_backward_count 1439130 real_backward_count 95216   6.616%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.840301/  1.153535, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   2  Sparsity: 69.7129%\n",
      "layer   3  Sparsity: 67.0097%\n",
      "total_backward_count 1448920 real_backward_count 95477   6.590%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.826580/  1.143033, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.73 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 69.7745%\n",
      "layer   3  Sparsity: 67.0957%\n",
      "total_backward_count 1458710 real_backward_count 95726   6.562%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.850946/  1.149846, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.86 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   2  Sparsity: 69.5123%\n",
      "layer   3  Sparsity: 67.0922%\n",
      "total_backward_count 1468500 real_backward_count 95987   6.536%\n",
      "fc layer 1 self.abs_max_out: 4704.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.831832/  1.155161, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.88 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1245%\n",
      "layer   2  Sparsity: 69.5914%\n",
      "layer   3  Sparsity: 67.6400%\n",
      "total_backward_count 1478290 real_backward_count 96238   6.510%\n",
      "lif layer 2 self.abs_max_v: 4441.5\n",
      "lif layer 1 self.abs_max_v: 7454.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.824351/  1.175011, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.46 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 69.5982%\n",
      "layer   3  Sparsity: 67.6943%\n",
      "total_backward_count 1488080 real_backward_count 96498   6.485%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.818698/  1.172521, val:  76.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   2  Sparsity: 69.4814%\n",
      "layer   3  Sparsity: 67.5628%\n",
      "total_backward_count 1497870 real_backward_count 96754   6.459%\n",
      "fc layer 1 self.abs_max_out: 4801.0\n",
      "lif layer 1 self.abs_max_v: 7509.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.825163/  1.132206, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.66 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   2  Sparsity: 69.4117%\n",
      "layer   3  Sparsity: 67.0831%\n",
      "total_backward_count 1507660 real_backward_count 97025   6.435%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.830800/  1.149673, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.07 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 69.1489%\n",
      "layer   3  Sparsity: 67.0791%\n",
      "total_backward_count 1517450 real_backward_count 97351   6.415%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.821620/  1.170663, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.31 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 69.0053%\n",
      "layer   3  Sparsity: 66.8610%\n",
      "total_backward_count 1527240 real_backward_count 97640   6.393%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.828024/  1.150804, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.89 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 69.0589%\n",
      "layer   3  Sparsity: 66.9962%\n",
      "total_backward_count 1537030 real_backward_count 97924   6.371%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.836642/  1.139311, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.30 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 69.1943%\n",
      "layer   3  Sparsity: 67.2961%\n",
      "total_backward_count 1546820 real_backward_count 98203   6.349%\n",
      "fc layer 1 self.abs_max_out: 4820.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.833686/  1.129837, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.43 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 69.2499%\n",
      "layer   3  Sparsity: 67.1886%\n",
      "total_backward_count 1556610 real_backward_count 98456   6.325%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.846450/  1.155480, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 69.2797%\n",
      "layer   3  Sparsity: 66.7532%\n",
      "total_backward_count 1566400 real_backward_count 98717   6.302%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.823259/  1.124009, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.36 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   2  Sparsity: 69.3257%\n",
      "layer   3  Sparsity: 67.0433%\n",
      "total_backward_count 1576190 real_backward_count 98964   6.279%\n",
      "lif layer 1 self.abs_max_v: 7621.5\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.813826/  1.130289, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.97 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 69.4114%\n",
      "layer   3  Sparsity: 67.4640%\n",
      "total_backward_count 1585980 real_backward_count 99222   6.256%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.821793/  1.130284, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.51 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 69.3072%\n",
      "layer   3  Sparsity: 67.2542%\n",
      "total_backward_count 1595770 real_backward_count 99501   6.235%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.821832/  1.129004, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 69.1692%\n",
      "layer   3  Sparsity: 67.1977%\n",
      "total_backward_count 1605560 real_backward_count 99727   6.211%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.824806/  1.119641, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 68.9822%\n",
      "layer   3  Sparsity: 67.6037%\n",
      "total_backward_count 1615350 real_backward_count 99994   6.190%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.821815/  1.120965, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 69.1267%\n",
      "layer   3  Sparsity: 67.6430%\n",
      "total_backward_count 1625140 real_backward_count 100229   6.167%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.827339/  1.130239, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.33 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   2  Sparsity: 69.3571%\n",
      "layer   3  Sparsity: 67.2738%\n",
      "total_backward_count 1634930 real_backward_count 100488   6.146%\n",
      "fc layer 1 self.abs_max_out: 4868.0\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.820307/  1.119436, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.45 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   2  Sparsity: 69.3507%\n",
      "layer   3  Sparsity: 67.1163%\n",
      "total_backward_count 1644720 real_backward_count 100740   6.125%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.809467/  1.136196, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.78 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 69.4883%\n",
      "layer   3  Sparsity: 67.3194%\n",
      "total_backward_count 1654510 real_backward_count 100932   6.100%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.804193/  1.166388, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   2  Sparsity: 69.4207%\n",
      "layer   3  Sparsity: 67.5495%\n",
      "total_backward_count 1664300 real_backward_count 101185   6.080%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.812413/  1.125959, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 69.2172%\n",
      "layer   3  Sparsity: 67.8686%\n",
      "total_backward_count 1674090 real_backward_count 101411   6.058%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.814366/  1.147070, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.1258%\n",
      "layer   2  Sparsity: 69.2867%\n",
      "layer   3  Sparsity: 67.7694%\n",
      "total_backward_count 1683880 real_backward_count 101661   6.037%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.808188/  1.121329, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   2  Sparsity: 69.1486%\n",
      "layer   3  Sparsity: 67.9965%\n",
      "total_backward_count 1693670 real_backward_count 101877   6.015%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.803023/  1.116969, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.34 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   2  Sparsity: 68.9457%\n",
      "layer   3  Sparsity: 68.1088%\n",
      "total_backward_count 1703460 real_backward_count 102102   5.994%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.795621/  1.116275, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.58 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.1170%\n",
      "layer   2  Sparsity: 69.0708%\n",
      "layer   3  Sparsity: 68.0207%\n",
      "total_backward_count 1713250 real_backward_count 102326   5.973%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.790076/  1.113330, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.87 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 69.0475%\n",
      "layer   3  Sparsity: 68.0936%\n",
      "total_backward_count 1723040 real_backward_count 102546   5.951%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.790165/  1.089870, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.12 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 69.0495%\n",
      "layer   3  Sparsity: 67.7137%\n",
      "total_backward_count 1732830 real_backward_count 102794   5.932%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.800980/  1.120966, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.38 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.1210%\n",
      "layer   2  Sparsity: 69.2352%\n",
      "layer   3  Sparsity: 67.5531%\n",
      "total_backward_count 1742620 real_backward_count 103065   5.914%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.801179/  1.108105, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 69.1785%\n",
      "layer   3  Sparsity: 67.7329%\n",
      "total_backward_count 1752410 real_backward_count 103303   5.895%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.794774/  1.146759, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 69.0658%\n",
      "layer   3  Sparsity: 67.8204%\n",
      "total_backward_count 1762200 real_backward_count 103506   5.874%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.799672/  1.107919, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.27 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   2  Sparsity: 69.0772%\n",
      "layer   3  Sparsity: 68.4185%\n",
      "total_backward_count 1771990 real_backward_count 103746   5.855%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.800406/  1.094491, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   2  Sparsity: 69.1793%\n",
      "layer   3  Sparsity: 68.0085%\n",
      "total_backward_count 1781780 real_backward_count 103969   5.835%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.806118/  1.109307, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.12 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 69.1439%\n",
      "layer   3  Sparsity: 67.7048%\n",
      "total_backward_count 1791570 real_backward_count 104189   5.816%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.800364/  1.134509, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.24 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 68.8936%\n",
      "layer   3  Sparsity: 67.7734%\n",
      "total_backward_count 1801360 real_backward_count 104408   5.796%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.801293/  1.102236, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 68.9833%\n",
      "layer   3  Sparsity: 68.1027%\n",
      "total_backward_count 1811150 real_backward_count 104625   5.777%\n",
      "fc layer 2 self.abs_max_out: 2642.0\n",
      "fc layer 1 self.abs_max_out: 4927.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.798007/  1.095721, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 68.9555%\n",
      "layer   3  Sparsity: 67.9592%\n",
      "total_backward_count 1820940 real_backward_count 104810   5.756%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.799930/  1.112372, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.22 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   2  Sparsity: 68.8429%\n",
      "layer   3  Sparsity: 67.8600%\n",
      "total_backward_count 1830730 real_backward_count 105044   5.738%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.798512/  1.113566, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.87 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 68.8685%\n",
      "layer   3  Sparsity: 68.1932%\n",
      "total_backward_count 1840520 real_backward_count 105251   5.719%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.789594/  1.133623, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.75 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 68.7874%\n",
      "layer   3  Sparsity: 68.0086%\n",
      "total_backward_count 1850310 real_backward_count 105472   5.700%\n",
      "fc layer 3 self.abs_max_out: 884.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.789405/  1.100595, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.39 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 69.1089%\n",
      "layer   3  Sparsity: 68.0263%\n",
      "total_backward_count 1860100 real_backward_count 105698   5.682%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.791133/  1.108694, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 69.2624%\n",
      "layer   3  Sparsity: 68.0514%\n",
      "total_backward_count 1869890 real_backward_count 105893   5.663%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.785282/  1.145583, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0342%\n",
      "layer   2  Sparsity: 69.1997%\n",
      "layer   3  Sparsity: 68.2459%\n",
      "total_backward_count 1879680 real_backward_count 106109   5.645%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.775316/  1.083458, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 69.3801%\n",
      "layer   3  Sparsity: 68.2955%\n",
      "total_backward_count 1889470 real_backward_count 106312   5.627%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.774468/  1.113046, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   2  Sparsity: 69.4395%\n",
      "layer   3  Sparsity: 67.8760%\n",
      "total_backward_count 1899260 real_backward_count 106535   5.609%\n",
      "fc layer 1 self.abs_max_out: 4940.0\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.785224/  1.098782, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1352%\n",
      "layer   2  Sparsity: 69.6318%\n",
      "layer   3  Sparsity: 67.7117%\n",
      "total_backward_count 1909050 real_backward_count 106765   5.593%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.783093/  1.090744, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   2  Sparsity: 69.7320%\n",
      "layer   3  Sparsity: 68.1502%\n",
      "total_backward_count 1918840 real_backward_count 106969   5.575%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.780179/  1.109950, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   2  Sparsity: 69.4731%\n",
      "layer   3  Sparsity: 68.0550%\n",
      "total_backward_count 1928630 real_backward_count 107166   5.557%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.790307/  1.089700, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 69.2358%\n",
      "layer   3  Sparsity: 67.7669%\n",
      "total_backward_count 1938420 real_backward_count 107366   5.539%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.792426/  1.102099, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 69.2249%\n",
      "layer   3  Sparsity: 67.4760%\n",
      "total_backward_count 1948210 real_backward_count 107586   5.522%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.788856/  1.099617, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 69.3452%\n",
      "layer   3  Sparsity: 67.7838%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b846423600e34163969c84f734d93fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.78886</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>1.09962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ti944tyk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ti944tyk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251120_094500-ti944tyk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cgpp21gv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 8411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251120_143512-cgpp21gv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgpp21gv' target=\"_blank\">noble-sweep-42</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/m019m4x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgpp21gv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgpp21gv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251120_143520_859', 'my_seed': 8411, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [14, 14, 14], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 14\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 14 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 14\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 14 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 14\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 14 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[14, 14, 14], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[14, 14, 14], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[14, 14, 14], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 305.0\n",
      "lif layer 1 self.abs_max_v: 305.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 206.0\n",
      "lif layer 2 self.abs_max_v: 206.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 407.5\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "lif layer 2 self.abs_max_v: 351.5\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "fc layer 1 self.abs_max_out: 314.0\n",
      "lif layer 1 self.abs_max_v: 497.0\n",
      "fc layer 2 self.abs_max_out: 325.0\n",
      "lif layer 2 self.abs_max_v: 379.5\n",
      "lif layer 1 self.abs_max_v: 534.5\n",
      "lif layer 2 self.abs_max_v: 466.5\n",
      "fc layer 1 self.abs_max_out: 338.0\n",
      "lif layer 1 self.abs_max_v: 605.5\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 643.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 550.5\n",
      "fc layer 1 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 602.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 1 self.abs_max_out: 411.0\n",
      "fc layer 1 self.abs_max_out: 487.0\n",
      "fc layer 1 self.abs_max_out: 543.0\n",
      "fc layer 2 self.abs_max_out: 431.0\n",
      "lif layer 2 self.abs_max_v: 718.5\n",
      "fc layer 1 self.abs_max_out: 609.0\n",
      "fc layer 1 self.abs_max_out: 612.0\n",
      "fc layer 1 self.abs_max_out: 624.0\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "fc layer 3 self.abs_max_out: 189.0\n",
      "lif layer 2 self.abs_max_v: 722.0\n",
      "lif layer 1 self.abs_max_v: 648.5\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 753.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 681.0\n",
      "lif layer 1 self.abs_max_v: 697.5\n",
      "lif layer 1 self.abs_max_v: 726.0\n",
      "fc layer 1 self.abs_max_out: 690.0\n",
      "lif layer 1 self.abs_max_v: 774.5\n",
      "lif layer 1 self.abs_max_v: 836.5\n",
      "lif layer 1 self.abs_max_v: 881.5\n",
      "fc layer 1 self.abs_max_out: 695.0\n",
      "fc layer 1 self.abs_max_out: 930.0\n",
      "lif layer 1 self.abs_max_v: 930.0\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "lif layer 1 self.abs_max_v: 1015.0\n",
      "lif layer 2 self.abs_max_v: 811.0\n",
      "lif layer 2 self.abs_max_v: 829.5\n",
      "lif layer 2 self.abs_max_v: 869.0\n",
      "lif layer 2 self.abs_max_v: 989.5\n",
      "lif layer 2 self.abs_max_v: 1048.0\n",
      "lif layer 1 self.abs_max_v: 1114.0\n",
      "lif layer 1 self.abs_max_v: 1133.0\n",
      "fc layer 2 self.abs_max_out: 618.0\n",
      "fc layer 2 self.abs_max_out: 648.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 1134.5\n",
      "lif layer 1 self.abs_max_v: 1203.5\n",
      "lif layer 1 self.abs_max_v: 1213.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6da1f039ea4caf9f1add534c660246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-sweep-42</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgpp21gv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cgpp21gv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251120_143512-cgpp21gv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [4,6,10,12,14,16]},\n",
    "        # \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-8]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_0],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'm019m4x7'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
