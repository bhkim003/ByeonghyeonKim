{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10277/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 8, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0,\n",
    "#                 lif_layer_v_threshold2 = 8,\n",
    "#                 learning_rate2 = 8,\n",
    "#                 init_scaling = [1/2,1/2,1/2],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: twim2zsu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251210_235530-twim2zsu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/twim2zsu' target=\"_blank\">daily-sweep-795</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/twim2zsu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/twim2zsu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251210_235538_571', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 2048, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.03125, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.5, 0.125, 0.5], 'learning_rate': 1, 'learning_rate2': 4} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 2048\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.03125, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.125, 0.5])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=2048, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.125, 0.5])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.03125, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.125, 0.5])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 742.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 791.0\n",
      "lif layer 1 self.abs_max_v: 1023.0\n",
      "fc layer 1 self.abs_max_out: 815.0\n",
      "lif layer 1 self.abs_max_v: 1163.5\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 1235.5\n",
      "lif layer 1 self.abs_max_v: 1444.5\n",
      "fc layer 1 self.abs_max_out: 1051.0\n",
      "lif layer 1 self.abs_max_v: 1703.5\n",
      "fc layer 1 self.abs_max_out: 1418.0\n",
      "lif layer 1 self.abs_max_v: 1893.0\n",
      "lif layer 1 self.abs_max_v: 1994.5\n",
      "fc layer 1 self.abs_max_out: 1478.0\n",
      "lif layer 1 self.abs_max_v: 2032.5\n",
      "fc layer 1 self.abs_max_out: 1525.0\n",
      "lif layer 1 self.abs_max_v: 2198.0\n",
      "fc layer 2 self.abs_max_out: 16.0\n",
      "lif layer 2 self.abs_max_v: 16.0\n",
      "fc layer 1 self.abs_max_out: 1533.0\n",
      "lif layer 1 self.abs_max_v: 2205.0\n",
      "fc layer 1 self.abs_max_out: 1849.0\n",
      "lif layer 1 self.abs_max_v: 2769.5\n",
      "lif layer 2 self.abs_max_v: 16.5\n",
      "lif layer 2 self.abs_max_v: 20.0\n",
      "fc layer 2 self.abs_max_out: 20.0\n",
      "lif layer 2 self.abs_max_v: 26.0\n",
      "fc layer 1 self.abs_max_out: 1951.0\n",
      "lif layer 1 self.abs_max_v: 2771.5\n",
      "lif layer 1 self.abs_max_v: 2816.0\n",
      "fc layer 2 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 30.0\n",
      "lif layer 1 self.abs_max_v: 2852.5\n",
      "fc layer 2 self.abs_max_out: 40.0\n",
      "lif layer 2 self.abs_max_v: 41.0\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "fc layer 2 self.abs_max_out: 51.0\n",
      "lif layer 2 self.abs_max_v: 51.0\n",
      "fc layer 1 self.abs_max_out: 2141.0\n",
      "lif layer 1 self.abs_max_v: 3099.0\n",
      "fc layer 2 self.abs_max_out: 66.0\n",
      "lif layer 2 self.abs_max_v: 66.5\n",
      "fc layer 2 self.abs_max_out: 82.0\n",
      "lif layer 2 self.abs_max_v: 80.5\n",
      "lif layer 2 self.abs_max_v: 86.0\n",
      "fc layer 2 self.abs_max_out: 84.0\n",
      "fc layer 2 self.abs_max_out: 88.0\n",
      "lif layer 2 self.abs_max_v: 88.0\n",
      "fc layer 2 self.abs_max_out: 92.0\n",
      "lif layer 2 self.abs_max_v: 125.0\n",
      "fc layer 2 self.abs_max_out: 96.0\n",
      "lif layer 2 self.abs_max_v: 127.5\n",
      "fc layer 2 self.abs_max_out: 100.0\n",
      "fc layer 2 self.abs_max_out: 119.0\n",
      "fc layer 2 self.abs_max_out: 120.0\n",
      "lif layer 2 self.abs_max_v: 149.0\n",
      "fc layer 2 self.abs_max_out: 124.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 160.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 1 self.abs_max_out: 2414.0\n",
      "fc layer 2 self.abs_max_out: 147.0\n",
      "lif layer 2 self.abs_max_v: 211.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "fc layer 2 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 215.5\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "fc layer 2 self.abs_max_out: 175.0\n",
      "fc layer 2 self.abs_max_out: 179.0\n",
      "fc layer 2 self.abs_max_out: 257.0\n",
      "lif layer 2 self.abs_max_v: 266.0\n",
      "lif layer 2 self.abs_max_v: 291.5\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 292.0\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "fc layer 1 self.abs_max_out: 2719.0\n",
      "lif layer 1 self.abs_max_v: 3504.5\n",
      "lif layer 2 self.abs_max_v: 324.0\n",
      "fc layer 1 self.abs_max_out: 2763.0\n",
      "lif layer 1 self.abs_max_v: 3760.0\n",
      "lif layer 2 self.abs_max_v: 324.5\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "fc layer 1 self.abs_max_out: 2809.0\n",
      "lif layer 2 self.abs_max_v: 338.0\n",
      "lif layer 2 self.abs_max_v: 338.5\n",
      "fc layer 2 self.abs_max_out: 268.0\n",
      "lif layer 2 self.abs_max_v: 343.5\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "lif layer 2 self.abs_max_v: 354.0\n",
      "fc layer 2 self.abs_max_out: 274.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 1 self.abs_max_out: 3049.0\n",
      "lif layer 2 self.abs_max_v: 457.5\n",
      "fc layer 1 self.abs_max_out: 4143.0\n",
      "lif layer 1 self.abs_max_v: 4143.0\n",
      "fc layer 2 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 464.0\n",
      "lif layer 2 self.abs_max_v: 491.0\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "lif layer 2 self.abs_max_v: 517.5\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "fc layer 2 self.abs_max_out: 289.0\n",
      "lif layer 1 self.abs_max_v: 4395.0\n",
      "fc layer 2 self.abs_max_out: 315.0\n",
      "fc layer 2 self.abs_max_out: 324.0\n",
      "fc layer 2 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 519.0\n",
      "lif layer 2 self.abs_max_v: 524.5\n",
      "fc layer 2 self.abs_max_out: 357.0\n",
      "lif layer 2 self.abs_max_v: 568.5\n",
      "fc layer 2 self.abs_max_out: 392.0\n",
      "lif layer 2 self.abs_max_v: 618.5\n",
      "lif layer 2 self.abs_max_v: 652.0\n",
      "fc layer 2 self.abs_max_out: 412.0\n",
      "lif layer 2 self.abs_max_v: 723.0\n",
      "fc layer 2 self.abs_max_out: 442.0\n",
      "fc layer 3 self.abs_max_out: 831.0\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "fc layer 1 self.abs_max_out: 4686.0\n",
      "lif layer 1 self.abs_max_v: 4686.0\n",
      "lif layer 2 self.abs_max_v: 773.5\n",
      "fc layer 2 self.abs_max_out: 459.0\n",
      "lif layer 2 self.abs_max_v: 828.5\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "lif layer 2 self.abs_max_v: 922.5\n",
      "fc layer 1 self.abs_max_out: 4799.0\n",
      "lif layer 1 self.abs_max_v: 4799.0\n",
      "fc layer 1 self.abs_max_out: 4890.0\n",
      "lif layer 1 self.abs_max_v: 4890.0\n",
      "fc layer 2 self.abs_max_out: 521.0\n",
      "fc layer 2 self.abs_max_out: 525.0\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "lif layer 2 self.abs_max_v: 976.5\n",
      "fc layer 2 self.abs_max_out: 630.0\n",
      "fc layer 2 self.abs_max_out: 844.0\n",
      "lif layer 2 self.abs_max_v: 1067.5\n",
      "lif layer 2 self.abs_max_v: 1134.0\n",
      "lif layer 2 self.abs_max_v: 1201.0\n",
      "fc layer 1 self.abs_max_out: 5109.0\n",
      "lif layer 1 self.abs_max_v: 5109.0\n",
      "lif layer 2 self.abs_max_v: 1250.5\n",
      "lif layer 2 self.abs_max_v: 1304.5\n",
      "fc layer 1 self.abs_max_out: 5172.0\n",
      "lif layer 1 self.abs_max_v: 5172.0\n",
      "lif layer 2 self.abs_max_v: 1364.5\n",
      "lif layer 2 self.abs_max_v: 1397.5\n",
      "lif layer 2 self.abs_max_v: 1475.5\n",
      "fc layer 1 self.abs_max_out: 5371.0\n",
      "lif layer 1 self.abs_max_v: 5371.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 31.907661/111.777748, val:  27.50%, val_best:  27.50%, tr:  47.09%, tr_best:  47.09%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 99.3882%\n",
      "layer   3  Sparsity: 85.4869%\n",
      "total_backward_count 9790 real_backward_count 6877  70.245%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 852.0\n",
      "fc layer 1 self.abs_max_out: 5436.0\n",
      "lif layer 1 self.abs_max_v: 5436.0\n",
      "fc layer 1 self.abs_max_out: 5556.0\n",
      "lif layer 1 self.abs_max_v: 5556.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 1502.5\n",
      "lif layer 2 self.abs_max_v: 1533.5\n",
      "lif layer 2 self.abs_max_v: 1551.5\n",
      "fc layer 2 self.abs_max_out: 1008.0\n",
      "lif layer 2 self.abs_max_v: 1688.0\n",
      "lif layer 2 self.abs_max_v: 1713.0\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "lif layer 2 self.abs_max_v: 1841.5\n",
      "fc layer 2 self.abs_max_out: 1046.0\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1194.0\n",
      "lif layer 2 self.abs_max_v: 2023.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "lif layer 2 self.abs_max_v: 2070.5\n",
      "lif layer 2 self.abs_max_v: 2194.5\n",
      "fc layer 2 self.abs_max_out: 1229.0\n",
      "lif layer 2 self.abs_max_v: 2230.5\n",
      "lif layer 2 self.abs_max_v: 2293.0\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "lif layer 2 self.abs_max_v: 2379.0\n",
      "lif layer 2 self.abs_max_v: 2383.5\n",
      "fc layer 2 self.abs_max_out: 1249.0\n",
      "lif layer 2 self.abs_max_v: 2431.5\n",
      "fc layer 1 self.abs_max_out: 5859.0\n",
      "lif layer 1 self.abs_max_v: 5859.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 28.377663/ 75.454033, val:  27.50%, val_best:  27.50%, tr:  76.92%, tr_best:  76.92%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 98.7028%\n",
      "layer   3  Sparsity: 76.7521%\n",
      "total_backward_count 19580 real_backward_count 11584  59.162%\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "fc layer 1 self.abs_max_out: 5885.0\n",
      "lif layer 1 self.abs_max_v: 5885.0\n",
      "fc layer 2 self.abs_max_out: 1358.0\n",
      "fc layer 3 self.abs_max_out: 857.0\n",
      "fc layer 1 self.abs_max_out: 6255.0\n",
      "lif layer 1 self.abs_max_v: 6255.0\n",
      "fc layer 2 self.abs_max_out: 1365.0\n",
      "lif layer 2 self.abs_max_v: 2479.5\n",
      "fc layer 2 self.abs_max_out: 1373.0\n",
      "lif layer 2 self.abs_max_v: 2571.0\n",
      "lif layer 2 self.abs_max_v: 2606.0\n",
      "lif layer 2 self.abs_max_v: 2643.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 22.487070/ 98.984337, val:  27.50%, val_best:  27.50%, tr:  83.45%, tr_best:  83.45%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 98.3639%\n",
      "layer   3  Sparsity: 73.0030%\n",
      "total_backward_count 29370 real_backward_count 15539  52.908%\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1421.0\n",
      "fc layer 2 self.abs_max_out: 1423.0\n",
      "fc layer 2 self.abs_max_out: 1506.0\n",
      "lif layer 2 self.abs_max_v: 2675.5\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "fc layer 2 self.abs_max_out: 1528.0\n",
      "lif layer 2 self.abs_max_v: 2789.0\n",
      "fc layer 3 self.abs_max_out: 862.0\n",
      "fc layer 2 self.abs_max_out: 1544.0\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "fc layer 2 self.abs_max_out: 1715.0\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "lif layer 2 self.abs_max_v: 3087.0\n",
      "fc layer 2 self.abs_max_out: 1729.0\n",
      "fc layer 2 self.abs_max_out: 1770.0\n",
      "lif layer 2 self.abs_max_v: 3151.5\n",
      "lif layer 2 self.abs_max_v: 3324.0\n",
      "lif layer 2 self.abs_max_v: 3387.0\n",
      "lif layer 2 self.abs_max_v: 3403.5\n",
      "fc layer 1 self.abs_max_out: 6471.0\n",
      "lif layer 1 self.abs_max_v: 6471.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 18.566223/ 76.478058, val:  26.25%, val_best:  27.50%, tr:  86.93%, tr_best:  86.93%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 98.0970%\n",
      "layer   3  Sparsity: 72.2750%\n",
      "total_backward_count 39160 real_backward_count 19041  48.624%\n",
      "fc layer 1 self.abs_max_out: 6636.0\n",
      "lif layer 1 self.abs_max_v: 6636.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 16.492435/111.439705, val:  15.00%, val_best:  27.50%, tr:  87.44%, tr_best:  87.44%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 98.0614%\n",
      "layer   3  Sparsity: 71.6475%\n",
      "total_backward_count 48950 real_backward_count 22351  45.661%\n",
      "fc layer 1 self.abs_max_out: 6997.0\n",
      "lif layer 1 self.abs_max_v: 6997.0\n",
      "fc layer 2 self.abs_max_out: 1835.0\n",
      "fc layer 2 self.abs_max_out: 1870.0\n",
      "lif layer 2 self.abs_max_v: 3408.0\n",
      "lif layer 2 self.abs_max_v: 3435.0\n",
      "lif layer 2 self.abs_max_v: 3581.5\n",
      "lif layer 2 self.abs_max_v: 3648.0\n",
      "lif layer 2 self.abs_max_v: 3667.0\n",
      "lif layer 2 self.abs_max_v: 3694.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 16.248457/ 88.013756, val:  31.25%, val_best:  31.25%, tr:  89.27%, tr_best:  89.27%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 98.0262%\n",
      "layer   3  Sparsity: 71.3998%\n",
      "total_backward_count 58740 real_backward_count 25628  43.630%\n",
      "fc layer 2 self.abs_max_out: 1877.0\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 14.893384/ 71.946564, val:  23.75%, val_best:  31.25%, tr:  90.81%, tr_best:  90.81%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 98.1965%\n",
      "layer   3  Sparsity: 72.2444%\n",
      "total_backward_count 68530 real_backward_count 28640  41.792%\n",
      "fc layer 1 self.abs_max_out: 7181.0\n",
      "lif layer 1 self.abs_max_v: 7181.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 14.362514/ 72.846695, val:  27.92%, val_best:  31.25%, tr:  91.42%, tr_best:  91.42%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 98.1798%\n",
      "layer   3  Sparsity: 73.0285%\n",
      "total_backward_count 78320 real_backward_count 31603  40.351%\n",
      "fc layer 1 self.abs_max_out: 7255.0\n",
      "lif layer 1 self.abs_max_v: 7255.0\n",
      "fc layer 1 self.abs_max_out: 7384.0\n",
      "lif layer 1 self.abs_max_v: 7384.0\n",
      "fc layer 1 self.abs_max_out: 7608.0\n",
      "lif layer 1 self.abs_max_v: 7608.0\n",
      "fc layer 1 self.abs_max_out: 7882.0\n",
      "lif layer 1 self.abs_max_v: 7882.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 13.930344/ 50.211002, val:  30.42%, val_best:  31.25%, tr:  91.62%, tr_best:  91.62%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 98.1578%\n",
      "layer   3  Sparsity: 72.9145%\n",
      "total_backward_count 88110 real_backward_count 34579  39.245%\n",
      "fc layer 1 self.abs_max_out: 7946.0\n",
      "lif layer 1 self.abs_max_v: 7946.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 13.301771/ 46.656033, val:  39.58%, val_best:  39.58%, tr:  92.13%, tr_best:  92.13%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 98.1393%\n",
      "layer   3  Sparsity: 72.4699%\n",
      "total_backward_count 97900 real_backward_count 37396  38.198%\n",
      "fc layer 1 self.abs_max_out: 7969.0\n",
      "lif layer 1 self.abs_max_v: 7969.0\n",
      "fc layer 1 self.abs_max_out: 7982.0\n",
      "lif layer 1 self.abs_max_v: 7982.0\n",
      "fc layer 1 self.abs_max_out: 8364.0\n",
      "lif layer 1 self.abs_max_v: 8364.0\n",
      "fc layer 1 self.abs_max_out: 8394.0\n",
      "lif layer 1 self.abs_max_v: 8394.0\n",
      "fc layer 1 self.abs_max_out: 8745.0\n",
      "lif layer 1 self.abs_max_v: 8745.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 13.261100/ 74.294945, val:  35.00%, val_best:  39.58%, tr:  93.26%, tr_best:  93.26%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 98.0635%\n",
      "layer   3  Sparsity: 70.8947%\n",
      "total_backward_count 107690 real_backward_count 40196  37.326%\n",
      "fc layer 1 self.abs_max_out: 8872.0\n",
      "lif layer 1 self.abs_max_v: 8872.0\n",
      "fc layer 1 self.abs_max_out: 9230.0\n",
      "lif layer 1 self.abs_max_v: 9230.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 13.100435/ 83.265556, val:  20.83%, val_best:  39.58%, tr:  92.44%, tr_best:  93.26%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 98.0311%\n",
      "layer   3  Sparsity: 71.4786%\n",
      "total_backward_count 117480 real_backward_count 42935  36.547%\n",
      "fc layer 1 self.abs_max_out: 9499.0\n",
      "lif layer 1 self.abs_max_v: 9499.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 12.433805/ 92.404015, val:  19.17%, val_best:  39.58%, tr:  92.34%, tr_best:  93.26%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 98.1427%\n",
      "layer   3  Sparsity: 72.0285%\n",
      "total_backward_count 127270 real_backward_count 45689  35.899%\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 12.718104/ 57.246746, val:  27.08%, val_best:  39.58%, tr:  91.52%, tr_best:  93.26%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 98.1282%\n",
      "layer   3  Sparsity: 72.2610%\n",
      "total_backward_count 137060 real_backward_count 48495  35.382%\n",
      "fc layer 1 self.abs_max_out: 9757.0\n",
      "lif layer 1 self.abs_max_v: 9757.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 12.198697/ 97.182198, val:  18.33%, val_best:  39.58%, tr:  90.50%, tr_best:  93.26%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 98.1884%\n",
      "layer   3  Sparsity: 72.4066%\n",
      "total_backward_count 146850 real_backward_count 51243  34.895%\n",
      "fc layer 1 self.abs_max_out: 9780.0\n",
      "lif layer 1 self.abs_max_v: 9780.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 12.760824/ 77.058395, val:  23.75%, val_best:  39.58%, tr:  92.34%, tr_best:  93.26%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 98.1473%\n",
      "layer   3  Sparsity: 71.7255%\n",
      "total_backward_count 156640 real_backward_count 54007  34.478%\n",
      "fc layer 1 self.abs_max_out: 10014.0\n",
      "lif layer 1 self.abs_max_v: 10014.0\n",
      "fc layer 1 self.abs_max_out: 10191.0\n",
      "lif layer 1 self.abs_max_v: 10191.0\n",
      "fc layer 2 self.abs_max_out: 1986.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 12.020790/ 60.865482, val:  36.25%, val_best:  39.58%, tr:  92.44%, tr_best:  93.26%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 97.9735%\n",
      "layer   3  Sparsity: 71.4267%\n",
      "total_backward_count 166430 real_backward_count 56648  34.037%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 12.623445/ 52.578987, val:  30.42%, val_best:  39.58%, tr:  92.75%, tr_best:  93.26%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 97.8979%\n",
      "layer   3  Sparsity: 70.2756%\n",
      "total_backward_count 176220 real_backward_count 59261  33.629%\n",
      "fc layer 1 self.abs_max_out: 10280.0\n",
      "lif layer 1 self.abs_max_v: 10280.0\n",
      "fc layer 1 self.abs_max_out: 10417.0\n",
      "lif layer 1 self.abs_max_v: 10417.0\n",
      "lif layer 2 self.abs_max_v: 3748.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 12.959383/ 60.487415, val:  27.50%, val_best:  39.58%, tr:  91.62%, tr_best:  93.26%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 97.8470%\n",
      "layer   3  Sparsity: 69.9139%\n",
      "total_backward_count 186010 real_backward_count 61931  33.294%\n",
      "fc layer 1 self.abs_max_out: 10499.0\n",
      "lif layer 1 self.abs_max_v: 10499.0\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "lif layer 2 self.abs_max_v: 3769.0\n",
      "fc layer 2 self.abs_max_out: 2070.0\n",
      "lif layer 2 self.abs_max_v: 3860.5\n",
      "fc layer 2 self.abs_max_out: 2118.0\n",
      "lif layer 2 self.abs_max_v: 4013.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 12.157872/ 80.430641, val:  21.67%, val_best:  39.58%, tr:  93.56%, tr_best:  93.56%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 97.7563%\n",
      "layer   3  Sparsity: 68.8322%\n",
      "total_backward_count 195800 real_backward_count 64368  32.874%\n",
      "fc layer 1 self.abs_max_out: 10578.0\n",
      "lif layer 1 self.abs_max_v: 10578.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 12.418004/ 84.313690, val:  30.42%, val_best:  39.58%, tr:  92.75%, tr_best:  93.56%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 97.7589%\n",
      "layer   3  Sparsity: 67.8427%\n",
      "total_backward_count 205590 real_backward_count 66770  32.477%\n",
      "fc layer 1 self.abs_max_out: 10647.0\n",
      "lif layer 1 self.abs_max_v: 10647.0\n",
      "lif layer 2 self.abs_max_v: 4028.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 12.710382/ 53.637180, val:  27.50%, val_best:  39.58%, tr:  94.79%, tr_best:  94.79%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 97.6513%\n",
      "layer   3  Sparsity: 67.5262%\n",
      "total_backward_count 215380 real_backward_count 69177  32.119%\n",
      "fc layer 1 self.abs_max_out: 10794.0\n",
      "lif layer 1 self.abs_max_v: 10794.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 12.775438/ 79.516190, val:  27.92%, val_best:  39.58%, tr:  95.30%, tr_best:  95.30%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 97.5800%\n",
      "layer   3  Sparsity: 67.8246%\n",
      "total_backward_count 225170 real_backward_count 71567  31.784%\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "fc layer 2 self.abs_max_out: 2311.0\n",
      "lif layer 2 self.abs_max_v: 4042.0\n",
      "fc layer 2 self.abs_max_out: 2356.0\n",
      "lif layer 2 self.abs_max_v: 4296.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 12.155088/ 48.718647, val:  34.58%, val_best:  39.58%, tr:  94.18%, tr_best:  95.30%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 97.6082%\n",
      "layer   3  Sparsity: 68.3018%\n",
      "total_backward_count 234960 real_backward_count 73944  31.471%\n",
      "lif layer 2 self.abs_max_v: 4300.5\n",
      "fc layer 2 self.abs_max_out: 2359.0\n",
      "lif layer 2 self.abs_max_v: 4407.5\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 12.341597/ 63.870258, val:  34.58%, val_best:  39.58%, tr:  94.08%, tr_best:  95.30%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 97.6299%\n",
      "layer   3  Sparsity: 69.4709%\n",
      "total_backward_count 244750 real_backward_count 76433  31.229%\n",
      "fc layer 2 self.abs_max_out: 2420.0\n",
      "fc layer 2 self.abs_max_out: 2608.0\n",
      "lif layer 2 self.abs_max_v: 4679.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 12.706023/ 64.167274, val:  23.75%, val_best:  39.58%, tr:  93.26%, tr_best:  95.30%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 97.5539%\n",
      "layer   3  Sparsity: 69.4310%\n",
      "total_backward_count 254540 real_backward_count 78936  31.011%\n",
      "fc layer 1 self.abs_max_out: 10987.0\n",
      "lif layer 1 self.abs_max_v: 10987.0\n",
      "lif layer 2 self.abs_max_v: 4733.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 12.652969/ 64.837837, val:  38.75%, val_best:  39.58%, tr:  92.95%, tr_best:  95.30%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 97.4901%\n",
      "layer   3  Sparsity: 69.2809%\n",
      "total_backward_count 264330 real_backward_count 81472  30.822%\n",
      "fc layer 1 self.abs_max_out: 11095.0\n",
      "lif layer 1 self.abs_max_v: 11095.0\n",
      "lif layer 2 self.abs_max_v: 4804.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 12.158762/ 52.986271, val:  34.17%, val_best:  39.58%, tr:  92.95%, tr_best:  95.30%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 97.4287%\n",
      "layer   3  Sparsity: 69.0830%\n",
      "total_backward_count 274120 real_backward_count 83950  30.625%\n",
      "fc layer 2 self.abs_max_out: 2723.0\n",
      "lif layer 2 self.abs_max_v: 4947.5\n",
      "lif layer 2 self.abs_max_v: 4953.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 11.888107/ 82.015564, val:  31.67%, val_best:  39.58%, tr:  93.05%, tr_best:  95.30%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 97.4415%\n",
      "layer   3  Sparsity: 69.0603%\n",
      "total_backward_count 283910 real_backward_count 86405  30.434%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 12.268086/ 58.065292, val:  27.08%, val_best:  39.58%, tr:  93.46%, tr_best:  95.30%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 97.4571%\n",
      "layer   3  Sparsity: 69.1757%\n",
      "total_backward_count 293700 real_backward_count 88910  30.272%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 11.681506/ 57.476841, val:  38.75%, val_best:  39.58%, tr:  92.24%, tr_best:  95.30%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 97.3612%\n",
      "layer   3  Sparsity: 68.7896%\n",
      "total_backward_count 303490 real_backward_count 91336  30.095%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 11.935210/ 47.052456, val:  31.25%, val_best:  39.58%, tr:  93.16%, tr_best:  95.30%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 97.3152%\n",
      "layer   3  Sparsity: 68.6671%\n",
      "total_backward_count 313280 real_backward_count 93772  29.932%\n",
      "fc layer 1 self.abs_max_out: 11356.0\n",
      "lif layer 1 self.abs_max_v: 11356.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 11.599189/ 65.328423, val:  21.67%, val_best:  39.58%, tr:  93.46%, tr_best:  95.30%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 97.3566%\n",
      "layer   3  Sparsity: 69.5378%\n",
      "total_backward_count 323070 real_backward_count 96180  29.771%\n",
      "fc layer 1 self.abs_max_out: 11536.0\n",
      "lif layer 1 self.abs_max_v: 11536.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 11.748129/ 63.723934, val:  32.50%, val_best:  39.58%, tr:  93.77%, tr_best:  95.30%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 97.3744%\n",
      "layer   3  Sparsity: 68.9208%\n",
      "total_backward_count 332860 real_backward_count 98591  29.619%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 11.369908/ 53.048042, val:  36.25%, val_best:  39.58%, tr:  97.24%, tr_best:  97.24%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 97.3032%\n",
      "layer   3  Sparsity: 69.9161%\n",
      "total_backward_count 342650 real_backward_count 100713  29.392%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 10.840558/ 48.682030, val:  34.58%, val_best:  39.58%, tr:  97.55%, tr_best:  97.55%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 97.3187%\n",
      "layer   3  Sparsity: 70.9060%\n",
      "total_backward_count 352440 real_backward_count 102696  29.139%\n",
      "fc layer 1 self.abs_max_out: 11601.0\n",
      "lif layer 1 self.abs_max_v: 11601.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 10.070135/ 57.873772, val:  34.17%, val_best:  39.58%, tr:  97.14%, tr_best:  97.55%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 97.3966%\n",
      "layer   3  Sparsity: 71.7192%\n",
      "total_backward_count 362230 real_backward_count 104614  28.881%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  9.810298/ 47.407749, val:  27.92%, val_best:  39.58%, tr:  96.42%, tr_best:  97.55%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 97.3679%\n",
      "layer   3  Sparsity: 71.9303%\n",
      "total_backward_count 372020 real_backward_count 106507  28.629%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  9.982899/ 53.295879, val:  30.83%, val_best:  39.58%, tr:  96.32%, tr_best:  97.55%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 97.3589%\n",
      "layer   3  Sparsity: 72.1128%\n",
      "total_backward_count 381810 real_backward_count 108429  28.399%\n",
      "fc layer 1 self.abs_max_out: 11661.0\n",
      "lif layer 1 self.abs_max_v: 11661.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  9.467294/ 60.912689, val:  25.00%, val_best:  39.58%, tr:  97.04%, tr_best:  97.55%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 97.3716%\n",
      "layer   3  Sparsity: 72.0214%\n",
      "total_backward_count 391600 real_backward_count 110290  28.164%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  9.246262/ 40.194023, val:  40.83%, val_best:  40.83%, tr:  96.63%, tr_best:  97.55%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 97.4635%\n",
      "layer   3  Sparsity: 72.3623%\n",
      "total_backward_count 401390 real_backward_count 112142  27.938%\n",
      "fc layer 1 self.abs_max_out: 11751.0\n",
      "lif layer 1 self.abs_max_v: 11751.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  9.034447/ 75.223122, val:  26.25%, val_best:  40.83%, tr:  96.22%, tr_best:  97.55%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 97.4409%\n",
      "layer   3  Sparsity: 73.0928%\n",
      "total_backward_count 411180 real_backward_count 113969  27.718%\n",
      "fc layer 1 self.abs_max_out: 11808.0\n",
      "lif layer 1 self.abs_max_v: 11808.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  9.025183/ 37.343246, val:  45.00%, val_best:  45.00%, tr:  96.22%, tr_best:  97.55%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 97.4421%\n",
      "layer   3  Sparsity: 73.1062%\n",
      "total_backward_count 420970 real_backward_count 115775  27.502%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  8.698291/ 30.493073, val:  45.42%, val_best:  45.42%, tr:  97.24%, tr_best:  97.55%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 97.4103%\n",
      "layer   3  Sparsity: 73.4932%\n",
      "total_backward_count 430760 real_backward_count 117555  27.290%\n",
      "fc layer 1 self.abs_max_out: 12009.0\n",
      "lif layer 1 self.abs_max_v: 12009.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  8.955586/ 44.467503, val:  41.25%, val_best:  45.42%, tr:  96.02%, tr_best:  97.55%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 97.4662%\n",
      "layer   3  Sparsity: 73.6779%\n",
      "total_backward_count 440550 real_backward_count 119396  27.102%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  8.495784/ 50.751095, val:  37.92%, val_best:  45.42%, tr:  97.14%, tr_best:  97.55%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 97.4023%\n",
      "layer   3  Sparsity: 74.1048%\n",
      "total_backward_count 450340 real_backward_count 121177  26.908%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  8.557227/ 28.203188, val:  45.00%, val_best:  45.42%, tr:  96.73%, tr_best:  97.55%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 97.4020%\n",
      "layer   3  Sparsity: 74.3754%\n",
      "total_backward_count 460130 real_backward_count 122994  26.730%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  9.000565/ 44.581894, val:  40.83%, val_best:  45.42%, tr:  97.14%, tr_best:  97.55%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 97.4219%\n",
      "layer   3  Sparsity: 74.6079%\n",
      "total_backward_count 469920 real_backward_count 124798  26.557%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  8.760699/ 42.554005, val:  31.25%, val_best:  45.42%, tr:  97.14%, tr_best:  97.55%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 97.4463%\n",
      "layer   3  Sparsity: 75.0959%\n",
      "total_backward_count 479710 real_backward_count 126639  26.399%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  8.765409/ 49.340504, val:  38.33%, val_best:  45.42%, tr:  96.02%, tr_best:  97.55%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 97.3973%\n",
      "layer   3  Sparsity: 75.3030%\n",
      "total_backward_count 489500 real_backward_count 128477  26.247%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  8.662291/ 42.378368, val:  37.92%, val_best:  45.42%, tr:  97.04%, tr_best:  97.55%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 97.5022%\n",
      "layer   3  Sparsity: 75.0838%\n",
      "total_backward_count 499290 real_backward_count 130303  26.098%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  8.867977/ 30.991350, val:  43.75%, val_best:  45.42%, tr:  96.12%, tr_best:  97.55%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 97.5083%\n",
      "layer   3  Sparsity: 74.8585%\n",
      "total_backward_count 509080 real_backward_count 132143  25.957%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  8.700518/ 59.255188, val:  35.00%, val_best:  45.42%, tr:  96.73%, tr_best:  97.55%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 97.4978%\n",
      "layer   3  Sparsity: 74.9588%\n",
      "total_backward_count 518870 real_backward_count 134006  25.827%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  9.540753/ 44.880814, val:  32.50%, val_best:  45.42%, tr:  95.91%, tr_best:  97.55%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 97.4756%\n",
      "layer   3  Sparsity: 74.5824%\n",
      "total_backward_count 528660 real_backward_count 135890  25.705%\n",
      "fc layer 1 self.abs_max_out: 12230.0\n",
      "lif layer 1 self.abs_max_v: 12230.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  9.100817/ 41.584442, val:  37.92%, val_best:  45.42%, tr:  97.04%, tr_best:  97.55%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 97.4375%\n",
      "layer   3  Sparsity: 74.2609%\n",
      "total_backward_count 538450 real_backward_count 137735  25.580%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  9.086651/ 45.824093, val:  41.67%, val_best:  45.42%, tr:  97.34%, tr_best:  97.55%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 97.3763%\n",
      "layer   3  Sparsity: 74.2004%\n",
      "total_backward_count 548240 real_backward_count 139553  25.455%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  9.023826/ 36.735161, val:  26.25%, val_best:  45.42%, tr:  96.22%, tr_best:  97.55%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 97.3806%\n",
      "layer   3  Sparsity: 74.1176%\n",
      "total_backward_count 558030 real_backward_count 141400  25.339%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  9.523156/ 45.830986, val:  26.67%, val_best:  45.42%, tr:  96.42%, tr_best:  97.55%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 97.2943%\n",
      "layer   3  Sparsity: 73.8659%\n",
      "total_backward_count 567820 real_backward_count 143244  25.227%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  9.199174/ 54.113724, val:  27.50%, val_best:  45.42%, tr:  95.71%, tr_best:  97.55%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 97.2536%\n",
      "layer   3  Sparsity: 73.7212%\n",
      "total_backward_count 577610 real_backward_count 145078  25.117%\n",
      "fc layer 2 self.abs_max_out: 2747.0\n",
      "fc layer 1 self.abs_max_out: 12422.0\n",
      "lif layer 1 self.abs_max_v: 12422.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  9.406332/ 43.181000, val:  41.25%, val_best:  45.42%, tr:  96.53%, tr_best:  97.55%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 97.2740%\n",
      "layer   3  Sparsity: 73.2934%\n",
      "total_backward_count 587400 real_backward_count 146886  25.006%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  9.594683/ 27.235355, val:  42.08%, val_best:  45.42%, tr:  97.65%, tr_best:  97.65%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 97.3441%\n",
      "layer   3  Sparsity: 72.8205%\n",
      "total_backward_count 597190 real_backward_count 148719  24.903%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  9.821194/ 69.789680, val:  32.50%, val_best:  45.42%, tr:  97.34%, tr_best:  97.65%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 97.2986%\n",
      "layer   3  Sparsity: 72.4807%\n",
      "total_backward_count 606980 real_backward_count 150570  24.806%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  9.518077/ 71.891792, val:  30.83%, val_best:  45.42%, tr:  96.63%, tr_best:  97.65%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 97.2794%\n",
      "layer   3  Sparsity: 71.5372%\n",
      "total_backward_count 616770 real_backward_count 152334  24.699%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  9.870452/ 36.709465, val:  45.00%, val_best:  45.42%, tr:  97.14%, tr_best:  97.65%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 97.2679%\n",
      "layer   3  Sparsity: 71.6834%\n",
      "total_backward_count 626560 real_backward_count 154152  24.603%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  9.606222/ 53.902397, val:  37.50%, val_best:  45.42%, tr:  97.96%, tr_best:  97.96%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 97.2481%\n",
      "layer   3  Sparsity: 71.6105%\n",
      "total_backward_count 636350 real_backward_count 155967  24.510%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  9.953259/ 56.962246, val:  33.33%, val_best:  45.42%, tr:  97.04%, tr_best:  97.96%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 97.2315%\n",
      "layer   3  Sparsity: 70.8251%\n",
      "total_backward_count 646140 real_backward_count 157736  24.412%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 10.461285/ 37.125076, val:  39.58%, val_best:  45.42%, tr:  97.96%, tr_best:  97.96%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 97.1845%\n",
      "layer   3  Sparsity: 70.9056%\n",
      "total_backward_count 655930 real_backward_count 159546  24.324%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  9.738112/ 51.655216, val:  41.25%, val_best:  45.42%, tr:  97.45%, tr_best:  97.96%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 97.2419%\n",
      "layer   3  Sparsity: 71.3057%\n",
      "total_backward_count 665720 real_backward_count 161282  24.227%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  9.611457/ 61.986248, val:  31.67%, val_best:  45.42%, tr:  97.65%, tr_best:  97.96%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 97.2923%\n",
      "layer   3  Sparsity: 71.3501%\n",
      "total_backward_count 675510 real_backward_count 163008  24.131%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  9.621230/ 51.339634, val:  41.67%, val_best:  45.42%, tr:  98.26%, tr_best:  98.26%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 97.2164%\n",
      "layer   3  Sparsity: 71.0961%\n",
      "total_backward_count 685300 real_backward_count 164715  24.035%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  9.438030/ 63.635448, val:  33.33%, val_best:  45.42%, tr:  96.83%, tr_best:  98.26%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 97.2176%\n",
      "layer   3  Sparsity: 71.2600%\n",
      "total_backward_count 695090 real_backward_count 166424  23.943%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  9.682774/ 53.244953, val:  44.17%, val_best:  45.42%, tr:  97.75%, tr_best:  98.26%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 97.2285%\n",
      "layer   3  Sparsity: 70.3523%\n",
      "total_backward_count 704880 real_backward_count 168110  23.849%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  9.892501/ 53.623589, val:  37.08%, val_best:  45.42%, tr:  96.32%, tr_best:  98.26%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 97.2059%\n",
      "layer   3  Sparsity: 70.2223%\n",
      "total_backward_count 714670 real_backward_count 169761  23.754%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  9.978923/ 58.269936, val:  35.42%, val_best:  45.42%, tr:  97.45%, tr_best:  98.26%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 97.2217%\n",
      "layer   3  Sparsity: 70.5673%\n",
      "total_backward_count 724460 real_backward_count 171462  23.668%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  9.864107/ 46.600258, val:  41.25%, val_best:  45.42%, tr:  97.24%, tr_best:  98.26%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 97.1871%\n",
      "layer   3  Sparsity: 70.2671%\n",
      "total_backward_count 734250 real_backward_count 173157  23.583%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  9.762465/ 58.763363, val:  37.08%, val_best:  45.42%, tr:  97.34%, tr_best:  98.26%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 97.2981%\n",
      "layer   3  Sparsity: 71.2108%\n",
      "total_backward_count 744040 real_backward_count 174869  23.503%\n",
      "fc layer 1 self.abs_max_out: 12563.0\n",
      "lif layer 1 self.abs_max_v: 12563.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  9.603676/ 34.817738, val:  41.25%, val_best:  45.42%, tr:  97.85%, tr_best:  98.26%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 97.2351%\n",
      "layer   3  Sparsity: 70.9927%\n",
      "total_backward_count 753830 real_backward_count 176569  23.423%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  9.066233/ 61.826180, val:  40.00%, val_best:  45.42%, tr:  97.45%, tr_best:  98.26%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 97.2195%\n",
      "layer   3  Sparsity: 70.9153%\n",
      "total_backward_count 763620 real_backward_count 178189  23.335%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  9.743551/ 57.862438, val:  38.75%, val_best:  45.42%, tr:  98.06%, tr_best:  98.26%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 97.2127%\n",
      "layer   3  Sparsity: 71.0747%\n",
      "total_backward_count 773410 real_backward_count 179901  23.261%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  9.277175/ 45.732121, val:  40.83%, val_best:  45.42%, tr:  97.55%, tr_best:  98.26%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 97.2018%\n",
      "layer   3  Sparsity: 70.6033%\n",
      "total_backward_count 783200 real_backward_count 181545  23.180%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  9.292830/ 49.168541, val:  37.08%, val_best:  45.42%, tr:  97.14%, tr_best:  98.26%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 97.1309%\n",
      "layer   3  Sparsity: 69.9376%\n",
      "total_backward_count 792990 real_backward_count 183167  23.098%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  9.742034/ 88.876434, val:  22.92%, val_best:  45.42%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 97.0883%\n",
      "layer   3  Sparsity: 69.9799%\n",
      "total_backward_count 802780 real_backward_count 184856  23.027%\n",
      "fc layer 1 self.abs_max_out: 12735.0\n",
      "lif layer 1 self.abs_max_v: 12735.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  9.495662/ 47.143337, val:  34.58%, val_best:  45.42%, tr:  97.45%, tr_best:  98.26%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 97.1467%\n",
      "layer   3  Sparsity: 70.7478%\n",
      "total_backward_count 812570 real_backward_count 186522  22.955%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  9.702817/ 51.235466, val:  43.33%, val_best:  45.42%, tr:  97.04%, tr_best:  98.26%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 97.1422%\n",
      "layer   3  Sparsity: 70.9525%\n",
      "total_backward_count 822360 real_backward_count 188242  22.890%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  9.627931/ 56.054577, val:  42.08%, val_best:  45.42%, tr:  96.83%, tr_best:  98.26%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 97.1514%\n",
      "layer   3  Sparsity: 71.5390%\n",
      "total_backward_count 832150 real_backward_count 189974  22.829%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  9.632918/ 56.203808, val:  30.83%, val_best:  45.42%, tr:  97.34%, tr_best:  98.26%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 97.1944%\n",
      "layer   3  Sparsity: 72.1044%\n",
      "total_backward_count 841940 real_backward_count 191704  22.769%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  8.755199/ 54.742065, val:  32.92%, val_best:  45.42%, tr:  95.30%, tr_best:  98.26%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 97.2160%\n",
      "layer   3  Sparsity: 73.2259%\n",
      "total_backward_count 851730 real_backward_count 193404  22.707%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  9.099085/ 50.721897, val:  35.42%, val_best:  45.42%, tr:  97.04%, tr_best:  98.26%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 97.1545%\n",
      "layer   3  Sparsity: 73.8224%\n",
      "total_backward_count 861520 real_backward_count 195160  22.653%\n",
      "fc layer 1 self.abs_max_out: 12746.0\n",
      "lif layer 1 self.abs_max_v: 12746.0\n",
      "fc layer 1 self.abs_max_out: 12755.0\n",
      "lif layer 1 self.abs_max_v: 12755.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  8.720979/ 68.152527, val:  29.58%, val_best:  45.42%, tr:  97.45%, tr_best:  98.26%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 97.1655%\n",
      "layer   3  Sparsity: 74.6754%\n",
      "total_backward_count 871310 real_backward_count 196872  22.595%\n",
      "fc layer 1 self.abs_max_out: 12759.0\n",
      "lif layer 1 self.abs_max_v: 12759.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  9.226658/ 59.106361, val:  29.17%, val_best:  45.42%, tr:  96.42%, tr_best:  98.26%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 97.2150%\n",
      "layer   3  Sparsity: 75.4073%\n",
      "total_backward_count 881100 real_backward_count 198673  22.548%\n",
      "fc layer 1 self.abs_max_out: 12768.0\n",
      "lif layer 1 self.abs_max_v: 12768.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  8.259068/ 48.684814, val:  36.67%, val_best:  45.42%, tr:  96.22%, tr_best:  98.26%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 97.2391%\n",
      "layer   3  Sparsity: 75.6592%\n",
      "total_backward_count 890890 real_backward_count 200376  22.492%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  8.312299/ 20.407475, val:  52.92%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 97.2224%\n",
      "layer   3  Sparsity: 75.3970%\n",
      "total_backward_count 900680 real_backward_count 202101  22.439%\n",
      "fc layer 1 self.abs_max_out: 12805.0\n",
      "lif layer 1 self.abs_max_v: 12805.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  8.609735/ 38.512054, val:  39.17%, val_best:  52.92%, tr:  94.99%, tr_best:  98.26%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 97.2148%\n",
      "layer   3  Sparsity: 74.5488%\n",
      "total_backward_count 910470 real_backward_count 203858  22.390%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  8.978676/ 66.995804, val:  30.42%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 97.2308%\n",
      "layer   3  Sparsity: 73.6840%\n",
      "total_backward_count 920260 real_backward_count 205574  22.339%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  8.459144/ 27.954924, val:  43.33%, val_best:  52.92%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 97.3300%\n",
      "layer   3  Sparsity: 74.6456%\n",
      "total_backward_count 930050 real_backward_count 207242  22.283%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  8.481331/ 44.186325, val:  30.00%, val_best:  52.92%, tr:  97.14%, tr_best:  98.26%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 97.2775%\n",
      "layer   3  Sparsity: 74.9339%\n",
      "total_backward_count 939840 real_backward_count 208938  22.231%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  8.428893/ 50.769951, val:  33.33%, val_best:  52.92%, tr:  95.81%, tr_best:  98.26%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 97.2019%\n",
      "layer   3  Sparsity: 75.0726%\n",
      "total_backward_count 949630 real_backward_count 210616  22.179%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  8.570605/ 43.591553, val:  38.75%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 97.2728%\n",
      "layer   3  Sparsity: 75.3652%\n",
      "total_backward_count 959420 real_backward_count 212319  22.130%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  8.244665/ 47.843765, val:  29.58%, val_best:  52.92%, tr:  96.53%, tr_best:  98.26%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 97.2713%\n",
      "layer   3  Sparsity: 76.0773%\n",
      "total_backward_count 969210 real_backward_count 214026  22.083%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  7.899653/ 43.959293, val:  37.50%, val_best:  52.92%, tr:  95.71%, tr_best:  98.26%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 97.2840%\n",
      "layer   3  Sparsity: 75.8927%\n",
      "total_backward_count 979000 real_backward_count 215713  22.034%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  7.980298/ 46.173271, val:  32.50%, val_best:  52.92%, tr:  97.24%, tr_best:  98.26%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 97.2692%\n",
      "layer   3  Sparsity: 76.0114%\n",
      "total_backward_count 988790 real_backward_count 217389  21.985%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  8.224430/ 44.955986, val:  35.42%, val_best:  52.92%, tr:  95.61%, tr_best:  98.26%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 97.2667%\n",
      "layer   3  Sparsity: 76.1151%\n",
      "total_backward_count 998580 real_backward_count 219092  21.940%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  8.008248/ 26.944731, val:  37.92%, val_best:  52.92%, tr:  95.91%, tr_best:  98.26%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 97.2664%\n",
      "layer   3  Sparsity: 75.8970%\n",
      "total_backward_count 1008370 real_backward_count 220763  21.893%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  7.891779/ 33.690933, val:  42.08%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 97.2586%\n",
      "layer   3  Sparsity: 76.1326%\n",
      "total_backward_count 1018160 real_backward_count 222455  21.849%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  7.871531/ 46.854492, val:  33.75%, val_best:  52.92%, tr:  97.04%, tr_best:  98.26%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 97.2458%\n",
      "layer   3  Sparsity: 76.6778%\n",
      "total_backward_count 1027950 real_backward_count 224182  21.809%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  8.014946/ 29.905218, val:  30.83%, val_best:  52.92%, tr:  96.73%, tr_best:  98.26%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 97.2859%\n",
      "layer   3  Sparsity: 76.9075%\n",
      "total_backward_count 1037740 real_backward_count 225931  21.771%\n",
      "fc layer 1 self.abs_max_out: 12873.0\n",
      "lif layer 1 self.abs_max_v: 12873.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  7.676004/ 37.350765, val:  32.50%, val_best:  52.92%, tr:  96.53%, tr_best:  98.26%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 97.3032%\n",
      "layer   3  Sparsity: 77.1419%\n",
      "total_backward_count 1047530 real_backward_count 227630  21.730%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  7.869060/ 23.914366, val:  47.92%, val_best:  52.92%, tr:  95.10%, tr_best:  98.26%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 97.3328%\n",
      "layer   3  Sparsity: 77.5100%\n",
      "total_backward_count 1057320 real_backward_count 229384  21.695%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  7.422947/ 53.160942, val:  22.08%, val_best:  52.92%, tr:  95.51%, tr_best:  98.26%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 97.3504%\n",
      "layer   3  Sparsity: 78.3153%\n",
      "total_backward_count 1067110 real_backward_count 231127  21.659%\n",
      "fc layer 1 self.abs_max_out: 12983.0\n",
      "lif layer 1 self.abs_max_v: 12983.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  6.798050/ 34.422295, val:  37.08%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 97.3804%\n",
      "layer   3  Sparsity: 79.3802%\n",
      "total_backward_count 1076900 real_backward_count 232826  21.620%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  7.183600/ 39.703590, val:  27.50%, val_best:  52.92%, tr:  96.22%, tr_best:  98.26%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 97.3605%\n",
      "layer   3  Sparsity: 78.6402%\n",
      "total_backward_count 1086690 real_backward_count 234561  21.585%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  7.183617/ 36.504612, val:  33.33%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 97.3827%\n",
      "layer   3  Sparsity: 78.5048%\n",
      "total_backward_count 1096480 real_backward_count 236285  21.549%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  7.411937/ 39.460876, val:  43.33%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 97.3669%\n",
      "layer   3  Sparsity: 78.0279%\n",
      "total_backward_count 1106270 real_backward_count 238083  21.521%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  7.443004/ 29.238022, val:  47.08%, val_best:  52.92%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 97.3258%\n",
      "layer   3  Sparsity: 77.7074%\n",
      "total_backward_count 1116060 real_backward_count 239781  21.485%\n",
      "fc layer 1 self.abs_max_out: 13067.0\n",
      "lif layer 1 self.abs_max_v: 13067.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  6.780538/ 49.628174, val:  18.75%, val_best:  52.92%, tr:  95.40%, tr_best:  98.26%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 97.3069%\n",
      "layer   3  Sparsity: 79.1779%\n",
      "total_backward_count 1125850 real_backward_count 241490  21.450%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  7.221323/ 39.286644, val:  33.75%, val_best:  52.92%, tr:  96.02%, tr_best:  98.26%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 97.3135%\n",
      "layer   3  Sparsity: 78.4670%\n",
      "total_backward_count 1135640 real_backward_count 243276  21.422%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  7.302937/ 47.777817, val:  36.67%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 97.3204%\n",
      "layer   3  Sparsity: 78.5502%\n",
      "total_backward_count 1145430 real_backward_count 245006  21.390%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  7.040637/ 41.527550, val:  42.50%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 97.3121%\n",
      "layer   3  Sparsity: 78.6059%\n",
      "total_backward_count 1155220 real_backward_count 246694  21.355%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  6.954930/ 49.431168, val:  40.00%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 97.2988%\n",
      "layer   3  Sparsity: 79.1578%\n",
      "total_backward_count 1165010 real_backward_count 248433  21.325%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  7.219874/ 40.222427, val:  40.00%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 97.2629%\n",
      "layer   3  Sparsity: 79.0710%\n",
      "total_backward_count 1174800 real_backward_count 250200  21.297%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  7.136135/ 28.784302, val:  30.00%, val_best:  52.92%, tr:  95.81%, tr_best:  98.26%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 97.2216%\n",
      "layer   3  Sparsity: 78.8496%\n",
      "total_backward_count 1184590 real_backward_count 251930  21.267%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  7.221316/ 36.403214, val:  44.17%, val_best:  52.92%, tr:  96.63%, tr_best:  98.26%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 97.2531%\n",
      "layer   3  Sparsity: 78.2824%\n",
      "total_backward_count 1194380 real_backward_count 253693  21.241%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  7.063981/ 39.539417, val:  34.17%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 97.2820%\n",
      "layer   3  Sparsity: 78.7283%\n",
      "total_backward_count 1204170 real_backward_count 255408  21.210%\n",
      "lif layer 1 self.abs_max_v: 13121.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  7.118145/ 33.738091, val:  42.50%, val_best:  52.92%, tr:  96.73%, tr_best:  98.26%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 97.2976%\n",
      "layer   3  Sparsity: 78.7377%\n",
      "total_backward_count 1213960 real_backward_count 257123  21.181%\n",
      "fc layer 1 self.abs_max_out: 13225.0\n",
      "lif layer 1 self.abs_max_v: 13225.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  7.069957/ 41.290264, val:  44.17%, val_best:  52.92%, tr:  97.24%, tr_best:  98.26%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 97.3446%\n",
      "layer   3  Sparsity: 78.3846%\n",
      "total_backward_count 1223750 real_backward_count 258835  21.151%\n",
      "fc layer 1 self.abs_max_out: 13244.0\n",
      "lif layer 1 self.abs_max_v: 13244.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  7.571728/ 44.833496, val:  36.67%, val_best:  52.92%, tr:  96.83%, tr_best:  98.26%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 97.2721%\n",
      "layer   3  Sparsity: 77.2347%\n",
      "total_backward_count 1233540 real_backward_count 260534  21.121%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  7.635774/ 44.434288, val:  34.17%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 97.2416%\n",
      "layer   3  Sparsity: 76.3706%\n",
      "total_backward_count 1243330 real_backward_count 262213  21.090%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  7.033268/ 52.120739, val:  31.25%, val_best:  52.92%, tr:  97.24%, tr_best:  98.26%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 97.2821%\n",
      "layer   3  Sparsity: 76.7315%\n",
      "total_backward_count 1253120 real_backward_count 263824  21.053%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  7.683391/ 27.109100, val:  42.50%, val_best:  52.92%, tr:  97.04%, tr_best:  98.26%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 97.2858%\n",
      "layer   3  Sparsity: 76.7987%\n",
      "total_backward_count 1262910 real_backward_count 265523  21.025%\n",
      "fc layer 1 self.abs_max_out: 13309.0\n",
      "lif layer 1 self.abs_max_v: 13309.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  7.371242/ 27.794432, val:  45.00%, val_best:  52.92%, tr:  97.45%, tr_best:  98.26%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 97.2959%\n",
      "layer   3  Sparsity: 77.5677%\n",
      "total_backward_count 1272700 real_backward_count 267227  20.997%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  7.305708/ 55.904770, val:  33.75%, val_best:  52.92%, tr:  97.04%, tr_best:  98.26%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 97.2892%\n",
      "layer   3  Sparsity: 76.9005%\n",
      "total_backward_count 1282490 real_backward_count 268891  20.966%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  7.595366/ 31.311201, val:  44.17%, val_best:  52.92%, tr:  97.34%, tr_best:  98.26%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 97.2916%\n",
      "layer   3  Sparsity: 77.2508%\n",
      "total_backward_count 1292280 real_backward_count 270591  20.939%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  7.081135/ 55.362129, val:  27.92%, val_best:  52.92%, tr:  96.83%, tr_best:  98.26%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 97.3362%\n",
      "layer   3  Sparsity: 77.4985%\n",
      "total_backward_count 1302070 real_backward_count 272252  20.909%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  7.461338/ 24.192995, val:  42.92%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 97.3248%\n",
      "layer   3  Sparsity: 77.3353%\n",
      "total_backward_count 1311860 real_backward_count 273949  20.882%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  7.398694/ 35.304138, val:  32.50%, val_best:  52.92%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 97.2501%\n",
      "layer   3  Sparsity: 77.4930%\n",
      "total_backward_count 1321650 real_backward_count 275654  20.857%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  7.549287/ 37.215828, val:  37.08%, val_best:  52.92%, tr:  96.73%, tr_best:  98.26%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 97.2639%\n",
      "layer   3  Sparsity: 77.1669%\n",
      "total_backward_count 1331440 real_backward_count 277414  20.836%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  7.009557/ 38.867764, val:  41.25%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 97.3616%\n",
      "layer   3  Sparsity: 77.6046%\n",
      "total_backward_count 1341230 real_backward_count 279045  20.805%\n",
      "fc layer 1 self.abs_max_out: 13355.0\n",
      "lif layer 1 self.abs_max_v: 13355.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  7.723448/ 35.867626, val:  43.75%, val_best:  52.92%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 97.3401%\n",
      "layer   3  Sparsity: 77.3237%\n",
      "total_backward_count 1351020 real_backward_count 280798  20.784%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  7.199343/ 22.509832, val:  45.83%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 97.3098%\n",
      "layer   3  Sparsity: 78.1782%\n",
      "total_backward_count 1360810 real_backward_count 282526  20.762%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  7.482277/ 38.671509, val:  41.25%, val_best:  52.92%, tr:  97.34%, tr_best:  98.26%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 97.2749%\n",
      "layer   3  Sparsity: 78.5477%\n",
      "total_backward_count 1370600 real_backward_count 284317  20.744%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  6.860947/ 40.168053, val:  33.33%, val_best:  52.92%, tr:  95.81%, tr_best:  98.26%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 97.2586%\n",
      "layer   3  Sparsity: 79.2811%\n",
      "total_backward_count 1380390 real_backward_count 286063  20.723%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  7.231939/ 34.613144, val:  45.83%, val_best:  52.92%, tr:  95.71%, tr_best:  98.26%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 97.3171%\n",
      "layer   3  Sparsity: 79.1290%\n",
      "total_backward_count 1390180 real_backward_count 287886  20.709%\n",
      "fc layer 1 self.abs_max_out: 13467.0\n",
      "lif layer 1 self.abs_max_v: 13467.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  6.833468/ 21.921387, val:  52.92%, val_best:  52.92%, tr:  96.22%, tr_best:  98.26%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 97.3426%\n",
      "layer   3  Sparsity: 80.3785%\n",
      "total_backward_count 1399970 real_backward_count 289743  20.696%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  6.480145/ 39.202042, val:  39.58%, val_best:  52.92%, tr:  96.22%, tr_best:  98.26%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 97.3275%\n",
      "layer   3  Sparsity: 80.5960%\n",
      "total_backward_count 1409760 real_backward_count 291475  20.676%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  6.565756/ 37.035545, val:  32.08%, val_best:  52.92%, tr:  96.42%, tr_best:  98.26%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 97.2922%\n",
      "layer   3  Sparsity: 80.4118%\n",
      "total_backward_count 1419550 real_backward_count 293188  20.654%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  6.685988/ 22.776907, val:  42.08%, val_best:  52.92%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 97.2723%\n",
      "layer   3  Sparsity: 80.2648%\n",
      "total_backward_count 1429340 real_backward_count 294969  20.637%\n",
      "fc layer 1 self.abs_max_out: 13492.0\n",
      "lif layer 1 self.abs_max_v: 13492.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  7.056941/ 29.356071, val:  42.50%, val_best:  52.92%, tr:  96.73%, tr_best:  98.26%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 97.3211%\n",
      "layer   3  Sparsity: 79.6473%\n",
      "total_backward_count 1439130 real_backward_count 296768  20.621%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  6.963440/ 59.665855, val:  26.67%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 97.3146%\n",
      "layer   3  Sparsity: 78.1435%\n",
      "total_backward_count 1448920 real_backward_count 298459  20.599%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  7.438454/ 33.603050, val:  41.25%, val_best:  52.92%, tr:  97.04%, tr_best:  98.26%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 97.2936%\n",
      "layer   3  Sparsity: 77.7576%\n",
      "total_backward_count 1458710 real_backward_count 300209  20.580%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  7.526732/ 36.481983, val:  35.42%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 97.2969%\n",
      "layer   3  Sparsity: 77.7801%\n",
      "total_backward_count 1468500 real_backward_count 301964  20.563%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  7.170580/ 37.017136, val:  34.58%, val_best:  52.92%, tr:  96.83%, tr_best:  98.26%, epoch time: 71.06 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 97.2835%\n",
      "layer   3  Sparsity: 77.5616%\n",
      "total_backward_count 1478290 real_backward_count 303667  20.542%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  7.697132/ 40.205654, val:  40.00%, val_best:  52.92%, tr:  96.63%, tr_best:  98.26%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 97.2536%\n",
      "layer   3  Sparsity: 76.7695%\n",
      "total_backward_count 1488080 real_backward_count 305394  20.523%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  7.690613/ 38.208031, val:  33.75%, val_best:  52.92%, tr:  97.14%, tr_best:  98.26%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 97.2961%\n",
      "layer   3  Sparsity: 77.6517%\n",
      "total_backward_count 1497870 real_backward_count 307144  20.505%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  7.885058/ 38.419685, val:  36.67%, val_best:  52.92%, tr:  96.22%, tr_best:  98.26%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 97.2875%\n",
      "layer   3  Sparsity: 76.5381%\n",
      "total_backward_count 1507660 real_backward_count 308905  20.489%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  7.624500/ 32.174633, val:  45.42%, val_best:  52.92%, tr:  95.91%, tr_best:  98.26%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 97.3228%\n",
      "layer   3  Sparsity: 76.5753%\n",
      "total_backward_count 1517450 real_backward_count 310619  20.470%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  7.398386/ 47.577625, val:  32.08%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 97.3015%\n",
      "layer   3  Sparsity: 76.5463%\n",
      "total_backward_count 1527240 real_backward_count 312303  20.449%\n",
      "fc layer 1 self.abs_max_out: 13536.0\n",
      "lif layer 1 self.abs_max_v: 13536.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  7.847296/ 43.314262, val:  41.67%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 97.3048%\n",
      "layer   3  Sparsity: 76.3163%\n",
      "total_backward_count 1537030 real_backward_count 314036  20.431%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  7.424974/ 34.081482, val:  45.83%, val_best:  52.92%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 97.3031%\n",
      "layer   3  Sparsity: 75.8274%\n",
      "total_backward_count 1546820 real_backward_count 315682  20.408%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  7.717189/ 39.932293, val:  26.67%, val_best:  52.92%, tr:  95.30%, tr_best:  98.26%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 97.2765%\n",
      "layer   3  Sparsity: 75.8462%\n",
      "total_backward_count 1556610 real_backward_count 317368  20.388%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  7.483323/ 38.235626, val:  43.75%, val_best:  52.92%, tr:  95.71%, tr_best:  98.26%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 97.3235%\n",
      "layer   3  Sparsity: 76.4493%\n",
      "total_backward_count 1566400 real_backward_count 319069  20.370%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  7.605612/ 29.597160, val:  50.00%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 97.2984%\n",
      "layer   3  Sparsity: 76.1253%\n",
      "total_backward_count 1576190 real_backward_count 320757  20.350%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  7.694366/ 51.477943, val:  30.00%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 97.2925%\n",
      "layer   3  Sparsity: 75.6084%\n",
      "total_backward_count 1585980 real_backward_count 322427  20.330%\n",
      "fc layer 1 self.abs_max_out: 13602.0\n",
      "lif layer 1 self.abs_max_v: 13602.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  7.959508/ 41.680519, val:  27.50%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 97.3265%\n",
      "layer   3  Sparsity: 75.3003%\n",
      "total_backward_count 1595770 real_backward_count 324108  20.310%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  7.552301/ 30.346130, val:  48.75%, val_best:  52.92%, tr:  95.71%, tr_best:  98.26%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 97.3384%\n",
      "layer   3  Sparsity: 75.6392%\n",
      "total_backward_count 1605560 real_backward_count 325733  20.288%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  7.414701/ 46.129578, val:  36.67%, val_best:  52.92%, tr:  96.22%, tr_best:  98.26%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 97.3099%\n",
      "layer   3  Sparsity: 75.2726%\n",
      "total_backward_count 1615350 real_backward_count 327359  20.266%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  7.623143/ 33.091347, val:  39.17%, val_best:  52.92%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 97.3023%\n",
      "layer   3  Sparsity: 76.1964%\n",
      "total_backward_count 1625140 real_backward_count 329103  20.251%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  7.255685/ 42.674351, val:  45.42%, val_best:  52.92%, tr:  96.94%, tr_best:  98.26%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 97.2941%\n",
      "layer   3  Sparsity: 76.8109%\n",
      "total_backward_count 1634930 real_backward_count 330785  20.232%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  7.326249/ 42.499134, val:  33.33%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 97.3013%\n",
      "layer   3  Sparsity: 76.9217%\n",
      "total_backward_count 1644720 real_backward_count 332487  20.215%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  7.190995/ 54.224209, val:  31.25%, val_best:  52.92%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 97.3229%\n",
      "layer   3  Sparsity: 76.7533%\n",
      "total_backward_count 1654510 real_backward_count 334185  20.198%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  7.394844/ 32.754658, val:  31.67%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 97.3013%\n",
      "layer   3  Sparsity: 76.5740%\n",
      "total_backward_count 1664300 real_backward_count 335899  20.183%\n",
      "fc layer 1 self.abs_max_out: 13646.0\n",
      "lif layer 1 self.abs_max_v: 13646.0\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  7.666442/ 36.000587, val:  36.67%, val_best:  52.92%, tr:  95.81%, tr_best:  98.26%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 97.3450%\n",
      "layer   3  Sparsity: 76.9096%\n",
      "total_backward_count 1674090 real_backward_count 337663  20.170%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  7.093587/ 35.419231, val:  45.00%, val_best:  52.92%, tr:  97.24%, tr_best:  98.26%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 97.3796%\n",
      "layer   3  Sparsity: 77.3968%\n",
      "total_backward_count 1683880 real_backward_count 339361  20.154%\n",
      "fc layer 1 self.abs_max_out: 13699.0\n",
      "lif layer 1 self.abs_max_v: 13699.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  7.450151/ 26.155245, val:  40.83%, val_best:  52.92%, tr:  95.81%, tr_best:  98.26%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 97.3799%\n",
      "layer   3  Sparsity: 77.5864%\n",
      "total_backward_count 1693670 real_backward_count 341111  20.140%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  7.035311/ 39.003235, val:  35.83%, val_best:  52.92%, tr:  96.32%, tr_best:  98.26%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 97.3532%\n",
      "layer   3  Sparsity: 76.8226%\n",
      "total_backward_count 1703460 real_backward_count 342763  20.122%\n",
      "fc layer 1 self.abs_max_out: 13712.0\n",
      "lif layer 1 self.abs_max_v: 13712.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  7.929536/ 23.413263, val:  54.58%, val_best:  54.58%, tr:  95.51%, tr_best:  98.26%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 97.3150%\n",
      "layer   3  Sparsity: 75.7160%\n",
      "total_backward_count 1713250 real_backward_count 344504  20.108%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  7.774985/ 43.327736, val:  29.58%, val_best:  54.58%, tr:  96.83%, tr_best:  98.26%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 97.3305%\n",
      "layer   3  Sparsity: 75.4884%\n",
      "total_backward_count 1723040 real_backward_count 346172  20.091%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  8.065069/ 52.775345, val:  26.67%, val_best:  54.58%, tr:  96.42%, tr_best:  98.26%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 97.3447%\n",
      "layer   3  Sparsity: 75.5009%\n",
      "total_backward_count 1732830 real_backward_count 347897  20.077%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  8.095772/ 43.785656, val:  43.75%, val_best:  54.58%, tr:  96.73%, tr_best:  98.26%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 97.3216%\n",
      "layer   3  Sparsity: 75.5295%\n",
      "total_backward_count 1742620 real_backward_count 349629  20.063%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  8.064650/ 47.327713, val:  42.08%, val_best:  54.58%, tr:  97.24%, tr_best:  98.26%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 97.3328%\n",
      "layer   3  Sparsity: 75.4865%\n",
      "total_backward_count 1752410 real_backward_count 351403  20.053%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  7.996277/ 37.693657, val:  35.83%, val_best:  54.58%, tr:  96.12%, tr_best:  98.26%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 97.3406%\n",
      "layer   3  Sparsity: 75.8382%\n",
      "total_backward_count 1762200 real_backward_count 353197  20.043%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  8.124312/ 32.336246, val:  50.83%, val_best:  54.58%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 97.3767%\n",
      "layer   3  Sparsity: 75.3293%\n",
      "total_backward_count 1771990 real_backward_count 354966  20.032%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  8.263039/ 48.466866, val:  38.33%, val_best:  54.58%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 97.3641%\n",
      "layer   3  Sparsity: 74.7721%\n",
      "total_backward_count 1781780 real_backward_count 356725  20.021%\n",
      "fc layer 1 self.abs_max_out: 13877.0\n",
      "lif layer 1 self.abs_max_v: 13877.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  8.528920/ 40.196915, val:  36.67%, val_best:  54.58%, tr:  96.22%, tr_best:  98.26%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 97.3583%\n",
      "layer   3  Sparsity: 74.1493%\n",
      "total_backward_count 1791570 real_backward_count 358506  20.011%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  8.724431/ 53.382954, val:  30.83%, val_best:  54.58%, tr:  96.53%, tr_best:  98.26%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 97.3400%\n",
      "layer   3  Sparsity: 73.2967%\n",
      "total_backward_count 1801360 real_backward_count 360236  19.998%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  9.081842/ 39.640736, val:  33.33%, val_best:  54.58%, tr:  96.63%, tr_best:  98.26%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 97.3610%\n",
      "layer   3  Sparsity: 73.2639%\n",
      "total_backward_count 1811150 real_backward_count 361995  19.987%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  9.036518/ 54.170101, val:  31.25%, val_best:  54.58%, tr:  96.73%, tr_best:  98.26%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 97.3621%\n",
      "layer   3  Sparsity: 73.2101%\n",
      "total_backward_count 1820940 real_backward_count 363733  19.975%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  8.917656/ 38.227821, val:  49.58%, val_best:  54.58%, tr:  97.75%, tr_best:  98.26%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 97.3824%\n",
      "layer   3  Sparsity: 73.3193%\n",
      "total_backward_count 1830730 real_backward_count 365474  19.963%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  9.052895/ 69.673645, val:  20.83%, val_best:  54.58%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 97.3711%\n",
      "layer   3  Sparsity: 73.0459%\n",
      "total_backward_count 1840520 real_backward_count 367202  19.951%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  8.577871/ 36.218788, val:  45.83%, val_best:  54.58%, tr:  96.53%, tr_best:  98.26%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 97.3444%\n",
      "layer   3  Sparsity: 73.0819%\n",
      "total_backward_count 1850310 real_backward_count 368881  19.936%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  8.679832/ 49.033295, val:  40.42%, val_best:  54.58%, tr:  96.83%, tr_best:  98.26%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 97.3492%\n",
      "layer   3  Sparsity: 73.5208%\n",
      "total_backward_count 1860100 real_backward_count 370584  19.923%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  8.566531/ 33.168240, val:  52.08%, val_best:  54.58%, tr:  96.63%, tr_best:  98.26%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 97.3575%\n",
      "layer   3  Sparsity: 74.0388%\n",
      "total_backward_count 1869890 real_backward_count 372308  19.911%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  8.698884/ 57.721828, val:  35.83%, val_best:  54.58%, tr:  96.83%, tr_best:  98.26%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 97.3626%\n",
      "layer   3  Sparsity: 74.7228%\n",
      "total_backward_count 1879680 real_backward_count 374095  19.902%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  8.590778/ 31.001284, val:  32.08%, val_best:  54.58%, tr:  96.12%, tr_best:  98.26%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 97.3270%\n",
      "layer   3  Sparsity: 74.0959%\n",
      "total_backward_count 1889470 real_backward_count 375845  19.892%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  8.439772/ 48.990463, val:  34.58%, val_best:  54.58%, tr:  96.32%, tr_best:  98.26%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 97.3522%\n",
      "layer   3  Sparsity: 73.6876%\n",
      "total_backward_count 1899260 real_backward_count 377509  19.877%\n",
      "lif layer 1 self.abs_max_v: 14073.5\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  8.572788/ 66.069031, val:  35.00%, val_best:  54.58%, tr:  96.83%, tr_best:  98.26%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 97.3592%\n",
      "layer   3  Sparsity: 73.4002%\n",
      "total_backward_count 1909050 real_backward_count 379217  19.864%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  8.979153/ 48.679810, val:  50.00%, val_best:  54.58%, tr:  96.94%, tr_best:  98.26%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 97.3823%\n",
      "layer   3  Sparsity: 73.1709%\n",
      "total_backward_count 1918840 real_backward_count 380955  19.853%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  8.866893/ 58.293781, val:  36.25%, val_best:  54.58%, tr:  96.63%, tr_best:  98.26%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 97.3506%\n",
      "layer   3  Sparsity: 73.9361%\n",
      "total_backward_count 1928630 real_backward_count 382713  19.844%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  8.440059/ 39.877518, val:  42.50%, val_best:  54.58%, tr:  95.61%, tr_best:  98.26%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 97.3338%\n",
      "layer   3  Sparsity: 73.9867%\n",
      "total_backward_count 1938420 real_backward_count 384439  19.833%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  8.611851/ 45.536488, val:  43.75%, val_best:  54.58%, tr:  96.83%, tr_best:  98.26%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 97.3775%\n",
      "layer   3  Sparsity: 73.9744%\n",
      "total_backward_count 1948210 real_backward_count 386137  19.820%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  8.469177/ 50.642883, val:  40.42%, val_best:  54.58%, tr:  96.22%, tr_best:  98.26%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 97.3053%\n",
      "layer   3  Sparsity: 73.8798%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151b69e78afc492b93fa3749cee0b006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñá‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.96221</td></tr><tr><td>tr_epoch_loss</td><td>8.46918</td></tr><tr><td>val_acc_best</td><td>0.54583</td></tr><tr><td>val_acc_now</td><td>0.40417</td></tr><tr><td>val_loss</td><td>50.64288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-795</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/twim2zsu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/twim2zsu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251210_235530-twim2zsu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g93qmj73 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.015625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251211_041602-g93qmj73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g93qmj73' target=\"_blank\">serene-sweep-809</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g93qmj73' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g93qmj73</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251211_041610_728', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.015625, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 32, 'lif_layer_v_threshold2': 512, 'init_scaling': [0.03125, 0.25, 0.25], 'learning_rate': 8, 'learning_rate2': 2} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.015625, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 32, self.v_threshold 512\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.25, 0.25])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=0.015625, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.25, 0.25])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=512, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.25, 0.25])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 8\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 182.0\n",
      "lif layer 1 self.abs_max_v: 210.0\n",
      "fc layer 2 self.abs_max_out: 184.0\n",
      "lif layer 2 self.abs_max_v: 184.0\n",
      "fc layer 1 self.abs_max_out: 449.0\n",
      "lif layer 1 self.abs_max_v: 546.0\n",
      "fc layer 2 self.abs_max_out: 259.0\n",
      "lif layer 2 self.abs_max_v: 334.0\n",
      "fc layer 1 self.abs_max_out: 795.0\n",
      "lif layer 1 self.abs_max_v: 1062.0\n",
      "lif layer 2 self.abs_max_v: 426.0\n",
      "fc layer 1 self.abs_max_out: 1094.0\n",
      "lif layer 1 self.abs_max_v: 1590.0\n",
      "lif layer 2 self.abs_max_v: 472.0\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "lif layer 1 self.abs_max_v: 2082.0\n",
      "lif layer 2 self.abs_max_v: 495.0\n",
      "fc layer 1 self.abs_max_out: 1651.0\n",
      "lif layer 1 self.abs_max_v: 2659.0\n",
      "lif layer 2 self.abs_max_v: 506.5\n",
      "lif layer 1 self.abs_max_v: 2668.0\n",
      "lif layer 2 self.abs_max_v: 512.5\n",
      "fc layer 1 self.abs_max_out: 2078.0\n",
      "lif layer 1 self.abs_max_v: 2914.0\n",
      "lif layer 1 self.abs_max_v: 3432.5\n",
      "fc layer 1 self.abs_max_out: 2470.0\n",
      "lif layer 1 self.abs_max_v: 4076.5\n",
      "lif layer 2 self.abs_max_v: 514.0\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "lif layer 1 self.abs_max_v: 6228.5\n",
      "lif layer 2 self.abs_max_v: 516.0\n",
      "fc layer 1 self.abs_max_out: 5458.0\n",
      "lif layer 1 self.abs_max_v: 8572.5\n",
      "fc layer 2 self.abs_max_out: 278.0\n",
      "lif layer 2 self.abs_max_v: 524.0\n",
      "fc layer 3 self.abs_max_out: 27.0\n",
      "lif layer 1 self.abs_max_v: 9253.5\n",
      "fc layer 1 self.abs_max_out: 5521.0\n",
      "fc layer 1 self.abs_max_out: 6852.0\n",
      "lif layer 1 self.abs_max_v: 11023.5\n",
      "fc layer 1 self.abs_max_out: 7612.0\n",
      "lif layer 1 self.abs_max_v: 12776.5\n",
      "fc layer 1 self.abs_max_out: 7802.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 566.5\n",
      "lif layer 2 self.abs_max_v: 612.5\n",
      "lif layer 2 self.abs_max_v: 649.5\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 687.0\n",
      "fc layer 1 self.abs_max_out: 9530.0\n",
      "lif layer 1 self.abs_max_v: 14974.0\n",
      "fc layer 1 self.abs_max_out: 10125.0\n",
      "lif layer 1 self.abs_max_v: 17050.0\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "lif layer 2 self.abs_max_v: 707.5\n",
      "fc layer 2 self.abs_max_out: 431.0\n",
      "lif layer 2 self.abs_max_v: 757.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "lif layer 2 self.abs_max_v: 804.0\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "lif layer 2 self.abs_max_v: 833.0\n",
      "lif layer 2 self.abs_max_v: 847.5\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 2 self.abs_max_out: 486.0\n",
      "lif layer 2 self.abs_max_v: 855.0\n",
      "lif layer 2 self.abs_max_v: 858.5\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "fc layer 3 self.abs_max_out: 67.0\n",
      "fc layer 2 self.abs_max_out: 737.0\n",
      "fc layer 1 self.abs_max_out: 10607.0\n",
      "lif layer 1 self.abs_max_v: 18956.0\n",
      "fc layer 1 self.abs_max_out: 10663.0\n",
      "lif layer 1 self.abs_max_v: 20134.0\n",
      "fc layer 1 self.abs_max_out: 11892.0\n",
      "lif layer 1 self.abs_max_v: 20606.5\n",
      "lif layer 2 self.abs_max_v: 860.0\n",
      "lif layer 1 self.abs_max_v: 20788.5\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "fc layer 1 self.abs_max_out: 12942.0\n",
      "lif layer 1 self.abs_max_v: 22109.5\n",
      "fc layer 1 self.abs_max_out: 15897.0\n",
      "lif layer 1 self.abs_max_v: 26930.5\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "lif layer 1 self.abs_max_v: 26972.5\n",
      "lif layer 2 self.abs_max_v: 882.5\n",
      "fc layer 2 self.abs_max_out: 796.0\n",
      "lif layer 2 self.abs_max_v: 944.5\n",
      "lif layer 1 self.abs_max_v: 28452.5\n",
      "lif layer 2 self.abs_max_v: 979.5\n",
      "lif layer 2 self.abs_max_v: 997.0\n",
      "lif layer 2 self.abs_max_v: 1005.5\n",
      "lif layer 2 self.abs_max_v: 1010.0\n",
      "fc layer 2 self.abs_max_out: 852.0\n",
      "lif layer 2 self.abs_max_v: 1012.0\n",
      "lif layer 2 self.abs_max_v: 1012.5\n",
      "lif layer 2 self.abs_max_v: 1013.5\n",
      "lif layer 2 self.abs_max_v: 1030.5\n",
      "lif layer 2 self.abs_max_v: 1046.5\n",
      "fc layer 3 self.abs_max_out: 99.0\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "lif layer 2 self.abs_max_v: 1058.0\n",
      "lif layer 2 self.abs_max_v: 1143.0\n",
      "lif layer 2 self.abs_max_v: 1185.5\n",
      "lif layer 2 self.abs_max_v: 1207.0\n",
      "lif layer 2 self.abs_max_v: 1217.5\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "lif layer 2 self.abs_max_v: 1225.5\n",
      "lif layer 2 self.abs_max_v: 1226.0\n",
      "lif layer 2 self.abs_max_v: 1227.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "fc layer 2 self.abs_max_out: 910.0\n",
      "fc layer 1 self.abs_max_out: 18326.0\n",
      "lif layer 1 self.abs_max_v: 29361.5\n",
      "lif layer 1 self.abs_max_v: 32058.0\n",
      "lif layer 1 self.abs_max_v: 33281.0\n",
      "lif layer 1 self.abs_max_v: 33702.5\n",
      "fc layer 1 self.abs_max_out: 19121.0\n",
      "lif layer 1 self.abs_max_v: 35963.0\n",
      "fc layer 2 self.abs_max_out: 989.0\n",
      "fc layer 2 self.abs_max_out: 1080.0\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "fc layer 2 self.abs_max_out: 1197.0\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "lif layer 2 self.abs_max_v: 1309.0\n",
      "fc layer 1 self.abs_max_out: 21078.0\n",
      "lif layer 1 self.abs_max_v: 37916.0\n",
      "lif layer 1 self.abs_max_v: 39038.0\n",
      "fc layer 2 self.abs_max_out: 1320.0\n",
      "lif layer 2 self.abs_max_v: 1320.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 1 self.abs_max_out: 24028.0\n",
      "lif layer 1 self.abs_max_v: 39199.5\n",
      "fc layer 2 self.abs_max_out: 1344.0\n",
      "lif layer 2 self.abs_max_v: 1344.0\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "fc layer 2 self.abs_max_out: 1472.0\n",
      "lif layer 2 self.abs_max_v: 1472.0\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "lif layer 2 self.abs_max_v: 1513.0\n",
      "fc layer 2 self.abs_max_out: 1533.0\n",
      "lif layer 2 self.abs_max_v: 1533.0\n",
      "fc layer 2 self.abs_max_out: 1576.0\n",
      "lif layer 2 self.abs_max_v: 1576.0\n",
      "fc layer 2 self.abs_max_out: 1644.0\n",
      "lif layer 2 self.abs_max_v: 1644.0\n",
      "fc layer 2 self.abs_max_out: 1685.0\n",
      "lif layer 2 self.abs_max_v: 1685.0\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "lif layer 2 self.abs_max_v: 1733.0\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "lif layer 2 self.abs_max_v: 1736.0\n",
      "lif layer 2 self.abs_max_v: 1741.5\n",
      "lif layer 2 self.abs_max_v: 1752.0\n",
      "lif layer 2 self.abs_max_v: 1757.0\n",
      "fc layer 1 self.abs_max_out: 24110.0\n",
      "lif layer 1 self.abs_max_v: 39972.0\n",
      "fc layer 1 self.abs_max_out: 27649.0\n",
      "lif layer 1 self.abs_max_v: 47631.0\n",
      "lif layer 1 self.abs_max_v: 50650.0\n",
      "fc layer 1 self.abs_max_out: 28596.0\n",
      "lif layer 1 self.abs_max_v: 53917.0\n",
      "lif layer 2 self.abs_max_v: 1833.0\n",
      "lif layer 2 self.abs_max_v: 1886.5\n",
      "lif layer 2 self.abs_max_v: 1913.5\n",
      "lif layer 2 self.abs_max_v: 1946.0\n",
      "lif layer 2 self.abs_max_v: 1958.0\n",
      "lif layer 2 self.abs_max_v: 1969.0\n",
      "lif layer 2 self.abs_max_v: 1972.5\n",
      "lif layer 2 self.abs_max_v: 1976.5\n",
      "lif layer 2 self.abs_max_v: 1978.5\n",
      "lif layer 2 self.abs_max_v: 1991.5\n",
      "lif layer 2 self.abs_max_v: 1992.5\n",
      "lif layer 2 self.abs_max_v: 2020.0\n",
      "lif layer 2 self.abs_max_v: 2040.0\n",
      "lif layer 2 self.abs_max_v: 2050.0\n",
      "lif layer 2 self.abs_max_v: 2051.5\n",
      "lif layer 2 self.abs_max_v: 2062.5\n",
      "lif layer 2 self.abs_max_v: 2107.5\n",
      "fc layer 1 self.abs_max_out: 30147.0\n",
      "lif layer 1 self.abs_max_v: 55999.5\n",
      "lif layer 1 self.abs_max_v: 56001.0\n",
      "epoch-0   lr=['8.0000000'], tr/val_loss:  6.089675/ 11.725621, val:  25.83%, val_best:  25.83%, tr:  82.33%, tr_best:  82.33%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 63.0678%\n",
      "layer   3  Sparsity: 96.8726%\n",
      "total_backward_count 9790 real_backward_count 3517  35.924%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 30599.0\n",
      "lif layer 1 self.abs_max_v: 57079.5\n",
      "fc layer 1 self.abs_max_out: 31171.0\n",
      "fc layer 1 self.abs_max_out: 32290.0\n",
      "lif layer 2 self.abs_max_v: 2231.5\n",
      "lif layer 2 self.abs_max_v: 2272.0\n",
      "lif layer 2 self.abs_max_v: 2336.0\n",
      "fc layer 1 self.abs_max_out: 32535.0\n",
      "lif layer 1 self.abs_max_v: 60227.5\n",
      "lif layer 1 self.abs_max_v: 60375.0\n",
      "epoch-1   lr=['8.0000000'], tr/val_loss:  1.591586/  7.977028, val:  27.50%, val_best:  27.50%, tr:  97.45%, tr_best:  97.45%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 65.8416%\n",
      "layer   3  Sparsity: 97.4187%\n",
      "total_backward_count 19580 real_backward_count 5704  29.132%\n",
      "fc layer 2 self.abs_max_out: 1751.0\n",
      "epoch-2   lr=['8.0000000'], tr/val_loss:  1.117990/  4.213888, val:  31.67%, val_best:  31.67%, tr:  97.14%, tr_best:  97.45%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 65.4812%\n",
      "layer   3  Sparsity: 97.5218%\n",
      "total_backward_count 29370 real_backward_count 7470  25.434%\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "lif layer 2 self.abs_max_v: 2379.5\n",
      "lif layer 2 self.abs_max_v: 2409.5\n",
      "lif layer 2 self.abs_max_v: 2420.0\n",
      "lif layer 2 self.abs_max_v: 2430.0\n",
      "lif layer 2 self.abs_max_v: 2435.0\n",
      "lif layer 2 self.abs_max_v: 2447.5\n",
      "lif layer 2 self.abs_max_v: 2459.5\n",
      "lif layer 2 self.abs_max_v: 2554.0\n",
      "lif layer 2 self.abs_max_v: 2639.0\n",
      "lif layer 2 self.abs_max_v: 2681.5\n",
      "lif layer 2 self.abs_max_v: 2703.0\n",
      "lif layer 2 self.abs_max_v: 2713.5\n",
      "lif layer 2 self.abs_max_v: 2719.0\n",
      "lif layer 2 self.abs_max_v: 2721.5\n",
      "fc layer 1 self.abs_max_out: 32733.0\n",
      "lif layer 1 self.abs_max_v: 60490.0\n",
      "lif layer 1 self.abs_max_v: 60986.0\n",
      "epoch-3   lr=['8.0000000'], tr/val_loss:  1.033705/  6.691908, val:  30.42%, val_best:  31.67%, tr:  97.96%, tr_best:  97.96%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 65.4261%\n",
      "layer   3  Sparsity: 97.5989%\n",
      "total_backward_count 39160 real_backward_count 9216  23.534%\n",
      "lif layer 2 self.abs_max_v: 2742.0\n",
      "lif layer 2 self.abs_max_v: 2759.0\n",
      "lif layer 2 self.abs_max_v: 2767.5\n",
      "lif layer 2 self.abs_max_v: 2830.0\n",
      "lif layer 2 self.abs_max_v: 2904.0\n",
      "lif layer 2 self.abs_max_v: 2960.0\n",
      "lif layer 2 self.abs_max_v: 2962.0\n",
      "lif layer 2 self.abs_max_v: 2989.0\n",
      "lif layer 2 self.abs_max_v: 3002.5\n",
      "lif layer 2 self.abs_max_v: 3009.5\n",
      "lif layer 2 self.abs_max_v: 3014.0\n",
      "lif layer 2 self.abs_max_v: 3035.0\n",
      "lif layer 2 self.abs_max_v: 3045.5\n",
      "lif layer 2 self.abs_max_v: 3051.0\n",
      "epoch-4   lr=['8.0000000'], tr/val_loss:  0.956153/  5.675903, val:  23.33%, val_best:  31.67%, tr:  97.85%, tr_best:  97.96%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 65.7361%\n",
      "layer   3  Sparsity: 97.6682%\n",
      "total_backward_count 48950 real_backward_count 10865  22.196%\n",
      "fc layer 2 self.abs_max_out: 1842.0\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "epoch-5   lr=['8.0000000'], tr/val_loss:  0.991690/  4.125073, val:  27.92%, val_best:  31.67%, tr:  96.73%, tr_best:  97.96%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 66.7493%\n",
      "layer   3  Sparsity: 97.9340%\n",
      "total_backward_count 58740 real_backward_count 12750  21.706%\n",
      "fc layer 2 self.abs_max_out: 1982.0\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "fc layer 2 self.abs_max_out: 2042.0\n",
      "lif layer 2 self.abs_max_v: 3089.0\n",
      "lif layer 2 self.abs_max_v: 3113.5\n",
      "lif layer 2 self.abs_max_v: 3126.0\n",
      "lif layer 2 self.abs_max_v: 3132.0\n",
      "lif layer 2 self.abs_max_v: 3135.0\n",
      "lif layer 2 self.abs_max_v: 3162.0\n",
      "lif layer 2 self.abs_max_v: 3213.0\n",
      "lif layer 2 self.abs_max_v: 3238.5\n",
      "lif layer 2 self.abs_max_v: 3251.5\n",
      "lif layer 2 self.abs_max_v: 3258.0\n",
      "lif layer 2 self.abs_max_v: 3261.0\n",
      "lif layer 2 self.abs_max_v: 3282.0\n",
      "lif layer 2 self.abs_max_v: 3373.0\n",
      "lif layer 2 self.abs_max_v: 3418.5\n",
      "lif layer 2 self.abs_max_v: 3441.5\n",
      "lif layer 2 self.abs_max_v: 3453.0\n",
      "lif layer 2 self.abs_max_v: 3456.0\n",
      "lif layer 2 self.abs_max_v: 3460.0\n",
      "lif layer 2 self.abs_max_v: 3461.0\n",
      "epoch-6   lr=['8.0000000'], tr/val_loss:  0.949730/  4.689044, val:  30.00%, val_best:  31.67%, tr:  97.55%, tr_best:  97.96%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 66.3069%\n",
      "layer   3  Sparsity: 97.9450%\n",
      "total_backward_count 68530 real_backward_count 14560  21.246%\n",
      "lif layer 2 self.abs_max_v: 3472.0\n",
      "lif layer 2 self.abs_max_v: 3485.0\n",
      "lif layer 2 self.abs_max_v: 3491.5\n",
      "lif layer 2 self.abs_max_v: 3498.0\n",
      "lif layer 2 self.abs_max_v: 3499.0\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "lif layer 2 self.abs_max_v: 3501.0\n",
      "fc layer 2 self.abs_max_out: 2132.0\n",
      "fc layer 2 self.abs_max_out: 2152.0\n",
      "fc layer 2 self.abs_max_out: 2192.0\n",
      "lif layer 2 self.abs_max_v: 3512.5\n",
      "lif layer 2 self.abs_max_v: 3519.5\n",
      "lif layer 2 self.abs_max_v: 3523.0\n",
      "fc layer 2 self.abs_max_out: 2266.0\n",
      "lif layer 1 self.abs_max_v: 61160.0\n",
      "epoch-7   lr=['8.0000000'], tr/val_loss:  0.890384/  3.984791, val:  36.25%, val_best:  36.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 64.9580%\n",
      "layer   3  Sparsity: 97.6893%\n",
      "total_backward_count 78320 real_backward_count 16200  20.684%\n",
      "lif layer 2 self.abs_max_v: 3532.5\n",
      "lif layer 2 self.abs_max_v: 3546.5\n",
      "lif layer 2 self.abs_max_v: 3548.5\n",
      "lif layer 2 self.abs_max_v: 3553.5\n",
      "epoch-8   lr=['8.0000000'], tr/val_loss:  1.027805/  5.160419, val:  28.33%, val_best:  36.25%, tr:  96.94%, tr_best:  99.28%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 65.2662%\n",
      "layer   3  Sparsity: 97.8417%\n",
      "total_backward_count 88110 real_backward_count 17991  20.419%\n",
      "lif layer 2 self.abs_max_v: 3557.0\n",
      "fc layer 2 self.abs_max_out: 2276.0\n",
      "lif layer 2 self.abs_max_v: 3561.5\n",
      "lif layer 2 self.abs_max_v: 3572.0\n",
      "lif layer 2 self.abs_max_v: 3586.0\n",
      "lif layer 2 self.abs_max_v: 3593.0\n",
      "lif layer 2 self.abs_max_v: 3596.5\n",
      "fc layer 1 self.abs_max_out: 32777.0\n",
      "fc layer 1 self.abs_max_out: 33149.0\n",
      "lif layer 1 self.abs_max_v: 62204.0\n",
      "lif layer 1 self.abs_max_v: 63005.0\n",
      "epoch-9   lr=['8.0000000'], tr/val_loss:  1.180172/  8.198182, val:  17.92%, val_best:  36.25%, tr:  96.94%, tr_best:  99.28%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 64.3041%\n",
      "layer   3  Sparsity: 97.4393%\n",
      "total_backward_count 97900 real_backward_count 19770  20.194%\n",
      "fc layer 1 self.abs_max_out: 33561.0\n",
      "lif layer 1 self.abs_max_v: 63476.0\n",
      "lif layer 1 self.abs_max_v: 64395.0\n",
      "epoch-10  lr=['8.0000000'], tr/val_loss:  1.099715/  3.813185, val:  36.67%, val_best:  36.67%, tr:  98.67%, tr_best:  99.28%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 64.2927%\n",
      "layer   3  Sparsity: 97.3716%\n",
      "total_backward_count 107690 real_backward_count 21503  19.967%\n",
      "fc layer 1 self.abs_max_out: 33992.0\n",
      "lif layer 1 self.abs_max_v: 65062.0\n",
      "epoch-11  lr=['8.0000000'], tr/val_loss:  0.954691/  5.607816, val:  20.83%, val_best:  36.67%, tr:  98.47%, tr_best:  99.28%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 64.6648%\n",
      "layer   3  Sparsity: 97.7345%\n",
      "total_backward_count 117480 real_backward_count 23230  19.774%\n",
      "fc layer 1 self.abs_max_out: 34578.0\n",
      "lif layer 1 self.abs_max_v: 65505.5\n",
      "lif layer 1 self.abs_max_v: 66202.5\n",
      "epoch-12  lr=['8.0000000'], tr/val_loss:  0.889386/  6.285170, val:  32.92%, val_best:  36.67%, tr:  98.16%, tr_best:  99.28%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 64.6367%\n",
      "layer   3  Sparsity: 97.6097%\n",
      "total_backward_count 127270 real_backward_count 24871  19.542%\n",
      "lif layer 2 self.abs_max_v: 3598.5\n",
      "lif layer 2 self.abs_max_v: 3613.5\n",
      "lif layer 2 self.abs_max_v: 3621.0\n",
      "lif layer 2 self.abs_max_v: 3624.5\n",
      "epoch-13  lr=['8.0000000'], tr/val_loss:  0.918074/  6.589209, val:  22.92%, val_best:  36.67%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 65.0098%\n",
      "layer   3  Sparsity: 97.3021%\n",
      "total_backward_count 137060 real_backward_count 26384  19.250%\n",
      "epoch-14  lr=['8.0000000'], tr/val_loss:  0.888671/  8.385052, val:  17.50%, val_best:  36.67%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 65.0611%\n",
      "layer   3  Sparsity: 97.2867%\n",
      "total_backward_count 146850 real_backward_count 27763  18.906%\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2316.0\n",
      "epoch-15  lr=['8.0000000'], tr/val_loss:  0.933672/  5.520498, val:  35.00%, val_best:  36.67%, tr:  98.77%, tr_best:  99.39%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 64.9763%\n",
      "layer   3  Sparsity: 97.2651%\n",
      "total_backward_count 156640 real_backward_count 29242  18.668%\n",
      "epoch-16  lr=['8.0000000'], tr/val_loss:  1.019054/  5.537776, val:  28.75%, val_best:  36.67%, tr:  98.16%, tr_best:  99.39%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 65.5368%\n",
      "layer   3  Sparsity: 97.3614%\n",
      "total_backward_count 166430 real_backward_count 30786  18.498%\n",
      "fc layer 2 self.abs_max_out: 2416.0\n",
      "lif layer 2 self.abs_max_v: 3634.5\n",
      "lif layer 2 self.abs_max_v: 3640.5\n",
      "lif layer 2 self.abs_max_v: 3642.0\n",
      "lif layer 2 self.abs_max_v: 3642.5\n",
      "epoch-17  lr=['8.0000000'], tr/val_loss:  1.005217/  6.674218, val:  24.58%, val_best:  36.67%, tr:  98.26%, tr_best:  99.39%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 65.1431%\n",
      "layer   3  Sparsity: 97.3267%\n",
      "total_backward_count 176220 real_backward_count 32349  18.357%\n",
      "epoch-18  lr=['8.0000000'], tr/val_loss:  0.944524/  5.716639, val:  33.75%, val_best:  36.67%, tr:  97.96%, tr_best:  99.39%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 64.3682%\n",
      "layer   3  Sparsity: 97.2741%\n",
      "total_backward_count 186010 real_backward_count 33856  18.201%\n",
      "lif layer 2 self.abs_max_v: 3657.5\n",
      "lif layer 2 self.abs_max_v: 3672.0\n",
      "lif layer 2 self.abs_max_v: 3679.0\n",
      "lif layer 2 self.abs_max_v: 3682.5\n",
      "epoch-19  lr=['8.0000000'], tr/val_loss:  0.927999/  5.990610, val:  23.33%, val_best:  36.67%, tr:  98.06%, tr_best:  99.39%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 64.7430%\n",
      "layer   3  Sparsity: 97.2347%\n",
      "total_backward_count 195800 real_backward_count 35311  18.034%\n",
      "lif layer 2 self.abs_max_v: 3713.0\n",
      "lif layer 2 self.abs_max_v: 3730.5\n",
      "lif layer 2 self.abs_max_v: 3739.5\n",
      "lif layer 2 self.abs_max_v: 3744.0\n",
      "epoch-20  lr=['8.0000000'], tr/val_loss:  0.930259/  6.929924, val:  26.25%, val_best:  36.67%, tr:  97.45%, tr_best:  99.39%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 64.7564%\n",
      "layer   3  Sparsity: 97.1881%\n",
      "total_backward_count 205590 real_backward_count 36738  17.870%\n",
      "lif layer 2 self.abs_max_v: 3766.0\n",
      "lif layer 2 self.abs_max_v: 3771.0\n",
      "lif layer 2 self.abs_max_v: 3779.5\n",
      "lif layer 2 self.abs_max_v: 3784.0\n",
      "lif layer 2 self.abs_max_v: 3784.5\n",
      "epoch-21  lr=['8.0000000'], tr/val_loss:  0.927960/  7.345694, val:  22.92%, val_best:  36.67%, tr:  97.85%, tr_best:  99.39%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 64.1072%\n",
      "layer   3  Sparsity: 97.3678%\n",
      "total_backward_count 215380 real_backward_count 38226  17.748%\n",
      "lif layer 2 self.abs_max_v: 3810.5\n",
      "lif layer 2 self.abs_max_v: 3839.5\n",
      "lif layer 2 self.abs_max_v: 3854.0\n",
      "lif layer 2 self.abs_max_v: 3861.0\n",
      "lif layer 2 self.abs_max_v: 3864.5\n",
      "lif layer 2 self.abs_max_v: 3876.0\n",
      "lif layer 2 self.abs_max_v: 3912.0\n",
      "lif layer 2 self.abs_max_v: 3930.0\n",
      "lif layer 2 self.abs_max_v: 3939.0\n",
      "epoch-22  lr=['8.0000000'], tr/val_loss:  0.845438/  4.723410, val:  32.50%, val_best:  36.67%, tr:  98.37%, tr_best:  99.39%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 64.3690%\n",
      "layer   3  Sparsity: 97.4553%\n",
      "total_backward_count 225170 real_backward_count 39612  17.592%\n",
      "lif layer 2 self.abs_max_v: 3941.0\n",
      "epoch-23  lr=['8.0000000'], tr/val_loss:  0.818789/  8.153444, val:  23.75%, val_best:  36.67%, tr:  97.45%, tr_best:  99.39%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 63.3013%\n",
      "layer   3  Sparsity: 97.4317%\n",
      "total_backward_count 234960 real_backward_count 40961  17.433%\n",
      "epoch-24  lr=['8.0000000'], tr/val_loss:  0.839266/  4.517061, val:  32.50%, val_best:  36.67%, tr:  98.57%, tr_best:  99.39%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 63.9806%\n",
      "layer   3  Sparsity: 97.4705%\n",
      "total_backward_count 244750 real_backward_count 42330  17.295%\n",
      "lif layer 2 self.abs_max_v: 3978.5\n",
      "lif layer 2 self.abs_max_v: 4035.5\n",
      "lif layer 2 self.abs_max_v: 4063.0\n",
      "epoch-25  lr=['8.0000000'], tr/val_loss:  0.847612/  7.042431, val:  20.83%, val_best:  36.67%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 63.9902%\n",
      "layer   3  Sparsity: 97.4764%\n",
      "total_backward_count 254540 real_backward_count 43709  17.172%\n",
      "lif layer 2 self.abs_max_v: 4088.5\n",
      "lif layer 2 self.abs_max_v: 4093.5\n",
      "epoch-26  lr=['8.0000000'], tr/val_loss:  0.802837/  3.985225, val:  34.17%, val_best:  36.67%, tr:  97.55%, tr_best:  99.39%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 63.9766%\n",
      "layer   3  Sparsity: 97.4974%\n",
      "total_backward_count 264330 real_backward_count 45039  17.039%\n",
      "lif layer 2 self.abs_max_v: 4138.5\n",
      "lif layer 2 self.abs_max_v: 4158.5\n",
      "fc layer 1 self.abs_max_out: 34682.0\n",
      "epoch-27  lr=['8.0000000'], tr/val_loss:  0.860200/  3.835849, val:  39.17%, val_best:  39.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 64.2163%\n",
      "layer   3  Sparsity: 97.5076%\n",
      "total_backward_count 274120 real_backward_count 46434  16.939%\n",
      "lif layer 2 self.abs_max_v: 4171.5\n",
      "lif layer 2 self.abs_max_v: 4172.0\n",
      "lif layer 2 self.abs_max_v: 4179.5\n",
      "lif layer 2 self.abs_max_v: 4196.0\n",
      "lif layer 2 self.abs_max_v: 4231.5\n",
      "lif layer 2 self.abs_max_v: 4262.0\n",
      "lif layer 2 self.abs_max_v: 4277.0\n",
      "lif layer 2 self.abs_max_v: 4284.5\n",
      "epoch-28  lr=['8.0000000'], tr/val_loss:  0.819973/  4.237458, val:  29.58%, val_best:  39.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 64.5437%\n",
      "layer   3  Sparsity: 97.5171%\n",
      "total_backward_count 283910 real_backward_count 47798  16.836%\n",
      "lif layer 2 self.abs_max_v: 4288.0\n",
      "lif layer 2 self.abs_max_v: 4305.5\n",
      "lif layer 2 self.abs_max_v: 4322.0\n",
      "lif layer 2 self.abs_max_v: 4330.0\n",
      "lif layer 2 self.abs_max_v: 4342.5\n",
      "lif layer 2 self.abs_max_v: 4360.5\n",
      "lif layer 2 self.abs_max_v: 4369.5\n",
      "lif layer 2 self.abs_max_v: 4374.0\n",
      "fc layer 1 self.abs_max_out: 34914.0\n",
      "lif layer 1 self.abs_max_v: 66537.0\n",
      "epoch-29  lr=['8.0000000'], tr/val_loss:  0.804249/  4.410035, val:  30.00%, val_best:  39.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 63.8921%\n",
      "layer   3  Sparsity: 97.4914%\n",
      "total_backward_count 293700 real_backward_count 49143  16.732%\n",
      "lif layer 2 self.abs_max_v: 4381.5\n",
      "lif layer 2 self.abs_max_v: 4386.0\n",
      "fc layer 1 self.abs_max_out: 35145.0\n",
      "lif layer 1 self.abs_max_v: 66633.0\n",
      "epoch-30  lr=['8.0000000'], tr/val_loss:  0.855592/  4.770882, val:  30.83%, val_best:  39.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 64.1460%\n",
      "layer   3  Sparsity: 97.5023%\n",
      "total_backward_count 303490 real_backward_count 50586  16.668%\n",
      "lif layer 2 self.abs_max_v: 4419.5\n",
      "lif layer 2 self.abs_max_v: 4445.0\n",
      "lif layer 2 self.abs_max_v: 4457.5\n",
      "lif layer 2 self.abs_max_v: 4464.0\n",
      "lif layer 2 self.abs_max_v: 4466.0\n",
      "lif layer 2 self.abs_max_v: 4502.5\n",
      "lif layer 2 self.abs_max_v: 4513.0\n",
      "lif layer 2 self.abs_max_v: 4518.5\n",
      "lif layer 2 self.abs_max_v: 4554.5\n",
      "lif layer 2 self.abs_max_v: 4572.5\n",
      "lif layer 2 self.abs_max_v: 4581.5\n",
      "lif layer 2 self.abs_max_v: 4586.0\n",
      "epoch-31  lr=['8.0000000'], tr/val_loss:  0.826323/  5.745732, val:  23.33%, val_best:  39.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 64.5185%\n",
      "layer   3  Sparsity: 97.5692%\n",
      "total_backward_count 313280 real_backward_count 51974  16.590%\n",
      "lif layer 2 self.abs_max_v: 4621.0\n",
      "lif layer 2 self.abs_max_v: 4645.0\n",
      "lif layer 2 self.abs_max_v: 4652.0\n",
      "lif layer 2 self.abs_max_v: 4661.0\n",
      "lif layer 2 self.abs_max_v: 4665.5\n",
      "epoch-32  lr=['8.0000000'], tr/val_loss:  0.816314/  4.383103, val:  31.67%, val_best:  39.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 65.0097%\n",
      "layer   3  Sparsity: 97.6838%\n",
      "total_backward_count 323070 real_backward_count 53425  16.537%\n",
      "lif layer 2 self.abs_max_v: 4676.0\n",
      "lif layer 2 self.abs_max_v: 4713.0\n",
      "lif layer 2 self.abs_max_v: 4731.5\n",
      "lif layer 2 self.abs_max_v: 4741.0\n",
      "lif layer 2 self.abs_max_v: 4745.5\n",
      "epoch-33  lr=['8.0000000'], tr/val_loss:  0.795029/  5.477377, val:  25.42%, val_best:  39.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 64.9852%\n",
      "layer   3  Sparsity: 97.6600%\n",
      "total_backward_count 332860 real_backward_count 54830  16.472%\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "epoch-34  lr=['8.0000000'], tr/val_loss:  0.788049/  3.434148, val:  32.08%, val_best:  39.17%, tr:  98.77%, tr_best:  99.39%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 64.6914%\n",
      "layer   3  Sparsity: 97.6399%\n",
      "total_backward_count 342650 real_backward_count 56214  16.406%\n",
      "fc layer 2 self.abs_max_out: 2447.0\n",
      "fc layer 2 self.abs_max_out: 2536.0\n",
      "epoch-35  lr=['8.0000000'], tr/val_loss:  0.814050/  4.848854, val:  36.25%, val_best:  39.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 65.4325%\n",
      "layer   3  Sparsity: 97.7203%\n",
      "total_backward_count 352440 real_backward_count 57668  16.363%\n",
      "fc layer 2 self.abs_max_out: 2556.0\n",
      "lif layer 2 self.abs_max_v: 4767.0\n",
      "lif layer 2 self.abs_max_v: 4778.5\n",
      "lif layer 2 self.abs_max_v: 4780.5\n",
      "lif layer 2 self.abs_max_v: 4785.5\n",
      "epoch-36  lr=['8.0000000'], tr/val_loss:  0.771993/  3.386449, val:  31.25%, val_best:  39.17%, tr:  98.37%, tr_best:  99.39%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 65.7972%\n",
      "layer   3  Sparsity: 97.6794%\n",
      "total_backward_count 362230 real_backward_count 59024  16.295%\n",
      "epoch-37  lr=['8.0000000'], tr/val_loss:  0.778259/  5.502576, val:  27.92%, val_best:  39.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 67.0671%\n",
      "layer   3  Sparsity: 97.7634%\n",
      "total_backward_count 372020 real_backward_count 60451  16.249%\n",
      "epoch-38  lr=['8.0000000'], tr/val_loss:  0.798891/  5.026859, val:  28.75%, val_best:  39.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 66.8500%\n",
      "layer   3  Sparsity: 97.6985%\n",
      "total_backward_count 381810 real_backward_count 61893  16.210%\n",
      "lif layer 2 self.abs_max_v: 4807.0\n",
      "lif layer 2 self.abs_max_v: 4818.5\n",
      "lif layer 2 self.abs_max_v: 4824.5\n",
      "lif layer 2 self.abs_max_v: 4825.5\n",
      "lif layer 2 self.abs_max_v: 4833.5\n",
      "lif layer 2 self.abs_max_v: 4872.0\n",
      "lif layer 2 self.abs_max_v: 4891.0\n",
      "lif layer 2 self.abs_max_v: 4900.5\n",
      "lif layer 2 self.abs_max_v: 4905.5\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "epoch-39  lr=['8.0000000'], tr/val_loss:  0.804397/  5.704292, val:  20.00%, val_best:  39.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 66.9244%\n",
      "layer   3  Sparsity: 97.6208%\n",
      "total_backward_count 391600 real_backward_count 63300  16.164%\n",
      "fc layer 2 self.abs_max_out: 2616.0\n",
      "lif layer 2 self.abs_max_v: 4946.5\n",
      "lif layer 2 self.abs_max_v: 4968.5\n",
      "lif layer 2 self.abs_max_v: 5050.5\n",
      "lif layer 2 self.abs_max_v: 5071.0\n",
      "epoch-40  lr=['8.0000000'], tr/val_loss:  0.829650/  5.626249, val:  22.08%, val_best:  39.17%, tr:  97.65%, tr_best:  99.39%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 66.5300%\n",
      "layer   3  Sparsity: 97.5702%\n",
      "total_backward_count 401390 real_backward_count 64704  16.120%\n",
      "epoch-41  lr=['8.0000000'], tr/val_loss:  0.831289/  5.011834, val:  29.17%, val_best:  39.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 65.5096%\n",
      "layer   3  Sparsity: 97.5619%\n",
      "total_backward_count 411180 real_backward_count 66143  16.086%\n",
      "fc layer 2 self.abs_max_out: 2702.0\n",
      "epoch-42  lr=['8.0000000'], tr/val_loss:  0.800041/  5.804376, val:  27.92%, val_best:  39.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 65.0649%\n",
      "layer   3  Sparsity: 97.5878%\n",
      "total_backward_count 420970 real_backward_count 67536  16.043%\n",
      "epoch-43  lr=['8.0000000'], tr/val_loss:  0.834848/  4.130349, val:  30.42%, val_best:  39.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 65.6475%\n",
      "layer   3  Sparsity: 97.6200%\n",
      "total_backward_count 430760 real_backward_count 68970  16.011%\n",
      "lif layer 2 self.abs_max_v: 5072.5\n",
      "lif layer 2 self.abs_max_v: 5091.5\n",
      "lif layer 2 self.abs_max_v: 5098.5\n",
      "lif layer 2 self.abs_max_v: 5104.5\n",
      "lif layer 2 self.abs_max_v: 5105.0\n",
      "epoch-44  lr=['8.0000000'], tr/val_loss:  0.805569/  4.271851, val:  35.83%, val_best:  39.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 65.4853%\n",
      "layer   3  Sparsity: 97.6212%\n",
      "total_backward_count 440550 real_backward_count 70398  15.980%\n",
      "epoch-45  lr=['8.0000000'], tr/val_loss:  0.791468/  6.178401, val:  31.25%, val_best:  39.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 65.4943%\n",
      "layer   3  Sparsity: 97.7233%\n",
      "total_backward_count 450340 real_backward_count 71785  15.940%\n",
      "epoch-46  lr=['8.0000000'], tr/val_loss:  0.817851/  6.706139, val:  16.67%, val_best:  39.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 65.1699%\n",
      "layer   3  Sparsity: 97.6309%\n",
      "total_backward_count 460130 real_backward_count 73226  15.914%\n",
      "epoch-47  lr=['8.0000000'], tr/val_loss:  0.787585/  3.943226, val:  32.08%, val_best:  39.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 65.1433%\n",
      "layer   3  Sparsity: 97.5827%\n",
      "total_backward_count 469920 real_backward_count 74596  15.874%\n",
      "epoch-48  lr=['8.0000000'], tr/val_loss:  0.826989/  5.051320, val:  25.83%, val_best:  39.17%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 64.7027%\n",
      "layer   3  Sparsity: 97.5529%\n",
      "total_backward_count 479710 real_backward_count 75974  15.837%\n",
      "epoch-49  lr=['8.0000000'], tr/val_loss:  0.823933/  4.108661, val:  37.50%, val_best:  39.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 64.2106%\n",
      "layer   3  Sparsity: 97.5358%\n",
      "total_backward_count 489500 real_backward_count 77373  15.807%\n",
      "epoch-50  lr=['8.0000000'], tr/val_loss:  0.826083/  4.922223, val:  25.83%, val_best:  39.17%, tr:  98.57%, tr_best:  99.39%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 65.8129%\n",
      "layer   3  Sparsity: 97.5870%\n",
      "total_backward_count 499290 real_backward_count 78799  15.782%\n",
      "epoch-51  lr=['8.0000000'], tr/val_loss:  0.825506/  4.281360, val:  25.83%, val_best:  39.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 65.4934%\n",
      "layer   3  Sparsity: 97.6557%\n",
      "total_backward_count 509080 real_backward_count 80247  15.763%\n",
      "epoch-52  lr=['8.0000000'], tr/val_loss:  0.809671/  4.225375, val:  32.50%, val_best:  39.17%, tr:  97.14%, tr_best:  99.39%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 64.7315%\n",
      "layer   3  Sparsity: 97.8256%\n",
      "total_backward_count 518870 real_backward_count 81707  15.747%\n",
      "epoch-53  lr=['8.0000000'], tr/val_loss:  0.748055/  5.617988, val:  28.33%, val_best:  39.17%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 65.3044%\n",
      "layer   3  Sparsity: 97.9281%\n",
      "total_backward_count 528660 real_backward_count 83139  15.726%\n",
      "epoch-54  lr=['8.0000000'], tr/val_loss:  0.764054/  3.530346, val:  21.67%, val_best:  39.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 64.7302%\n",
      "layer   3  Sparsity: 97.8855%\n",
      "total_backward_count 538450 real_backward_count 84613  15.714%\n",
      "epoch-55  lr=['8.0000000'], tr/val_loss:  0.757890/  5.151190, val:  20.83%, val_best:  39.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 65.5019%\n",
      "layer   3  Sparsity: 97.8866%\n",
      "total_backward_count 548240 real_backward_count 86044  15.695%\n",
      "epoch-56  lr=['8.0000000'], tr/val_loss:  0.747526/  4.060004, val:  25.83%, val_best:  39.17%, tr:  98.67%, tr_best:  99.39%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 64.2932%\n",
      "layer   3  Sparsity: 97.9158%\n",
      "total_backward_count 558030 real_backward_count 87505  15.681%\n",
      "lif layer 2 self.abs_max_v: 5134.0\n",
      "lif layer 2 self.abs_max_v: 5229.0\n",
      "lif layer 2 self.abs_max_v: 5235.5\n",
      "lif layer 2 self.abs_max_v: 5320.0\n",
      "lif layer 2 self.abs_max_v: 5362.0\n",
      "lif layer 2 self.abs_max_v: 5383.0\n",
      "lif layer 2 self.abs_max_v: 5393.5\n",
      "lif layer 2 self.abs_max_v: 5399.0\n",
      "fc layer 2 self.abs_max_out: 2802.0\n",
      "lif layer 2 self.abs_max_v: 5449.5\n",
      "lif layer 2 self.abs_max_v: 5527.0\n",
      "lif layer 2 self.abs_max_v: 5560.5\n",
      "lif layer 2 self.abs_max_v: 5562.0\n",
      "lif layer 2 self.abs_max_v: 5572.5\n",
      "lif layer 2 self.abs_max_v: 5588.5\n",
      "fc layer 2 self.abs_max_out: 2822.0\n",
      "lif layer 2 self.abs_max_v: 5606.0\n",
      "epoch-57  lr=['8.0000000'], tr/val_loss:  0.736350/  3.835625, val:  33.33%, val_best:  39.17%, tr:  98.98%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 65.4447%\n",
      "layer   3  Sparsity: 97.9217%\n",
      "total_backward_count 567820 real_backward_count 88938  15.663%\n",
      "lif layer 2 self.abs_max_v: 5620.5\n",
      "lif layer 2 self.abs_max_v: 5632.5\n",
      "epoch-58  lr=['8.0000000'], tr/val_loss:  0.736241/  3.092520, val:  37.08%, val_best:  39.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 65.2095%\n",
      "layer   3  Sparsity: 97.9603%\n",
      "total_backward_count 577610 real_backward_count 90320  15.637%\n",
      "fc layer 2 self.abs_max_out: 2842.0\n",
      "epoch-59  lr=['8.0000000'], tr/val_loss:  0.722175/  3.554899, val:  32.92%, val_best:  39.17%, tr:  98.26%, tr_best:  99.39%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 66.1733%\n",
      "layer   3  Sparsity: 97.9571%\n",
      "total_backward_count 587400 real_backward_count 91733  15.617%\n",
      "fc layer 2 self.abs_max_out: 2862.0\n",
      "fc layer 2 self.abs_max_out: 2902.0\n",
      "lif layer 2 self.abs_max_v: 5702.5\n",
      "lif layer 2 self.abs_max_v: 5724.0\n",
      "epoch-60  lr=['8.0000000'], tr/val_loss:  0.757462/  3.534420, val:  27.08%, val_best:  39.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 65.4985%\n",
      "layer   3  Sparsity: 97.8343%\n",
      "total_backward_count 597190 real_backward_count 93189  15.605%\n",
      "lif layer 2 self.abs_max_v: 5750.0\n",
      "lif layer 2 self.abs_max_v: 5777.0\n",
      "epoch-61  lr=['8.0000000'], tr/val_loss:  0.798196/  5.507582, val:  22.08%, val_best:  39.17%, tr:  98.47%, tr_best:  99.39%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 64.7839%\n",
      "layer   3  Sparsity: 97.8484%\n",
      "total_backward_count 606980 real_backward_count 94672  15.597%\n",
      "fc layer 2 self.abs_max_out: 2922.0\n",
      "lif layer 2 self.abs_max_v: 5783.0\n",
      "lif layer 2 self.abs_max_v: 5813.5\n",
      "lif layer 2 self.abs_max_v: 5829.0\n",
      "lif layer 2 self.abs_max_v: 5836.5\n",
      "lif layer 2 self.abs_max_v: 5838.5\n",
      "epoch-62  lr=['8.0000000'], tr/val_loss:  0.772218/  5.102779, val:  20.00%, val_best:  39.17%, tr:  96.73%, tr_best:  99.39%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 65.2648%\n",
      "layer   3  Sparsity: 97.8752%\n",
      "total_backward_count 616770 real_backward_count 96107  15.582%\n",
      "epoch-63  lr=['8.0000000'], tr/val_loss:  0.737024/  6.352399, val:  16.25%, val_best:  39.17%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 65.4635%\n",
      "layer   3  Sparsity: 97.8724%\n",
      "total_backward_count 626560 real_backward_count 97473  15.557%\n",
      "epoch-64  lr=['8.0000000'], tr/val_loss:  0.760366/  3.614086, val:  35.00%, val_best:  39.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 65.7538%\n",
      "layer   3  Sparsity: 97.8502%\n",
      "total_backward_count 636350 real_backward_count 98881  15.539%\n",
      "epoch-65  lr=['8.0000000'], tr/val_loss:  0.736391/  4.898582, val:  31.67%, val_best:  39.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 64.3499%\n",
      "layer   3  Sparsity: 97.9388%\n",
      "total_backward_count 646140 real_backward_count 100274  15.519%\n",
      "fc layer 2 self.abs_max_out: 2928.0\n",
      "lif layer 2 self.abs_max_v: 5846.0\n",
      "lif layer 2 self.abs_max_v: 5850.5\n",
      "fc layer 2 self.abs_max_out: 2948.0\n",
      "lif layer 2 self.abs_max_v: 5868.0\n",
      "lif layer 2 self.abs_max_v: 5882.0\n",
      "fc layer 2 self.abs_max_out: 2968.0\n",
      "lif layer 2 self.abs_max_v: 5890.0\n",
      "lif layer 2 self.abs_max_v: 5913.0\n",
      "lif layer 2 self.abs_max_v: 5924.5\n",
      "lif layer 2 self.abs_max_v: 5930.5\n",
      "fc layer 2 self.abs_max_out: 3008.0\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "lif layer 2 self.abs_max_v: 5969.0\n",
      "lif layer 2 self.abs_max_v: 5992.5\n",
      "lif layer 2 self.abs_max_v: 6004.5\n",
      "epoch-66  lr=['8.0000000'], tr/val_loss:  0.749935/  3.831922, val:  35.42%, val_best:  39.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 64.8778%\n",
      "layer   3  Sparsity: 97.9527%\n",
      "total_backward_count 655930 real_backward_count 101701  15.505%\n",
      "lif layer 2 self.abs_max_v: 6010.5\n",
      "lif layer 2 self.abs_max_v: 6014.5\n",
      "lif layer 2 self.abs_max_v: 6035.5\n",
      "lif layer 2 self.abs_max_v: 6044.5\n",
      "lif layer 2 self.abs_max_v: 6050.5\n",
      "epoch-67  lr=['8.0000000'], tr/val_loss:  0.742792/  3.590484, val:  30.83%, val_best:  39.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 65.2508%\n",
      "layer   3  Sparsity: 97.9606%\n",
      "total_backward_count 665720 real_backward_count 103138  15.493%\n",
      "fc layer 2 self.abs_max_out: 3068.0\n",
      "lif layer 2 self.abs_max_v: 6088.5\n",
      "lif layer 2 self.abs_max_v: 6112.5\n",
      "lif layer 2 self.abs_max_v: 6124.5\n",
      "lif layer 2 self.abs_max_v: 6130.5\n",
      "epoch-68  lr=['8.0000000'], tr/val_loss:  0.736870/  3.939557, val:  37.92%, val_best:  39.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 64.7174%\n",
      "layer   3  Sparsity: 97.9719%\n",
      "total_backward_count 675510 real_backward_count 104500  15.470%\n",
      "epoch-69  lr=['8.0000000'], tr/val_loss:  0.750296/  3.840735, val:  27.50%, val_best:  39.17%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 64.3150%\n",
      "layer   3  Sparsity: 98.0636%\n",
      "total_backward_count 685300 real_backward_count 105901  15.453%\n",
      "epoch-70  lr=['8.0000000'], tr/val_loss:  0.774091/  2.910692, val:  33.33%, val_best:  39.17%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 64.4918%\n",
      "layer   3  Sparsity: 98.1365%\n",
      "total_backward_count 695090 real_backward_count 107425  15.455%\n",
      "epoch-71  lr=['8.0000000'], tr/val_loss:  0.757443/  3.553465, val:  25.42%, val_best:  39.17%, tr:  98.37%, tr_best:  99.39%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 64.2799%\n",
      "layer   3  Sparsity: 97.9766%\n",
      "total_backward_count 704880 real_backward_count 108884  15.447%\n",
      "epoch-72  lr=['8.0000000'], tr/val_loss:  0.741397/  3.558288, val:  36.67%, val_best:  39.17%, tr:  97.34%, tr_best:  99.39%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 64.3171%\n",
      "layer   3  Sparsity: 98.0366%\n",
      "total_backward_count 714670 real_backward_count 110346  15.440%\n",
      "epoch-73  lr=['8.0000000'], tr/val_loss:  0.751407/  3.808124, val:  32.08%, val_best:  39.17%, tr:  98.16%, tr_best:  99.39%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 65.6089%\n",
      "layer   3  Sparsity: 98.0352%\n",
      "total_backward_count 724460 real_backward_count 111791  15.431%\n",
      "epoch-74  lr=['8.0000000'], tr/val_loss:  0.737770/  2.257528, val:  34.58%, val_best:  39.17%, tr:  97.65%, tr_best:  99.39%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 66.1715%\n",
      "layer   3  Sparsity: 97.9958%\n",
      "total_backward_count 734250 real_backward_count 113232  15.421%\n",
      "epoch-75  lr=['8.0000000'], tr/val_loss:  0.732893/  3.941239, val:  31.67%, val_best:  39.17%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 65.5029%\n",
      "layer   3  Sparsity: 98.0094%\n",
      "total_backward_count 744040 real_backward_count 114640  15.408%\n",
      "epoch-76  lr=['8.0000000'], tr/val_loss:  0.765792/  3.484920, val:  34.58%, val_best:  39.17%, tr:  97.85%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 66.8163%\n",
      "layer   3  Sparsity: 97.9880%\n",
      "total_backward_count 753830 real_backward_count 116158  15.409%\n",
      "epoch-77  lr=['8.0000000'], tr/val_loss:  0.759696/  5.135583, val:  25.42%, val_best:  39.17%, tr:  97.96%, tr_best:  99.39%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 66.7726%\n",
      "layer   3  Sparsity: 97.9552%\n",
      "total_backward_count 763620 real_backward_count 117642  15.406%\n",
      "epoch-78  lr=['8.0000000'], tr/val_loss:  0.740046/  4.612743, val:  28.75%, val_best:  39.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 66.4658%\n",
      "layer   3  Sparsity: 98.0050%\n",
      "total_backward_count 773410 real_backward_count 119087  15.398%\n",
      "epoch-79  lr=['8.0000000'], tr/val_loss:  0.754651/  4.001211, val:  30.42%, val_best:  39.17%, tr:  98.06%, tr_best:  99.39%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 65.6565%\n",
      "layer   3  Sparsity: 98.0134%\n",
      "total_backward_count 783200 real_backward_count 120590  15.397%\n",
      "epoch-80  lr=['8.0000000'], tr/val_loss:  0.735749/  3.014893, val:  39.58%, val_best:  39.58%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 65.2146%\n",
      "layer   3  Sparsity: 97.9443%\n",
      "total_backward_count 792990 real_backward_count 122030  15.389%\n",
      "epoch-81  lr=['8.0000000'], tr/val_loss:  0.744596/  4.826345, val:  24.17%, val_best:  39.58%, tr:  98.47%, tr_best:  99.39%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 64.6100%\n",
      "layer   3  Sparsity: 97.9382%\n",
      "total_backward_count 802780 real_backward_count 123493  15.383%\n",
      "epoch-82  lr=['8.0000000'], tr/val_loss:  0.776814/  3.870007, val:  37.50%, val_best:  39.58%, tr:  97.65%, tr_best:  99.39%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 63.5655%\n",
      "layer   3  Sparsity: 97.9296%\n",
      "total_backward_count 812570 real_backward_count 124997  15.383%\n",
      "epoch-83  lr=['8.0000000'], tr/val_loss:  0.771439/  3.647720, val:  38.33%, val_best:  39.58%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 64.1968%\n",
      "layer   3  Sparsity: 98.0820%\n",
      "total_backward_count 822360 real_backward_count 126512  15.384%\n",
      "epoch-84  lr=['8.0000000'], tr/val_loss:  0.776511/  4.901279, val:  27.92%, val_best:  39.58%, tr:  96.63%, tr_best:  99.39%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 64.6213%\n",
      "layer   3  Sparsity: 98.1990%\n",
      "total_backward_count 832150 real_backward_count 128028  15.385%\n",
      "epoch-85  lr=['8.0000000'], tr/val_loss:  0.740523/  2.939790, val:  28.75%, val_best:  39.58%, tr:  96.94%, tr_best:  99.39%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 65.7804%\n",
      "layer   3  Sparsity: 98.2429%\n",
      "total_backward_count 841940 real_backward_count 129486  15.379%\n",
      "epoch-86  lr=['8.0000000'], tr/val_loss:  0.777895/  4.334257, val:  27.08%, val_best:  39.58%, tr:  97.04%, tr_best:  99.39%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 65.0259%\n",
      "layer   3  Sparsity: 98.1512%\n",
      "total_backward_count 851730 real_backward_count 131060  15.388%\n",
      "epoch-87  lr=['8.0000000'], tr/val_loss:  0.793417/  3.680031, val:  25.00%, val_best:  39.58%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 65.0009%\n",
      "layer   3  Sparsity: 98.1592%\n",
      "total_backward_count 861520 real_backward_count 132634  15.395%\n",
      "epoch-88  lr=['8.0000000'], tr/val_loss:  0.757909/  3.572686, val:  30.83%, val_best:  39.58%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 64.7477%\n",
      "layer   3  Sparsity: 98.2767%\n",
      "total_backward_count 871310 real_backward_count 134147  15.396%\n",
      "epoch-89  lr=['8.0000000'], tr/val_loss:  0.749765/  3.037076, val:  38.75%, val_best:  39.58%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 64.6388%\n",
      "layer   3  Sparsity: 98.1816%\n",
      "total_backward_count 881100 real_backward_count 135655  15.396%\n",
      "epoch-90  lr=['8.0000000'], tr/val_loss:  0.754719/  3.415889, val:  38.75%, val_best:  39.58%, tr:  98.16%, tr_best:  99.39%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 64.7938%\n",
      "layer   3  Sparsity: 98.2296%\n",
      "total_backward_count 890890 real_backward_count 137212  15.402%\n",
      "epoch-91  lr=['8.0000000'], tr/val_loss:  0.734083/  2.345220, val:  33.75%, val_best:  39.58%, tr:  97.55%, tr_best:  99.39%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 65.5591%\n",
      "layer   3  Sparsity: 98.3135%\n",
      "total_backward_count 900680 real_backward_count 138733  15.403%\n",
      "epoch-92  lr=['8.0000000'], tr/val_loss:  0.740660/  3.068267, val:  32.92%, val_best:  39.58%, tr:  97.65%, tr_best:  99.39%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 65.1467%\n",
      "layer   3  Sparsity: 98.3308%\n",
      "total_backward_count 910470 real_backward_count 140230  15.402%\n",
      "epoch-93  lr=['8.0000000'], tr/val_loss:  0.706764/  3.305732, val:  23.33%, val_best:  39.58%, tr:  97.45%, tr_best:  99.39%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 65.3928%\n",
      "layer   3  Sparsity: 98.2831%\n",
      "total_backward_count 920260 real_backward_count 141725  15.401%\n",
      "epoch-94  lr=['8.0000000'], tr/val_loss:  0.721592/  3.256584, val:  29.17%, val_best:  39.58%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 64.9699%\n",
      "layer   3  Sparsity: 98.3565%\n",
      "total_backward_count 930050 real_backward_count 143196  15.397%\n",
      "epoch-95  lr=['8.0000000'], tr/val_loss:  0.751165/  2.820873, val:  37.50%, val_best:  39.58%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 65.7122%\n",
      "layer   3  Sparsity: 98.4260%\n",
      "total_backward_count 939840 real_backward_count 144800  15.407%\n",
      "fc layer 2 self.abs_max_out: 3088.0\n",
      "fc layer 2 self.abs_max_out: 3168.0\n",
      "epoch-96  lr=['8.0000000'], tr/val_loss:  0.743059/  3.126077, val:  36.25%, val_best:  39.58%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 65.4084%\n",
      "layer   3  Sparsity: 98.4297%\n",
      "total_backward_count 949630 real_backward_count 146338  15.410%\n",
      "lif layer 2 self.abs_max_v: 6192.0\n",
      "lif layer 2 self.abs_max_v: 6224.0\n",
      "lif layer 2 self.abs_max_v: 6240.0\n",
      "lif layer 2 self.abs_max_v: 6248.0\n",
      "epoch-97  lr=['8.0000000'], tr/val_loss:  0.709930/  2.692410, val:  37.08%, val_best:  39.58%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 65.9815%\n",
      "layer   3  Sparsity: 98.3497%\n",
      "total_backward_count 959420 real_backward_count 147787  15.404%\n",
      "fc layer 2 self.abs_max_out: 3228.0\n",
      "lif layer 2 self.abs_max_v: 6254.5\n",
      "lif layer 2 self.abs_max_v: 6355.5\n",
      "lif layer 2 self.abs_max_v: 6406.0\n",
      "lif layer 2 self.abs_max_v: 6431.0\n",
      "lif layer 2 self.abs_max_v: 6443.5\n",
      "fc layer 2 self.abs_max_out: 3308.0\n",
      "lif layer 2 self.abs_max_v: 6449.0\n",
      "lif layer 2 self.abs_max_v: 6477.0\n",
      "lif layer 2 self.abs_max_v: 6513.0\n",
      "lif layer 2 self.abs_max_v: 6564.5\n",
      "lif layer 2 self.abs_max_v: 6590.5\n",
      "lif layer 2 self.abs_max_v: 6603.5\n",
      "lif layer 2 self.abs_max_v: 6610.0\n",
      "epoch-98  lr=['8.0000000'], tr/val_loss:  0.724347/  3.078532, val:  27.92%, val_best:  39.58%, tr:  98.26%, tr_best:  99.39%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 65.3998%\n",
      "layer   3  Sparsity: 98.4208%\n",
      "total_backward_count 969210 real_backward_count 149249  15.399%\n",
      "epoch-99  lr=['8.0000000'], tr/val_loss:  0.744967/  2.449797, val:  37.50%, val_best:  39.58%, tr:  97.45%, tr_best:  99.39%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 65.0944%\n",
      "layer   3  Sparsity: 98.5667%\n",
      "total_backward_count 979000 real_backward_count 150807  15.404%\n",
      "fc layer 2 self.abs_max_out: 3326.0\n",
      "fc layer 2 self.abs_max_out: 3406.0\n",
      "lif layer 2 self.abs_max_v: 6622.5\n",
      "lif layer 2 self.abs_max_v: 6637.5\n",
      "lif layer 2 self.abs_max_v: 6725.0\n",
      "lif layer 2 self.abs_max_v: 6768.5\n",
      "lif layer 2 self.abs_max_v: 6779.0\n",
      "lif layer 2 self.abs_max_v: 6795.5\n",
      "lif layer 2 self.abs_max_v: 6804.0\n",
      "fc layer 2 self.abs_max_out: 3446.0\n",
      "lif layer 2 self.abs_max_v: 6808.5\n",
      "lif layer 2 self.abs_max_v: 6850.5\n",
      "lif layer 2 self.abs_max_v: 6865.5\n",
      "lif layer 2 self.abs_max_v: 6879.0\n",
      "lif layer 2 self.abs_max_v: 6885.5\n",
      "epoch-100 lr=['8.0000000'], tr/val_loss:  0.764824/  3.116420, val:  33.33%, val_best:  39.58%, tr:  95.10%, tr_best:  99.39%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 64.1646%\n",
      "layer   3  Sparsity: 98.6058%\n",
      "total_backward_count 988790 real_backward_count 152355  15.408%\n",
      "epoch-101 lr=['8.0000000'], tr/val_loss:  0.774324/  3.422290, val:  21.67%, val_best:  39.58%, tr:  95.40%, tr_best:  99.39%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 63.8541%\n",
      "layer   3  Sparsity: 98.6238%\n",
      "total_backward_count 998580 real_backward_count 153973  15.419%\n",
      "epoch-102 lr=['8.0000000'], tr/val_loss:  0.751610/  3.350408, val:  23.33%, val_best:  39.58%, tr:  95.91%, tr_best:  99.39%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 63.8560%\n",
      "layer   3  Sparsity: 98.6443%\n",
      "total_backward_count 1008370 real_backward_count 155504  15.421%\n",
      "epoch-103 lr=['8.0000000'], tr/val_loss:  0.765193/  3.499937, val:  32.08%, val_best:  39.58%, tr:  95.71%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 63.7928%\n",
      "layer   3  Sparsity: 98.6154%\n",
      "total_backward_count 1018160 real_backward_count 157071  15.427%\n",
      "epoch-104 lr=['8.0000000'], tr/val_loss:  0.777163/  2.442589, val:  31.25%, val_best:  39.58%, tr:  94.89%, tr_best:  99.39%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 62.7176%\n",
      "layer   3  Sparsity: 98.5849%\n",
      "total_backward_count 1027950 real_backward_count 158603  15.429%\n",
      "epoch-105 lr=['8.0000000'], tr/val_loss:  0.784634/  2.633426, val:  37.92%, val_best:  39.58%, tr:  94.99%, tr_best:  99.39%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 62.8585%\n",
      "layer   3  Sparsity: 98.5830%\n",
      "total_backward_count 1037740 real_backward_count 160179  15.435%\n",
      "epoch-106 lr=['8.0000000'], tr/val_loss:  0.782459/  3.632245, val:  22.08%, val_best:  39.58%, tr:  93.87%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 62.9745%\n",
      "layer   3  Sparsity: 98.5603%\n",
      "total_backward_count 1047530 real_backward_count 161751  15.441%\n",
      "fc layer 2 self.abs_max_out: 3466.0\n",
      "fc layer 2 self.abs_max_out: 3526.0\n",
      "fc layer 2 self.abs_max_out: 3566.0\n",
      "lif layer 2 self.abs_max_v: 6909.5\n",
      "lif layer 2 self.abs_max_v: 7021.0\n",
      "lif layer 2 self.abs_max_v: 7076.5\n",
      "lif layer 2 self.abs_max_v: 7104.5\n",
      "lif layer 2 self.abs_max_v: 7118.5\n",
      "lif layer 2 self.abs_max_v: 7125.5\n",
      "epoch-107 lr=['8.0000000'], tr/val_loss:  0.771454/  2.969469, val:  32.92%, val_best:  39.58%, tr:  95.81%, tr_best:  99.39%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 62.8944%\n",
      "layer   3  Sparsity: 98.5466%\n",
      "total_backward_count 1057320 real_backward_count 163334  15.448%\n",
      "epoch-108 lr=['8.0000000'], tr/val_loss:  0.764481/  3.553218, val:  25.00%, val_best:  39.58%, tr:  94.89%, tr_best:  99.39%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 64.0064%\n",
      "layer   3  Sparsity: 98.5708%\n",
      "total_backward_count 1067110 real_backward_count 164843  15.448%\n",
      "epoch-109 lr=['8.0000000'], tr/val_loss:  0.770372/  2.331810, val:  37.08%, val_best:  39.58%, tr:  95.81%, tr_best:  99.39%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 63.9421%\n",
      "layer   3  Sparsity: 98.6311%\n",
      "total_backward_count 1076900 real_backward_count 166343  15.446%\n",
      "fc layer 2 self.abs_max_out: 3586.0\n",
      "fc layer 2 self.abs_max_out: 3646.0\n",
      "lif layer 2 self.abs_max_v: 7152.5\n",
      "lif layer 2 self.abs_max_v: 7178.5\n",
      "lif layer 2 self.abs_max_v: 7235.5\n",
      "lif layer 2 self.abs_max_v: 7264.0\n",
      "lif layer 2 self.abs_max_v: 7278.0\n",
      "lif layer 2 self.abs_max_v: 7285.0\n",
      "fc layer 2 self.abs_max_out: 3666.0\n",
      "fc layer 2 self.abs_max_out: 3706.0\n",
      "epoch-110 lr=['8.0000000'], tr/val_loss:  0.770136/  4.152479, val:  25.83%, val_best:  39.58%, tr:  94.59%, tr_best:  99.39%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 63.3603%\n",
      "layer   3  Sparsity: 98.6602%\n",
      "total_backward_count 1086690 real_backward_count 167894  15.450%\n",
      "lif layer 2 self.abs_max_v: 7336.0\n",
      "lif layer 2 self.abs_max_v: 7338.0\n",
      "epoch-111 lr=['8.0000000'], tr/val_loss:  0.790850/  3.568395, val:  26.25%, val_best:  39.58%, tr:  94.79%, tr_best:  99.39%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 62.7852%\n",
      "layer   3  Sparsity: 98.5411%\n",
      "total_backward_count 1096480 real_backward_count 169428  15.452%\n",
      "lif layer 2 self.abs_max_v: 7355.0\n",
      "lif layer 2 self.abs_max_v: 7363.5\n",
      "lif layer 2 self.abs_max_v: 7365.0\n",
      "epoch-112 lr=['8.0000000'], tr/val_loss:  0.767957/  2.137484, val:  42.08%, val_best:  42.08%, tr:  95.61%, tr_best:  99.39%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 63.4567%\n",
      "layer   3  Sparsity: 98.4252%\n",
      "total_backward_count 1106270 real_backward_count 171007  15.458%\n",
      "lif layer 2 self.abs_max_v: 7377.5\n",
      "epoch-113 lr=['8.0000000'], tr/val_loss:  0.756450/  3.163817, val:  28.33%, val_best:  42.08%, tr:  94.38%, tr_best:  99.39%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 63.6877%\n",
      "layer   3  Sparsity: 98.4611%\n",
      "total_backward_count 1116060 real_backward_count 172541  15.460%\n",
      "epoch-114 lr=['8.0000000'], tr/val_loss:  0.727945/  3.818639, val:  24.17%, val_best:  42.08%, tr:  96.12%, tr_best:  99.39%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 64.0326%\n",
      "layer   3  Sparsity: 98.4141%\n",
      "total_backward_count 1125850 real_backward_count 174034  15.458%\n",
      "epoch-115 lr=['8.0000000'], tr/val_loss:  0.723483/  3.111805, val:  26.67%, val_best:  42.08%, tr:  96.42%, tr_best:  99.39%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 64.7338%\n",
      "layer   3  Sparsity: 98.4123%\n",
      "total_backward_count 1135640 real_backward_count 175505  15.454%\n",
      "epoch-116 lr=['8.0000000'], tr/val_loss:  0.723104/  2.995635, val:  27.50%, val_best:  42.08%, tr:  95.81%, tr_best:  99.39%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 63.8336%\n",
      "layer   3  Sparsity: 98.4351%\n",
      "total_backward_count 1145430 real_backward_count 176984  15.451%\n",
      "epoch-117 lr=['8.0000000'], tr/val_loss:  0.714575/  3.515851, val:  29.58%, val_best:  42.08%, tr:  96.22%, tr_best:  99.39%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 63.8250%\n",
      "layer   3  Sparsity: 98.4222%\n",
      "total_backward_count 1155220 real_backward_count 178481  15.450%\n",
      "fc layer 2 self.abs_max_out: 3726.0\n",
      "lif layer 2 self.abs_max_v: 7408.0\n",
      "lif layer 2 self.abs_max_v: 7423.0\n",
      "lif layer 2 self.abs_max_v: 7437.5\n",
      "lif layer 2 self.abs_max_v: 7445.0\n",
      "epoch-118 lr=['8.0000000'], tr/val_loss:  0.701926/  2.794338, val:  31.67%, val_best:  42.08%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 63.3001%\n",
      "layer   3  Sparsity: 98.4194%\n",
      "total_backward_count 1165010 real_backward_count 179949  15.446%\n",
      "epoch-119 lr=['8.0000000'], tr/val_loss:  0.713907/  2.892696, val:  35.42%, val_best:  42.08%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 63.5929%\n",
      "layer   3  Sparsity: 98.3891%\n",
      "total_backward_count 1174800 real_backward_count 181378  15.439%\n",
      "fc layer 2 self.abs_max_out: 3746.0\n",
      "fc layer 2 self.abs_max_out: 3806.0\n",
      "lif layer 2 self.abs_max_v: 7480.5\n",
      "epoch-120 lr=['8.0000000'], tr/val_loss:  0.697336/  2.773000, val:  36.67%, val_best:  42.08%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 63.9493%\n",
      "layer   3  Sparsity: 98.3979%\n",
      "total_backward_count 1184590 real_backward_count 182823  15.433%\n",
      "lif layer 2 self.abs_max_v: 7503.0\n",
      "lif layer 2 self.abs_max_v: 7517.5\n",
      "lif layer 2 self.abs_max_v: 7525.0\n",
      "epoch-121 lr=['8.0000000'], tr/val_loss:  0.718395/  3.419011, val:  25.42%, val_best:  42.08%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 64.0653%\n",
      "layer   3  Sparsity: 98.4164%\n",
      "total_backward_count 1194380 real_backward_count 184302  15.431%\n",
      "epoch-122 lr=['8.0000000'], tr/val_loss:  0.719945/  3.805703, val:  29.17%, val_best:  42.08%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 64.8847%\n",
      "layer   3  Sparsity: 98.4936%\n",
      "total_backward_count 1204170 real_backward_count 185854  15.434%\n",
      "epoch-123 lr=['8.0000000'], tr/val_loss:  0.742325/  2.958034, val:  32.50%, val_best:  42.08%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 64.4692%\n",
      "layer   3  Sparsity: 98.5293%\n",
      "total_backward_count 1213960 real_backward_count 187427  15.439%\n",
      "epoch-124 lr=['8.0000000'], tr/val_loss:  0.730087/  2.553292, val:  39.17%, val_best:  42.08%, tr:  95.40%, tr_best:  99.39%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 64.9949%\n",
      "layer   3  Sparsity: 98.5221%\n",
      "total_backward_count 1223750 real_backward_count 188921  15.438%\n",
      "lif layer 2 self.abs_max_v: 7550.5\n",
      "lif layer 2 self.abs_max_v: 7553.0\n",
      "lif layer 2 self.abs_max_v: 7582.5\n",
      "lif layer 2 self.abs_max_v: 7597.5\n",
      "lif layer 2 self.abs_max_v: 7605.0\n",
      "epoch-125 lr=['8.0000000'], tr/val_loss:  0.728236/  3.926711, val:  28.75%, val_best:  42.08%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 64.5634%\n",
      "layer   3  Sparsity: 98.5316%\n",
      "total_backward_count 1233540 real_backward_count 190441  15.439%\n",
      "epoch-126 lr=['8.0000000'], tr/val_loss:  0.728814/  3.193517, val:  26.25%, val_best:  42.08%, tr:  96.73%, tr_best:  99.39%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 64.6628%\n",
      "layer   3  Sparsity: 98.5153%\n",
      "total_backward_count 1243330 real_backward_count 191982  15.441%\n",
      "epoch-127 lr=['8.0000000'], tr/val_loss:  0.699588/  2.405930, val:  34.58%, val_best:  42.08%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 65.1871%\n",
      "layer   3  Sparsity: 98.5514%\n",
      "total_backward_count 1253120 real_backward_count 193412  15.434%\n",
      "fc layer 2 self.abs_max_out: 3834.0\n",
      "epoch-128 lr=['8.0000000'], tr/val_loss:  0.712882/  3.702017, val:  17.50%, val_best:  42.08%, tr:  96.73%, tr_best:  99.39%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 65.1913%\n",
      "layer   3  Sparsity: 98.5649%\n",
      "total_backward_count 1262910 real_backward_count 194841  15.428%\n",
      "epoch-129 lr=['8.0000000'], tr/val_loss:  0.714177/  2.963839, val:  31.67%, val_best:  42.08%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 65.3175%\n",
      "layer   3  Sparsity: 98.5879%\n",
      "total_backward_count 1272700 real_backward_count 196336  15.427%\n",
      "lif layer 2 self.abs_max_v: 7613.5\n",
      "lif layer 2 self.abs_max_v: 7621.0\n",
      "epoch-130 lr=['8.0000000'], tr/val_loss:  0.725012/  3.251644, val:  34.17%, val_best:  42.08%, tr:  96.94%, tr_best:  99.39%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 64.9370%\n",
      "layer   3  Sparsity: 98.5628%\n",
      "total_backward_count 1282490 real_backward_count 197856  15.427%\n",
      "epoch-131 lr=['8.0000000'], tr/val_loss:  0.718623/  3.608063, val:  24.58%, val_best:  42.08%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 65.0444%\n",
      "layer   3  Sparsity: 98.5458%\n",
      "total_backward_count 1292280 real_backward_count 199330  15.425%\n",
      "epoch-132 lr=['8.0000000'], tr/val_loss:  0.707377/  3.062199, val:  32.50%, val_best:  42.08%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 65.3586%\n",
      "layer   3  Sparsity: 98.5391%\n",
      "total_backward_count 1302070 real_backward_count 200768  15.419%\n",
      "epoch-133 lr=['8.0000000'], tr/val_loss:  0.728206/  2.688843, val:  31.25%, val_best:  42.08%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 64.8135%\n",
      "layer   3  Sparsity: 98.5124%\n",
      "total_backward_count 1311860 real_backward_count 202264  15.418%\n",
      "epoch-134 lr=['8.0000000'], tr/val_loss:  0.724182/  2.687104, val:  30.83%, val_best:  42.08%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 64.0350%\n",
      "layer   3  Sparsity: 98.5182%\n",
      "total_backward_count 1321650 real_backward_count 203736  15.415%\n",
      "epoch-135 lr=['8.0000000'], tr/val_loss:  0.753925/  2.440557, val:  34.58%, val_best:  42.08%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 64.0626%\n",
      "layer   3  Sparsity: 98.6132%\n",
      "total_backward_count 1331440 real_backward_count 205323  15.421%\n",
      "fc layer 2 self.abs_max_out: 3854.0\n",
      "fc layer 2 self.abs_max_out: 3954.0\n",
      "lif layer 2 self.abs_max_v: 7729.5\n",
      "lif layer 2 self.abs_max_v: 7795.5\n",
      "lif layer 2 self.abs_max_v: 7846.5\n",
      "lif layer 2 self.abs_max_v: 7865.0\n",
      "lif layer 2 self.abs_max_v: 7877.5\n",
      "lif layer 2 self.abs_max_v: 7893.0\n",
      "lif layer 2 self.abs_max_v: 7900.5\n",
      "epoch-136 lr=['8.0000000'], tr/val_loss:  0.692129/  2.351477, val:  35.83%, val_best:  42.08%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 64.1132%\n",
      "layer   3  Sparsity: 98.5277%\n",
      "total_backward_count 1341230 real_backward_count 206737  15.414%\n",
      "fc layer 2 self.abs_max_out: 3974.0\n",
      "fc layer 2 self.abs_max_out: 3994.0\n",
      "lif layer 2 self.abs_max_v: 7942.0\n",
      "fc layer 2 self.abs_max_out: 4014.0\n",
      "lif layer 2 self.abs_max_v: 7965.5\n",
      "lif layer 2 self.abs_max_v: 7997.0\n",
      "lif layer 2 self.abs_max_v: 8012.5\n",
      "lif layer 2 self.abs_max_v: 8020.5\n",
      "epoch-137 lr=['8.0000000'], tr/val_loss:  0.715441/  3.494763, val:  35.42%, val_best:  42.08%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 64.1418%\n",
      "layer   3  Sparsity: 98.5551%\n",
      "total_backward_count 1351020 real_backward_count 208232  15.413%\n",
      "epoch-138 lr=['8.0000000'], tr/val_loss:  0.727694/  2.654063, val:  35.42%, val_best:  42.08%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 64.2805%\n",
      "layer   3  Sparsity: 98.6237%\n",
      "total_backward_count 1360810 real_backward_count 209772  15.415%\n",
      "fc layer 2 self.abs_max_out: 4034.0\n",
      "epoch-139 lr=['8.0000000'], tr/val_loss:  0.759580/  2.795909, val:  32.92%, val_best:  42.08%, tr:  95.30%, tr_best:  99.39%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 64.8923%\n",
      "layer   3  Sparsity: 98.6922%\n",
      "total_backward_count 1370600 real_backward_count 211317  15.418%\n",
      "epoch-140 lr=['8.0000000'], tr/val_loss:  0.746894/  3.611309, val:  24.58%, val_best:  42.08%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 64.1018%\n",
      "layer   3  Sparsity: 98.6733%\n",
      "total_backward_count 1380390 real_backward_count 212895  15.423%\n",
      "epoch-141 lr=['8.0000000'], tr/val_loss:  0.741543/  2.841039, val:  30.00%, val_best:  42.08%, tr:  95.51%, tr_best:  99.39%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 64.9055%\n",
      "layer   3  Sparsity: 98.6427%\n",
      "total_backward_count 1390180 real_backward_count 214462  15.427%\n",
      "epoch-142 lr=['8.0000000'], tr/val_loss:  0.715123/  2.948421, val:  35.83%, val_best:  42.08%, tr:  96.83%, tr_best:  99.39%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 64.3427%\n",
      "layer   3  Sparsity: 98.5056%\n",
      "total_backward_count 1399970 real_backward_count 215988  15.428%\n",
      "epoch-143 lr=['8.0000000'], tr/val_loss:  0.717777/  2.488296, val:  37.08%, val_best:  42.08%, tr:  96.94%, tr_best:  99.39%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 64.3715%\n",
      "layer   3  Sparsity: 98.5271%\n",
      "total_backward_count 1409760 real_backward_count 217431  15.423%\n",
      "epoch-144 lr=['8.0000000'], tr/val_loss:  0.717908/  3.539927, val:  30.83%, val_best:  42.08%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 64.7355%\n",
      "layer   3  Sparsity: 98.5251%\n",
      "total_backward_count 1419550 real_backward_count 218919  15.422%\n",
      "epoch-145 lr=['8.0000000'], tr/val_loss:  0.743054/  2.283122, val:  33.33%, val_best:  42.08%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 64.3869%\n",
      "layer   3  Sparsity: 98.5074%\n",
      "total_backward_count 1429340 real_backward_count 220454  15.423%\n",
      "epoch-146 lr=['8.0000000'], tr/val_loss:  0.723050/  2.729518, val:  37.92%, val_best:  42.08%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 64.0145%\n",
      "layer   3  Sparsity: 98.5369%\n",
      "total_backward_count 1439130 real_backward_count 221958  15.423%\n",
      "epoch-147 lr=['8.0000000'], tr/val_loss:  0.741260/  3.262996, val:  23.33%, val_best:  42.08%, tr:  96.63%, tr_best:  99.39%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 64.4443%\n",
      "layer   3  Sparsity: 98.6212%\n",
      "total_backward_count 1448920 real_backward_count 223525  15.427%\n",
      "epoch-148 lr=['8.0000000'], tr/val_loss:  0.747130/  3.312487, val:  32.08%, val_best:  42.08%, tr:  95.81%, tr_best:  99.39%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 64.4612%\n",
      "layer   3  Sparsity: 98.6285%\n",
      "total_backward_count 1458710 real_backward_count 225077  15.430%\n",
      "epoch-149 lr=['8.0000000'], tr/val_loss:  0.755934/  4.080935, val:  22.92%, val_best:  42.08%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 64.6189%\n",
      "layer   3  Sparsity: 98.6204%\n",
      "total_backward_count 1468500 real_backward_count 226630  15.433%\n",
      "epoch-150 lr=['8.0000000'], tr/val_loss:  0.756617/  3.108777, val:  27.92%, val_best:  42.08%, tr:  96.12%, tr_best:  99.39%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 65.0952%\n",
      "layer   3  Sparsity: 98.5854%\n",
      "total_backward_count 1478290 real_backward_count 228150  15.433%\n",
      "epoch-151 lr=['8.0000000'], tr/val_loss:  0.744142/  2.209072, val:  32.08%, val_best:  42.08%, tr:  95.81%, tr_best:  99.39%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 65.6638%\n",
      "layer   3  Sparsity: 98.5329%\n",
      "total_backward_count 1488080 real_backward_count 229702  15.436%\n",
      "epoch-152 lr=['8.0000000'], tr/val_loss:  0.742908/  2.558856, val:  31.67%, val_best:  42.08%, tr:  96.22%, tr_best:  99.39%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 65.7124%\n",
      "layer   3  Sparsity: 98.5782%\n",
      "total_backward_count 1497870 real_backward_count 231222  15.437%\n",
      "epoch-153 lr=['8.0000000'], tr/val_loss:  0.723003/  2.540672, val:  30.42%, val_best:  42.08%, tr:  96.12%, tr_best:  99.39%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 65.7581%\n",
      "layer   3  Sparsity: 98.5461%\n",
      "total_backward_count 1507660 real_backward_count 232725  15.436%\n",
      "epoch-154 lr=['8.0000000'], tr/val_loss:  0.713822/  2.823416, val:  32.08%, val_best:  42.08%, tr:  97.85%, tr_best:  99.39%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 65.3562%\n",
      "layer   3  Sparsity: 98.4842%\n",
      "total_backward_count 1517450 real_backward_count 234247  15.437%\n",
      "epoch-155 lr=['8.0000000'], tr/val_loss:  0.702643/  2.736608, val:  30.83%, val_best:  42.08%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 64.9758%\n",
      "layer   3  Sparsity: 98.4639%\n",
      "total_backward_count 1527240 real_backward_count 235748  15.436%\n",
      "epoch-156 lr=['8.0000000'], tr/val_loss:  0.698997/  3.352243, val:  35.42%, val_best:  42.08%, tr:  97.24%, tr_best:  99.39%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 64.8464%\n",
      "layer   3  Sparsity: 98.5066%\n",
      "total_backward_count 1537030 real_backward_count 237173  15.431%\n",
      "epoch-157 lr=['8.0000000'], tr/val_loss:  0.708000/  2.414649, val:  29.58%, val_best:  42.08%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 65.1539%\n",
      "layer   3  Sparsity: 98.5188%\n",
      "total_backward_count 1546820 real_backward_count 238637  15.428%\n",
      "epoch-158 lr=['8.0000000'], tr/val_loss:  0.712376/  3.059085, val:  23.75%, val_best:  42.08%, tr:  97.04%, tr_best:  99.39%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 65.6592%\n",
      "layer   3  Sparsity: 98.5788%\n",
      "total_backward_count 1556610 real_backward_count 240121  15.426%\n",
      "epoch-159 lr=['8.0000000'], tr/val_loss:  0.718612/  2.982303, val:  20.83%, val_best:  42.08%, tr:  96.53%, tr_best:  99.39%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 65.6379%\n",
      "layer   3  Sparsity: 98.5418%\n",
      "total_backward_count 1566400 real_backward_count 241643  15.427%\n",
      "epoch-160 lr=['8.0000000'], tr/val_loss:  0.724774/  3.552159, val:  29.58%, val_best:  42.08%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 65.1993%\n",
      "layer   3  Sparsity: 98.5349%\n",
      "total_backward_count 1576190 real_backward_count 243175  15.428%\n",
      "epoch-161 lr=['8.0000000'], tr/val_loss:  0.715824/  2.857646, val:  34.58%, val_best:  42.08%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 65.6278%\n",
      "layer   3  Sparsity: 98.5083%\n",
      "total_backward_count 1585980 real_backward_count 244729  15.431%\n",
      "epoch-162 lr=['8.0000000'], tr/val_loss:  0.720070/  3.466531, val:  27.92%, val_best:  42.08%, tr:  96.63%, tr_best:  99.39%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 65.4021%\n",
      "layer   3  Sparsity: 98.5192%\n",
      "total_backward_count 1595770 real_backward_count 246210  15.429%\n",
      "epoch-163 lr=['8.0000000'], tr/val_loss:  0.719250/  3.308417, val:  29.17%, val_best:  42.08%, tr:  97.55%, tr_best:  99.39%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 65.9072%\n",
      "layer   3  Sparsity: 98.5011%\n",
      "total_backward_count 1605560 real_backward_count 247685  15.427%\n",
      "fc layer 2 self.abs_max_out: 4049.0\n",
      "lif layer 2 self.abs_max_v: 8046.5\n",
      "lif layer 2 self.abs_max_v: 8072.5\n",
      "epoch-164 lr=['8.0000000'], tr/val_loss:  0.718026/  2.686591, val:  38.75%, val_best:  42.08%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 66.1583%\n",
      "layer   3  Sparsity: 98.4882%\n",
      "total_backward_count 1615350 real_backward_count 249230  15.429%\n",
      "fc layer 2 self.abs_max_out: 4069.0\n",
      "lif layer 2 self.abs_max_v: 8102.0\n",
      "lif layer 2 self.abs_max_v: 8120.0\n",
      "fc layer 2 self.abs_max_out: 4089.0\n",
      "lif layer 2 self.abs_max_v: 8128.5\n",
      "epoch-165 lr=['8.0000000'], tr/val_loss:  0.730636/  3.854409, val:  27.92%, val_best:  42.08%, tr:  97.75%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 65.4568%\n",
      "layer   3  Sparsity: 98.5006%\n",
      "total_backward_count 1625140 real_backward_count 250765  15.430%\n",
      "lif layer 2 self.abs_max_v: 8130.5\n",
      "fc layer 2 self.abs_max_out: 4109.0\n",
      "lif layer 2 self.abs_max_v: 8131.5\n",
      "lif layer 2 self.abs_max_v: 8175.0\n",
      "lif layer 2 self.abs_max_v: 8196.5\n",
      "epoch-166 lr=['8.0000000'], tr/val_loss:  0.729818/  3.866242, val:  27.92%, val_best:  42.08%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 65.5285%\n",
      "layer   3  Sparsity: 98.5461%\n",
      "total_backward_count 1634930 real_backward_count 252345  15.435%\n",
      "epoch-167 lr=['8.0000000'], tr/val_loss:  0.735861/  3.525381, val:  27.92%, val_best:  42.08%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 64.9623%\n",
      "layer   3  Sparsity: 98.5633%\n",
      "total_backward_count 1644720 real_backward_count 253881  15.436%\n",
      "epoch-168 lr=['8.0000000'], tr/val_loss:  0.700081/  3.395132, val:  35.00%, val_best:  42.08%, tr:  96.83%, tr_best:  99.39%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 65.6372%\n",
      "layer   3  Sparsity: 98.5763%\n",
      "total_backward_count 1654510 real_backward_count 255358  15.434%\n",
      "epoch-169 lr=['8.0000000'], tr/val_loss:  0.699889/  1.885154, val:  42.50%, val_best:  42.50%, tr:  96.02%, tr_best:  99.39%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 65.3567%\n",
      "layer   3  Sparsity: 98.5774%\n",
      "total_backward_count 1664300 real_backward_count 256784  15.429%\n",
      "fc layer 2 self.abs_max_out: 4149.0\n",
      "epoch-170 lr=['8.0000000'], tr/val_loss:  0.702910/  2.927860, val:  28.33%, val_best:  42.50%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 65.3898%\n",
      "layer   3  Sparsity: 98.5785%\n",
      "total_backward_count 1674090 real_backward_count 258181  15.422%\n",
      "epoch-171 lr=['8.0000000'], tr/val_loss:  0.697993/  3.400374, val:  26.25%, val_best:  42.50%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 65.6119%\n",
      "layer   3  Sparsity: 98.5737%\n",
      "total_backward_count 1683880 real_backward_count 259623  15.418%\n",
      "lif layer 2 self.abs_max_v: 8203.0\n",
      "lif layer 2 self.abs_max_v: 8220.0\n",
      "lif layer 2 self.abs_max_v: 8259.0\n",
      "lif layer 2 self.abs_max_v: 8278.5\n",
      "lif layer 2 self.abs_max_v: 8288.5\n",
      "lif layer 2 self.abs_max_v: 8290.0\n",
      "epoch-172 lr=['8.0000000'], tr/val_loss:  0.736004/  2.259109, val:  34.58%, val_best:  42.50%, tr:  96.22%, tr_best:  99.39%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 65.3569%\n",
      "layer   3  Sparsity: 98.5961%\n",
      "total_backward_count 1693670 real_backward_count 261097  15.416%\n",
      "fc layer 2 self.abs_max_out: 4169.0\n",
      "fc layer 2 self.abs_max_out: 4209.0\n",
      "fc layer 2 self.abs_max_out: 4229.0\n",
      "lif layer 2 self.abs_max_v: 8352.5\n",
      "lif layer 2 self.abs_max_v: 8385.5\n",
      "lif layer 2 self.abs_max_v: 8402.0\n",
      "lif layer 2 self.abs_max_v: 8410.0\n",
      "lif layer 2 self.abs_max_v: 8425.0\n",
      "lif layer 2 self.abs_max_v: 8441.5\n",
      "lif layer 2 self.abs_max_v: 8450.0\n",
      "fc layer 2 self.abs_max_out: 4249.0\n",
      "fc layer 2 self.abs_max_out: 4269.0\n",
      "lif layer 2 self.abs_max_v: 8462.0\n",
      "lif layer 2 self.abs_max_v: 8500.0\n",
      "lif layer 2 self.abs_max_v: 8519.0\n",
      "epoch-173 lr=['8.0000000'], tr/val_loss:  0.681198/  3.144440, val:  34.58%, val_best:  42.50%, tr:  96.42%, tr_best:  99.39%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 65.3095%\n",
      "layer   3  Sparsity: 98.6044%\n",
      "total_backward_count 1703460 real_backward_count 262480  15.409%\n",
      "lif layer 2 self.abs_max_v: 8521.0\n",
      "lif layer 2 self.abs_max_v: 8521.5\n",
      "lif layer 2 self.abs_max_v: 8530.0\n",
      "epoch-174 lr=['8.0000000'], tr/val_loss:  0.708732/  2.754985, val:  35.42%, val_best:  42.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 65.2898%\n",
      "layer   3  Sparsity: 98.6049%\n",
      "total_backward_count 1713250 real_backward_count 263923  15.405%\n",
      "epoch-175 lr=['8.0000000'], tr/val_loss:  0.734920/  2.359859, val:  39.17%, val_best:  42.50%, tr:  97.14%, tr_best:  99.39%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 64.6358%\n",
      "layer   3  Sparsity: 98.6481%\n",
      "total_backward_count 1723040 real_backward_count 265473  15.407%\n",
      "epoch-176 lr=['8.0000000'], tr/val_loss:  0.717954/  2.969951, val:  30.83%, val_best:  42.50%, tr:  96.83%, tr_best:  99.39%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 65.1676%\n",
      "layer   3  Sparsity: 98.6116%\n",
      "total_backward_count 1732830 real_backward_count 266914  15.403%\n",
      "epoch-177 lr=['8.0000000'], tr/val_loss:  0.708562/  2.072119, val:  38.33%, val_best:  42.50%, tr:  97.04%, tr_best:  99.39%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 65.1993%\n",
      "layer   3  Sparsity: 98.5918%\n",
      "total_backward_count 1742620 real_backward_count 268340  15.399%\n",
      "epoch-178 lr=['8.0000000'], tr/val_loss:  0.731172/  3.593268, val:  32.92%, val_best:  42.50%, tr:  96.12%, tr_best:  99.39%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 65.4592%\n",
      "layer   3  Sparsity: 98.6159%\n",
      "total_backward_count 1752410 real_backward_count 269833  15.398%\n",
      "epoch-179 lr=['8.0000000'], tr/val_loss:  0.727991/  3.173011, val:  28.75%, val_best:  42.50%, tr:  96.32%, tr_best:  99.39%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 64.9459%\n",
      "layer   3  Sparsity: 98.6261%\n",
      "total_backward_count 1762200 real_backward_count 271315  15.396%\n",
      "epoch-180 lr=['8.0000000'], tr/val_loss:  0.721849/  2.897084, val:  35.42%, val_best:  42.50%, tr:  96.22%, tr_best:  99.39%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 66.4091%\n",
      "layer   3  Sparsity: 98.6563%\n",
      "total_backward_count 1771990 real_backward_count 272797  15.395%\n",
      "epoch-181 lr=['8.0000000'], tr/val_loss:  0.757763/  2.516727, val:  43.33%, val_best:  43.33%, tr:  94.99%, tr_best:  99.39%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 67.1423%\n",
      "layer   3  Sparsity: 98.5681%\n",
      "total_backward_count 1781780 real_backward_count 274360  15.398%\n",
      "epoch-182 lr=['8.0000000'], tr/val_loss:  0.756320/  3.106613, val:  28.75%, val_best:  43.33%, tr:  97.14%, tr_best:  99.39%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 66.1426%\n",
      "layer   3  Sparsity: 98.4352%\n",
      "total_backward_count 1791570 real_backward_count 275926  15.401%\n",
      "epoch-183 lr=['8.0000000'], tr/val_loss:  0.723289/  4.167846, val:  22.92%, val_best:  43.33%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 64.9664%\n",
      "layer   3  Sparsity: 98.3856%\n",
      "total_backward_count 1801360 real_backward_count 277391  15.399%\n",
      "epoch-184 lr=['8.0000000'], tr/val_loss:  0.708921/  4.117207, val:  25.00%, val_best:  43.33%, tr:  97.75%, tr_best:  99.39%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 64.7863%\n",
      "layer   3  Sparsity: 98.3585%\n",
      "total_backward_count 1811150 real_backward_count 278835  15.395%\n",
      "epoch-185 lr=['8.0000000'], tr/val_loss:  0.716104/  3.449648, val:  25.83%, val_best:  43.33%, tr:  97.65%, tr_best:  99.39%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 64.8614%\n",
      "layer   3  Sparsity: 98.4031%\n",
      "total_backward_count 1820940 real_backward_count 280318  15.394%\n",
      "epoch-186 lr=['8.0000000'], tr/val_loss:  0.727569/  3.622200, val:  35.00%, val_best:  43.33%, tr:  96.73%, tr_best:  99.39%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 65.6797%\n",
      "layer   3  Sparsity: 98.4132%\n",
      "total_backward_count 1830730 real_backward_count 281789  15.392%\n",
      "epoch-187 lr=['8.0000000'], tr/val_loss:  0.724029/  3.314020, val:  27.92%, val_best:  43.33%, tr:  97.24%, tr_best:  99.39%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 66.0395%\n",
      "layer   3  Sparsity: 98.3839%\n",
      "total_backward_count 1840520 real_backward_count 283211  15.388%\n",
      "epoch-188 lr=['8.0000000'], tr/val_loss:  0.724461/  3.497278, val:  20.00%, val_best:  43.33%, tr:  97.34%, tr_best:  99.39%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 66.5412%\n",
      "layer   3  Sparsity: 98.4208%\n",
      "total_backward_count 1850310 real_backward_count 284693  15.386%\n",
      "epoch-189 lr=['8.0000000'], tr/val_loss:  0.709890/  2.569710, val:  30.42%, val_best:  43.33%, tr:  97.55%, tr_best:  99.39%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 66.6183%\n",
      "layer   3  Sparsity: 98.4445%\n",
      "total_backward_count 1860100 real_backward_count 286147  15.383%\n",
      "epoch-190 lr=['8.0000000'], tr/val_loss:  0.710823/  2.523639, val:  36.67%, val_best:  43.33%, tr:  96.22%, tr_best:  99.39%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 65.8849%\n",
      "layer   3  Sparsity: 98.5015%\n",
      "total_backward_count 1869890 real_backward_count 287589  15.380%\n",
      "epoch-191 lr=['8.0000000'], tr/val_loss:  0.725643/  3.534559, val:  27.92%, val_best:  43.33%, tr:  96.94%, tr_best:  99.39%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 65.6000%\n",
      "layer   3  Sparsity: 98.5372%\n",
      "total_backward_count 1879680 real_backward_count 289039  15.377%\n",
      "epoch-192 lr=['8.0000000'], tr/val_loss:  0.715536/  2.572692, val:  29.58%, val_best:  43.33%, tr:  96.32%, tr_best:  99.39%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 65.6701%\n",
      "layer   3  Sparsity: 98.5376%\n",
      "total_backward_count 1889470 real_backward_count 290434  15.371%\n",
      "epoch-193 lr=['8.0000000'], tr/val_loss:  0.703055/  2.749140, val:  33.33%, val_best:  43.33%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 66.1636%\n",
      "layer   3  Sparsity: 98.5183%\n",
      "total_backward_count 1899260 real_backward_count 291859  15.367%\n",
      "epoch-194 lr=['8.0000000'], tr/val_loss:  0.698888/  3.190870, val:  23.75%, val_best:  43.33%, tr:  97.45%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 65.4758%\n",
      "layer   3  Sparsity: 98.5267%\n",
      "total_backward_count 1909050 real_backward_count 293305  15.364%\n",
      "epoch-195 lr=['8.0000000'], tr/val_loss:  0.706585/  3.370438, val:  32.08%, val_best:  43.33%, tr:  96.42%, tr_best:  99.39%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 65.6061%\n",
      "layer   3  Sparsity: 98.5437%\n",
      "total_backward_count 1918840 real_backward_count 294771  15.362%\n",
      "epoch-196 lr=['8.0000000'], tr/val_loss:  0.739577/  3.095506, val:  31.25%, val_best:  43.33%, tr:  97.34%, tr_best:  99.39%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 65.7139%\n",
      "layer   3  Sparsity: 98.5731%\n",
      "total_backward_count 1928630 real_backward_count 296296  15.363%\n",
      "epoch-197 lr=['8.0000000'], tr/val_loss:  0.708997/  3.014476, val:  30.42%, val_best:  43.33%, tr:  97.75%, tr_best:  99.39%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 65.8709%\n",
      "layer   3  Sparsity: 98.5373%\n",
      "total_backward_count 1938420 real_backward_count 297787  15.362%\n",
      "epoch-198 lr=['8.0000000'], tr/val_loss:  0.751022/  3.115297, val:  35.42%, val_best:  43.33%, tr:  97.75%, tr_best:  99.39%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 65.7649%\n",
      "layer   3  Sparsity: 98.6156%\n",
      "total_backward_count 1948210 real_backward_count 299383  15.367%\n",
      "epoch-199 lr=['8.0000000'], tr/val_loss:  0.741668/  2.884500, val:  28.75%, val_best:  43.33%, tr:  97.96%, tr_best:  99.39%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 65.8289%\n",
      "layer   3  Sparsity: 98.6039%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6876c9b326854d44a2ce50c72d6b0982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÑ‚ñÅ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñá‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñÑ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÑ‚ñÅ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñá‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñÑ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97957</td></tr><tr><td>tr_epoch_loss</td><td>0.74167</td></tr><tr><td>val_acc_best</td><td>0.43333</td></tr><tr><td>val_acc_now</td><td>0.2875</td></tr><tr><td>val_loss</td><td>2.8845</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-sweep-809</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g93qmj73' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g93qmj73</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251211_041602-g93qmj73/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x6l8tlhr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251211_083612-x6l8tlhr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x6l8tlhr' target=\"_blank\">vital-sweep-818</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x6l8tlhr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x6l8tlhr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251211_083621_745', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 32, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.25, 0.5, 0.5], 'learning_rate': 1, 'learning_rate2': 8} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.5, 0.5])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.5, 0.5])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.5, 0.5])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 373.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 844.0\n",
      "lif layer 2 self.abs_max_v: 844.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 457.5\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "lif layer 2 self.abs_max_v: 1453.0\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "fc layer 1 self.abs_max_out: 410.0\n",
      "lif layer 1 self.abs_max_v: 542.0\n",
      "lif layer 2 self.abs_max_v: 1476.5\n",
      "fc layer 1 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 560.0\n",
      "lif layer 1 self.abs_max_v: 654.0\n",
      "fc layer 1 self.abs_max_out: 528.0\n",
      "lif layer 1 self.abs_max_v: 832.0\n",
      "lif layer 2 self.abs_max_v: 1485.5\n",
      "lif layer 2 self.abs_max_v: 1543.0\n",
      "fc layer 1 self.abs_max_out: 690.0\n",
      "lif layer 1 self.abs_max_v: 851.0\n",
      "lif layer 2 self.abs_max_v: 1659.5\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 1 self.abs_max_out: 744.0\n",
      "lif layer 1 self.abs_max_v: 995.5\n",
      "fc layer 2 self.abs_max_out: 1135.0\n",
      "lif layer 2 self.abs_max_v: 1710.0\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "fc layer 1 self.abs_max_out: 756.0\n",
      "fc layer 1 self.abs_max_out: 765.0\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "lif layer 1 self.abs_max_v: 1104.5\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "lif layer 1 self.abs_max_v: 1394.5\n",
      "lif layer 2 self.abs_max_v: 1902.5\n",
      "lif layer 2 self.abs_max_v: 1905.5\n",
      "lif layer 2 self.abs_max_v: 2007.0\n",
      "lif layer 2 self.abs_max_v: 2150.5\n",
      "fc layer 1 self.abs_max_out: 974.0\n",
      "lif layer 1 self.abs_max_v: 1417.0\n",
      "lif layer 1 self.abs_max_v: 1419.0\n",
      "fc layer 2 self.abs_max_out: 1408.0\n",
      "lif layer 2 self.abs_max_v: 2162.5\n",
      "lif layer 2 self.abs_max_v: 2173.0\n",
      "fc layer 2 self.abs_max_out: 1426.0\n",
      "lif layer 2 self.abs_max_v: 2339.0\n",
      "fc layer 2 self.abs_max_out: 1551.0\n",
      "fc layer 1 self.abs_max_out: 1019.0\n",
      "fc layer 1 self.abs_max_out: 1073.0\n",
      "lif layer 1 self.abs_max_v: 1555.5\n",
      "fc layer 2 self.abs_max_out: 1599.0\n",
      "lif layer 2 self.abs_max_v: 2349.5\n",
      "lif layer 2 self.abs_max_v: 2366.0\n",
      "fc layer 2 self.abs_max_out: 1622.0\n",
      "fc layer 2 self.abs_max_out: 1651.0\n",
      "fc layer 2 self.abs_max_out: 1670.0\n",
      "lif layer 2 self.abs_max_v: 2378.0\n",
      "lif layer 2 self.abs_max_v: 2386.0\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "lif layer 2 self.abs_max_v: 2387.5\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "lif layer 2 self.abs_max_v: 2564.0\n",
      "lif layer 2 self.abs_max_v: 2749.0\n",
      "lif layer 2 self.abs_max_v: 2763.0\n",
      "lif layer 2 self.abs_max_v: 2781.5\n",
      "lif layer 2 self.abs_max_v: 2946.0\n",
      "fc layer 1 self.abs_max_out: 1221.0\n",
      "lif layer 1 self.abs_max_v: 1790.0\n",
      "lif layer 2 self.abs_max_v: 2988.0\n",
      "lif layer 2 self.abs_max_v: 3008.0\n",
      "fc layer 1 self.abs_max_out: 1380.0\n",
      "lif layer 1 self.abs_max_v: 2167.0\n",
      "lif layer 2 self.abs_max_v: 3057.0\n",
      "lif layer 2 self.abs_max_v: 3140.5\n",
      "lif layer 2 self.abs_max_v: 3147.0\n",
      "lif layer 2 self.abs_max_v: 3214.0\n",
      "lif layer 2 self.abs_max_v: 3294.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "lif layer 2 self.abs_max_v: 3359.0\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "fc layer 2 self.abs_max_out: 1939.0\n",
      "lif layer 2 self.abs_max_v: 3532.5\n",
      "fc layer 2 self.abs_max_out: 1960.0\n",
      "lif layer 2 self.abs_max_v: 3679.0\n",
      "fc layer 2 self.abs_max_out: 2057.0\n",
      "fc layer 2 self.abs_max_out: 2110.0\n",
      "lif layer 2 self.abs_max_v: 3835.0\n",
      "fc layer 2 self.abs_max_out: 2130.0\n",
      "lif layer 1 self.abs_max_v: 2240.5\n",
      "fc layer 2 self.abs_max_out: 2376.0\n",
      "lif layer 2 self.abs_max_v: 4002.0\n",
      "fc layer 1 self.abs_max_out: 1412.0\n",
      "lif layer 2 self.abs_max_v: 4122.0\n",
      "lif layer 2 self.abs_max_v: 4366.0\n",
      "lif layer 2 self.abs_max_v: 4394.5\n",
      "fc layer 2 self.abs_max_out: 2524.0\n",
      "lif layer 1 self.abs_max_v: 2264.5\n",
      "lif layer 1 self.abs_max_v: 2288.0\n",
      "fc layer 1 self.abs_max_out: 1424.0\n",
      "lif layer 2 self.abs_max_v: 4414.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 47.126198/139.038147, val:  32.08%, val_best:  32.08%, tr:  89.07%, tr_best:  89.07%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 66.1888%\n",
      "layer   3  Sparsity: 63.1185%\n",
      "total_backward_count 9790 real_backward_count 2947  30.102%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 2327.5\n",
      "lif layer 2 self.abs_max_v: 4440.0\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "lif layer 2 self.abs_max_v: 4483.0\n",
      "lif layer 1 self.abs_max_v: 2378.0\n",
      "fc layer 2 self.abs_max_out: 2608.0\n",
      "fc layer 1 self.abs_max_out: 1441.0\n",
      "lif layer 1 self.abs_max_v: 2393.0\n",
      "fc layer 1 self.abs_max_out: 1496.0\n",
      "lif layer 2 self.abs_max_v: 4510.5\n",
      "lif layer 1 self.abs_max_v: 2416.5\n",
      "lif layer 1 self.abs_max_v: 2482.5\n",
      "fc layer 1 self.abs_max_out: 1602.0\n",
      "lif layer 1 self.abs_max_v: 2601.0\n",
      "lif layer 1 self.abs_max_v: 2640.5\n",
      "lif layer 1 self.abs_max_v: 2735.5\n",
      "lif layer 1 self.abs_max_v: 2771.0\n",
      "fc layer 1 self.abs_max_out: 1619.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 21.239614/ 78.626938, val:  44.17%, val_best:  44.17%, tr:  97.85%, tr_best:  97.85%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 68.5581%\n",
      "layer   3  Sparsity: 65.1463%\n",
      "total_backward_count 19580 real_backward_count 4940  25.230%\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "lif layer 2 self.abs_max_v: 4520.5\n",
      "lif layer 2 self.abs_max_v: 4636.0\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "lif layer 1 self.abs_max_v: 2776.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "fc layer 1 self.abs_max_out: 1692.0\n",
      "lif layer 1 self.abs_max_v: 2807.5\n",
      "lif layer 1 self.abs_max_v: 3071.0\n",
      "lif layer 1 self.abs_max_v: 3134.5\n",
      "lif layer 1 self.abs_max_v: 3238.5\n",
      "fc layer 1 self.abs_max_out: 1711.0\n",
      "lif layer 1 self.abs_max_v: 3330.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 18.219166/ 88.927963, val:  37.08%, val_best:  44.17%, tr:  97.55%, tr_best:  97.85%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 71.0155%\n",
      "layer   3  Sparsity: 65.0520%\n",
      "total_backward_count 29370 real_backward_count 6739  22.945%\n",
      "lif layer 2 self.abs_max_v: 4677.0\n",
      "fc layer 1 self.abs_max_out: 1818.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 15.899786/ 81.420403, val:  38.75%, val_best:  44.17%, tr:  98.37%, tr_best:  98.37%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 72.1521%\n",
      "layer   3  Sparsity: 66.8628%\n",
      "total_backward_count 39160 real_backward_count 8440  21.553%\n",
      "lif layer 2 self.abs_max_v: 4694.0\n",
      "lif layer 2 self.abs_max_v: 4737.5\n",
      "fc layer 1 self.abs_max_out: 1834.0\n",
      "lif layer 2 self.abs_max_v: 4760.0\n",
      "fc layer 2 self.abs_max_out: 2732.0\n",
      "lif layer 2 self.abs_max_v: 4772.0\n",
      "lif layer 2 self.abs_max_v: 4806.0\n",
      "lif layer 2 self.abs_max_v: 4873.0\n",
      "lif layer 2 self.abs_max_v: 4995.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 13.402534/ 55.486465, val:  46.67%, val_best:  46.67%, tr:  98.67%, tr_best:  98.67%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 71.9959%\n",
      "layer   3  Sparsity: 67.7462%\n",
      "total_backward_count 48950 real_backward_count 10004  20.437%\n",
      "lif layer 2 self.abs_max_v: 5029.0\n",
      "fc layer 2 self.abs_max_out: 3026.0\n",
      "lif layer 2 self.abs_max_v: 5337.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 11.927929/ 57.493359, val:  45.42%, val_best:  46.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 72.2115%\n",
      "layer   3  Sparsity: 68.4777%\n",
      "total_backward_count 58740 real_backward_count 11542  19.649%\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 11.299704/ 61.721424, val:  52.50%, val_best:  52.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 72.9318%\n",
      "layer   3  Sparsity: 67.8626%\n",
      "total_backward_count 68530 real_backward_count 12945  18.890%\n",
      "fc layer 2 self.abs_max_out: 3043.0\n",
      "lif layer 2 self.abs_max_v: 5574.0\n",
      "fc layer 1 self.abs_max_out: 1865.0\n",
      "lif layer 1 self.abs_max_v: 3349.0\n",
      "lif layer 1 self.abs_max_v: 3407.5\n",
      "lif layer 1 self.abs_max_v: 3449.0\n",
      "lif layer 1 self.abs_max_v: 3532.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 10.575541/ 61.731445, val:  45.83%, val_best:  52.50%, tr:  99.49%, tr_best:  99.59%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 73.0023%\n",
      "layer   3  Sparsity: 67.6110%\n",
      "total_backward_count 78320 real_backward_count 14295  18.252%\n",
      "fc layer 1 self.abs_max_out: 1885.0\n",
      "fc layer 1 self.abs_max_out: 1899.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 10.964299/ 63.264076, val:  50.00%, val_best:  52.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 73.9186%\n",
      "layer   3  Sparsity: 67.0039%\n",
      "total_backward_count 88110 real_backward_count 15674  17.789%\n",
      "fc layer 1 self.abs_max_out: 2073.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 10.888868/ 83.779381, val:  47.08%, val_best:  52.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 74.8583%\n",
      "layer   3  Sparsity: 66.7239%\n",
      "total_backward_count 97900 real_backward_count 17054  17.420%\n",
      "lif layer 1 self.abs_max_v: 3549.0\n",
      "fc layer 2 self.abs_max_out: 3148.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 10.040621/ 83.755753, val:  42.92%, val_best:  52.50%, tr:  99.28%, tr_best:  99.59%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 74.4886%\n",
      "layer   3  Sparsity: 66.9135%\n",
      "total_backward_count 107690 real_backward_count 18318  17.010%\n",
      "fc layer 1 self.abs_max_out: 2115.0\n",
      "lif layer 1 self.abs_max_v: 3566.5\n",
      "lif layer 1 self.abs_max_v: 3754.5\n",
      "lif layer 1 self.abs_max_v: 3902.5\n",
      "lif layer 1 self.abs_max_v: 3949.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  9.350448/ 55.107059, val:  51.25%, val_best:  52.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 74.9988%\n",
      "layer   3  Sparsity: 66.6057%\n",
      "total_backward_count 117480 real_backward_count 19535  16.628%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  9.124961/ 73.607948, val:  48.75%, val_best:  52.50%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 75.4609%\n",
      "layer   3  Sparsity: 66.5028%\n",
      "total_backward_count 127270 real_backward_count 20717  16.278%\n",
      "fc layer 2 self.abs_max_out: 3159.0\n",
      "fc layer 3 self.abs_max_out: 993.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  9.220360/ 80.431290, val:  40.83%, val_best:  52.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 75.4918%\n",
      "layer   3  Sparsity: 66.6769%\n",
      "total_backward_count 137060 real_backward_count 21961  16.023%\n",
      "fc layer 2 self.abs_max_out: 3391.0\n",
      "fc layer 2 self.abs_max_out: 3401.0\n",
      "fc layer 2 self.abs_max_out: 3472.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  9.214544/ 48.086609, val:  51.25%, val_best:  52.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 75.2034%\n",
      "layer   3  Sparsity: 66.6603%\n",
      "total_backward_count 146850 real_backward_count 23148  15.763%\n",
      "fc layer 1 self.abs_max_out: 2163.0\n",
      "fc layer 3 self.abs_max_out: 1012.0\n",
      "lif layer 1 self.abs_max_v: 4049.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  9.138713/ 77.051842, val:  44.17%, val_best:  52.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 75.5478%\n",
      "layer   3  Sparsity: 67.3679%\n",
      "total_backward_count 156640 real_backward_count 24359  15.551%\n",
      "fc layer 3 self.abs_max_out: 1047.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.288093/ 54.688564, val:  56.25%, val_best:  56.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 75.6614%\n",
      "layer   3  Sparsity: 67.7833%\n",
      "total_backward_count 166430 real_backward_count 25507  15.326%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  8.310349/ 41.816933, val:  58.75%, val_best:  58.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 75.4703%\n",
      "layer   3  Sparsity: 67.8490%\n",
      "total_backward_count 176220 real_backward_count 26672  15.136%\n",
      "fc layer 1 self.abs_max_out: 2214.0\n",
      "lif layer 1 self.abs_max_v: 4056.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.075987/ 61.660229, val:  45.42%, val_best:  58.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 75.1621%\n",
      "layer   3  Sparsity: 67.9801%\n",
      "total_backward_count 186010 real_backward_count 27824  14.958%\n",
      "lif layer 2 self.abs_max_v: 5603.5\n",
      "lif layer 2 self.abs_max_v: 5641.0\n",
      "lif layer 2 self.abs_max_v: 5735.5\n",
      "fc layer 1 self.abs_max_out: 2250.0\n",
      "fc layer 1 self.abs_max_out: 2270.0\n",
      "lif layer 1 self.abs_max_v: 4244.5\n",
      "lif layer 1 self.abs_max_v: 4282.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.604701/ 53.953346, val:  42.92%, val_best:  58.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 74.6518%\n",
      "layer   3  Sparsity: 68.1889%\n",
      "total_backward_count 195800 real_backward_count 28901  14.760%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.462964/ 54.783432, val:  52.50%, val_best:  58.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 74.2975%\n",
      "layer   3  Sparsity: 68.0289%\n",
      "total_backward_count 205590 real_backward_count 29995  14.590%\n",
      "fc layer 1 self.abs_max_out: 2287.0\n",
      "lif layer 1 self.abs_max_v: 4309.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.311579/ 66.172653, val:  46.25%, val_best:  58.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 75.3016%\n",
      "layer   3  Sparsity: 67.4800%\n",
      "total_backward_count 215380 real_backward_count 31152  14.464%\n",
      "lif layer 2 self.abs_max_v: 5771.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.214115/ 56.149456, val:  56.67%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 75.4505%\n",
      "layer   3  Sparsity: 68.0043%\n",
      "total_backward_count 225170 real_backward_count 32231  14.314%\n",
      "fc layer 1 self.abs_max_out: 2461.0\n",
      "lif layer 1 self.abs_max_v: 4356.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.889819/ 60.697212, val:  51.67%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 74.8764%\n",
      "layer   3  Sparsity: 68.6156%\n",
      "total_backward_count 234960 real_backward_count 33371  14.203%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.591815/ 51.203903, val:  54.58%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 74.7035%\n",
      "layer   3  Sparsity: 68.8632%\n",
      "total_backward_count 244750 real_backward_count 34452  14.076%\n",
      "lif layer 2 self.abs_max_v: 5836.5\n",
      "fc layer 1 self.abs_max_out: 2514.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.919212/ 51.009964, val:  53.33%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 74.9001%\n",
      "layer   3  Sparsity: 69.4159%\n",
      "total_backward_count 254540 real_backward_count 35577  13.977%\n",
      "lif layer 2 self.abs_max_v: 5848.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.673489/ 55.405266, val:  55.83%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 74.9624%\n",
      "layer   3  Sparsity: 69.8713%\n",
      "total_backward_count 264330 real_backward_count 36624  13.855%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  7.286152/ 36.270576, val:  60.42%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 74.6315%\n",
      "layer   3  Sparsity: 69.5568%\n",
      "total_backward_count 274120 real_backward_count 37750  13.771%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.975131/ 64.694153, val:  50.42%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 74.3427%\n",
      "layer   3  Sparsity: 69.5695%\n",
      "total_backward_count 283910 real_backward_count 38816  13.672%\n",
      "fc layer 2 self.abs_max_out: 3479.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.977663/ 72.112617, val:  46.25%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 74.3652%\n",
      "layer   3  Sparsity: 69.5501%\n",
      "total_backward_count 293700 real_backward_count 39913  13.590%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.602100/ 47.502590, val:  54.58%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 74.5492%\n",
      "layer   3  Sparsity: 69.6553%\n",
      "total_backward_count 303490 real_backward_count 40913  13.481%\n",
      "lif layer 2 self.abs_max_v: 5912.5\n",
      "lif layer 2 self.abs_max_v: 6003.0\n",
      "lif layer 2 self.abs_max_v: 6025.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.367718/ 58.087658, val:  50.00%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 74.1029%\n",
      "layer   3  Sparsity: 69.9993%\n",
      "total_backward_count 313280 real_backward_count 41957  13.393%\n",
      "fc layer 2 self.abs_max_out: 3584.0\n",
      "fc layer 2 self.abs_max_out: 3895.0\n",
      "lif layer 2 self.abs_max_v: 6049.5\n",
      "lif layer 2 self.abs_max_v: 6282.5\n",
      "lif layer 2 self.abs_max_v: 6714.5\n",
      "fc layer 1 self.abs_max_out: 2515.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.551537/ 63.745716, val:  47.92%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 74.6433%\n",
      "layer   3  Sparsity: 70.1530%\n",
      "total_backward_count 323070 real_backward_count 43000  13.310%\n",
      "fc layer 1 self.abs_max_out: 2685.0\n",
      "lif layer 1 self.abs_max_v: 4776.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  7.124721/ 52.384392, val:  57.50%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 75.0162%\n",
      "layer   3  Sparsity: 69.9540%\n",
      "total_backward_count 332860 real_backward_count 44081  13.243%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.273462/ 80.544907, val:  41.25%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 75.2269%\n",
      "layer   3  Sparsity: 69.2757%\n",
      "total_backward_count 342650 real_backward_count 45105  13.164%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  7.042585/ 42.079990, val:  57.92%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 75.7761%\n",
      "layer   3  Sparsity: 69.0313%\n",
      "total_backward_count 352440 real_backward_count 46136  13.090%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.756313/ 41.601376, val:  64.17%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 75.2121%\n",
      "layer   3  Sparsity: 69.2468%\n",
      "total_backward_count 362230 real_backward_count 47149  13.016%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.144248/ 54.447956, val:  60.83%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 75.2692%\n",
      "layer   3  Sparsity: 69.1240%\n",
      "total_backward_count 372020 real_backward_count 48136  12.939%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.801874/ 39.605709, val:  60.83%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 75.2907%\n",
      "layer   3  Sparsity: 68.8489%\n",
      "total_backward_count 381810 real_backward_count 49183  12.882%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  7.047393/ 58.456078, val:  48.75%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 75.3075%\n",
      "layer   3  Sparsity: 69.3000%\n",
      "total_backward_count 391600 real_backward_count 50253  12.833%\n",
      "lif layer 1 self.abs_max_v: 4796.0\n",
      "fc layer 1 self.abs_max_out: 2697.0\n",
      "lif layer 1 self.abs_max_v: 5076.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  6.813915/ 37.181580, val:  70.42%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 75.3982%\n",
      "layer   3  Sparsity: 68.9434%\n",
      "total_backward_count 401390 real_backward_count 51326  12.787%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  6.671123/ 38.143085, val:  71.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 75.5938%\n",
      "layer   3  Sparsity: 69.0633%\n",
      "total_backward_count 411180 real_backward_count 52364  12.735%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  6.754305/ 47.592842, val:  57.92%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 76.0531%\n",
      "layer   3  Sparsity: 69.1366%\n",
      "total_backward_count 420970 real_backward_count 53368  12.677%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  6.587017/ 34.260128, val:  67.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 75.6679%\n",
      "layer   3  Sparsity: 68.7849%\n",
      "total_backward_count 430760 real_backward_count 54361  12.620%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  6.560375/ 62.330326, val:  51.67%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 75.2148%\n",
      "layer   3  Sparsity: 68.3932%\n",
      "total_backward_count 440550 real_backward_count 55323  12.558%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  6.659184/ 63.812214, val:  50.83%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 75.1890%\n",
      "layer   3  Sparsity: 68.8000%\n",
      "total_backward_count 450340 real_backward_count 56324  12.507%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  6.671107/ 49.184761, val:  57.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 75.2301%\n",
      "layer   3  Sparsity: 67.9698%\n",
      "total_backward_count 460130 real_backward_count 57316  12.456%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  6.564792/ 72.444855, val:  46.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 75.4797%\n",
      "layer   3  Sparsity: 68.0841%\n",
      "total_backward_count 469920 real_backward_count 58297  12.406%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.975013/ 49.260002, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 75.1381%\n",
      "layer   3  Sparsity: 68.6114%\n",
      "total_backward_count 479710 real_backward_count 59239  12.349%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  6.005517/ 37.132107, val:  66.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 75.4118%\n",
      "layer   3  Sparsity: 68.8000%\n",
      "total_backward_count 489500 real_backward_count 60209  12.300%\n",
      "fc layer 1 self.abs_max_out: 2730.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  6.413836/ 62.007065, val:  52.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 75.2219%\n",
      "layer   3  Sparsity: 69.2122%\n",
      "total_backward_count 499290 real_backward_count 61191  12.256%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  6.324075/ 42.669247, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 75.2818%\n",
      "layer   3  Sparsity: 69.2092%\n",
      "total_backward_count 509080 real_backward_count 62171  12.212%\n",
      "fc layer 1 self.abs_max_out: 2731.0\n",
      "lif layer 1 self.abs_max_v: 5117.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  6.205609/ 38.409763, val:  67.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 75.5379%\n",
      "layer   3  Sparsity: 69.8891%\n",
      "total_backward_count 518870 real_backward_count 63184  12.177%\n",
      "fc layer 1 self.abs_max_out: 2741.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.925842/ 45.380547, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 75.7163%\n",
      "layer   3  Sparsity: 69.9156%\n",
      "total_backward_count 528660 real_backward_count 64156  12.136%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  6.045264/ 52.311401, val:  55.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 75.9644%\n",
      "layer   3  Sparsity: 70.4600%\n",
      "total_backward_count 538450 real_backward_count 65099  12.090%\n",
      "fc layer 1 self.abs_max_out: 2745.0\n",
      "fc layer 1 self.abs_max_out: 2764.0\n",
      "lif layer 1 self.abs_max_v: 5126.5\n",
      "fc layer 1 self.abs_max_out: 2883.0\n",
      "lif layer 1 self.abs_max_v: 5446.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  6.484838/ 43.272980, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 75.6641%\n",
      "layer   3  Sparsity: 70.9335%\n",
      "total_backward_count 548240 real_backward_count 66100  12.057%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  6.179249/ 64.397324, val:  53.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 75.5172%\n",
      "layer   3  Sparsity: 70.3256%\n",
      "total_backward_count 558030 real_backward_count 67096  12.024%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  5.684397/ 56.029137, val:  52.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 75.5928%\n",
      "layer   3  Sparsity: 70.3615%\n",
      "total_backward_count 567820 real_backward_count 68004  11.976%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  5.808686/ 37.316792, val:  69.17%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 75.5609%\n",
      "layer   3  Sparsity: 70.6780%\n",
      "total_backward_count 577610 real_backward_count 68947  11.937%\n",
      "fc layer 1 self.abs_max_out: 2902.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  5.666217/ 68.274063, val:  52.92%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 75.3890%\n",
      "layer   3  Sparsity: 70.4627%\n",
      "total_backward_count 587400 real_backward_count 69875  11.896%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  5.801167/ 62.248238, val:  52.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 75.3212%\n",
      "layer   3  Sparsity: 70.0523%\n",
      "total_backward_count 597190 real_backward_count 70803  11.856%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  5.812756/ 32.825138, val:  70.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 75.5603%\n",
      "layer   3  Sparsity: 70.5240%\n",
      "total_backward_count 606980 real_backward_count 71718  11.816%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  6.152112/ 44.656303, val:  61.67%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 75.6157%\n",
      "layer   3  Sparsity: 70.4116%\n",
      "total_backward_count 616770 real_backward_count 72697  11.787%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  5.353208/ 50.748035, val:  55.83%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 75.5726%\n",
      "layer   3  Sparsity: 70.6001%\n",
      "total_backward_count 626560 real_backward_count 73575  11.743%\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  6.129226/ 38.962753, val:  69.17%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 75.8438%\n",
      "layer   3  Sparsity: 69.8205%\n",
      "total_backward_count 636350 real_backward_count 74480  11.704%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  5.931885/ 45.807564, val:  67.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 76.3861%\n",
      "layer   3  Sparsity: 69.5140%\n",
      "total_backward_count 646140 real_backward_count 75415  11.672%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  5.785119/ 52.106911, val:  59.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 76.3343%\n",
      "layer   3  Sparsity: 69.3614%\n",
      "total_backward_count 655930 real_backward_count 76316  11.635%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  5.423766/ 42.496876, val:  66.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 75.8744%\n",
      "layer   3  Sparsity: 69.4421%\n",
      "total_backward_count 665720 real_backward_count 77193  11.595%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  5.699686/ 40.676804, val:  70.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 75.7654%\n",
      "layer   3  Sparsity: 69.3377%\n",
      "total_backward_count 675510 real_backward_count 78094  11.561%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  5.077978/ 71.233170, val:  53.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 75.6164%\n",
      "layer   3  Sparsity: 69.7598%\n",
      "total_backward_count 685300 real_backward_count 78955  11.521%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  5.927493/ 42.854561, val:  74.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 75.5670%\n",
      "layer   3  Sparsity: 70.0553%\n",
      "total_backward_count 695090 real_backward_count 79864  11.490%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  5.352619/ 53.382725, val:  58.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 76.1043%\n",
      "layer   3  Sparsity: 70.2512%\n",
      "total_backward_count 704880 real_backward_count 80729  11.453%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  5.163949/ 35.763432, val:  73.75%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 76.3651%\n",
      "layer   3  Sparsity: 70.1946%\n",
      "total_backward_count 714670 real_backward_count 81567  11.413%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  5.047837/ 63.459332, val:  61.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 76.3626%\n",
      "layer   3  Sparsity: 70.3618%\n",
      "total_backward_count 724460 real_backward_count 82417  11.376%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  5.766835/ 42.750412, val:  67.92%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 75.9351%\n",
      "layer   3  Sparsity: 70.2418%\n",
      "total_backward_count 734250 real_backward_count 83327  11.349%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  5.208473/ 40.253357, val:  72.92%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 76.0439%\n",
      "layer   3  Sparsity: 70.6844%\n",
      "total_backward_count 744040 real_backward_count 84170  11.313%\n",
      "fc layer 1 self.abs_max_out: 2984.0\n",
      "lif layer 1 self.abs_max_v: 5462.5\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  5.843948/ 35.433971, val:  69.58%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 75.5726%\n",
      "layer   3  Sparsity: 70.8686%\n",
      "total_backward_count 753830 real_backward_count 85097  11.289%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  5.273829/ 68.055542, val:  51.25%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 76.0652%\n",
      "layer   3  Sparsity: 71.0318%\n",
      "total_backward_count 763620 real_backward_count 85979  11.259%\n",
      "fc layer 1 self.abs_max_out: 3016.0\n",
      "fc layer 1 self.abs_max_out: 3020.0\n",
      "lif layer 1 self.abs_max_v: 5487.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  5.110254/ 57.061951, val:  56.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 75.6788%\n",
      "layer   3  Sparsity: 71.4280%\n",
      "total_backward_count 773410 real_backward_count 86829  11.227%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  4.727203/ 39.876232, val:  70.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 75.6669%\n",
      "layer   3  Sparsity: 71.6604%\n",
      "total_backward_count 783200 real_backward_count 87614  11.187%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  4.437183/ 41.696064, val:  67.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 75.7004%\n",
      "layer   3  Sparsity: 71.7498%\n",
      "total_backward_count 792990 real_backward_count 88414  11.149%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  4.703039/ 56.845360, val:  52.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 76.1105%\n",
      "layer   3  Sparsity: 71.6478%\n",
      "total_backward_count 802780 real_backward_count 89201  11.112%\n",
      "fc layer 1 self.abs_max_out: 3025.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  5.194679/ 39.015297, val:  67.92%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 75.9174%\n",
      "layer   3  Sparsity: 71.5231%\n",
      "total_backward_count 812570 real_backward_count 90073  11.085%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.589539/ 42.349846, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 75.9095%\n",
      "layer   3  Sparsity: 71.4555%\n",
      "total_backward_count 822360 real_backward_count 90879  11.051%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  5.024079/ 42.235420, val:  70.00%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 75.9128%\n",
      "layer   3  Sparsity: 71.7281%\n",
      "total_backward_count 832150 real_backward_count 91707  11.020%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  5.080653/ 42.189484, val:  73.75%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 75.8389%\n",
      "layer   3  Sparsity: 71.9331%\n",
      "total_backward_count 841940 real_backward_count 92544  10.992%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  4.456362/ 34.224773, val:  68.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 75.6661%\n",
      "layer   3  Sparsity: 71.0648%\n",
      "total_backward_count 851730 real_backward_count 93323  10.957%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  5.038367/ 44.498371, val:  62.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 75.6150%\n",
      "layer   3  Sparsity: 70.7025%\n",
      "total_backward_count 861520 real_backward_count 94211  10.935%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  4.629258/ 38.563751, val:  73.75%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 75.7070%\n",
      "layer   3  Sparsity: 70.1779%\n",
      "total_backward_count 871310 real_backward_count 94987  10.902%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  5.153716/ 31.941490, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 75.5465%\n",
      "layer   3  Sparsity: 69.9821%\n",
      "total_backward_count 881100 real_backward_count 95860  10.880%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  5.624916/ 39.550945, val:  71.25%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 75.1373%\n",
      "layer   3  Sparsity: 69.3701%\n",
      "total_backward_count 890890 real_backward_count 96707  10.855%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  4.499905/ 35.495090, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 75.7165%\n",
      "layer   3  Sparsity: 69.8973%\n",
      "total_backward_count 900680 real_backward_count 97448  10.819%\n",
      "fc layer 1 self.abs_max_out: 3027.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  5.196692/ 33.222980, val:  74.58%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 75.6934%\n",
      "layer   3  Sparsity: 70.0126%\n",
      "total_backward_count 910470 real_backward_count 98263  10.793%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  4.876030/ 37.472115, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 75.5251%\n",
      "layer   3  Sparsity: 69.7471%\n",
      "total_backward_count 920260 real_backward_count 99055  10.764%\n",
      "lif layer 1 self.abs_max_v: 5540.5\n",
      "lif layer 1 self.abs_max_v: 5704.5\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  5.021564/ 45.883945, val:  54.58%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 75.6095%\n",
      "layer   3  Sparsity: 70.3359%\n",
      "total_backward_count 930050 real_backward_count 99887  10.740%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  5.128848/ 41.684635, val:  63.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 75.5841%\n",
      "layer   3  Sparsity: 70.1996%\n",
      "total_backward_count 939840 real_backward_count 100702  10.715%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  5.321888/ 42.565620, val:  68.75%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 75.4668%\n",
      "layer   3  Sparsity: 70.5782%\n",
      "total_backward_count 949630 real_backward_count 101536  10.692%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  4.308344/ 34.502155, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 75.4105%\n",
      "layer   3  Sparsity: 70.9600%\n",
      "total_backward_count 959420 real_backward_count 102268  10.659%\n",
      "fc layer 1 self.abs_max_out: 3077.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  4.705989/ 49.699028, val:  67.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 75.7334%\n",
      "layer   3  Sparsity: 71.0526%\n",
      "total_backward_count 969210 real_backward_count 103045  10.632%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  4.555903/ 44.856514, val:  69.58%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 76.0326%\n",
      "layer   3  Sparsity: 71.0358%\n",
      "total_backward_count 979000 real_backward_count 103809  10.604%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  4.871210/ 43.610329, val:  64.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 75.6646%\n",
      "layer   3  Sparsity: 70.1043%\n",
      "total_backward_count 988790 real_backward_count 104589  10.577%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  4.994155/ 42.220623, val:  71.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 76.0502%\n",
      "layer   3  Sparsity: 69.9920%\n",
      "total_backward_count 998580 real_backward_count 105412  10.556%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  4.604254/ 31.358038, val:  83.75%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 76.0974%\n",
      "layer   3  Sparsity: 69.9759%\n",
      "total_backward_count 1008370 real_backward_count 106138  10.526%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  5.006636/ 47.041744, val:  67.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 75.6478%\n",
      "layer   3  Sparsity: 70.1120%\n",
      "total_backward_count 1018160 real_backward_count 106896  10.499%\n",
      "fc layer 1 self.abs_max_out: 3110.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  4.759167/ 40.772690, val:  68.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 75.1975%\n",
      "layer   3  Sparsity: 70.0604%\n",
      "total_backward_count 1027950 real_backward_count 107677  10.475%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  4.675302/ 47.274441, val:  64.58%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 75.6310%\n",
      "layer   3  Sparsity: 70.4041%\n",
      "total_backward_count 1037740 real_backward_count 108444  10.450%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  4.307560/ 55.402794, val:  60.00%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 75.6502%\n",
      "layer   3  Sparsity: 70.3259%\n",
      "total_backward_count 1047530 real_backward_count 109144  10.419%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  4.839362/ 44.502388, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 75.5790%\n",
      "layer   3  Sparsity: 70.2114%\n",
      "total_backward_count 1057320 real_backward_count 109905  10.395%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  4.608873/ 40.141155, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 75.5467%\n",
      "layer   3  Sparsity: 69.6691%\n",
      "total_backward_count 1067110 real_backward_count 110675  10.371%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  4.397637/ 39.058788, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 75.6073%\n",
      "layer   3  Sparsity: 70.5069%\n",
      "total_backward_count 1076900 real_backward_count 111444  10.349%\n",
      "fc layer 1 self.abs_max_out: 3118.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  4.347271/ 38.859814, val:  74.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 76.0648%\n",
      "layer   3  Sparsity: 70.3910%\n",
      "total_backward_count 1086690 real_backward_count 112158  10.321%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  4.214717/ 41.703083, val:  72.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 75.5415%\n",
      "layer   3  Sparsity: 70.3663%\n",
      "total_backward_count 1096480 real_backward_count 112904  10.297%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  4.492695/ 35.482090, val:  78.33%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 75.2953%\n",
      "layer   3  Sparsity: 70.7953%\n",
      "total_backward_count 1106270 real_backward_count 113618  10.270%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  4.532962/ 34.120850, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 74.9194%\n",
      "layer   3  Sparsity: 70.3425%\n",
      "total_backward_count 1116060 real_backward_count 114368  10.247%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  4.506683/ 50.562248, val:  72.50%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 75.4967%\n",
      "layer   3  Sparsity: 70.3867%\n",
      "total_backward_count 1125850 real_backward_count 115126  10.226%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  4.512432/ 32.430222, val:  81.25%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 75.9793%\n",
      "layer   3  Sparsity: 70.4276%\n",
      "total_backward_count 1135640 real_backward_count 115865  10.203%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  4.709626/ 35.404091, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 75.7450%\n",
      "layer   3  Sparsity: 71.3110%\n",
      "total_backward_count 1145430 real_backward_count 116626  10.182%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  4.497235/ 37.821545, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 75.4499%\n",
      "layer   3  Sparsity: 71.7635%\n",
      "total_backward_count 1155220 real_backward_count 117361  10.159%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  4.832183/ 46.381088, val:  65.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 75.1607%\n",
      "layer   3  Sparsity: 71.6000%\n",
      "total_backward_count 1165010 real_backward_count 118129  10.140%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  4.649075/ 30.690689, val:  80.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 75.0973%\n",
      "layer   3  Sparsity: 71.0935%\n",
      "total_backward_count 1174800 real_backward_count 118923  10.123%\n",
      "fc layer 3 self.abs_max_out: 1067.0\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  4.302908/ 40.704632, val:  73.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 75.4598%\n",
      "layer   3  Sparsity: 71.1195%\n",
      "total_backward_count 1184590 real_backward_count 119630  10.099%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  4.548444/ 38.231216, val:  72.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 75.6570%\n",
      "layer   3  Sparsity: 71.0127%\n",
      "total_backward_count 1194380 real_backward_count 120426  10.083%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  4.298886/ 34.729469, val:  81.25%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 75.3119%\n",
      "layer   3  Sparsity: 70.6128%\n",
      "total_backward_count 1204170 real_backward_count 121115  10.058%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  4.174407/ 41.103611, val:  75.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 75.5298%\n",
      "layer   3  Sparsity: 70.8692%\n",
      "total_backward_count 1213960 real_backward_count 121815  10.035%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  4.140303/ 39.418663, val:  72.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 75.2130%\n",
      "layer   3  Sparsity: 71.2705%\n",
      "total_backward_count 1223750 real_backward_count 122521  10.012%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  4.728510/ 36.076771, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 75.1062%\n",
      "layer   3  Sparsity: 70.7276%\n",
      "total_backward_count 1233540 real_backward_count 123249   9.991%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  4.148285/ 36.280785, val:  75.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 75.3234%\n",
      "layer   3  Sparsity: 70.3398%\n",
      "total_backward_count 1243330 real_backward_count 123954   9.970%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.840206/ 41.661560, val:  70.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 75.1585%\n",
      "layer   3  Sparsity: 70.1929%\n",
      "total_backward_count 1253120 real_backward_count 124630   9.946%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  4.616663/ 39.088867, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 75.1108%\n",
      "layer   3  Sparsity: 70.1236%\n",
      "total_backward_count 1262910 real_backward_count 125353   9.926%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  4.304941/ 35.773090, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 75.6273%\n",
      "layer   3  Sparsity: 69.9725%\n",
      "total_backward_count 1272700 real_backward_count 126064   9.905%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  4.483690/ 46.806206, val:  73.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 75.5117%\n",
      "layer   3  Sparsity: 69.8620%\n",
      "total_backward_count 1282490 real_backward_count 126793   9.886%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  4.293361/ 44.940308, val:  72.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 75.4171%\n",
      "layer   3  Sparsity: 69.6206%\n",
      "total_backward_count 1292280 real_backward_count 127463   9.863%\n",
      "fc layer 3 self.abs_max_out: 1110.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  3.699226/ 42.496944, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 75.1990%\n",
      "layer   3  Sparsity: 69.6119%\n",
      "total_backward_count 1302070 real_backward_count 128086   9.837%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  4.292252/ 42.143028, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 75.3067%\n",
      "layer   3  Sparsity: 69.5233%\n",
      "total_backward_count 1311860 real_backward_count 128757   9.815%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  3.951261/ 48.568832, val:  67.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 75.2777%\n",
      "layer   3  Sparsity: 69.1634%\n",
      "total_backward_count 1321650 real_backward_count 129417   9.792%\n",
      "fc layer 3 self.abs_max_out: 1140.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  4.026577/ 41.282345, val:  76.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 74.9090%\n",
      "layer   3  Sparsity: 69.3377%\n",
      "total_backward_count 1331440 real_backward_count 130114   9.772%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  3.938566/ 32.512779, val:  80.83%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 75.1596%\n",
      "layer   3  Sparsity: 69.3866%\n",
      "total_backward_count 1341230 real_backward_count 130745   9.748%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  4.375396/ 37.058029, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 75.1948%\n",
      "layer   3  Sparsity: 69.1125%\n",
      "total_backward_count 1351020 real_backward_count 131433   9.728%\n",
      "fc layer 3 self.abs_max_out: 1179.0\n",
      "fc layer 3 self.abs_max_out: 1189.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  3.746511/ 34.838764, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 75.0132%\n",
      "layer   3  Sparsity: 69.0769%\n",
      "total_backward_count 1360810 real_backward_count 132084   9.706%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  4.284369/ 47.736740, val:  70.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 75.2570%\n",
      "layer   3  Sparsity: 68.9112%\n",
      "total_backward_count 1370600 real_backward_count 132744   9.685%\n",
      "fc layer 3 self.abs_max_out: 1210.0\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  4.186115/ 39.849903, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 75.2184%\n",
      "layer   3  Sparsity: 69.1793%\n",
      "total_backward_count 1380390 real_backward_count 133407   9.664%\n",
      "fc layer 1 self.abs_max_out: 3154.0\n",
      "lif layer 1 self.abs_max_v: 5743.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  4.009283/ 37.259289, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 75.1689%\n",
      "layer   3  Sparsity: 69.7424%\n",
      "total_backward_count 1390180 real_backward_count 134063   9.644%\n",
      "fc layer 3 self.abs_max_out: 1215.0\n",
      "fc layer 3 self.abs_max_out: 1245.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  3.873196/ 39.408943, val:  75.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 75.1149%\n",
      "layer   3  Sparsity: 70.1535%\n",
      "total_backward_count 1399970 real_backward_count 134713   9.623%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  3.595494/ 55.510941, val:  64.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 75.0070%\n",
      "layer   3  Sparsity: 69.7760%\n",
      "total_backward_count 1409760 real_backward_count 135326   9.599%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  4.149014/ 44.361946, val:  73.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 74.8633%\n",
      "layer   3  Sparsity: 69.7800%\n",
      "total_backward_count 1419550 real_backward_count 135996   9.580%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  3.869917/ 45.347046, val:  66.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 74.8245%\n",
      "layer   3  Sparsity: 69.9625%\n",
      "total_backward_count 1429340 real_backward_count 136654   9.561%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  3.692374/ 40.820972, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 75.0414%\n",
      "layer   3  Sparsity: 70.1205%\n",
      "total_backward_count 1439130 real_backward_count 137292   9.540%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  3.451257/ 81.030838, val:  62.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 74.9528%\n",
      "layer   3  Sparsity: 69.9710%\n",
      "total_backward_count 1448920 real_backward_count 137895   9.517%\n",
      "fc layer 1 self.abs_max_out: 3171.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  4.379642/ 51.526264, val:  71.25%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 75.2174%\n",
      "layer   3  Sparsity: 69.9517%\n",
      "total_backward_count 1458710 real_backward_count 138574   9.500%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  3.837598/ 55.111179, val:  70.83%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 75.3236%\n",
      "layer   3  Sparsity: 69.7242%\n",
      "total_backward_count 1468500 real_backward_count 139184   9.478%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  3.903834/ 36.983471, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 74.9270%\n",
      "layer   3  Sparsity: 69.8608%\n",
      "total_backward_count 1478290 real_backward_count 139833   9.459%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  4.315567/ 43.652504, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 74.6729%\n",
      "layer   3  Sparsity: 70.8148%\n",
      "total_backward_count 1488080 real_backward_count 140480   9.440%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  3.643415/ 51.212120, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 75.0699%\n",
      "layer   3  Sparsity: 70.2274%\n",
      "total_backward_count 1497870 real_backward_count 141086   9.419%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  4.508765/ 49.635864, val:  73.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 75.3838%\n",
      "layer   3  Sparsity: 69.4037%\n",
      "total_backward_count 1507660 real_backward_count 141756   9.402%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  3.751029/ 45.353355, val:  76.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 75.6195%\n",
      "layer   3  Sparsity: 69.5783%\n",
      "total_backward_count 1517450 real_backward_count 142344   9.380%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  3.566884/ 55.654461, val:  70.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 75.5393%\n",
      "layer   3  Sparsity: 69.4718%\n",
      "total_backward_count 1527240 real_backward_count 142921   9.358%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  4.280931/ 40.104633, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 75.4673%\n",
      "layer   3  Sparsity: 68.9566%\n",
      "total_backward_count 1537030 real_backward_count 143575   9.341%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  3.577971/ 33.904099, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 75.2620%\n",
      "layer   3  Sparsity: 69.4957%\n",
      "total_backward_count 1546820 real_backward_count 144155   9.319%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  3.683370/ 37.134563, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 75.2059%\n",
      "layer   3  Sparsity: 70.3600%\n",
      "total_backward_count 1556610 real_backward_count 144788   9.301%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  3.968490/ 35.034431, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 75.3062%\n",
      "layer   3  Sparsity: 70.0863%\n",
      "total_backward_count 1566400 real_backward_count 145440   9.285%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  3.912292/ 39.392235, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 75.4930%\n",
      "layer   3  Sparsity: 69.9760%\n",
      "total_backward_count 1576190 real_backward_count 146055   9.266%\n",
      "fc layer 1 self.abs_max_out: 3252.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  4.331104/ 39.901299, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 75.3163%\n",
      "layer   3  Sparsity: 69.5834%\n",
      "total_backward_count 1585980 real_backward_count 146726   9.251%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  3.951481/ 40.935734, val:  78.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 75.0132%\n",
      "layer   3  Sparsity: 69.6410%\n",
      "total_backward_count 1595770 real_backward_count 147352   9.234%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  3.654593/ 50.917423, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 74.8938%\n",
      "layer   3  Sparsity: 68.8620%\n",
      "total_backward_count 1605560 real_backward_count 147935   9.214%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  3.945398/ 38.320679, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 75.1786%\n",
      "layer   3  Sparsity: 68.8851%\n",
      "total_backward_count 1615350 real_backward_count 148534   9.195%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  4.047269/ 41.499336, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 75.3645%\n",
      "layer   3  Sparsity: 69.4687%\n",
      "total_backward_count 1625140 real_backward_count 149175   9.179%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  3.357127/ 52.722496, val:  71.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 75.2089%\n",
      "layer   3  Sparsity: 69.7907%\n",
      "total_backward_count 1634930 real_backward_count 149714   9.157%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  3.452324/ 42.559940, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 75.3229%\n",
      "layer   3  Sparsity: 69.6094%\n",
      "total_backward_count 1644720 real_backward_count 150274   9.137%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  3.587888/ 38.672829, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 75.2144%\n",
      "layer   3  Sparsity: 69.8889%\n",
      "total_backward_count 1654510 real_backward_count 150819   9.116%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  3.841832/ 37.862225, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 75.0169%\n",
      "layer   3  Sparsity: 69.6772%\n",
      "total_backward_count 1664300 real_backward_count 151420   9.098%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  3.540468/ 40.142651, val:  77.50%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 75.2883%\n",
      "layer   3  Sparsity: 70.1108%\n",
      "total_backward_count 1674090 real_backward_count 151992   9.079%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  3.964644/ 46.616344, val:  70.00%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 75.0221%\n",
      "layer   3  Sparsity: 70.4783%\n",
      "total_backward_count 1683880 real_backward_count 152608   9.063%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  3.426083/ 59.100666, val:  67.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 74.8633%\n",
      "layer   3  Sparsity: 70.1263%\n",
      "total_backward_count 1693670 real_backward_count 153211   9.046%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  3.251953/ 44.316151, val:  77.50%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 75.1548%\n",
      "layer   3  Sparsity: 70.2483%\n",
      "total_backward_count 1703460 real_backward_count 153758   9.026%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  3.452407/ 55.052395, val:  65.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 74.9690%\n",
      "layer   3  Sparsity: 70.4676%\n",
      "total_backward_count 1713250 real_backward_count 154363   9.010%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  3.841805/ 53.218906, val:  71.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 74.6919%\n",
      "layer   3  Sparsity: 70.2558%\n",
      "total_backward_count 1723040 real_backward_count 154984   8.995%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  3.685885/ 43.779354, val:  79.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 74.7783%\n",
      "layer   3  Sparsity: 70.0023%\n",
      "total_backward_count 1732830 real_backward_count 155567   8.978%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  3.550358/ 37.801598, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 74.7073%\n",
      "layer   3  Sparsity: 70.2607%\n",
      "total_backward_count 1742620 real_backward_count 156153   8.961%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  3.274537/ 44.144993, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 74.9878%\n",
      "layer   3  Sparsity: 69.9823%\n",
      "total_backward_count 1752410 real_backward_count 156688   8.941%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  3.985696/ 49.494968, val:  70.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 75.2657%\n",
      "layer   3  Sparsity: 70.0630%\n",
      "total_backward_count 1762200 real_backward_count 157346   8.929%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  3.531362/ 50.732918, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 75.5224%\n",
      "layer   3  Sparsity: 70.4874%\n",
      "total_backward_count 1771990 real_backward_count 157913   8.912%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  3.476109/ 32.713978, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 75.6756%\n",
      "layer   3  Sparsity: 70.8331%\n",
      "total_backward_count 1781780 real_backward_count 158479   8.894%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  3.502826/ 41.796444, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 75.9185%\n",
      "layer   3  Sparsity: 70.6782%\n",
      "total_backward_count 1791570 real_backward_count 159036   8.877%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  3.151069/ 55.739330, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 76.0103%\n",
      "layer   3  Sparsity: 70.5257%\n",
      "total_backward_count 1801360 real_backward_count 159579   8.859%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  3.308686/ 69.824074, val:  63.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 75.9299%\n",
      "layer   3  Sparsity: 70.2847%\n",
      "total_backward_count 1811150 real_backward_count 160133   8.842%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  3.608476/ 33.813747, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 75.7244%\n",
      "layer   3  Sparsity: 70.3908%\n",
      "total_backward_count 1820940 real_backward_count 160679   8.824%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  3.428854/ 42.503193, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 75.4683%\n",
      "layer   3  Sparsity: 70.3747%\n",
      "total_backward_count 1830730 real_backward_count 161230   8.807%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  3.456549/ 47.018562, val:  75.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 75.0585%\n",
      "layer   3  Sparsity: 70.4863%\n",
      "total_backward_count 1840520 real_backward_count 161779   8.790%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  3.153633/ 43.903450, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 75.1822%\n",
      "layer   3  Sparsity: 70.5356%\n",
      "total_backward_count 1850310 real_backward_count 162297   8.771%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  3.530412/ 41.324795, val:  81.25%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 75.1562%\n",
      "layer   3  Sparsity: 70.5695%\n",
      "total_backward_count 1860100 real_backward_count 162839   8.754%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  3.394447/ 39.372986, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 75.2849%\n",
      "layer   3  Sparsity: 70.2588%\n",
      "total_backward_count 1869890 real_backward_count 163393   8.738%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  3.698290/ 58.913967, val:  65.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 75.1425%\n",
      "layer   3  Sparsity: 70.1750%\n",
      "total_backward_count 1879680 real_backward_count 163966   8.723%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  3.422753/ 38.369644, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 75.3286%\n",
      "layer   3  Sparsity: 70.9724%\n",
      "total_backward_count 1889470 real_backward_count 164520   8.707%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  3.269844/ 43.511841, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 75.5765%\n",
      "layer   3  Sparsity: 70.7241%\n",
      "total_backward_count 1899260 real_backward_count 165049   8.690%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  3.052647/ 57.827328, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 75.3789%\n",
      "layer   3  Sparsity: 70.3317%\n",
      "total_backward_count 1909050 real_backward_count 165567   8.673%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  3.098044/ 62.429134, val:  67.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 75.1962%\n",
      "layer   3  Sparsity: 69.8765%\n",
      "total_backward_count 1918840 real_backward_count 166070   8.655%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  3.601214/ 40.218861, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 74.9965%\n",
      "layer   3  Sparsity: 69.9527%\n",
      "total_backward_count 1928630 real_backward_count 166632   8.640%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  3.003625/ 54.530693, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 74.5835%\n",
      "layer   3  Sparsity: 70.5329%\n",
      "total_backward_count 1938420 real_backward_count 167135   8.622%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  3.215548/ 47.500095, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 74.7422%\n",
      "layer   3  Sparsity: 70.8979%\n",
      "total_backward_count 1948210 real_backward_count 167648   8.605%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  3.395294/ 57.430305, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 74.9646%\n",
      "layer   3  Sparsity: 70.5128%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f931280f82c452fafcc55bcf0e9e2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>3.39529</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.70417</td></tr><tr><td>val_loss</td><td>57.43031</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-sweep-818</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x6l8tlhr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x6l8tlhr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251211_083612-x6l8tlhr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tkn51e5s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251211_125709-tkn51e5s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkn51e5s' target=\"_blank\">easy-sweep-827</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkn51e5s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkn51e5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251211_125719_431', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 16, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 32, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.0625, 1, 0.125], 'learning_rate': 1, 'learning_rate2': 2} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 16, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.0625, 1, 0.125])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=16, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.0625, 1, 0.125])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.0625, 1, 0.125])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 88.0\n",
      "lif layer 1 self.abs_max_v: 88.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 125.0\n",
      "lif layer 2 self.abs_max_v: 125.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 1 self.abs_max_out: 102.0\n",
      "lif layer 1 self.abs_max_v: 131.0\n",
      "fc layer 2 self.abs_max_out: 1029.0\n",
      "lif layer 2 self.abs_max_v: 1029.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 1 self.abs_max_out: 103.0\n",
      "lif layer 1 self.abs_max_v: 137.0\n",
      "fc layer 1 self.abs_max_out: 121.0\n",
      "lif layer 1 self.abs_max_v: 157.5\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "lif layer 2 self.abs_max_v: 1175.0\n",
      "fc layer 1 self.abs_max_out: 139.0\n",
      "lif layer 1 self.abs_max_v: 180.0\n",
      "lif layer 2 self.abs_max_v: 1268.0\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 188.0\n",
      "lif layer 2 self.abs_max_v: 1336.5\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "fc layer 1 self.abs_max_out: 247.0\n",
      "lif layer 1 self.abs_max_v: 247.0\n",
      "fc layer 2 self.abs_max_out: 1395.0\n",
      "lif layer 2 self.abs_max_v: 2044.5\n",
      "lif layer 2 self.abs_max_v: 2096.5\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 2 self.abs_max_out: 1422.0\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 261.5\n",
      "fc layer 1 self.abs_max_out: 294.0\n",
      "lif layer 1 self.abs_max_v: 294.0\n",
      "fc layer 2 self.abs_max_out: 1708.0\n",
      "lif layer 2 self.abs_max_v: 2307.5\n",
      "fc layer 1 self.abs_max_out: 297.0\n",
      "lif layer 1 self.abs_max_v: 297.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 2568.5\n",
      "fc layer 1 self.abs_max_out: 323.0\n",
      "lif layer 1 self.abs_max_v: 323.0\n",
      "fc layer 1 self.abs_max_out: 362.0\n",
      "lif layer 1 self.abs_max_v: 362.0\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 1 self.abs_max_out: 372.0\n",
      "lif layer 1 self.abs_max_v: 372.0\n",
      "lif layer 1 self.abs_max_v: 448.0\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "lif layer 2 self.abs_max_v: 2991.5\n",
      "fc layer 2 self.abs_max_out: 1997.0\n",
      "lif layer 1 self.abs_max_v: 458.0\n",
      "lif layer 1 self.abs_max_v: 496.0\n",
      "lif layer 2 self.abs_max_v: 3020.0\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 507.0\n",
      "lif layer 2 self.abs_max_v: 3077.0\n",
      "fc layer 1 self.abs_max_out: 413.0\n",
      "fc layer 1 self.abs_max_out: 465.0\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "lif layer 2 self.abs_max_v: 3591.5\n",
      "lif layer 2 self.abs_max_v: 3599.0\n",
      "fc layer 1 self.abs_max_out: 487.0\n",
      "lif layer 1 self.abs_max_v: 511.5\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "lif layer 1 self.abs_max_v: 545.5\n",
      "lif layer 2 self.abs_max_v: 3851.0\n",
      "lif layer 2 self.abs_max_v: 3993.5\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "fc layer 2 self.abs_max_out: 2236.0\n",
      "fc layer 2 self.abs_max_out: 2277.0\n",
      "lif layer 1 self.abs_max_v: 664.5\n",
      "lif layer 1 self.abs_max_v: 685.5\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "fc layer 1 self.abs_max_out: 603.0\n",
      "lif layer 1 self.abs_max_v: 706.0\n",
      "lif layer 1 self.abs_max_v: 858.0\n",
      "fc layer 2 self.abs_max_out: 2331.0\n",
      "lif layer 1 self.abs_max_v: 918.5\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 1 self.abs_max_out: 646.0\n",
      "fc layer 1 self.abs_max_out: 649.0\n",
      "lif layer 1 self.abs_max_v: 933.5\n",
      "lif layer 1 self.abs_max_v: 945.0\n",
      "lif layer 1 self.abs_max_v: 995.5\n",
      "lif layer 1 self.abs_max_v: 1074.5\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 1 self.abs_max_out: 703.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "lif layer 2 self.abs_max_v: 4002.0\n",
      "fc layer 1 self.abs_max_out: 724.0\n",
      "lif layer 1 self.abs_max_v: 1176.0\n",
      "fc layer 1 self.abs_max_out: 843.0\n",
      "lif layer 1 self.abs_max_v: 1183.5\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "lif layer 1 self.abs_max_v: 1269.5\n",
      "lif layer 1 self.abs_max_v: 1308.5\n",
      "lif layer 1 self.abs_max_v: 1341.0\n",
      "lif layer 1 self.abs_max_v: 1405.5\n",
      "fc layer 1 self.abs_max_out: 900.0\n",
      "lif layer 1 self.abs_max_v: 1478.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "lif layer 1 self.abs_max_v: 1528.5\n",
      "fc layer 1 self.abs_max_out: 931.0\n",
      "lif layer 1 self.abs_max_v: 1552.0\n",
      "fc layer 1 self.abs_max_out: 993.0\n",
      "lif layer 1 self.abs_max_v: 1769.0\n",
      "fc layer 1 self.abs_max_out: 1195.0\n",
      "lif layer 1 self.abs_max_v: 1785.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 3 self.abs_max_out: 517.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "lif layer 1 self.abs_max_v: 1809.5\n",
      "fc layer 3 self.abs_max_out: 550.0\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "lif layer 1 self.abs_max_v: 1847.0\n",
      "lif layer 1 self.abs_max_v: 1880.5\n",
      "lif layer 1 self.abs_max_v: 1952.5\n",
      "lif layer 1 self.abs_max_v: 2146.0\n",
      "lif layer 1 self.abs_max_v: 2246.0\n",
      "fc layer 1 self.abs_max_out: 1204.0\n",
      "lif layer 1 self.abs_max_v: 2327.0\n",
      "lif layer 1 self.abs_max_v: 2366.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.757210/ 78.167908, val:  38.33%, val_best:  38.33%, tr:  98.47%, tr_best:  98.47%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 77.5960%\n",
      "layer   3  Sparsity: 61.5325%\n",
      "total_backward_count 9790 real_backward_count 1820  18.590%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 4079.0\n",
      "fc layer 2 self.abs_max_out: 2495.0\n",
      "fc layer 1 self.abs_max_out: 1216.0\n",
      "fc layer 1 self.abs_max_out: 1254.0\n",
      "fc layer 1 self.abs_max_out: 1396.0\n",
      "lif layer 1 self.abs_max_v: 2456.5\n",
      "fc layer 3 self.abs_max_out: 605.0\n",
      "lif layer 1 self.abs_max_v: 2615.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.945335/ 58.186981, val:  43.33%, val_best:  43.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 77.3850%\n",
      "layer   3  Sparsity: 60.7577%\n",
      "total_backward_count 19580 real_backward_count 3241  16.553%\n",
      "fc layer 1 self.abs_max_out: 1435.0\n",
      "fc layer 1 self.abs_max_out: 1558.0\n",
      "fc layer 1 self.abs_max_out: 1618.0\n",
      "lif layer 1 self.abs_max_v: 2781.5\n",
      "lif layer 1 self.abs_max_v: 2955.0\n",
      "lif layer 1 self.abs_max_v: 3015.5\n",
      "lif layer 1 self.abs_max_v: 3035.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 11.114745/ 46.676777, val:  50.00%, val_best:  50.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 77.2203%\n",
      "layer   3  Sparsity: 61.6202%\n",
      "total_backward_count 29370 real_backward_count 4695  15.986%\n",
      "fc layer 2 self.abs_max_out: 2496.0\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 1 self.abs_max_out: 1646.0\n",
      "fc layer 1 self.abs_max_out: 1703.0\n",
      "lif layer 1 self.abs_max_v: 3140.0\n",
      "lif layer 1 self.abs_max_v: 3230.0\n",
      "lif layer 1 self.abs_max_v: 3274.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.701298/ 79.503960, val:  41.67%, val_best:  50.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 77.2596%\n",
      "layer   3  Sparsity: 60.7158%\n",
      "total_backward_count 39160 real_backward_count 6086  15.541%\n",
      "fc layer 2 self.abs_max_out: 2604.0\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "lif layer 2 self.abs_max_v: 4083.5\n",
      "lif layer 2 self.abs_max_v: 4097.0\n",
      "lif layer 2 self.abs_max_v: 4256.5\n",
      "lif layer 2 self.abs_max_v: 4328.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 10.776107/ 67.730347, val:  44.17%, val_best:  50.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 75.9582%\n",
      "layer   3  Sparsity: 60.5287%\n",
      "total_backward_count 48950 real_backward_count 7469  15.258%\n",
      "fc layer 1 self.abs_max_out: 1824.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 10.410206/ 58.287067, val:  50.00%, val_best:  50.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 75.9220%\n",
      "layer   3  Sparsity: 60.0901%\n",
      "total_backward_count 58740 real_backward_count 8810  14.998%\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "fc layer 1 self.abs_max_out: 1877.0\n",
      "lif layer 1 self.abs_max_v: 3316.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.051388/ 79.208336, val:  51.25%, val_best:  51.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 76.3034%\n",
      "layer   3  Sparsity: 59.9137%\n",
      "total_backward_count 68530 real_backward_count 10099  14.737%\n",
      "fc layer 3 self.abs_max_out: 695.0\n",
      "fc layer 3 self.abs_max_out: 742.0\n",
      "fc layer 1 self.abs_max_out: 1903.0\n",
      "fc layer 1 self.abs_max_out: 1943.0\n",
      "lif layer 1 self.abs_max_v: 3492.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.277810/ 86.341545, val:  50.00%, val_best:  51.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 76.4945%\n",
      "layer   3  Sparsity: 60.4909%\n",
      "total_backward_count 78320 real_backward_count 11312  14.443%\n",
      "fc layer 1 self.abs_max_out: 2004.0\n",
      "lif layer 1 self.abs_max_v: 3543.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  9.686353/ 49.645084, val:  55.83%, val_best:  55.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 76.1379%\n",
      "layer   3  Sparsity: 60.4161%\n",
      "total_backward_count 88110 real_backward_count 12565  14.261%\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 3 self.abs_max_out: 778.0\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "fc layer 1 self.abs_max_out: 2107.0\n",
      "fc layer 1 self.abs_max_out: 2134.0\n",
      "lif layer 1 self.abs_max_v: 3860.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  9.071721/ 66.003548, val:  46.25%, val_best:  55.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 75.8448%\n",
      "layer   3  Sparsity: 60.6671%\n",
      "total_backward_count 97900 real_backward_count 13769  14.064%\n",
      "lif layer 2 self.abs_max_v: 4436.0\n",
      "lif layer 2 self.abs_max_v: 4483.0\n",
      "fc layer 2 self.abs_max_out: 2621.0\n",
      "lif layer 2 self.abs_max_v: 4490.5\n",
      "lif layer 2 self.abs_max_v: 4512.5\n",
      "lif layer 2 self.abs_max_v: 4764.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  9.546981/ 54.379356, val:  51.25%, val_best:  55.83%, tr:  99.18%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 75.1862%\n",
      "layer   3  Sparsity: 60.4229%\n",
      "total_backward_count 107690 real_backward_count 14970  13.901%\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  9.242613/ 41.319065, val:  61.25%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 75.6920%\n",
      "layer   3  Sparsity: 60.8872%\n",
      "total_backward_count 117480 real_backward_count 16195  13.785%\n",
      "fc layer 1 self.abs_max_out: 2153.0\n",
      "lif layer 1 self.abs_max_v: 3895.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.636254/ 70.472595, val:  49.17%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 75.9187%\n",
      "layer   3  Sparsity: 61.2937%\n",
      "total_backward_count 127270 real_backward_count 17331  13.618%\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  9.129948/ 87.781006, val:  45.00%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 75.4732%\n",
      "layer   3  Sparsity: 60.1590%\n",
      "total_backward_count 137060 real_backward_count 18527  13.517%\n",
      "fc layer 2 self.abs_max_out: 2681.0\n",
      "fc layer 1 self.abs_max_out: 2226.0\n",
      "lif layer 1 self.abs_max_v: 3996.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.947399/ 47.942242, val:  63.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 75.4867%\n",
      "layer   3  Sparsity: 60.6797%\n",
      "total_backward_count 146850 real_backward_count 19691  13.409%\n",
      "fc layer 3 self.abs_max_out: 992.0\n",
      "fc layer 1 self.abs_max_out: 2329.0\n",
      "lif layer 1 self.abs_max_v: 4152.0\n",
      "fc layer 2 self.abs_max_out: 2832.0\n",
      "lif layer 2 self.abs_max_v: 5024.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  8.979910/ 66.078377, val:  48.33%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 75.3208%\n",
      "layer   3  Sparsity: 61.3749%\n",
      "total_backward_count 156640 real_backward_count 20858  13.316%\n",
      "fc layer 1 self.abs_max_out: 2348.0\n",
      "lif layer 1 self.abs_max_v: 4189.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.359539/ 56.156330, val:  54.58%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 75.8723%\n",
      "layer   3  Sparsity: 63.1818%\n",
      "total_backward_count 166430 real_backward_count 22003  13.221%\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "lif layer 1 self.abs_max_v: 4248.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  8.004202/ 38.809498, val:  66.67%, val_best:  66.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 75.5655%\n",
      "layer   3  Sparsity: 63.4777%\n",
      "total_backward_count 176220 real_backward_count 23137  13.130%\n",
      "fc layer 3 self.abs_max_out: 1032.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  8.606558/ 56.519379, val:  50.42%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 75.5365%\n",
      "layer   3  Sparsity: 63.4427%\n",
      "total_backward_count 186010 real_backward_count 24324  13.077%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.863372/ 57.339951, val:  55.83%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 75.2978%\n",
      "layer   3  Sparsity: 63.8356%\n",
      "total_backward_count 195800 real_backward_count 25390  12.967%\n",
      "fc layer 1 self.abs_max_out: 2404.0\n",
      "fc layer 1 self.abs_max_out: 2413.0\n",
      "fc layer 1 self.abs_max_out: 2474.0\n",
      "lif layer 1 self.abs_max_v: 4441.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.813807/ 75.902390, val:  45.42%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 75.1762%\n",
      "layer   3  Sparsity: 63.8203%\n",
      "total_backward_count 205590 real_backward_count 26473  12.877%\n",
      "fc layer 1 self.abs_max_out: 2487.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  8.558325/ 44.391869, val:  64.17%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 74.9852%\n",
      "layer   3  Sparsity: 63.4346%\n",
      "total_backward_count 215380 real_backward_count 27656  12.841%\n",
      "fc layer 2 self.abs_max_out: 2916.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.832855/ 72.814537, val:  55.00%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 74.9545%\n",
      "layer   3  Sparsity: 62.9797%\n",
      "total_backward_count 225170 real_backward_count 28763  12.774%\n",
      "fc layer 2 self.abs_max_out: 2962.0\n",
      "fc layer 3 self.abs_max_out: 1037.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  8.140187/ 55.428913, val:  56.25%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 75.6293%\n",
      "layer   3  Sparsity: 62.9156%\n",
      "total_backward_count 234960 real_backward_count 29843  12.701%\n",
      "fc layer 1 self.abs_max_out: 2552.0\n",
      "lif layer 1 self.abs_max_v: 4511.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.773229/ 53.614250, val:  62.08%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 75.4350%\n",
      "layer   3  Sparsity: 63.2375%\n",
      "total_backward_count 244750 real_backward_count 30903  12.626%\n",
      "lif layer 1 self.abs_max_v: 4523.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.893737/ 44.436661, val:  71.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 74.9301%\n",
      "layer   3  Sparsity: 63.0522%\n",
      "total_backward_count 254540 real_backward_count 31997  12.571%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  7.537836/ 44.427883, val:  67.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 74.2445%\n",
      "layer   3  Sparsity: 63.1128%\n",
      "total_backward_count 264330 real_backward_count 33049  12.503%\n",
      "fc layer 1 self.abs_max_out: 2614.0\n",
      "lif layer 1 self.abs_max_v: 4668.0\n",
      "fc layer 1 self.abs_max_out: 2680.0\n",
      "lif layer 1 self.abs_max_v: 4722.0\n",
      "lif layer 2 self.abs_max_v: 5353.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  8.230227/ 43.554234, val:  69.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 74.2031%\n",
      "layer   3  Sparsity: 62.6926%\n",
      "total_backward_count 274120 real_backward_count 34147  12.457%\n",
      "fc layer 3 self.abs_max_out: 1103.0\n",
      "fc layer 1 self.abs_max_out: 2766.0\n",
      "lif layer 1 self.abs_max_v: 4869.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  7.731316/ 85.714127, val:  47.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 75.0365%\n",
      "layer   3  Sparsity: 62.8370%\n",
      "total_backward_count 283910 real_backward_count 35221  12.406%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  7.746309/ 72.093208, val:  59.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 75.0938%\n",
      "layer   3  Sparsity: 63.2523%\n",
      "total_backward_count 293700 real_backward_count 36255  12.344%\n",
      "fc layer 2 self.abs_max_out: 3020.0\n",
      "lif layer 2 self.abs_max_v: 5455.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  7.106137/ 36.010872, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 74.7870%\n",
      "layer   3  Sparsity: 63.2932%\n",
      "total_backward_count 303490 real_backward_count 37224  12.265%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  7.549533/ 46.432205, val:  67.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 74.4320%\n",
      "layer   3  Sparsity: 62.9608%\n",
      "total_backward_count 313280 real_backward_count 38245  12.208%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.802248/ 43.033306, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 74.2486%\n",
      "layer   3  Sparsity: 63.1318%\n",
      "total_backward_count 323070 real_backward_count 39204  12.135%\n",
      "fc layer 1 self.abs_max_out: 2861.0\n",
      "lif layer 1 self.abs_max_v: 4966.5\n",
      "lif layer 1 self.abs_max_v: 5110.5\n",
      "fc layer 2 self.abs_max_out: 3038.0\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "lif layer 2 self.abs_max_v: 5762.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  7.157312/ 47.254025, val:  66.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 74.3936%\n",
      "layer   3  Sparsity: 63.1050%\n",
      "total_backward_count 332860 real_backward_count 40192  12.075%\n",
      "fc layer 3 self.abs_max_out: 1160.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.529018/ 65.450333, val:  52.50%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 74.5362%\n",
      "layer   3  Sparsity: 62.8722%\n",
      "total_backward_count 342650 real_backward_count 41139  12.006%\n",
      "fc layer 3 self.abs_max_out: 1175.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  7.328672/ 46.188751, val:  69.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 74.3055%\n",
      "layer   3  Sparsity: 62.1949%\n",
      "total_backward_count 352440 real_backward_count 42119  11.951%\n",
      "fc layer 3 self.abs_max_out: 1245.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.778217/ 56.836597, val:  63.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 74.3447%\n",
      "layer   3  Sparsity: 62.6952%\n",
      "total_backward_count 362230 real_backward_count 43051  11.885%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.959395/ 70.442955, val:  61.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 74.5261%\n",
      "layer   3  Sparsity: 62.7576%\n",
      "total_backward_count 372020 real_backward_count 43973  11.820%\n",
      "fc layer 2 self.abs_max_out: 3141.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.666276/ 53.819462, val:  63.75%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 74.1123%\n",
      "layer   3  Sparsity: 62.8075%\n",
      "total_backward_count 381810 real_backward_count 44936  11.769%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  7.200709/ 56.872055, val:  61.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 74.3320%\n",
      "layer   3  Sparsity: 62.6584%\n",
      "total_backward_count 391600 real_backward_count 45895  11.720%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  6.614586/ 59.676029, val:  58.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 74.3347%\n",
      "layer   3  Sparsity: 62.9657%\n",
      "total_backward_count 401390 real_backward_count 46807  11.661%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  6.612211/ 56.939678, val:  63.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 74.6414%\n",
      "layer   3  Sparsity: 63.7355%\n",
      "total_backward_count 411180 real_backward_count 47713  11.604%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  6.447321/ 49.105953, val:  67.50%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 74.0865%\n",
      "layer   3  Sparsity: 63.7054%\n",
      "total_backward_count 420970 real_backward_count 48615  11.548%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  6.513657/ 28.824869, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 74.0507%\n",
      "layer   3  Sparsity: 63.5252%\n",
      "total_backward_count 430760 real_backward_count 49512  11.494%\n",
      "fc layer 1 self.abs_max_out: 2985.0\n",
      "lif layer 1 self.abs_max_v: 5200.5\n",
      "lif layer 1 self.abs_max_v: 5280.5\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  6.856597/ 50.999256, val:  69.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 73.6663%\n",
      "layer   3  Sparsity: 63.1101%\n",
      "total_backward_count 440550 real_backward_count 50393  11.439%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  6.914048/ 52.672134, val:  71.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 73.5776%\n",
      "layer   3  Sparsity: 63.0999%\n",
      "total_backward_count 450340 real_backward_count 51337  11.400%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  6.575984/ 40.742794, val:  75.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 73.8166%\n",
      "layer   3  Sparsity: 62.8548%\n",
      "total_backward_count 460130 real_backward_count 52234  11.352%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.999401/ 54.031296, val:  64.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 74.0066%\n",
      "layer   3  Sparsity: 63.0040%\n",
      "total_backward_count 469920 real_backward_count 53071  11.294%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  6.309317/ 35.426861, val:  81.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 73.7077%\n",
      "layer   3  Sparsity: 62.6973%\n",
      "total_backward_count 479710 real_backward_count 53931  11.242%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  6.180921/ 44.851460, val:  74.58%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 73.9120%\n",
      "layer   3  Sparsity: 62.3610%\n",
      "total_backward_count 489500 real_backward_count 54777  11.190%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  6.220255/ 46.679153, val:  70.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 73.8583%\n",
      "layer   3  Sparsity: 62.5709%\n",
      "total_backward_count 499290 real_backward_count 55596  11.135%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.719580/ 40.870781, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 73.7016%\n",
      "layer   3  Sparsity: 62.6373%\n",
      "total_backward_count 509080 real_backward_count 56405  11.080%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  6.489727/ 49.606628, val:  71.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 73.8106%\n",
      "layer   3  Sparsity: 62.8740%\n",
      "total_backward_count 518870 real_backward_count 57280  11.039%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  6.086426/ 68.271851, val:  71.67%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 74.1815%\n",
      "layer   3  Sparsity: 63.1834%\n",
      "total_backward_count 528660 real_backward_count 58098  10.990%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  6.392490/ 43.743961, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 73.7604%\n",
      "layer   3  Sparsity: 63.5736%\n",
      "total_backward_count 538450 real_backward_count 58936  10.945%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  6.165522/ 45.695908, val:  74.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 73.8391%\n",
      "layer   3  Sparsity: 63.4425%\n",
      "total_backward_count 548240 real_backward_count 59769  10.902%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  5.932859/ 48.574356, val:  75.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 73.6677%\n",
      "layer   3  Sparsity: 63.4221%\n",
      "total_backward_count 558030 real_backward_count 60626  10.864%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  5.768533/ 44.765064, val:  74.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 73.5354%\n",
      "layer   3  Sparsity: 63.9724%\n",
      "total_backward_count 567820 real_backward_count 61400  10.813%\n",
      "fc layer 1 self.abs_max_out: 3053.0\n",
      "lif layer 1 self.abs_max_v: 5389.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  6.092059/ 38.185715, val:  82.50%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 73.0395%\n",
      "layer   3  Sparsity: 63.7770%\n",
      "total_backward_count 577610 real_backward_count 62160  10.762%\n",
      "fc layer 1 self.abs_max_out: 3055.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  5.589024/ 78.976112, val:  53.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 73.4132%\n",
      "layer   3  Sparsity: 63.8886%\n",
      "total_backward_count 587400 real_backward_count 62945  10.716%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  5.405418/ 90.787186, val:  59.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 73.5605%\n",
      "layer   3  Sparsity: 64.2878%\n",
      "total_backward_count 597190 real_backward_count 63702  10.667%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  5.293849/ 36.223469, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 73.6885%\n",
      "layer   3  Sparsity: 64.8096%\n",
      "total_backward_count 606980 real_backward_count 64471  10.622%\n",
      "fc layer 2 self.abs_max_out: 3187.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  5.908325/ 47.551006, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 73.3831%\n",
      "layer   3  Sparsity: 64.4334%\n",
      "total_backward_count 616770 real_backward_count 65283  10.585%\n",
      "fc layer 1 self.abs_max_out: 3071.0\n",
      "lif layer 1 self.abs_max_v: 5395.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  5.845627/ 51.576035, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 73.8234%\n",
      "layer   3  Sparsity: 64.4241%\n",
      "total_backward_count 626560 real_backward_count 66075  10.546%\n",
      "fc layer 1 self.abs_max_out: 3102.0\n",
      "lif layer 1 self.abs_max_v: 5482.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  5.479144/ 42.556438, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 73.7404%\n",
      "layer   3  Sparsity: 64.7260%\n",
      "total_backward_count 636350 real_backward_count 66844  10.504%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.986746/ 48.109356, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 73.7416%\n",
      "layer   3  Sparsity: 65.0466%\n",
      "total_backward_count 646140 real_backward_count 67561  10.456%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  5.830397/ 43.864967, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 73.5756%\n",
      "layer   3  Sparsity: 65.0076%\n",
      "total_backward_count 655930 real_backward_count 68333  10.418%\n",
      "fc layer 1 self.abs_max_out: 3128.0\n",
      "lif layer 1 self.abs_max_v: 5526.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  5.090792/ 42.035454, val:  77.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 73.3977%\n",
      "layer   3  Sparsity: 64.5976%\n",
      "total_backward_count 665720 real_backward_count 69049  10.372%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.955855/ 32.627777, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 73.8705%\n",
      "layer   3  Sparsity: 64.9783%\n",
      "total_backward_count 675510 real_backward_count 69747  10.325%\n",
      "fc layer 1 self.abs_max_out: 3157.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.427127/ 49.056408, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 73.8245%\n",
      "layer   3  Sparsity: 65.3508%\n",
      "total_backward_count 685300 real_backward_count 70408  10.274%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.779780/ 46.118603, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 73.6580%\n",
      "layer   3  Sparsity: 65.9662%\n",
      "total_backward_count 695090 real_backward_count 71124  10.232%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  5.378262/ 45.248623, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 73.6936%\n",
      "layer   3  Sparsity: 65.6790%\n",
      "total_backward_count 704880 real_backward_count 71865  10.195%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  4.706396/ 36.642502, val:  82.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 73.8424%\n",
      "layer   3  Sparsity: 65.9717%\n",
      "total_backward_count 714670 real_backward_count 72536  10.150%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  4.554857/ 53.210075, val:  75.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 73.6321%\n",
      "layer   3  Sparsity: 65.9484%\n",
      "total_backward_count 724460 real_backward_count 73216  10.106%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.112591/ 47.546936, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 73.7854%\n",
      "layer   3  Sparsity: 65.8065%\n",
      "total_backward_count 734250 real_backward_count 73862  10.060%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  4.564067/ 39.509483, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 73.5740%\n",
      "layer   3  Sparsity: 65.5316%\n",
      "total_backward_count 744040 real_backward_count 74525  10.016%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  5.121022/ 38.478844, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 73.6010%\n",
      "layer   3  Sparsity: 65.3919%\n",
      "total_backward_count 753830 real_backward_count 75233   9.980%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.588567/ 70.572731, val:  62.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 73.8924%\n",
      "layer   3  Sparsity: 65.6545%\n",
      "total_backward_count 763620 real_backward_count 75865   9.935%\n",
      "lif layer 1 self.abs_max_v: 5573.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  4.724419/ 57.756809, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 73.9133%\n",
      "layer   3  Sparsity: 65.7110%\n",
      "total_backward_count 773410 real_backward_count 76538   9.896%\n",
      "lif layer 1 self.abs_max_v: 5603.5\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  4.889883/ 45.774063, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 73.9091%\n",
      "layer   3  Sparsity: 65.2057%\n",
      "total_backward_count 783200 real_backward_count 77191   9.856%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  4.308171/ 51.162643, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 73.8549%\n",
      "layer   3  Sparsity: 65.4281%\n",
      "total_backward_count 792990 real_backward_count 77785   9.809%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  4.188785/ 43.696823, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 73.4687%\n",
      "layer   3  Sparsity: 65.0997%\n",
      "total_backward_count 802780 real_backward_count 78417   9.768%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  4.375459/ 37.972862, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 73.6015%\n",
      "layer   3  Sparsity: 65.0240%\n",
      "total_backward_count 812570 real_backward_count 79039   9.727%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  4.537971/ 38.972775, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 73.4823%\n",
      "layer   3  Sparsity: 64.8062%\n",
      "total_backward_count 822360 real_backward_count 79687   9.690%\n",
      "fc layer 1 self.abs_max_out: 3288.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  4.785613/ 32.909073, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 73.6326%\n",
      "layer   3  Sparsity: 64.6240%\n",
      "total_backward_count 832150 real_backward_count 80357   9.657%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  4.557770/ 41.289185, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 73.3490%\n",
      "layer   3  Sparsity: 64.4269%\n",
      "total_backward_count 841940 real_backward_count 81017   9.623%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  4.479259/ 38.404091, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 73.4549%\n",
      "layer   3  Sparsity: 64.5978%\n",
      "total_backward_count 851730 real_backward_count 81633   9.584%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  4.370763/ 49.053333, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 73.3964%\n",
      "layer   3  Sparsity: 64.6215%\n",
      "total_backward_count 861520 real_backward_count 82274   9.550%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  4.426981/ 44.283794, val:  80.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 73.4482%\n",
      "layer   3  Sparsity: 64.0495%\n",
      "total_backward_count 871310 real_backward_count 82894   9.514%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  4.499774/ 34.870747, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 73.5182%\n",
      "layer   3  Sparsity: 63.6099%\n",
      "total_backward_count 881100 real_backward_count 83546   9.482%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  4.739919/ 48.437706, val:  77.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 73.2186%\n",
      "layer   3  Sparsity: 63.5264%\n",
      "total_backward_count 890890 real_backward_count 84188   9.450%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  4.436776/ 42.947723, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 73.3058%\n",
      "layer   3  Sparsity: 63.9888%\n",
      "total_backward_count 900680 real_backward_count 84790   9.414%\n",
      "lif layer 1 self.abs_max_v: 5777.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  4.220014/ 47.356205, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 73.3951%\n",
      "layer   3  Sparsity: 63.6414%\n",
      "total_backward_count 910470 real_backward_count 85384   9.378%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  4.581605/ 44.356846, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 73.2872%\n",
      "layer   3  Sparsity: 63.4817%\n",
      "total_backward_count 920260 real_backward_count 86002   9.345%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  4.501763/ 41.657555, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 73.4326%\n",
      "layer   3  Sparsity: 63.8281%\n",
      "total_backward_count 930050 real_backward_count 86599   9.311%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  4.499923/ 54.606853, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 73.4852%\n",
      "layer   3  Sparsity: 63.8922%\n",
      "total_backward_count 939840 real_backward_count 87216   9.280%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.770154/ 46.076427, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 74.0087%\n",
      "layer   3  Sparsity: 64.0382%\n",
      "total_backward_count 949630 real_backward_count 87757   9.241%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.789654/ 43.454323, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 73.7562%\n",
      "layer   3  Sparsity: 64.2843%\n",
      "total_backward_count 959420 real_backward_count 88292   9.203%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  4.000471/ 48.486412, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 73.6435%\n",
      "layer   3  Sparsity: 64.4270%\n",
      "total_backward_count 969210 real_backward_count 88848   9.167%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.975856/ 40.948162, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 73.3829%\n",
      "layer   3  Sparsity: 64.8345%\n",
      "total_backward_count 979000 real_backward_count 89383   9.130%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.754475/ 40.100159, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 73.1886%\n",
      "layer   3  Sparsity: 64.7133%\n",
      "total_backward_count 988790 real_backward_count 89923   9.094%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  4.050021/ 40.272129, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 73.2444%\n",
      "layer   3  Sparsity: 64.6081%\n",
      "total_backward_count 998580 real_backward_count 90490   9.062%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.601175/ 39.271217, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 73.3949%\n",
      "layer   3  Sparsity: 64.5787%\n",
      "total_backward_count 1008370 real_backward_count 91006   9.025%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.741478/ 37.767982, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 73.6078%\n",
      "layer   3  Sparsity: 64.4188%\n",
      "total_backward_count 1018160 real_backward_count 91548   8.992%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.590710/ 41.057461, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 73.3251%\n",
      "layer   3  Sparsity: 65.1015%\n",
      "total_backward_count 1027950 real_backward_count 92068   8.956%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  3.961900/ 50.767796, val:  74.58%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 73.4185%\n",
      "layer   3  Sparsity: 65.3715%\n",
      "total_backward_count 1037740 real_backward_count 92601   8.923%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.565464/ 47.500290, val:  81.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 73.5791%\n",
      "layer   3  Sparsity: 65.4289%\n",
      "total_backward_count 1047530 real_backward_count 93125   8.890%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  3.539697/ 43.564873, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 73.7453%\n",
      "layer   3  Sparsity: 65.4052%\n",
      "total_backward_count 1057320 real_backward_count 93635   8.856%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  3.295819/ 47.435375, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 73.6233%\n",
      "layer   3  Sparsity: 65.2011%\n",
      "total_backward_count 1067110 real_backward_count 94140   8.822%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  3.952663/ 37.325062, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 73.5199%\n",
      "layer   3  Sparsity: 65.2217%\n",
      "total_backward_count 1076900 real_backward_count 94674   8.791%\n",
      "fc layer 1 self.abs_max_out: 3470.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.181048/ 49.734665, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 73.6031%\n",
      "layer   3  Sparsity: 65.0536%\n",
      "total_backward_count 1086690 real_backward_count 95130   8.754%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  3.361509/ 49.578285, val:  82.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 73.7277%\n",
      "layer   3  Sparsity: 64.8525%\n",
      "total_backward_count 1096480 real_backward_count 95629   8.721%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  3.324099/ 44.033230, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 73.5605%\n",
      "layer   3  Sparsity: 64.5070%\n",
      "total_backward_count 1106270 real_backward_count 96116   8.688%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  3.432184/ 42.874371, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 73.3572%\n",
      "layer   3  Sparsity: 64.8389%\n",
      "total_backward_count 1116060 real_backward_count 96614   8.657%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  3.603675/ 50.139172, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 73.4736%\n",
      "layer   3  Sparsity: 65.3641%\n",
      "total_backward_count 1125850 real_backward_count 97120   8.626%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  3.471184/ 39.758381, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 73.3713%\n",
      "layer   3  Sparsity: 65.5753%\n",
      "total_backward_count 1135640 real_backward_count 97598   8.594%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  3.753529/ 43.687908, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 73.5055%\n",
      "layer   3  Sparsity: 65.6390%\n",
      "total_backward_count 1145430 real_backward_count 98106   8.565%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  3.490158/ 42.372967, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 73.3648%\n",
      "layer   3  Sparsity: 65.4644%\n",
      "total_backward_count 1155220 real_backward_count 98571   8.533%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  3.834273/ 44.425167, val:  79.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 73.5797%\n",
      "layer   3  Sparsity: 65.7957%\n",
      "total_backward_count 1165010 real_backward_count 99080   8.505%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  3.480161/ 42.559937, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 73.4842%\n",
      "layer   3  Sparsity: 66.0541%\n",
      "total_backward_count 1174800 real_backward_count 99563   8.475%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.306309/ 59.081451, val:  73.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 73.4489%\n",
      "layer   3  Sparsity: 66.6187%\n",
      "total_backward_count 1184590 real_backward_count 100053   8.446%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  3.425597/ 33.908867, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 73.6543%\n",
      "layer   3  Sparsity: 66.6458%\n",
      "total_backward_count 1194380 real_backward_count 100539   8.418%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  3.122924/ 41.480427, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 73.4114%\n",
      "layer   3  Sparsity: 66.4921%\n",
      "total_backward_count 1204170 real_backward_count 100979   8.386%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  3.288255/ 35.734699, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 73.6339%\n",
      "layer   3  Sparsity: 66.9371%\n",
      "total_backward_count 1213960 real_backward_count 101432   8.355%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.816860/ 32.079651, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 73.4846%\n",
      "layer   3  Sparsity: 66.7453%\n",
      "total_backward_count 1223750 real_backward_count 101828   8.321%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  3.321379/ 45.176422, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 73.6609%\n",
      "layer   3  Sparsity: 66.5717%\n",
      "total_backward_count 1233540 real_backward_count 102285   8.292%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  3.009060/ 46.221535, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 73.7619%\n",
      "layer   3  Sparsity: 66.0005%\n",
      "total_backward_count 1243330 real_backward_count 102710   8.261%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  3.093030/ 40.403168, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 73.5519%\n",
      "layer   3  Sparsity: 65.6160%\n",
      "total_backward_count 1253120 real_backward_count 103159   8.232%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  3.150037/ 37.732628, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 73.5422%\n",
      "layer   3  Sparsity: 65.6415%\n",
      "total_backward_count 1262910 real_backward_count 103613   8.204%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.846754/ 41.543964, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 73.7559%\n",
      "layer   3  Sparsity: 65.9867%\n",
      "total_backward_count 1272700 real_backward_count 104022   8.173%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  3.267428/ 43.031185, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 73.8304%\n",
      "layer   3  Sparsity: 65.9208%\n",
      "total_backward_count 1282490 real_backward_count 104495   8.148%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  3.449977/ 44.246696, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 73.5220%\n",
      "layer   3  Sparsity: 65.3172%\n",
      "total_backward_count 1292280 real_backward_count 104964   8.122%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.929997/ 42.049057, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 73.2739%\n",
      "layer   3  Sparsity: 65.8998%\n",
      "total_backward_count 1302070 real_backward_count 105398   8.095%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.672432/ 54.507828, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 73.1507%\n",
      "layer   3  Sparsity: 65.9424%\n",
      "total_backward_count 1311860 real_backward_count 105827   8.067%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.577913/ 51.147209, val:  80.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 73.4981%\n",
      "layer   3  Sparsity: 65.9477%\n",
      "total_backward_count 1321650 real_backward_count 106218   8.037%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.848770/ 39.980782, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 73.3825%\n",
      "layer   3  Sparsity: 66.0940%\n",
      "total_backward_count 1331440 real_backward_count 106617   8.008%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.410592/ 33.223270, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 73.5256%\n",
      "layer   3  Sparsity: 66.5004%\n",
      "total_backward_count 1341230 real_backward_count 106959   7.975%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.648824/ 52.592365, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 73.6500%\n",
      "layer   3  Sparsity: 66.5901%\n",
      "total_backward_count 1351020 real_backward_count 107367   7.947%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.567862/ 37.164711, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 73.6844%\n",
      "layer   3  Sparsity: 66.6489%\n",
      "total_backward_count 1360810 real_backward_count 107723   7.916%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.599017/ 38.975746, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 73.6043%\n",
      "layer   3  Sparsity: 66.4847%\n",
      "total_backward_count 1370600 real_backward_count 108097   7.887%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.486505/ 38.356823, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 73.4676%\n",
      "layer   3  Sparsity: 66.8068%\n",
      "total_backward_count 1380390 real_backward_count 108472   7.858%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.840556/ 38.048157, val:  87.08%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 73.6002%\n",
      "layer   3  Sparsity: 66.7696%\n",
      "total_backward_count 1390180 real_backward_count 108885   7.832%\n",
      "lif layer 1 self.abs_max_v: 5821.5\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.597628/ 45.195290, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.30 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 73.6363%\n",
      "layer   3  Sparsity: 66.5534%\n",
      "total_backward_count 1399970 real_backward_count 109271   7.805%\n",
      "fc layer 2 self.abs_max_out: 3189.0\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.336358/ 42.040409, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.72 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 73.4317%\n",
      "layer   3  Sparsity: 66.5151%\n",
      "total_backward_count 1409760 real_backward_count 109637   7.777%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.427842/ 41.982697, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 73.3681%\n",
      "layer   3  Sparsity: 66.6322%\n",
      "total_backward_count 1419550 real_backward_count 110015   7.750%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.431756/ 44.860756, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 73.4308%\n",
      "layer   3  Sparsity: 66.5198%\n",
      "total_backward_count 1429340 real_backward_count 110391   7.723%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.597835/ 43.958279, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 73.4485%\n",
      "layer   3  Sparsity: 66.3849%\n",
      "total_backward_count 1439130 real_backward_count 110803   7.699%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.687509/ 33.464745, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 73.5281%\n",
      "layer   3  Sparsity: 66.4682%\n",
      "total_backward_count 1448920 real_backward_count 111177   7.673%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.673875/ 46.799717, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 73.6116%\n",
      "layer   3  Sparsity: 66.2893%\n",
      "total_backward_count 1458710 real_backward_count 111575   7.649%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.755086/ 43.268684, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 73.6227%\n",
      "layer   3  Sparsity: 65.4480%\n",
      "total_backward_count 1468500 real_backward_count 111980   7.625%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.698278/ 44.541607, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 73.6221%\n",
      "layer   3  Sparsity: 65.8061%\n",
      "total_backward_count 1478290 real_backward_count 112362   7.601%\n",
      "lif layer 1 self.abs_max_v: 5863.5\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.649348/ 47.065872, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 73.6500%\n",
      "layer   3  Sparsity: 66.1450%\n",
      "total_backward_count 1488080 real_backward_count 112731   7.576%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.755673/ 39.829746, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 73.4004%\n",
      "layer   3  Sparsity: 65.6084%\n",
      "total_backward_count 1497870 real_backward_count 113114   7.552%\n",
      "lif layer 1 self.abs_max_v: 5903.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.363683/ 36.156124, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 73.5789%\n",
      "layer   3  Sparsity: 65.6478%\n",
      "total_backward_count 1507660 real_backward_count 113477   7.527%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.328957/ 38.318340, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 73.3841%\n",
      "layer   3  Sparsity: 65.4913%\n",
      "total_backward_count 1517450 real_backward_count 113828   7.501%\n",
      "fc layer 1 self.abs_max_out: 3486.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.547851/ 49.469997, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 73.5298%\n",
      "layer   3  Sparsity: 65.6361%\n",
      "total_backward_count 1527240 real_backward_count 114193   7.477%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.459030/ 50.800049, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 73.5414%\n",
      "layer   3  Sparsity: 66.3385%\n",
      "total_backward_count 1537030 real_backward_count 114555   7.453%\n",
      "fc layer 1 self.abs_max_out: 3506.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.344707/ 46.973499, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 73.5571%\n",
      "layer   3  Sparsity: 66.0198%\n",
      "total_backward_count 1546820 real_backward_count 114883   7.427%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  2.269973/ 38.372490, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 73.4665%\n",
      "layer   3  Sparsity: 66.2793%\n",
      "total_backward_count 1556610 real_backward_count 115231   7.403%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  2.567594/ 43.916454, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 73.4646%\n",
      "layer   3  Sparsity: 66.1686%\n",
      "total_backward_count 1566400 real_backward_count 115582   7.379%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  2.307293/ 42.203014, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 73.6582%\n",
      "layer   3  Sparsity: 66.1571%\n",
      "total_backward_count 1576190 real_backward_count 115929   7.355%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  2.502507/ 40.567341, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 73.8345%\n",
      "layer   3  Sparsity: 65.9629%\n",
      "total_backward_count 1585980 real_backward_count 116264   7.331%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  2.068141/ 41.456947, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 73.8506%\n",
      "layer   3  Sparsity: 65.9165%\n",
      "total_backward_count 1595770 real_backward_count 116574   7.305%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.971648/ 42.050224, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 73.8168%\n",
      "layer   3  Sparsity: 66.0191%\n",
      "total_backward_count 1605560 real_backward_count 116892   7.280%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.062189/ 38.644615, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 73.8116%\n",
      "layer   3  Sparsity: 66.0275%\n",
      "total_backward_count 1615350 real_backward_count 117196   7.255%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.386207/ 46.322186, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 73.7634%\n",
      "layer   3  Sparsity: 66.1022%\n",
      "total_backward_count 1625140 real_backward_count 117537   7.232%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  2.371725/ 50.835243, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 73.6183%\n",
      "layer   3  Sparsity: 66.3000%\n",
      "total_backward_count 1634930 real_backward_count 117868   7.209%\n",
      "fc layer 1 self.abs_max_out: 3570.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  2.177630/ 49.587997, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 73.4662%\n",
      "layer   3  Sparsity: 66.2705%\n",
      "total_backward_count 1644720 real_backward_count 118194   7.186%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  2.148670/ 41.077385, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 73.3817%\n",
      "layer   3  Sparsity: 66.2576%\n",
      "total_backward_count 1654510 real_backward_count 118525   7.164%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  2.041941/ 38.599815, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 73.5802%\n",
      "layer   3  Sparsity: 66.3450%\n",
      "total_backward_count 1664300 real_backward_count 118853   7.141%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  2.088604/ 40.479248, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 73.5625%\n",
      "layer   3  Sparsity: 66.3609%\n",
      "total_backward_count 1674090 real_backward_count 119147   7.117%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  2.165686/ 39.004532, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 73.6607%\n",
      "layer   3  Sparsity: 66.1724%\n",
      "total_backward_count 1683880 real_backward_count 119457   7.094%\n",
      "lif layer 1 self.abs_max_v: 5911.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  2.070904/ 44.368374, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 73.7515%\n",
      "layer   3  Sparsity: 66.0235%\n",
      "total_backward_count 1693670 real_backward_count 119787   7.073%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  2.116019/ 43.031197, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 73.8514%\n",
      "layer   3  Sparsity: 65.5164%\n",
      "total_backward_count 1703460 real_backward_count 120107   7.051%\n",
      "lif layer 1 self.abs_max_v: 5992.5\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.993853/ 43.234081, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 74.0026%\n",
      "layer   3  Sparsity: 65.6495%\n",
      "total_backward_count 1713250 real_backward_count 120410   7.028%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  2.150770/ 40.176582, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 74.0454%\n",
      "layer   3  Sparsity: 66.0181%\n",
      "total_backward_count 1723040 real_backward_count 120724   7.006%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  2.333229/ 41.525970, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 73.9936%\n",
      "layer   3  Sparsity: 65.8833%\n",
      "total_backward_count 1732830 real_backward_count 121071   6.987%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.937667/ 41.858341, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 73.9177%\n",
      "layer   3  Sparsity: 66.0598%\n",
      "total_backward_count 1742620 real_backward_count 121367   6.965%\n",
      "fc layer 3 self.abs_max_out: 1250.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.831835/ 42.111488, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 73.7054%\n",
      "layer   3  Sparsity: 66.3892%\n",
      "total_backward_count 1752410 real_backward_count 121641   6.941%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  2.150848/ 44.355225, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 73.6960%\n",
      "layer   3  Sparsity: 66.6943%\n",
      "total_backward_count 1762200 real_backward_count 121950   6.920%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.830841/ 43.510017, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 73.7554%\n",
      "layer   3  Sparsity: 66.3016%\n",
      "total_backward_count 1771990 real_backward_count 122230   6.898%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  2.025347/ 39.872540, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 73.6698%\n",
      "layer   3  Sparsity: 66.1146%\n",
      "total_backward_count 1781780 real_backward_count 122536   6.877%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.666869/ 39.470863, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 73.7688%\n",
      "layer   3  Sparsity: 66.3978%\n",
      "total_backward_count 1791570 real_backward_count 122791   6.854%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.950659/ 55.599583, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 73.7074%\n",
      "layer   3  Sparsity: 66.2561%\n",
      "total_backward_count 1801360 real_backward_count 123072   6.832%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.899752/ 37.789501, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 73.8365%\n",
      "layer   3  Sparsity: 66.2696%\n",
      "total_backward_count 1811150 real_backward_count 123350   6.811%\n",
      "fc layer 3 self.abs_max_out: 1264.0\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.899554/ 37.145790, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 73.5694%\n",
      "layer   3  Sparsity: 66.0735%\n",
      "total_backward_count 1820940 real_backward_count 123628   6.789%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.603228/ 46.019447, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 73.6801%\n",
      "layer   3  Sparsity: 65.8277%\n",
      "total_backward_count 1830730 real_backward_count 123883   6.767%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.935990/ 40.719200, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 73.6907%\n",
      "layer   3  Sparsity: 65.8488%\n",
      "total_backward_count 1840520 real_backward_count 124181   6.747%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.762073/ 38.231136, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 73.8491%\n",
      "layer   3  Sparsity: 65.7107%\n",
      "total_backward_count 1850310 real_backward_count 124465   6.727%\n",
      "lif layer 1 self.abs_max_v: 6094.5\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.486967/ 43.791412, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 73.9032%\n",
      "layer   3  Sparsity: 66.0854%\n",
      "total_backward_count 1860100 real_backward_count 124716   6.705%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.704369/ 40.692268, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 74.0256%\n",
      "layer   3  Sparsity: 66.4250%\n",
      "total_backward_count 1869890 real_backward_count 124967   6.683%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.792917/ 65.001320, val:  76.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 73.7790%\n",
      "layer   3  Sparsity: 65.9816%\n",
      "total_backward_count 1879680 real_backward_count 125236   6.663%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.805418/ 46.549290, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 73.9373%\n",
      "layer   3  Sparsity: 66.1247%\n",
      "total_backward_count 1889470 real_backward_count 125509   6.643%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.725522/ 43.490536, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 73.9726%\n",
      "layer   3  Sparsity: 66.1542%\n",
      "total_backward_count 1899260 real_backward_count 125777   6.622%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.742095/ 43.572327, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 73.9879%\n",
      "layer   3  Sparsity: 65.7530%\n",
      "total_backward_count 1909050 real_backward_count 126032   6.602%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.644941/ 42.028988, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 74.0502%\n",
      "layer   3  Sparsity: 65.4406%\n",
      "total_backward_count 1918840 real_backward_count 126287   6.581%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.907111/ 40.458878, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 73.8030%\n",
      "layer   3  Sparsity: 65.5160%\n",
      "total_backward_count 1928630 real_backward_count 126551   6.562%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.768402/ 40.836906, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 73.8153%\n",
      "layer   3  Sparsity: 65.3660%\n",
      "total_backward_count 1938420 real_backward_count 126839   6.543%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.725282/ 41.781784, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 73.7370%\n",
      "layer   3  Sparsity: 65.3794%\n",
      "total_backward_count 1948210 real_backward_count 127079   6.523%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.769326/ 46.493290, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 73.9502%\n",
      "layer   3  Sparsity: 64.8934%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785bd77b7cb64cfe893993544aeb7bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.76933</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>46.49329</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-sweep-827</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkn51e5s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tkn51e5s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251211_125709-tkn51e5s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3ky115ts with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251211_171834-3ky115ts</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3ky115ts' target=\"_blank\">lunar-sweep-835</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3ky115ts' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3ky115ts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251211_171842_692', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 64, 'lif_layer_v_threshold2': 32, 'init_scaling': [0.5, 1, 0.5], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 64, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 1, 0.5])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 1, 0.5])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 1, 0.5])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 742.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1642.0\n",
      "lif layer 2 self.abs_max_v: 1642.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 1 self.abs_max_out: 791.0\n",
      "lif layer 1 self.abs_max_v: 896.5\n",
      "fc layer 2 self.abs_max_out: 2132.0\n",
      "lif layer 2 self.abs_max_v: 2775.0\n",
      "fc layer 3 self.abs_max_out: 816.0\n",
      "fc layer 1 self.abs_max_out: 815.0\n",
      "lif layer 1 self.abs_max_v: 1078.0\n",
      "lif layer 2 self.abs_max_v: 2935.5\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 1123.0\n",
      "lif layer 1 self.abs_max_v: 1316.5\n",
      "fc layer 3 self.abs_max_out: 822.0\n",
      "fc layer 1 self.abs_max_out: 1051.0\n",
      "lif layer 1 self.abs_max_v: 1671.5\n",
      "lif layer 2 self.abs_max_v: 3201.0\n",
      "lif layer 2 self.abs_max_v: 3220.5\n",
      "fc layer 1 self.abs_max_out: 1418.0\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "lif layer 1 self.abs_max_v: 1701.5\n",
      "fc layer 1 self.abs_max_out: 1478.0\n",
      "lif layer 1 self.abs_max_v: 2008.5\n",
      "fc layer 2 self.abs_max_out: 2644.0\n",
      "fc layer 1 self.abs_max_out: 1525.0\n",
      "lif layer 2 self.abs_max_v: 3471.0\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "fc layer 1 self.abs_max_out: 1533.0\n",
      "lif layer 1 self.abs_max_v: 2205.0\n",
      "fc layer 1 self.abs_max_out: 1849.0\n",
      "lif layer 1 self.abs_max_v: 2769.5\n",
      "fc layer 1 self.abs_max_out: 1951.0\n",
      "fc layer 3 self.abs_max_out: 1081.0\n",
      "lif layer 1 self.abs_max_v: 2771.5\n",
      "lif layer 1 self.abs_max_v: 2816.0\n",
      "lif layer 1 self.abs_max_v: 2852.5\n",
      "lif layer 2 self.abs_max_v: 3780.5\n",
      "lif layer 2 self.abs_max_v: 4140.0\n",
      "fc layer 2 self.abs_max_out: 2668.0\n",
      "fc layer 1 self.abs_max_out: 2141.0\n",
      "lif layer 1 self.abs_max_v: 3099.0\n",
      "lif layer 2 self.abs_max_v: 4207.5\n",
      "fc layer 2 self.abs_max_out: 2697.0\n",
      "fc layer 2 self.abs_max_out: 2711.0\n",
      "lif layer 2 self.abs_max_v: 4213.5\n",
      "lif layer 2 self.abs_max_v: 4486.5\n",
      "lif layer 2 self.abs_max_v: 4758.5\n",
      "fc layer 2 self.abs_max_out: 2716.0\n",
      "fc layer 2 self.abs_max_out: 2767.0\n",
      "fc layer 2 self.abs_max_out: 3139.0\n",
      "fc layer 1 self.abs_max_out: 2187.0\n",
      "lif layer 1 self.abs_max_v: 3528.5\n",
      "fc layer 1 self.abs_max_out: 2236.0\n",
      "fc layer 2 self.abs_max_out: 3164.0\n",
      "lif layer 2 self.abs_max_v: 5193.5\n",
      "fc layer 1 self.abs_max_out: 2269.0\n",
      "lif layer 1 self.abs_max_v: 3532.5\n",
      "lif layer 1 self.abs_max_v: 3698.5\n",
      "fc layer 3 self.abs_max_out: 1156.0\n",
      "fc layer 1 self.abs_max_out: 2349.0\n",
      "fc layer 1 self.abs_max_out: 2472.0\n",
      "lif layer 1 self.abs_max_v: 4036.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 76.178703/185.162689, val:  32.92%, val_best:  32.92%, tr:  82.74%, tr_best:  82.74%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 61.7885%\n",
      "layer   3  Sparsity: 61.3723%\n",
      "total_backward_count 9790 real_backward_count 3785  38.662%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2571.0\n",
      "lif layer 1 self.abs_max_v: 4082.5\n",
      "lif layer 1 self.abs_max_v: 4094.5\n",
      "fc layer 2 self.abs_max_out: 3181.0\n",
      "fc layer 2 self.abs_max_out: 3247.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 36.990063/131.004852, val:  40.00%, val_best:  40.00%, tr:  93.67%, tr_best:  93.67%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 62.2737%\n",
      "layer   3  Sparsity: 61.6971%\n",
      "total_backward_count 19580 real_backward_count 6391  32.640%\n",
      "lif layer 1 self.abs_max_v: 4124.5\n",
      "lif layer 2 self.abs_max_v: 5312.5\n",
      "fc layer 3 self.abs_max_out: 1176.0\n",
      "lif layer 1 self.abs_max_v: 4191.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 27.775909/116.486763, val:  39.58%, val_best:  40.00%, tr:  96.02%, tr_best:  96.02%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 63.5814%\n",
      "layer   3  Sparsity: 61.5748%\n",
      "total_backward_count 29370 real_backward_count 8657  29.476%\n",
      "lif layer 2 self.abs_max_v: 5398.0\n",
      "fc layer 1 self.abs_max_out: 2598.0\n",
      "lif layer 1 self.abs_max_v: 4263.5\n",
      "fc layer 1 self.abs_max_out: 2599.0\n",
      "fc layer 1 self.abs_max_out: 2603.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 21.968708/ 93.634552, val:  40.00%, val_best:  40.00%, tr:  97.34%, tr_best:  97.34%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 64.3773%\n",
      "layer   3  Sparsity: 62.0019%\n",
      "total_backward_count 39160 real_backward_count 10665  27.234%\n",
      "fc layer 1 self.abs_max_out: 2686.0\n",
      "lif layer 1 self.abs_max_v: 4290.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 18.472782/ 82.298996, val:  45.00%, val_best:  45.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 64.8088%\n",
      "layer   3  Sparsity: 61.1885%\n",
      "total_backward_count 48950 real_backward_count 12527  25.591%\n",
      "fc layer 1 self.abs_max_out: 2689.0\n",
      "lif layer 1 self.abs_max_v: 4328.0\n",
      "lif layer 1 self.abs_max_v: 4368.0\n",
      "fc layer 1 self.abs_max_out: 2775.0\n",
      "fc layer 1 self.abs_max_out: 2990.0\n",
      "lif layer 1 self.abs_max_v: 4710.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 17.854660/122.568527, val:  33.75%, val_best:  45.00%, tr:  98.77%, tr_best:  99.18%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 65.5213%\n",
      "layer   3  Sparsity: 61.5502%\n",
      "total_backward_count 58740 real_backward_count 14317  24.374%\n",
      "fc layer 2 self.abs_max_out: 3329.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 15.771086/ 81.936966, val:  46.25%, val_best:  46.25%, tr:  98.67%, tr_best:  99.18%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 66.2158%\n",
      "layer   3  Sparsity: 62.0521%\n",
      "total_backward_count 68530 real_backward_count 16050  23.420%\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 15.596395/ 80.554024, val:  48.75%, val_best:  48.75%, tr:  98.88%, tr_best:  99.18%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 66.7128%\n",
      "layer   3  Sparsity: 62.5273%\n",
      "total_backward_count 78320 real_backward_count 17763  22.680%\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 15.668533/ 88.317009, val:  44.58%, val_best:  48.75%, tr:  98.88%, tr_best:  99.18%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 67.0547%\n",
      "layer   3  Sparsity: 61.4005%\n",
      "total_backward_count 88110 real_backward_count 19463  22.089%\n",
      "lif layer 1 self.abs_max_v: 4721.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 14.421290/ 84.952301, val:  45.83%, val_best:  48.75%, tr:  98.77%, tr_best:  99.18%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 66.8633%\n",
      "layer   3  Sparsity: 60.6183%\n",
      "total_backward_count 97900 real_backward_count 21111  21.564%\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 13.676639/ 92.075653, val:  43.75%, val_best:  48.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 67.1099%\n",
      "layer   3  Sparsity: 60.6117%\n",
      "total_backward_count 107690 real_backward_count 22669  21.050%\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 13.627712/ 64.794724, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 67.1095%\n",
      "layer   3  Sparsity: 60.6829%\n",
      "total_backward_count 117480 real_backward_count 24212  20.609%\n",
      "lif layer 1 self.abs_max_v: 4739.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 12.419293/ 79.422760, val:  42.08%, val_best:  49.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 67.1116%\n",
      "layer   3  Sparsity: 60.8095%\n",
      "total_backward_count 127270 real_backward_count 25681  20.178%\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 12.163672/111.960884, val:  34.17%, val_best:  49.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 67.1231%\n",
      "layer   3  Sparsity: 61.2313%\n",
      "total_backward_count 137060 real_backward_count 27148  19.807%\n",
      "fc layer 1 self.abs_max_out: 3121.0\n",
      "lif layer 1 self.abs_max_v: 4970.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 12.250566/ 60.500973, val:  51.25%, val_best:  51.25%, tr:  98.88%, tr_best:  99.59%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 67.1053%\n",
      "layer   3  Sparsity: 60.8501%\n",
      "total_backward_count 146850 real_backward_count 28584  19.465%\n",
      "fc layer 1 self.abs_max_out: 3162.0\n",
      "lif layer 1 self.abs_max_v: 5042.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 12.284372/ 88.131943, val:  43.33%, val_best:  51.25%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 66.9349%\n",
      "layer   3  Sparsity: 60.4553%\n",
      "total_backward_count 156640 real_backward_count 30031  19.172%\n",
      "lif layer 1 self.abs_max_v: 5088.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 11.808925/ 53.602104, val:  55.42%, val_best:  55.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 67.8189%\n",
      "layer   3  Sparsity: 60.2251%\n",
      "total_backward_count 166430 real_backward_count 31428  18.884%\n",
      "lif layer 1 self.abs_max_v: 5159.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 11.988838/ 56.475224, val:  55.42%, val_best:  55.42%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 68.2436%\n",
      "layer   3  Sparsity: 60.9076%\n",
      "total_backward_count 176220 real_backward_count 32841  18.636%\n",
      "lif layer 1 self.abs_max_v: 5203.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 12.181247/ 61.174587, val:  55.00%, val_best:  55.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 68.6514%\n",
      "layer   3  Sparsity: 60.8271%\n",
      "total_backward_count 186010 real_backward_count 34294  18.437%\n",
      "fc layer 1 self.abs_max_out: 3169.0\n",
      "fc layer 1 self.abs_max_out: 3192.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 11.130376/ 63.163826, val:  49.58%, val_best:  55.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 68.7781%\n",
      "layer   3  Sparsity: 60.7813%\n",
      "total_backward_count 195800 real_backward_count 35644  18.204%\n",
      "fc layer 1 self.abs_max_out: 3201.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 11.503109/ 81.027756, val:  44.17%, val_best:  55.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 68.8218%\n",
      "layer   3  Sparsity: 60.8263%\n",
      "total_backward_count 205590 real_backward_count 37041  18.017%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 11.829458/ 74.743607, val:  46.67%, val_best:  55.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 69.3491%\n",
      "layer   3  Sparsity: 61.3709%\n",
      "total_backward_count 215380 real_backward_count 38506  17.878%\n",
      "lif layer 1 self.abs_max_v: 5379.5\n",
      "lif layer 1 self.abs_max_v: 5613.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 11.167727/ 62.407028, val:  52.50%, val_best:  55.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 69.9702%\n",
      "layer   3  Sparsity: 61.4158%\n",
      "total_backward_count 225170 real_backward_count 39893  17.717%\n",
      "fc layer 1 self.abs_max_out: 3270.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 11.367909/ 58.490406, val:  57.92%, val_best:  57.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 69.6679%\n",
      "layer   3  Sparsity: 61.4098%\n",
      "total_backward_count 234960 real_backward_count 41276  17.567%\n",
      "fc layer 1 self.abs_max_out: 3276.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 10.943698/ 78.155891, val:  42.50%, val_best:  57.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 69.0868%\n",
      "layer   3  Sparsity: 61.9116%\n",
      "total_backward_count 244750 real_backward_count 42620  17.414%\n",
      "fc layer 1 self.abs_max_out: 3326.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 11.816413/ 49.165627, val:  56.67%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 69.0062%\n",
      "layer   3  Sparsity: 61.6829%\n",
      "total_backward_count 254540 real_backward_count 44057  17.308%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 10.821650/ 41.647038, val:  64.17%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 69.1914%\n",
      "layer   3  Sparsity: 61.5329%\n",
      "total_backward_count 264330 real_backward_count 45377  17.167%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 11.473309/ 51.810696, val:  56.67%, val_best:  64.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 69.0121%\n",
      "layer   3  Sparsity: 61.9213%\n",
      "total_backward_count 274120 real_backward_count 46789  17.069%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 10.500144/ 60.634705, val:  51.67%, val_best:  64.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 69.2086%\n",
      "layer   3  Sparsity: 62.5786%\n",
      "total_backward_count 283910 real_backward_count 48135  16.954%\n",
      "fc layer 1 self.abs_max_out: 3368.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 10.346605/ 67.499771, val:  50.83%, val_best:  64.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 69.4147%\n",
      "layer   3  Sparsity: 62.4377%\n",
      "total_backward_count 293700 real_backward_count 49483  16.848%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 10.455692/ 65.844231, val:  49.58%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 69.4125%\n",
      "layer   3  Sparsity: 62.6746%\n",
      "total_backward_count 303490 real_backward_count 50814  16.743%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 10.571053/ 75.844337, val:  53.33%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 69.4622%\n",
      "layer   3  Sparsity: 62.7682%\n",
      "total_backward_count 313280 real_backward_count 52154  16.648%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 10.731556/ 61.552479, val:  60.42%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 69.3215%\n",
      "layer   3  Sparsity: 62.4360%\n",
      "total_backward_count 323070 real_backward_count 53486  16.556%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 10.981560/ 70.621986, val:  46.25%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 69.8957%\n",
      "layer   3  Sparsity: 62.4450%\n",
      "total_backward_count 332860 real_backward_count 54848  16.478%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 10.011291/ 75.360405, val:  47.08%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 69.2196%\n",
      "layer   3  Sparsity: 62.9785%\n",
      "total_backward_count 342650 real_backward_count 56171  16.393%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  9.664679/ 58.503742, val:  53.33%, val_best:  64.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 69.6044%\n",
      "layer   3  Sparsity: 63.0823%\n",
      "total_backward_count 352440 real_backward_count 57434  16.296%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  9.832314/ 47.378006, val:  57.50%, val_best:  64.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 69.7508%\n",
      "layer   3  Sparsity: 62.9386%\n",
      "total_backward_count 362230 real_backward_count 58708  16.207%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  9.286419/ 63.136208, val:  51.67%, val_best:  64.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 69.8608%\n",
      "layer   3  Sparsity: 62.8091%\n",
      "total_backward_count 372020 real_backward_count 59957  16.117%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 10.144897/ 50.225124, val:  56.67%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 70.0100%\n",
      "layer   3  Sparsity: 62.8715%\n",
      "total_backward_count 381810 real_backward_count 61291  16.053%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  9.893087/ 91.964706, val:  47.08%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 70.2816%\n",
      "layer   3  Sparsity: 62.9278%\n",
      "total_backward_count 391600 real_backward_count 62601  15.986%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  9.559723/ 57.827156, val:  57.50%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 70.4597%\n",
      "layer   3  Sparsity: 63.5155%\n",
      "total_backward_count 401390 real_backward_count 63893  15.918%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  9.790518/ 43.333122, val:  59.58%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 70.4199%\n",
      "layer   3  Sparsity: 63.6678%\n",
      "total_backward_count 411180 real_backward_count 65172  15.850%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  9.812749/ 56.918442, val:  51.25%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 70.1799%\n",
      "layer   3  Sparsity: 63.6764%\n",
      "total_backward_count 420970 real_backward_count 66424  15.779%\n",
      "fc layer 1 self.abs_max_out: 3386.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  9.971750/ 58.102119, val:  58.75%, val_best:  64.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 70.2872%\n",
      "layer   3  Sparsity: 63.5627%\n",
      "total_backward_count 430760 real_backward_count 67721  15.721%\n",
      "fc layer 1 self.abs_max_out: 3438.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 10.001893/ 40.810913, val:  63.75%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 70.2178%\n",
      "layer   3  Sparsity: 63.8389%\n",
      "total_backward_count 440550 real_backward_count 69034  15.670%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  9.221339/ 50.598843, val:  56.67%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 70.1642%\n",
      "layer   3  Sparsity: 63.5281%\n",
      "total_backward_count 450340 real_backward_count 70286  15.607%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  9.608343/ 54.059345, val:  56.67%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 70.3844%\n",
      "layer   3  Sparsity: 63.1000%\n",
      "total_backward_count 460130 real_backward_count 71547  15.549%\n",
      "lif layer 1 self.abs_max_v: 5755.5\n",
      "lif layer 1 self.abs_max_v: 5839.0\n",
      "lif layer 1 self.abs_max_v: 5843.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  9.799088/ 98.322525, val:  42.50%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 70.2672%\n",
      "layer   3  Sparsity: 62.8519%\n",
      "total_backward_count 469920 real_backward_count 72815  15.495%\n",
      "fc layer 1 self.abs_max_out: 3456.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  9.187326/ 35.903000, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 69.9833%\n",
      "layer   3  Sparsity: 62.8417%\n",
      "total_backward_count 479710 real_backward_count 74058  15.438%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  9.530189/ 55.320236, val:  57.92%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 70.3565%\n",
      "layer   3  Sparsity: 62.9179%\n",
      "total_backward_count 489500 real_backward_count 75320  15.387%\n",
      "lif layer 1 self.abs_max_v: 5987.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  9.690671/ 74.306839, val:  58.75%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 70.9315%\n",
      "layer   3  Sparsity: 63.0684%\n",
      "total_backward_count 499290 real_backward_count 76592  15.340%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  9.551261/ 49.449932, val:  56.25%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 71.1180%\n",
      "layer   3  Sparsity: 63.4182%\n",
      "total_backward_count 509080 real_backward_count 77844  15.291%\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  9.256035/ 50.518776, val:  64.17%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 71.0342%\n",
      "layer   3  Sparsity: 63.8260%\n",
      "total_backward_count 518870 real_backward_count 79072  15.239%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  9.088395/ 73.182159, val:  53.75%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 71.2612%\n",
      "layer   3  Sparsity: 64.1099%\n",
      "total_backward_count 528660 real_backward_count 80304  15.190%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  9.407657/ 52.341175, val:  60.00%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 71.0702%\n",
      "layer   3  Sparsity: 64.2662%\n",
      "total_backward_count 538450 real_backward_count 81543  15.144%\n",
      "lif layer 1 self.abs_max_v: 6085.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  9.190701/ 57.630756, val:  57.50%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 70.7186%\n",
      "layer   3  Sparsity: 64.0943%\n",
      "total_backward_count 548240 real_backward_count 82811  15.105%\n",
      "fc layer 1 self.abs_max_out: 3482.0\n",
      "lif layer 1 self.abs_max_v: 6172.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  9.116355/ 62.328171, val:  53.75%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 70.9040%\n",
      "layer   3  Sparsity: 63.8589%\n",
      "total_backward_count 558030 real_backward_count 84067  15.065%\n",
      "lif layer 1 self.abs_max_v: 6239.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  8.891343/ 56.283031, val:  56.67%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 70.8114%\n",
      "layer   3  Sparsity: 64.3758%\n",
      "total_backward_count 567820 real_backward_count 85277  15.018%\n",
      "lif layer 1 self.abs_max_v: 6329.5\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  8.074002/ 53.948166, val:  57.92%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 70.8635%\n",
      "layer   3  Sparsity: 64.5555%\n",
      "total_backward_count 577610 real_backward_count 86426  14.963%\n",
      "lif layer 1 self.abs_max_v: 6435.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  9.201407/ 69.123802, val:  47.08%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 71.0753%\n",
      "layer   3  Sparsity: 64.5434%\n",
      "total_backward_count 587400 real_backward_count 87681  14.927%\n",
      "fc layer 1 self.abs_max_out: 3570.0\n",
      "lif layer 1 self.abs_max_v: 6781.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  8.721581/ 67.033180, val:  52.92%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 71.0705%\n",
      "layer   3  Sparsity: 64.5555%\n",
      "total_backward_count 597190 real_backward_count 88898  14.886%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  9.280665/ 43.501278, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 71.3424%\n",
      "layer   3  Sparsity: 64.5056%\n",
      "total_backward_count 606980 real_backward_count 90169  14.855%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  9.112561/ 53.591560, val:  57.50%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 71.5018%\n",
      "layer   3  Sparsity: 64.7737%\n",
      "total_backward_count 616770 real_backward_count 91428  14.824%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  8.768639/ 57.209152, val:  55.42%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 71.5032%\n",
      "layer   3  Sparsity: 64.7768%\n",
      "total_backward_count 626560 real_backward_count 92612  14.781%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  8.264915/ 49.950027, val:  61.67%, val_best:  71.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 71.4623%\n",
      "layer   3  Sparsity: 65.1308%\n",
      "total_backward_count 636350 real_backward_count 93805  14.741%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  8.343885/ 43.110023, val:  63.75%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 71.3191%\n",
      "layer   3  Sparsity: 65.1640%\n",
      "total_backward_count 646140 real_backward_count 94958  14.696%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  8.735511/ 44.782558, val:  62.50%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 71.2756%\n",
      "layer   3  Sparsity: 65.1761%\n",
      "total_backward_count 655930 real_backward_count 96156  14.659%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  8.411611/ 62.194748, val:  60.42%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 71.1580%\n",
      "layer   3  Sparsity: 64.9426%\n",
      "total_backward_count 665720 real_backward_count 97369  14.626%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  8.836784/ 48.504051, val:  66.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 71.6897%\n",
      "layer   3  Sparsity: 65.2695%\n",
      "total_backward_count 675510 real_backward_count 98576  14.593%\n",
      "fc layer 1 self.abs_max_out: 3672.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  7.909926/ 69.527962, val:  52.08%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 71.5069%\n",
      "layer   3  Sparsity: 65.5578%\n",
      "total_backward_count 685300 real_backward_count 99694  14.547%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  8.435117/ 59.696434, val:  62.50%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 71.4006%\n",
      "layer   3  Sparsity: 65.6474%\n",
      "total_backward_count 695090 real_backward_count 100896  14.516%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  8.332415/ 39.110813, val:  67.50%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 70.9928%\n",
      "layer   3  Sparsity: 65.6750%\n",
      "total_backward_count 704880 real_backward_count 102062  14.479%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  8.260908/ 35.834492, val:  76.25%, val_best:  76.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 71.1605%\n",
      "layer   3  Sparsity: 65.6305%\n",
      "total_backward_count 714670 real_backward_count 103211  14.442%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  8.187251/ 73.210442, val:  57.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 71.2727%\n",
      "layer   3  Sparsity: 66.0409%\n",
      "total_backward_count 724460 real_backward_count 104355  14.405%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  8.258281/ 49.014534, val:  63.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 71.2215%\n",
      "layer   3  Sparsity: 66.1935%\n",
      "total_backward_count 734250 real_backward_count 105580  14.379%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  7.987576/ 41.156395, val:  66.67%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 71.0491%\n",
      "layer   3  Sparsity: 66.6016%\n",
      "total_backward_count 744040 real_backward_count 106750  14.347%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  8.569572/ 48.519127, val:  61.67%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 71.1329%\n",
      "layer   3  Sparsity: 66.6426%\n",
      "total_backward_count 753830 real_backward_count 107997  14.326%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  7.544416/ 74.223373, val:  50.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 71.4552%\n",
      "layer   3  Sparsity: 66.3242%\n",
      "total_backward_count 763620 real_backward_count 109131  14.291%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  8.230959/ 48.485535, val:  60.42%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 71.4061%\n",
      "layer   3  Sparsity: 65.8498%\n",
      "total_backward_count 773410 real_backward_count 110294  14.261%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  8.121659/ 51.813427, val:  61.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 71.6694%\n",
      "layer   3  Sparsity: 66.1388%\n",
      "total_backward_count 783200 real_backward_count 111434  14.228%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  7.913654/ 42.643074, val:  62.50%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 71.7034%\n",
      "layer   3  Sparsity: 66.5544%\n",
      "total_backward_count 792990 real_backward_count 112574  14.196%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  7.516156/ 92.512344, val:  39.58%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 71.4704%\n",
      "layer   3  Sparsity: 66.6626%\n",
      "total_backward_count 802780 real_backward_count 113679  14.161%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  8.318564/ 63.281170, val:  58.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 71.6156%\n",
      "layer   3  Sparsity: 66.9116%\n",
      "total_backward_count 812570 real_backward_count 114857  14.135%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  7.968053/ 57.512936, val:  67.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 71.7651%\n",
      "layer   3  Sparsity: 67.0486%\n",
      "total_backward_count 822360 real_backward_count 116007  14.107%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  7.635671/ 46.869083, val:  62.50%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 71.9197%\n",
      "layer   3  Sparsity: 67.3247%\n",
      "total_backward_count 832150 real_backward_count 117161  14.079%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  7.572277/ 47.863106, val:  65.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 72.0709%\n",
      "layer   3  Sparsity: 67.4934%\n",
      "total_backward_count 841940 real_backward_count 118332  14.055%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  7.603490/ 52.130852, val:  58.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 72.1098%\n",
      "layer   3  Sparsity: 67.0408%\n",
      "total_backward_count 851730 real_backward_count 119450  14.024%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  7.951398/ 52.894691, val:  63.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 71.9902%\n",
      "layer   3  Sparsity: 66.5335%\n",
      "total_backward_count 861520 real_backward_count 120616  14.000%\n",
      "fc layer 1 self.abs_max_out: 3684.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  6.997987/ 47.793056, val:  66.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 71.9099%\n",
      "layer   3  Sparsity: 66.8244%\n",
      "total_backward_count 871310 real_backward_count 121646  13.961%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  7.789955/ 38.300743, val:  72.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 71.5996%\n",
      "layer   3  Sparsity: 66.6304%\n",
      "total_backward_count 881100 real_backward_count 122776  13.934%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  7.770599/ 55.273441, val:  62.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 71.5291%\n",
      "layer   3  Sparsity: 66.5234%\n",
      "total_backward_count 890890 real_backward_count 123901  13.908%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  7.369204/ 38.414932, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 71.4754%\n",
      "layer   3  Sparsity: 66.6511%\n",
      "total_backward_count 900680 real_backward_count 124979  13.876%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  7.679874/ 69.327217, val:  53.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 71.3290%\n",
      "layer   3  Sparsity: 66.7568%\n",
      "total_backward_count 910470 real_backward_count 126079  13.848%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  7.605858/ 45.408714, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 71.6177%\n",
      "layer   3  Sparsity: 66.3598%\n",
      "total_backward_count 920260 real_backward_count 127217  13.824%\n",
      "fc layer 1 self.abs_max_out: 3706.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  7.431966/ 48.215103, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 71.5681%\n",
      "layer   3  Sparsity: 66.2440%\n",
      "total_backward_count 930050 real_backward_count 128305  13.795%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  7.510248/ 44.901245, val:  72.50%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 71.4747%\n",
      "layer   3  Sparsity: 65.9711%\n",
      "total_backward_count 939840 real_backward_count 129401  13.768%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  7.494906/ 58.659199, val:  60.00%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 71.6067%\n",
      "layer   3  Sparsity: 66.0748%\n",
      "total_backward_count 949630 real_backward_count 130498  13.742%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  7.030078/ 52.293701, val:  65.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 71.3400%\n",
      "layer   3  Sparsity: 66.3096%\n",
      "total_backward_count 959420 real_backward_count 131526  13.709%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  7.344327/ 40.073536, val:  69.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 71.3870%\n",
      "layer   3  Sparsity: 66.2468%\n",
      "total_backward_count 969210 real_backward_count 132554  13.676%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  6.842928/ 47.196682, val:  70.83%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 71.4308%\n",
      "layer   3  Sparsity: 66.0452%\n",
      "total_backward_count 979000 real_backward_count 133572  13.644%\n",
      "fc layer 1 self.abs_max_out: 3810.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  6.880869/ 47.226017, val:  67.08%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 71.2484%\n",
      "layer   3  Sparsity: 66.0817%\n",
      "total_backward_count 988790 real_backward_count 134648  13.617%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  7.477986/ 37.095795, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 71.4050%\n",
      "layer   3  Sparsity: 66.1813%\n",
      "total_backward_count 998580 real_backward_count 135687  13.588%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  6.929610/ 39.687901, val:  73.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 71.3062%\n",
      "layer   3  Sparsity: 66.2219%\n",
      "total_backward_count 1008370 real_backward_count 136715  13.558%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  7.020656/ 50.630230, val:  65.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 71.1981%\n",
      "layer   3  Sparsity: 66.2890%\n",
      "total_backward_count 1018160 real_backward_count 137731  13.527%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  7.292324/ 44.988258, val:  71.25%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 70.8878%\n",
      "layer   3  Sparsity: 66.1424%\n",
      "total_backward_count 1027950 real_backward_count 138794  13.502%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  6.839851/ 48.500900, val:  70.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 70.9277%\n",
      "layer   3  Sparsity: 66.2705%\n",
      "total_backward_count 1037740 real_backward_count 139811  13.473%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  6.777430/ 80.722496, val:  56.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 70.8422%\n",
      "layer   3  Sparsity: 66.3340%\n",
      "total_backward_count 1047530 real_backward_count 140830  13.444%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  7.339533/ 48.166569, val:  70.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 71.6183%\n",
      "layer   3  Sparsity: 66.3195%\n",
      "total_backward_count 1057320 real_backward_count 141876  13.418%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  7.133908/ 39.593105, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 71.8919%\n",
      "layer   3  Sparsity: 66.3427%\n",
      "total_backward_count 1067110 real_backward_count 142904  13.392%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  7.396591/ 53.677635, val:  66.25%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.42 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 71.8269%\n",
      "layer   3  Sparsity: 67.2205%\n",
      "total_backward_count 1076900 real_backward_count 143975  13.369%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  7.020509/ 40.705887, val:  73.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 71.6918%\n",
      "layer   3  Sparsity: 67.1837%\n",
      "total_backward_count 1086690 real_backward_count 145023  13.345%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  6.918405/ 47.725872, val:  63.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 71.7703%\n",
      "layer   3  Sparsity: 67.2014%\n",
      "total_backward_count 1096480 real_backward_count 146046  13.320%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  7.239664/ 36.567616, val:  77.92%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 71.5869%\n",
      "layer   3  Sparsity: 66.9529%\n",
      "total_backward_count 1106270 real_backward_count 147106  13.297%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  6.950027/ 46.015972, val:  69.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 71.5039%\n",
      "layer   3  Sparsity: 66.8543%\n",
      "total_backward_count 1116060 real_backward_count 148113  13.271%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  6.767265/ 43.721916, val:  77.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 71.6999%\n",
      "layer   3  Sparsity: 66.8391%\n",
      "total_backward_count 1125850 real_backward_count 149127  13.246%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  6.462283/ 34.739952, val:  77.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 71.4654%\n",
      "layer   3  Sparsity: 67.2094%\n",
      "total_backward_count 1135640 real_backward_count 150098  13.217%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  6.428702/ 48.859451, val:  66.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 71.2485%\n",
      "layer   3  Sparsity: 67.2640%\n",
      "total_backward_count 1145430 real_backward_count 151086  13.190%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  7.003178/ 43.448055, val:  74.17%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 71.1531%\n",
      "layer   3  Sparsity: 67.2802%\n",
      "total_backward_count 1155220 real_backward_count 152090  13.165%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  6.817006/ 43.211884, val:  67.08%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 71.5532%\n",
      "layer   3  Sparsity: 67.2565%\n",
      "total_backward_count 1165010 real_backward_count 153102  13.142%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  6.325212/ 32.389240, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 71.6824%\n",
      "layer   3  Sparsity: 67.6464%\n",
      "total_backward_count 1174800 real_backward_count 154113  13.118%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  6.389807/ 39.114151, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 71.8849%\n",
      "layer   3  Sparsity: 67.7947%\n",
      "total_backward_count 1184590 real_backward_count 155047  13.089%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  6.532010/ 46.648705, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 71.7977%\n",
      "layer   3  Sparsity: 67.9255%\n",
      "total_backward_count 1194380 real_backward_count 156057  13.066%\n",
      "fc layer 1 self.abs_max_out: 3881.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  7.038204/ 40.409401, val:  76.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 71.8356%\n",
      "layer   3  Sparsity: 67.6584%\n",
      "total_backward_count 1204170 real_backward_count 157082  13.045%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  6.741052/ 40.954205, val:  76.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 71.7495%\n",
      "layer   3  Sparsity: 67.3426%\n",
      "total_backward_count 1213960 real_backward_count 158055  13.020%\n",
      "fc layer 1 self.abs_max_out: 3884.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  6.181933/ 42.814560, val:  67.50%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 71.5906%\n",
      "layer   3  Sparsity: 67.1785%\n",
      "total_backward_count 1223750 real_backward_count 159013  12.994%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  6.362799/ 37.489170, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 71.5167%\n",
      "layer   3  Sparsity: 67.4769%\n",
      "total_backward_count 1233540 real_backward_count 159975  12.969%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  6.477632/ 64.464180, val:  65.83%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 71.2624%\n",
      "layer   3  Sparsity: 67.9279%\n",
      "total_backward_count 1243330 real_backward_count 160945  12.945%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  6.232037/ 44.493103, val:  70.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 71.2948%\n",
      "layer   3  Sparsity: 68.0641%\n",
      "total_backward_count 1253120 real_backward_count 161850  12.916%\n",
      "fc layer 1 self.abs_max_out: 3930.0\n",
      "lif layer 1 self.abs_max_v: 6862.0\n",
      "fc layer 1 self.abs_max_out: 3952.0\n",
      "lif layer 1 self.abs_max_v: 7214.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  6.675240/ 48.879650, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 71.2515%\n",
      "layer   3  Sparsity: 68.2483%\n",
      "total_backward_count 1262910 real_backward_count 162831  12.893%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  5.738186/ 38.108273, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 71.6627%\n",
      "layer   3  Sparsity: 68.2529%\n",
      "total_backward_count 1272700 real_backward_count 163717  12.864%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  6.047080/ 63.034805, val:  65.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 71.6331%\n",
      "layer   3  Sparsity: 68.1886%\n",
      "total_backward_count 1282490 real_backward_count 164622  12.836%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  6.394450/ 45.768993, val:  69.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 71.6279%\n",
      "layer   3  Sparsity: 68.0994%\n",
      "total_backward_count 1292280 real_backward_count 165606  12.815%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  5.917343/ 45.644806, val:  76.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 71.7106%\n",
      "layer   3  Sparsity: 68.0121%\n",
      "total_backward_count 1302070 real_backward_count 166503  12.788%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  5.737514/ 41.033703, val:  75.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 71.7048%\n",
      "layer   3  Sparsity: 68.2440%\n",
      "total_backward_count 1311860 real_backward_count 167423  12.762%\n",
      "fc layer 1 self.abs_max_out: 3963.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  6.342694/ 55.397301, val:  65.42%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 71.6048%\n",
      "layer   3  Sparsity: 67.9408%\n",
      "total_backward_count 1321650 real_backward_count 168344  12.737%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  6.462427/ 47.180729, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 71.6192%\n",
      "layer   3  Sparsity: 67.9400%\n",
      "total_backward_count 1331440 real_backward_count 169293  12.715%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  5.587599/ 37.058773, val:  77.50%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 71.8378%\n",
      "layer   3  Sparsity: 68.2474%\n",
      "total_backward_count 1341230 real_backward_count 170135  12.685%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  5.870656/ 63.169407, val:  65.83%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 71.8936%\n",
      "layer   3  Sparsity: 68.1126%\n",
      "total_backward_count 1351020 real_backward_count 171017  12.658%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  5.598325/ 37.769909, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 71.9465%\n",
      "layer   3  Sparsity: 68.0523%\n",
      "total_backward_count 1360810 real_backward_count 171901  12.632%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  5.953616/ 38.955078, val:  75.83%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 71.6842%\n",
      "layer   3  Sparsity: 68.2314%\n",
      "total_backward_count 1370600 real_backward_count 172772  12.606%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  5.752757/ 41.048206, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 71.2587%\n",
      "layer   3  Sparsity: 68.1428%\n",
      "total_backward_count 1380390 real_backward_count 173691  12.583%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  5.652791/ 36.416355, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 71.4155%\n",
      "layer   3  Sparsity: 68.4106%\n",
      "total_backward_count 1390180 real_backward_count 174594  12.559%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  6.012538/ 39.660988, val:  76.67%, val_best:  82.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 71.4342%\n",
      "layer   3  Sparsity: 68.1018%\n",
      "total_backward_count 1399970 real_backward_count 175497  12.536%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  5.479168/ 39.386745, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 71.4324%\n",
      "layer   3  Sparsity: 67.9902%\n",
      "total_backward_count 1409760 real_backward_count 176375  12.511%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  5.550928/ 38.521812, val:  77.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 71.6134%\n",
      "layer   3  Sparsity: 68.2949%\n",
      "total_backward_count 1419550 real_backward_count 177250  12.486%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  5.545459/ 72.292488, val:  60.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 71.6225%\n",
      "layer   3  Sparsity: 68.2773%\n",
      "total_backward_count 1429340 real_backward_count 178132  12.463%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  6.471499/ 39.266953, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.83 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 71.5298%\n",
      "layer   3  Sparsity: 68.4067%\n",
      "total_backward_count 1439130 real_backward_count 179083  12.444%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  5.403979/ 53.226662, val:  68.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 71.5212%\n",
      "layer   3  Sparsity: 68.4342%\n",
      "total_backward_count 1448920 real_backward_count 179911  12.417%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  5.612271/ 44.085075, val:  79.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 71.9158%\n",
      "layer   3  Sparsity: 68.5255%\n",
      "total_backward_count 1458710 real_backward_count 180774  12.393%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  5.900176/ 49.285347, val:  74.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 72.0513%\n",
      "layer   3  Sparsity: 68.1872%\n",
      "total_backward_count 1468500 real_backward_count 181663  12.371%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  5.571845/ 39.541782, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 72.0291%\n",
      "layer   3  Sparsity: 68.0209%\n",
      "total_backward_count 1478290 real_backward_count 182509  12.346%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  5.935817/ 44.242424, val:  70.00%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.62 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 71.5896%\n",
      "layer   3  Sparsity: 68.0272%\n",
      "total_backward_count 1488080 real_backward_count 183388  12.324%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  5.435839/ 50.682266, val:  74.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 71.7606%\n",
      "layer   3  Sparsity: 68.0435%\n",
      "total_backward_count 1497870 real_backward_count 184233  12.300%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  5.811208/ 50.138355, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 71.7168%\n",
      "layer   3  Sparsity: 68.4261%\n",
      "total_backward_count 1507660 real_backward_count 185119  12.279%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  5.515280/ 48.388916, val:  70.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 71.5783%\n",
      "layer   3  Sparsity: 68.2116%\n",
      "total_backward_count 1517450 real_backward_count 185975  12.256%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  5.226812/ 57.851471, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 71.6932%\n",
      "layer   3  Sparsity: 68.0032%\n",
      "total_backward_count 1527240 real_backward_count 186803  12.231%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  5.496965/ 44.822353, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 71.6001%\n",
      "layer   3  Sparsity: 67.9169%\n",
      "total_backward_count 1537030 real_backward_count 187641  12.208%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  5.089686/ 40.811520, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 71.6957%\n",
      "layer   3  Sparsity: 67.9257%\n",
      "total_backward_count 1546820 real_backward_count 188444  12.183%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  5.258887/ 36.927025, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 71.9276%\n",
      "layer   3  Sparsity: 68.1075%\n",
      "total_backward_count 1556610 real_backward_count 189238  12.157%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  5.207694/ 33.561077, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 72.0329%\n",
      "layer   3  Sparsity: 68.1718%\n",
      "total_backward_count 1566400 real_backward_count 190068  12.134%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  4.877328/ 35.980190, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 71.8804%\n",
      "layer   3  Sparsity: 68.3745%\n",
      "total_backward_count 1576190 real_backward_count 190837  12.107%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  5.514028/ 35.285252, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 71.8573%\n",
      "layer   3  Sparsity: 68.4280%\n",
      "total_backward_count 1585980 real_backward_count 191675  12.086%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  5.192530/ 67.102943, val:  59.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 71.9258%\n",
      "layer   3  Sparsity: 68.2379%\n",
      "total_backward_count 1595770 real_backward_count 192491  12.063%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  5.111800/ 35.604630, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 72.0892%\n",
      "layer   3  Sparsity: 68.1004%\n",
      "total_backward_count 1605560 real_backward_count 193267  12.037%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  4.966097/ 36.930481, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 71.8575%\n",
      "layer   3  Sparsity: 68.3136%\n",
      "total_backward_count 1615350 real_backward_count 194038  12.012%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  5.556479/ 43.609325, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 71.7751%\n",
      "layer   3  Sparsity: 68.7130%\n",
      "total_backward_count 1625140 real_backward_count 194893  11.992%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  5.190242/ 51.371243, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.06 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 71.9369%\n",
      "layer   3  Sparsity: 68.5669%\n",
      "total_backward_count 1634930 real_backward_count 195709  11.970%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  5.420234/ 53.579861, val:  68.75%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 71.8217%\n",
      "layer   3  Sparsity: 68.7381%\n",
      "total_backward_count 1644720 real_backward_count 196523  11.949%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  5.017417/ 44.821339, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 71.8362%\n",
      "layer   3  Sparsity: 68.8420%\n",
      "total_backward_count 1654510 real_backward_count 197293  11.925%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  4.769071/ 49.645676, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 72.1419%\n",
      "layer   3  Sparsity: 68.9114%\n",
      "total_backward_count 1664300 real_backward_count 198080  11.902%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  5.098833/ 48.155674, val:  76.67%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 72.1123%\n",
      "layer   3  Sparsity: 68.7046%\n",
      "total_backward_count 1674090 real_backward_count 198839  11.877%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  5.197611/ 48.309837, val:  69.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 72.4257%\n",
      "layer   3  Sparsity: 68.6313%\n",
      "total_backward_count 1683880 real_backward_count 199604  11.854%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  5.012180/ 41.335926, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 72.1668%\n",
      "layer   3  Sparsity: 68.6208%\n",
      "total_backward_count 1693670 real_backward_count 200374  11.831%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  4.647393/ 45.002438, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 71.9808%\n",
      "layer   3  Sparsity: 68.1893%\n",
      "total_backward_count 1703460 real_backward_count 201116  11.806%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  4.872761/ 50.946091, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 71.8416%\n",
      "layer   3  Sparsity: 68.2237%\n",
      "total_backward_count 1713250 real_backward_count 201878  11.783%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  5.189360/ 33.489994, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 72.0573%\n",
      "layer   3  Sparsity: 68.1233%\n",
      "total_backward_count 1723040 real_backward_count 202663  11.762%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  5.233318/ 41.679947, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 71.8928%\n",
      "layer   3  Sparsity: 68.0903%\n",
      "total_backward_count 1732830 real_backward_count 203444  11.741%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  4.694499/ 36.674191, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 72.1558%\n",
      "layer   3  Sparsity: 67.9361%\n",
      "total_backward_count 1742620 real_backward_count 204196  11.718%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  4.939166/ 45.758709, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 72.1657%\n",
      "layer   3  Sparsity: 68.0521%\n",
      "total_backward_count 1752410 real_backward_count 204953  11.695%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  5.591164/ 44.853745, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 72.0653%\n",
      "layer   3  Sparsity: 68.1756%\n",
      "total_backward_count 1762200 real_backward_count 205778  11.677%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  5.097901/ 46.332565, val:  74.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 72.1007%\n",
      "layer   3  Sparsity: 68.4608%\n",
      "total_backward_count 1771990 real_backward_count 206555  11.657%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  4.742321/ 37.572994, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 72.1244%\n",
      "layer   3  Sparsity: 68.7843%\n",
      "total_backward_count 1781780 real_backward_count 207290  11.634%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  4.043133/ 37.090549, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 71.9491%\n",
      "layer   3  Sparsity: 68.8611%\n",
      "total_backward_count 1791570 real_backward_count 207974  11.608%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  4.367703/ 63.432770, val:  65.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 72.2614%\n",
      "layer   3  Sparsity: 68.9852%\n",
      "total_backward_count 1801360 real_backward_count 208634  11.582%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  4.061324/ 39.431561, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.52 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 72.3523%\n",
      "layer   3  Sparsity: 68.8514%\n",
      "total_backward_count 1811150 real_backward_count 209308  11.557%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  4.848148/ 39.700840, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 72.2884%\n",
      "layer   3  Sparsity: 68.5254%\n",
      "total_backward_count 1820940 real_backward_count 210021  11.534%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  4.808319/ 50.628662, val:  73.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 72.2847%\n",
      "layer   3  Sparsity: 68.4954%\n",
      "total_backward_count 1830730 real_backward_count 210741  11.511%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  4.874700/ 44.232342, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 71.9297%\n",
      "layer   3  Sparsity: 68.9344%\n",
      "total_backward_count 1840520 real_backward_count 211481  11.490%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  4.501607/ 43.247089, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 71.8940%\n",
      "layer   3  Sparsity: 69.2488%\n",
      "total_backward_count 1850310 real_backward_count 212190  11.468%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  4.486945/ 41.851940, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 71.9489%\n",
      "layer   3  Sparsity: 68.9998%\n",
      "total_backward_count 1860100 real_backward_count 212883  11.445%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  4.352816/ 36.368786, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 71.6477%\n",
      "layer   3  Sparsity: 69.1393%\n",
      "total_backward_count 1869890 real_backward_count 213565  11.421%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  4.239637/ 51.598427, val:  73.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 71.7514%\n",
      "layer   3  Sparsity: 69.2166%\n",
      "total_backward_count 1879680 real_backward_count 214270  11.399%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  4.324058/ 33.011921, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 71.7292%\n",
      "layer   3  Sparsity: 69.2509%\n",
      "total_backward_count 1889470 real_backward_count 214973  11.377%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  4.294017/ 44.866951, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 71.6925%\n",
      "layer   3  Sparsity: 69.2518%\n",
      "total_backward_count 1899260 real_backward_count 215638  11.354%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  4.249177/ 44.827126, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 72.0610%\n",
      "layer   3  Sparsity: 68.9910%\n",
      "total_backward_count 1909050 real_backward_count 216330  11.332%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  4.486506/ 46.632599, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 72.1279%\n",
      "layer   3  Sparsity: 69.0564%\n",
      "total_backward_count 1918840 real_backward_count 217025  11.310%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  4.568968/ 45.745907, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 72.0744%\n",
      "layer   3  Sparsity: 69.1084%\n",
      "total_backward_count 1928630 real_backward_count 217729  11.289%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  4.833076/ 51.451817, val:  69.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 72.1699%\n",
      "layer   3  Sparsity: 69.1850%\n",
      "total_backward_count 1938420 real_backward_count 218458  11.270%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  4.219248/ 40.372730, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 72.4425%\n",
      "layer   3  Sparsity: 69.3690%\n",
      "total_backward_count 1948210 real_backward_count 219098  11.246%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  3.769690/ 65.151680, val:  62.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 72.1085%\n",
      "layer   3  Sparsity: 69.1166%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857a107614542558651a91aaad1fb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñá‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñá‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>3.76969</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.625</td></tr><tr><td>val_loss</td><td>65.15168</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sweep-835</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3ky115ts' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3ky115ts</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251211_171834-3ky115ts/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 10p6wqda with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251211_214200-10p6wqda</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/10p6wqda' target=\"_blank\">visionary-sweep-845</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/10p6wqda' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/10p6wqda</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251211_214210_295', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 2048, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.0625, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 64, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.03125, 0.0625, 0.5], 'learning_rate': 4, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.0625, self.v_threshold 2048\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 64, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.0625, 0.5])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=2048, v_reset=10000, sg_width=0.0625, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.0625, 0.5])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.03125, 0.0625, 0.5])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 4\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 46.0\n",
      "lif layer 1 self.abs_max_v: 46.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 51.0\n",
      "lif layer 1 self.abs_max_v: 59.5\n",
      "fc layer 1 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 69.0\n",
      "fc layer 1 self.abs_max_out: 122.0\n",
      "lif layer 1 self.abs_max_v: 150.0\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 1 self.abs_max_v: 342.0\n",
      "fc layer 1 self.abs_max_out: 471.0\n",
      "lif layer 1 self.abs_max_v: 638.0\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 961.0\n",
      "fc layer 1 self.abs_max_out: 756.0\n",
      "lif layer 1 self.abs_max_v: 1223.0\n",
      "fc layer 1 self.abs_max_out: 964.0\n",
      "lif layer 1 self.abs_max_v: 1567.5\n",
      "lif layer 1 self.abs_max_v: 1577.5\n",
      "fc layer 1 self.abs_max_out: 1208.0\n",
      "lif layer 1 self.abs_max_v: 1699.0\n",
      "lif layer 1 self.abs_max_v: 1991.0\n",
      "fc layer 1 self.abs_max_out: 1429.0\n",
      "lif layer 1 self.abs_max_v: 2363.5\n",
      "fc layer 2 self.abs_max_out: 64.0\n",
      "lif layer 2 self.abs_max_v: 64.0\n",
      "fc layer 1 self.abs_max_out: 2414.0\n",
      "lif layer 1 self.abs_max_v: 3596.0\n",
      "lif layer 2 self.abs_max_v: 96.0\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "lif layer 1 self.abs_max_v: 4937.0\n",
      "lif layer 2 self.abs_max_v: 112.0\n",
      "lif layer 1 self.abs_max_v: 5326.5\n",
      "lif layer 2 self.abs_max_v: 120.0\n",
      "fc layer 1 self.abs_max_out: 3193.0\n",
      "lif layer 1 self.abs_max_v: 5357.5\n",
      "lif layer 1 self.abs_max_v: 5788.0\n",
      "fc layer 1 self.abs_max_out: 4035.0\n",
      "lif layer 1 self.abs_max_v: 6897.0\n",
      "fc layer 1 self.abs_max_out: 4470.0\n",
      "lif layer 1 self.abs_max_v: 7905.5\n",
      "lif layer 2 self.abs_max_v: 124.0\n",
      "lif layer 2 self.abs_max_v: 126.0\n",
      "fc layer 1 self.abs_max_out: 4747.0\n",
      "fc layer 1 self.abs_max_out: 6197.0\n",
      "lif layer 1 self.abs_max_v: 9560.0\n",
      "fc layer 2 self.abs_max_out: 85.0\n",
      "lif layer 2 self.abs_max_v: 130.0\n",
      "fc layer 3 self.abs_max_out: 55.0\n",
      "fc layer 1 self.abs_max_out: 6203.0\n",
      "lif layer 1 self.abs_max_v: 10565.0\n",
      "fc layer 2 self.abs_max_out: 90.0\n",
      "lif layer 2 self.abs_max_v: 145.0\n",
      "lif layer 2 self.abs_max_v: 157.5\n",
      "fc layer 1 self.abs_max_out: 6809.0\n",
      "lif layer 1 self.abs_max_v: 11858.0\n",
      "fc layer 2 self.abs_max_out: 103.0\n",
      "lif layer 2 self.abs_max_v: 182.0\n",
      "lif layer 1 self.abs_max_v: 11900.0\n",
      "fc layer 1 self.abs_max_out: 7360.0\n",
      "lif layer 1 self.abs_max_v: 13295.5\n",
      "fc layer 3 self.abs_max_out: 61.0\n",
      "lif layer 1 self.abs_max_v: 13514.0\n",
      "fc layer 2 self.abs_max_out: 130.0\n",
      "fc layer 1 self.abs_max_out: 7746.0\n",
      "fc layer 2 self.abs_max_out: 210.0\n",
      "lif layer 2 self.abs_max_v: 247.5\n",
      "lif layer 1 self.abs_max_v: 13683.0\n",
      "lif layer 2 self.abs_max_v: 257.5\n",
      "fc layer 1 self.abs_max_out: 9075.0\n",
      "lif layer 1 self.abs_max_v: 15270.0\n",
      "fc layer 1 self.abs_max_out: 11366.0\n",
      "lif layer 1 self.abs_max_v: 18997.5\n",
      "fc layer 2 self.abs_max_out: 260.0\n",
      "lif layer 2 self.abs_max_v: 260.0\n",
      "lif layer 1 self.abs_max_v: 19078.5\n",
      "lif layer 1 self.abs_max_v: 19589.0\n",
      "fc layer 1 self.abs_max_out: 11567.0\n",
      "lif layer 1 self.abs_max_v: 21357.5\n",
      "fc layer 2 self.abs_max_out: 319.0\n",
      "lif layer 2 self.abs_max_v: 319.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "lif layer 1 self.abs_max_v: 21387.5\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 331.0\n",
      "lif layer 2 self.abs_max_v: 347.5\n",
      "fc layer 2 self.abs_max_out: 334.0\n",
      "fc layer 1 self.abs_max_out: 13038.0\n",
      "fc layer 1 self.abs_max_out: 15472.0\n",
      "lif layer 1 self.abs_max_v: 21983.5\n",
      "fc layer 1 self.abs_max_out: 17538.0\n",
      "lif layer 1 self.abs_max_v: 28530.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "lif layer 1 self.abs_max_v: 30272.0\n",
      "lif layer 1 self.abs_max_v: 30479.0\n",
      "lif layer 1 self.abs_max_v: 31262.0\n",
      "fc layer 1 self.abs_max_out: 19154.0\n",
      "lif layer 1 self.abs_max_v: 34762.0\n",
      "fc layer 2 self.abs_max_out: 380.0\n",
      "lif layer 2 self.abs_max_v: 380.0\n",
      "fc layer 2 self.abs_max_out: 393.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "fc layer 1 self.abs_max_out: 19960.0\n",
      "lif layer 1 self.abs_max_v: 36411.0\n",
      "lif layer 1 self.abs_max_v: 36939.5\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 405.5\n",
      "fc layer 1 self.abs_max_out: 21174.0\n",
      "fc layer 1 self.abs_max_out: 26236.0\n",
      "lif layer 1 self.abs_max_v: 41662.5\n",
      "lif layer 1 self.abs_max_v: 41712.0\n",
      "fc layer 2 self.abs_max_out: 422.0\n",
      "lif layer 2 self.abs_max_v: 422.0\n",
      "lif layer 1 self.abs_max_v: 42570.5\n",
      "fc layer 2 self.abs_max_out: 431.0\n",
      "lif layer 2 self.abs_max_v: 431.0\n",
      "lif layer 2 self.abs_max_v: 436.0\n",
      "fc layer 2 self.abs_max_out: 451.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "fc layer 2 self.abs_max_out: 461.0\n",
      "lif layer 2 self.abs_max_v: 461.0\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 462.0\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "lif layer 2 self.abs_max_v: 490.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 2 self.abs_max_out: 491.0\n",
      "lif layer 2 self.abs_max_v: 491.0\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "lif layer 2 self.abs_max_v: 506.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 532.0\n",
      "lif layer 2 self.abs_max_v: 554.0\n",
      "lif layer 2 self.abs_max_v: 565.0\n",
      "fc layer 2 self.abs_max_out: 511.0\n",
      "fc layer 2 self.abs_max_out: 529.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 583.0\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "lif layer 2 self.abs_max_v: 612.0\n",
      "lif layer 2 self.abs_max_v: 655.5\n",
      "lif layer 1 self.abs_max_v: 42791.5\n",
      "lif layer 1 self.abs_max_v: 44807.0\n",
      "epoch-0   lr=['4.0000000'], tr/val_loss: 12.608388/ 18.940760, val:  31.25%, val_best:  31.25%, tr:  70.68%, tr_best:  70.68%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 72.7048%\n",
      "layer   3  Sparsity: 96.6704%\n",
      "total_backward_count 9790 real_backward_count 4146  42.349%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "fc layer 2 self.abs_max_out: 627.0\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "lif layer 2 self.abs_max_v: 667.0\n",
      "fc layer 2 self.abs_max_out: 673.0\n",
      "lif layer 2 self.abs_max_v: 673.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "lif layer 2 self.abs_max_v: 703.0\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "lif layer 2 self.abs_max_v: 709.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 714.0\n",
      "lif layer 1 self.abs_max_v: 47022.0\n",
      "lif layer 1 self.abs_max_v: 48391.0\n",
      "epoch-1   lr=['4.0000000'], tr/val_loss:  4.669496/ 11.666598, val:  32.08%, val_best:  32.08%, tr:  90.30%, tr_best:  90.30%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 71.5171%\n",
      "layer   3  Sparsity: 96.5788%\n",
      "total_backward_count 19580 real_backward_count 6885  35.163%\n",
      "lif layer 2 self.abs_max_v: 725.5\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "lif layer 2 self.abs_max_v: 769.0\n",
      "epoch-2   lr=['4.0000000'], tr/val_loss:  3.974256/ 16.003376, val:  35.00%, val_best:  35.00%, tr:  90.70%, tr_best:  90.70%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 72.0808%\n",
      "layer   3  Sparsity: 96.3023%\n",
      "total_backward_count 29370 real_backward_count 9462  32.217%\n",
      "lif layer 2 self.abs_max_v: 781.0\n",
      "lif layer 2 self.abs_max_v: 797.5\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "lif layer 2 self.abs_max_v: 870.5\n",
      "lif layer 2 self.abs_max_v: 883.5\n",
      "lif layer 2 self.abs_max_v: 905.0\n",
      "lif layer 2 self.abs_max_v: 914.5\n",
      "lif layer 2 self.abs_max_v: 929.5\n",
      "lif layer 2 self.abs_max_v: 937.0\n",
      "lif layer 2 self.abs_max_v: 937.5\n",
      "lif layer 2 self.abs_max_v: 940.5\n",
      "lif layer 2 self.abs_max_v: 942.5\n",
      "lif layer 2 self.abs_max_v: 943.5\n",
      "epoch-3   lr=['4.0000000'], tr/val_loss:  2.579952/ 10.975826, val:  27.08%, val_best:  35.00%, tr:  96.94%, tr_best:  96.94%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 72.0391%\n",
      "layer   3  Sparsity: 96.4906%\n",
      "total_backward_count 39160 real_backward_count 11796  30.123%\n",
      "fc layer 2 self.abs_max_out: 787.0\n",
      "lif layer 1 self.abs_max_v: 48516.0\n",
      "epoch-4   lr=['4.0000000'], tr/val_loss:  1.502262/  8.772525, val:  27.08%, val_best:  35.00%, tr:  98.37%, tr_best:  98.37%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 71.5274%\n",
      "layer   3  Sparsity: 96.6948%\n",
      "total_backward_count 48950 real_backward_count 13756  28.102%\n",
      "lif layer 2 self.abs_max_v: 1001.5\n",
      "lif layer 2 self.abs_max_v: 1040.0\n",
      "lif layer 2 self.abs_max_v: 1060.0\n",
      "lif layer 2 self.abs_max_v: 1068.5\n",
      "lif layer 2 self.abs_max_v: 1074.5\n",
      "lif layer 2 self.abs_max_v: 1091.5\n",
      "lif layer 2 self.abs_max_v: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1143.0\n",
      "lif layer 2 self.abs_max_v: 1165.5\n",
      "lif layer 2 self.abs_max_v: 1203.0\n",
      "lif layer 2 self.abs_max_v: 1211.5\n",
      "lif layer 1 self.abs_max_v: 49021.5\n",
      "epoch-5   lr=['4.0000000'], tr/val_loss:  1.357688/  4.711874, val:  31.25%, val_best:  35.00%, tr:  97.24%, tr_best:  98.37%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 71.4129%\n",
      "layer   3  Sparsity: 96.8383%\n",
      "total_backward_count 58740 real_backward_count 15640  26.626%\n",
      "lif layer 1 self.abs_max_v: 50256.0\n",
      "epoch-6   lr=['4.0000000'], tr/val_loss:  1.394131/  6.496661, val:  29.58%, val_best:  35.00%, tr:  97.04%, tr_best:  98.37%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 72.1518%\n",
      "layer   3  Sparsity: 97.1091%\n",
      "total_backward_count 68530 real_backward_count 17502  25.539%\n",
      "fc layer 1 self.abs_max_out: 26869.0\n",
      "lif layer 1 self.abs_max_v: 51244.5\n",
      "epoch-7   lr=['4.0000000'], tr/val_loss:  1.090304/ 10.209476, val:  22.92%, val_best:  35.00%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 72.3644%\n",
      "layer   3  Sparsity: 97.1450%\n",
      "total_backward_count 78320 real_backward_count 19237  24.562%\n",
      "lif layer 1 self.abs_max_v: 51430.5\n",
      "epoch-8   lr=['4.0000000'], tr/val_loss:  1.214654/  5.652952, val:  32.08%, val_best:  35.00%, tr:  98.57%, tr_best:  98.98%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 71.8765%\n",
      "layer   3  Sparsity: 97.1363%\n",
      "total_backward_count 88110 real_backward_count 21147  24.001%\n",
      "fc layer 2 self.abs_max_out: 788.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "epoch-9   lr=['4.0000000'], tr/val_loss:  1.129134/  6.394594, val:  30.42%, val_best:  35.00%, tr:  97.96%, tr_best:  98.98%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 71.8684%\n",
      "layer   3  Sparsity: 97.2275%\n",
      "total_backward_count 97900 real_backward_count 22982  23.475%\n",
      "fc layer 1 self.abs_max_out: 27033.0\n",
      "lif layer 1 self.abs_max_v: 52397.0\n",
      "epoch-10  lr=['4.0000000'], tr/val_loss:  1.132880/  3.651246, val:  32.08%, val_best:  35.00%, tr:  98.57%, tr_best:  98.98%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 71.7943%\n",
      "layer   3  Sparsity: 97.5069%\n",
      "total_backward_count 107690 real_backward_count 24837  23.063%\n",
      "lif layer 2 self.abs_max_v: 1243.0\n",
      "fc layer 1 self.abs_max_out: 28312.0\n",
      "epoch-11  lr=['4.0000000'], tr/val_loss:  0.986591/  7.238130, val:  22.08%, val_best:  35.00%, tr:  97.65%, tr_best:  98.98%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 72.1571%\n",
      "layer   3  Sparsity: 97.5774%\n",
      "total_backward_count 117480 real_backward_count 26618  22.657%\n",
      "lif layer 2 self.abs_max_v: 1261.0\n",
      "lif layer 2 self.abs_max_v: 1262.5\n",
      "epoch-12  lr=['4.0000000'], tr/val_loss:  0.974195/  5.792512, val:  33.33%, val_best:  35.00%, tr:  97.45%, tr_best:  98.98%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 72.4045%\n",
      "layer   3  Sparsity: 97.5717%\n",
      "total_backward_count 127270 real_backward_count 28359  22.283%\n",
      "lif layer 2 self.abs_max_v: 1289.5\n",
      "lif layer 2 self.abs_max_v: 1371.0\n",
      "lif layer 1 self.abs_max_v: 53064.5\n",
      "epoch-13  lr=['4.0000000'], tr/val_loss:  0.950749/  3.801902, val:  30.42%, val_best:  35.00%, tr:  97.85%, tr_best:  98.98%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 72.2929%\n",
      "layer   3  Sparsity: 97.7374%\n",
      "total_backward_count 137060 real_backward_count 30190  22.027%\n",
      "fc layer 1 self.abs_max_out: 28715.0\n",
      "fc layer 1 self.abs_max_out: 28842.0\n",
      "lif layer 2 self.abs_max_v: 1405.0\n",
      "lif layer 2 self.abs_max_v: 1460.0\n",
      "lif layer 2 self.abs_max_v: 1494.0\n",
      "lif layer 2 self.abs_max_v: 1550.0\n",
      "lif layer 2 self.abs_max_v: 1555.0\n",
      "lif layer 2 self.abs_max_v: 1559.5\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "lif layer 2 self.abs_max_v: 1594.5\n",
      "lif layer 1 self.abs_max_v: 54096.0\n",
      "epoch-14  lr=['4.0000000'], tr/val_loss:  0.898814/  6.922206, val:  23.75%, val_best:  35.00%, tr:  98.26%, tr_best:  98.98%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 72.7698%\n",
      "layer   3  Sparsity: 97.6927%\n",
      "total_backward_count 146850 real_backward_count 31909  21.729%\n",
      "fc layer 1 self.abs_max_out: 29301.0\n",
      "fc layer 2 self.abs_max_out: 846.0\n",
      "lif layer 2 self.abs_max_v: 1606.5\n",
      "lif layer 2 self.abs_max_v: 1616.5\n",
      "lif layer 1 self.abs_max_v: 54765.0\n",
      "lif layer 1 self.abs_max_v: 56118.5\n",
      "epoch-15  lr=['4.0000000'], tr/val_loss:  0.921861/  4.227721, val:  32.50%, val_best:  35.00%, tr:  97.65%, tr_best:  98.98%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 72.9379%\n",
      "layer   3  Sparsity: 97.8197%\n",
      "total_backward_count 156640 real_backward_count 33644  21.479%\n",
      "fc layer 1 self.abs_max_out: 29450.0\n",
      "lif layer 1 self.abs_max_v: 56809.0\n",
      "epoch-16  lr=['4.0000000'], tr/val_loss:  0.855920/  3.394947, val:  30.00%, val_best:  35.00%, tr:  97.45%, tr_best:  98.98%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 73.1431%\n",
      "layer   3  Sparsity: 98.0064%\n",
      "total_backward_count 166430 real_backward_count 35388  21.263%\n",
      "fc layer 1 self.abs_max_out: 30123.0\n",
      "lif layer 1 self.abs_max_v: 57059.0\n",
      "lif layer 1 self.abs_max_v: 58295.5\n",
      "epoch-17  lr=['4.0000000'], tr/val_loss:  0.876764/  4.906244, val:  13.75%, val_best:  35.00%, tr:  97.24%, tr_best:  98.98%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 72.8769%\n",
      "layer   3  Sparsity: 98.1568%\n",
      "total_backward_count 176220 real_backward_count 37231  21.128%\n",
      "fc layer 1 self.abs_max_out: 30425.0\n",
      "epoch-18  lr=['4.0000000'], tr/val_loss:  0.858378/  3.569526, val:  28.33%, val_best:  35.00%, tr:  97.65%, tr_best:  98.98%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 72.5182%\n",
      "layer   3  Sparsity: 98.2727%\n",
      "total_backward_count 186010 real_backward_count 39201  21.075%\n",
      "fc layer 1 self.abs_max_out: 31357.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "epoch-19  lr=['4.0000000'], tr/val_loss:  0.836412/  5.427223, val:  25.83%, val_best:  35.00%, tr:  98.06%, tr_best:  98.98%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 72.4315%\n",
      "layer   3  Sparsity: 98.2754%\n",
      "total_backward_count 195800 real_backward_count 41044  20.962%\n",
      "fc layer 2 self.abs_max_out: 850.0\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 899.0\n",
      "epoch-20  lr=['4.0000000'], tr/val_loss:  0.852773/  5.103749, val:  22.92%, val_best:  35.00%, tr:  96.42%, tr_best:  98.98%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 72.2721%\n",
      "layer   3  Sparsity: 98.2222%\n",
      "total_backward_count 205590 real_backward_count 42877  20.856%\n",
      "fc layer 2 self.abs_max_out: 935.0\n",
      "lif layer 2 self.abs_max_v: 1632.5\n",
      "lif layer 2 self.abs_max_v: 1645.5\n",
      "lif layer 2 self.abs_max_v: 1652.0\n",
      "lif layer 2 self.abs_max_v: 1655.0\n",
      "lif layer 2 self.abs_max_v: 1656.5\n",
      "epoch-21  lr=['4.0000000'], tr/val_loss:  0.855202/  3.698953, val:  29.17%, val_best:  35.00%, tr:  97.75%, tr_best:  98.98%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 72.1116%\n",
      "layer   3  Sparsity: 98.1073%\n",
      "total_backward_count 215380 real_backward_count 44655  20.733%\n",
      "lif layer 2 self.abs_max_v: 1657.5\n",
      "lif layer 2 self.abs_max_v: 1678.0\n",
      "lif layer 2 self.abs_max_v: 1704.0\n",
      "lif layer 2 self.abs_max_v: 1761.0\n",
      "epoch-22  lr=['4.0000000'], tr/val_loss:  0.850524/  3.498212, val:  36.67%, val_best:  36.67%, tr:  98.37%, tr_best:  98.98%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 72.6134%\n",
      "layer   3  Sparsity: 98.1295%\n",
      "total_backward_count 225170 real_backward_count 46414  20.613%\n",
      "lif layer 2 self.abs_max_v: 1764.0\n",
      "lif layer 2 self.abs_max_v: 1764.5\n",
      "lif layer 2 self.abs_max_v: 1771.5\n",
      "lif layer 2 self.abs_max_v: 1794.5\n",
      "epoch-23  lr=['4.0000000'], tr/val_loss:  0.824132/  2.526761, val:  43.33%, val_best:  43.33%, tr:  97.55%, tr_best:  98.98%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 72.6380%\n",
      "layer   3  Sparsity: 98.2759%\n",
      "total_backward_count 234960 real_backward_count 48180  20.506%\n",
      "fc layer 2 self.abs_max_out: 952.0\n",
      "lif layer 2 self.abs_max_v: 1798.0\n",
      "epoch-24  lr=['4.0000000'], tr/val_loss:  0.836606/  3.426031, val:  32.08%, val_best:  43.33%, tr:  96.22%, tr_best:  98.98%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 72.5013%\n",
      "layer   3  Sparsity: 98.4813%\n",
      "total_backward_count 244750 real_backward_count 50051  20.450%\n",
      "epoch-25  lr=['4.0000000'], tr/val_loss:  0.856449/  3.942627, val:  32.50%, val_best:  43.33%, tr:  96.73%, tr_best:  98.98%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 72.2370%\n",
      "layer   3  Sparsity: 98.4271%\n",
      "total_backward_count 254540 real_backward_count 51919  20.397%\n",
      "fc layer 2 self.abs_max_out: 965.0\n",
      "lif layer 2 self.abs_max_v: 1826.0\n",
      "lif layer 2 self.abs_max_v: 1842.0\n",
      "epoch-26  lr=['4.0000000'], tr/val_loss:  0.813553/  3.052722, val:  35.83%, val_best:  43.33%, tr:  96.63%, tr_best:  98.98%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 71.4052%\n",
      "layer   3  Sparsity: 98.6122%\n",
      "total_backward_count 264330 real_backward_count 53783  20.347%\n",
      "fc layer 2 self.abs_max_out: 985.0\n",
      "lif layer 2 self.abs_max_v: 1896.0\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 2 self.abs_max_out: 1020.0\n",
      "lif layer 2 self.abs_max_v: 1898.5\n",
      "lif layer 2 self.abs_max_v: 1945.0\n",
      "lif layer 2 self.abs_max_v: 1951.0\n",
      "fc layer 2 self.abs_max_out: 1058.0\n",
      "epoch-27  lr=['4.0000000'], tr/val_loss:  0.828027/  2.706040, val:  32.50%, val_best:  43.33%, tr:  96.12%, tr_best:  98.98%, epoch time: 80.71 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 71.6169%\n",
      "layer   3  Sparsity: 98.6369%\n",
      "total_backward_count 274120 real_backward_count 55674  20.310%\n",
      "fc layer 2 self.abs_max_out: 1060.0\n",
      "lif layer 2 self.abs_max_v: 1978.5\n",
      "lif layer 2 self.abs_max_v: 2027.5\n",
      "lif layer 2 self.abs_max_v: 2052.0\n",
      "fc layer 2 self.abs_max_out: 1080.0\n",
      "epoch-28  lr=['4.0000000'], tr/val_loss:  0.814047/  2.897238, val:  26.25%, val_best:  43.33%, tr:  94.89%, tr_best:  98.98%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 71.9304%\n",
      "layer   3  Sparsity: 98.6562%\n",
      "total_backward_count 283910 real_backward_count 57511  20.257%\n",
      "fc layer 2 self.abs_max_out: 1092.0\n",
      "fc layer 2 self.abs_max_out: 1183.0\n",
      "lif layer 2 self.abs_max_v: 2063.0\n",
      "lif layer 2 self.abs_max_v: 2088.5\n",
      "lif layer 2 self.abs_max_v: 2095.5\n",
      "epoch-29  lr=['4.0000000'], tr/val_loss:  0.823880/  3.150435, val:  18.75%, val_best:  43.33%, tr:  95.81%, tr_best:  98.98%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 71.9018%\n",
      "layer   3  Sparsity: 98.6701%\n",
      "total_backward_count 293700 real_backward_count 59379  20.218%\n",
      "lif layer 2 self.abs_max_v: 2123.5\n",
      "epoch-30  lr=['4.0000000'], tr/val_loss:  0.847884/  2.580485, val:  33.33%, val_best:  43.33%, tr:  95.40%, tr_best:  98.98%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 72.2836%\n",
      "layer   3  Sparsity: 98.6294%\n",
      "total_backward_count 303490 real_backward_count 61278  20.191%\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "lif layer 2 self.abs_max_v: 2131.0\n",
      "epoch-31  lr=['4.0000000'], tr/val_loss:  0.830078/  2.526324, val:  39.17%, val_best:  43.33%, tr:  96.42%, tr_best:  98.98%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 72.6208%\n",
      "layer   3  Sparsity: 98.5801%\n",
      "total_backward_count 313280 real_backward_count 63144  20.156%\n",
      "lif layer 2 self.abs_max_v: 2136.0\n",
      "lif layer 2 self.abs_max_v: 2145.0\n",
      "lif layer 2 self.abs_max_v: 2149.5\n",
      "fc layer 2 self.abs_max_out: 1221.0\n",
      "epoch-32  lr=['4.0000000'], tr/val_loss:  0.839172/  2.645436, val:  32.08%, val_best:  43.33%, tr:  95.40%, tr_best:  98.98%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 72.3553%\n",
      "layer   3  Sparsity: 98.5495%\n",
      "total_backward_count 323070 real_backward_count 64995  20.118%\n",
      "epoch-33  lr=['4.0000000'], tr/val_loss:  0.837933/  2.711936, val:  33.33%, val_best:  43.33%, tr:  95.91%, tr_best:  98.98%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 72.1380%\n",
      "layer   3  Sparsity: 98.6137%\n",
      "total_backward_count 332860 real_backward_count 66855  20.085%\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "epoch-34  lr=['4.0000000'], tr/val_loss:  0.838535/  3.570286, val:  31.67%, val_best:  43.33%, tr:  95.71%, tr_best:  98.98%, epoch time: 80.58 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 71.7073%\n",
      "layer   3  Sparsity: 98.5839%\n",
      "total_backward_count 342650 real_backward_count 68728  20.058%\n",
      "epoch-35  lr=['4.0000000'], tr/val_loss:  0.856576/  2.976936, val:  30.42%, val_best:  43.33%, tr:  95.20%, tr_best:  98.98%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 72.0190%\n",
      "layer   3  Sparsity: 98.5962%\n",
      "total_backward_count 352440 real_backward_count 70578  20.026%\n",
      "epoch-36  lr=['4.0000000'], tr/val_loss:  0.833532/  2.971621, val:  35.00%, val_best:  43.33%, tr:  95.71%, tr_best:  98.98%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 72.0076%\n",
      "layer   3  Sparsity: 98.6918%\n",
      "total_backward_count 362230 real_backward_count 72481  20.010%\n",
      "epoch-37  lr=['4.0000000'], tr/val_loss:  0.844359/  3.708828, val:  24.58%, val_best:  43.33%, tr:  94.59%, tr_best:  98.98%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 72.4727%\n",
      "layer   3  Sparsity: 98.6816%\n",
      "total_backward_count 372020 real_backward_count 74338  19.982%\n",
      "epoch-38  lr=['4.0000000'], tr/val_loss:  0.853339/  3.302170, val:  27.50%, val_best:  43.33%, tr:  94.28%, tr_best:  98.98%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 71.6149%\n",
      "layer   3  Sparsity: 98.7066%\n",
      "total_backward_count 381810 real_backward_count 76267  19.975%\n",
      "epoch-39  lr=['4.0000000'], tr/val_loss:  0.812156/  2.958977, val:  22.50%, val_best:  43.33%, tr:  95.10%, tr_best:  98.98%, epoch time: 80.84 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 71.8537%\n",
      "layer   3  Sparsity: 98.7053%\n",
      "total_backward_count 391600 real_backward_count 78076  19.938%\n",
      "lif layer 2 self.abs_max_v: 2154.0\n",
      "epoch-40  lr=['4.0000000'], tr/val_loss:  0.869947/  2.673924, val:  25.00%, val_best:  43.33%, tr:  93.97%, tr_best:  98.98%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 71.7637%\n",
      "layer   3  Sparsity: 98.9063%\n",
      "total_backward_count 401390 real_backward_count 79821  19.886%\n",
      "epoch-41  lr=['4.0000000'], tr/val_loss:  0.931953/  2.724632, val:  33.75%, val_best:  43.33%, tr:  91.01%, tr_best:  98.98%, epoch time: 80.64 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 71.9311%\n",
      "layer   3  Sparsity: 98.9463%\n",
      "total_backward_count 411180 real_backward_count 81652  19.858%\n",
      "epoch-42  lr=['4.0000000'], tr/val_loss:  0.917409/  2.367514, val:  36.25%, val_best:  43.33%, tr:  91.73%, tr_best:  98.98%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 72.3688%\n",
      "layer   3  Sparsity: 98.9422%\n",
      "total_backward_count 420970 real_backward_count 83536  19.844%\n",
      "lif layer 2 self.abs_max_v: 2155.0\n",
      "epoch-43  lr=['4.0000000'], tr/val_loss:  0.949787/  2.414073, val:  28.33%, val_best:  43.33%, tr:  92.54%, tr_best:  98.98%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 72.7648%\n",
      "layer   3  Sparsity: 98.9452%\n",
      "total_backward_count 430760 real_backward_count 85406  19.827%\n",
      "lif layer 1 self.abs_max_v: 58386.5\n",
      "epoch-44  lr=['4.0000000'], tr/val_loss:  0.941930/  2.739779, val:  21.25%, val_best:  43.33%, tr:  92.95%, tr_best:  98.98%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 72.7044%\n",
      "layer   3  Sparsity: 98.9029%\n",
      "total_backward_count 440550 real_backward_count 87373  19.833%\n",
      "lif layer 1 self.abs_max_v: 59190.0\n",
      "epoch-45  lr=['4.0000000'], tr/val_loss:  0.976697/  3.011169, val:  28.75%, val_best:  43.33%, tr:  89.68%, tr_best:  98.98%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 72.3565%\n",
      "layer   3  Sparsity: 98.8369%\n",
      "total_backward_count 450340 real_backward_count 89308  19.831%\n",
      "epoch-46  lr=['4.0000000'], tr/val_loss:  1.010327/  2.266522, val:  31.67%, val_best:  43.33%, tr:  88.87%, tr_best:  98.98%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 72.3268%\n",
      "layer   3  Sparsity: 98.8566%\n",
      "total_backward_count 460130 real_backward_count 91225  19.826%\n",
      "lif layer 2 self.abs_max_v: 2178.5\n",
      "lif layer 2 self.abs_max_v: 2206.5\n",
      "lif layer 2 self.abs_max_v: 2207.5\n",
      "epoch-47  lr=['4.0000000'], tr/val_loss:  0.963756/  2.823768, val:  28.75%, val_best:  43.33%, tr:  91.93%, tr_best:  98.98%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 72.4068%\n",
      "layer   3  Sparsity: 98.8513%\n",
      "total_backward_count 469920 real_backward_count 93075  19.807%\n",
      "epoch-48  lr=['4.0000000'], tr/val_loss:  0.930189/  2.705320, val:  25.83%, val_best:  43.33%, tr:  92.95%, tr_best:  98.98%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 72.7896%\n",
      "layer   3  Sparsity: 98.8346%\n",
      "total_backward_count 479710 real_backward_count 94923  19.788%\n",
      "epoch-49  lr=['4.0000000'], tr/val_loss:  0.947058/  2.457208, val:  39.17%, val_best:  43.33%, tr:  92.24%, tr_best:  98.98%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 72.5428%\n",
      "layer   3  Sparsity: 98.8249%\n",
      "total_backward_count 489500 real_backward_count 96774  19.770%\n",
      "epoch-50  lr=['4.0000000'], tr/val_loss:  0.957100/  2.785873, val:  28.33%, val_best:  43.33%, tr:  93.46%, tr_best:  98.98%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 72.8717%\n",
      "layer   3  Sparsity: 98.8211%\n",
      "total_backward_count 499290 real_backward_count 98746  19.777%\n",
      "epoch-51  lr=['4.0000000'], tr/val_loss:  0.933616/  2.801708, val:  26.25%, val_best:  43.33%, tr:  93.77%, tr_best:  98.98%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 72.7850%\n",
      "layer   3  Sparsity: 98.8164%\n",
      "total_backward_count 509080 real_backward_count 100751  19.791%\n",
      "epoch-52  lr=['4.0000000'], tr/val_loss:  0.977720/  2.785560, val:  31.25%, val_best:  43.33%, tr:  91.73%, tr_best:  98.98%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 72.6894%\n",
      "layer   3  Sparsity: 98.8171%\n",
      "total_backward_count 518870 real_backward_count 102717  19.796%\n",
      "epoch-53  lr=['4.0000000'], tr/val_loss:  0.918374/  3.433416, val:  30.42%, val_best:  43.33%, tr:  93.67%, tr_best:  98.98%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 72.7514%\n",
      "layer   3  Sparsity: 98.8555%\n",
      "total_backward_count 528660 real_backward_count 104621  19.790%\n",
      "epoch-54  lr=['4.0000000'], tr/val_loss:  0.981728/  2.325166, val:  37.08%, val_best:  43.33%, tr:  91.32%, tr_best:  98.98%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 73.0877%\n",
      "layer   3  Sparsity: 98.8717%\n",
      "total_backward_count 538450 real_backward_count 106543  19.787%\n",
      "epoch-55  lr=['4.0000000'], tr/val_loss:  0.962430/  3.019482, val:  23.33%, val_best:  43.33%, tr:  90.70%, tr_best:  98.98%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 73.7184%\n",
      "layer   3  Sparsity: 98.8579%\n",
      "total_backward_count 548240 real_backward_count 108400  19.772%\n",
      "epoch-56  lr=['4.0000000'], tr/val_loss:  0.971116/  2.830528, val:  22.92%, val_best:  43.33%, tr:  91.32%, tr_best:  98.98%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 73.3159%\n",
      "layer   3  Sparsity: 98.8500%\n",
      "total_backward_count 558030 real_backward_count 110409  19.785%\n",
      "epoch-57  lr=['4.0000000'], tr/val_loss:  0.936202/  2.552186, val:  30.42%, val_best:  43.33%, tr:  92.65%, tr_best:  98.98%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 72.8716%\n",
      "layer   3  Sparsity: 98.8199%\n",
      "total_backward_count 567820 real_backward_count 112346  19.785%\n",
      "epoch-58  lr=['4.0000000'], tr/val_loss:  0.921396/  3.301650, val:  29.17%, val_best:  43.33%, tr:  92.65%, tr_best:  98.98%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 73.0783%\n",
      "layer   3  Sparsity: 98.8210%\n",
      "total_backward_count 577610 real_backward_count 114241  19.778%\n",
      "lif layer 2 self.abs_max_v: 2214.5\n",
      "lif layer 2 self.abs_max_v: 2290.5\n",
      "epoch-59  lr=['4.0000000'], tr/val_loss:  0.877189/  2.927438, val:  29.58%, val_best:  43.33%, tr:  95.40%, tr_best:  98.98%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 73.4301%\n",
      "layer   3  Sparsity: 98.8079%\n",
      "total_backward_count 587400 real_backward_count 116095  19.764%\n",
      "epoch-60  lr=['4.0000000'], tr/val_loss:  0.861114/  3.086759, val:  15.83%, val_best:  43.33%, tr:  95.91%, tr_best:  98.98%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 73.0279%\n",
      "layer   3  Sparsity: 98.8140%\n",
      "total_backward_count 597190 real_backward_count 117958  19.752%\n",
      "epoch-61  lr=['4.0000000'], tr/val_loss:  0.879619/  2.625575, val:  32.50%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 72.8451%\n",
      "layer   3  Sparsity: 98.7787%\n",
      "total_backward_count 606980 real_backward_count 119841  19.744%\n",
      "epoch-62  lr=['4.0000000'], tr/val_loss:  0.840356/  2.588165, val:  27.50%, val_best:  43.33%, tr:  94.38%, tr_best:  98.98%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 72.8253%\n",
      "layer   3  Sparsity: 98.8007%\n",
      "total_backward_count 616770 real_backward_count 121573  19.711%\n",
      "epoch-63  lr=['4.0000000'], tr/val_loss:  0.890872/  2.232287, val:  28.33%, val_best:  43.33%, tr:  94.99%, tr_best:  98.98%, epoch time: 80.61 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 73.1575%\n",
      "layer   3  Sparsity: 98.8338%\n",
      "total_backward_count 626560 real_backward_count 123372  19.690%\n",
      "lif layer 2 self.abs_max_v: 2300.0\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "epoch-64  lr=['4.0000000'], tr/val_loss:  0.944741/  2.915058, val:  30.83%, val_best:  43.33%, tr:  92.03%, tr_best:  98.98%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 73.4148%\n",
      "layer   3  Sparsity: 98.8482%\n",
      "total_backward_count 636350 real_backward_count 125245  19.682%\n",
      "epoch-65  lr=['4.0000000'], tr/val_loss:  0.922344/  2.686862, val:  27.92%, val_best:  43.33%, tr:  92.85%, tr_best:  98.98%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 73.2738%\n",
      "layer   3  Sparsity: 98.8120%\n",
      "total_backward_count 646140 real_backward_count 127188  19.684%\n",
      "epoch-66  lr=['4.0000000'], tr/val_loss:  0.932832/  2.400651, val:  29.17%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 73.3562%\n",
      "layer   3  Sparsity: 98.8328%\n",
      "total_backward_count 655930 real_backward_count 129173  19.693%\n",
      "fc layer 2 self.abs_max_out: 1269.0\n",
      "lif layer 2 self.abs_max_v: 2311.5\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "lif layer 2 self.abs_max_v: 2380.5\n",
      "epoch-67  lr=['4.0000000'], tr/val_loss:  0.940368/  2.167359, val:  38.75%, val_best:  43.33%, tr:  92.24%, tr_best:  98.98%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 73.8202%\n",
      "layer   3  Sparsity: 98.8571%\n",
      "total_backward_count 665720 real_backward_count 131073  19.689%\n",
      "fc layer 2 self.abs_max_out: 1309.0\n",
      "epoch-68  lr=['4.0000000'], tr/val_loss:  0.899238/  3.010349, val:  29.17%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 73.6308%\n",
      "layer   3  Sparsity: 98.8260%\n",
      "total_backward_count 675510 real_backward_count 132933  19.679%\n",
      "lif layer 2 self.abs_max_v: 2419.5\n",
      "lif layer 2 self.abs_max_v: 2429.0\n",
      "lif layer 2 self.abs_max_v: 2474.5\n",
      "lif layer 2 self.abs_max_v: 2497.5\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "lif layer 2 self.abs_max_v: 2534.5\n",
      "lif layer 2 self.abs_max_v: 2596.5\n",
      "lif layer 2 self.abs_max_v: 2627.5\n",
      "epoch-69  lr=['4.0000000'], tr/val_loss:  0.879795/  3.106869, val:  30.00%, val_best:  43.33%, tr:  94.59%, tr_best:  98.98%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 73.6938%\n",
      "layer   3  Sparsity: 98.8352%\n",
      "total_backward_count 685300 real_backward_count 134756  19.664%\n",
      "fc layer 2 self.abs_max_out: 1349.0\n",
      "epoch-70  lr=['4.0000000'], tr/val_loss:  0.874394/  2.776232, val:  29.17%, val_best:  43.33%, tr:  93.26%, tr_best:  98.98%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 73.7048%\n",
      "layer   3  Sparsity: 98.8486%\n",
      "total_backward_count 695090 real_backward_count 136563  19.647%\n",
      "epoch-71  lr=['4.0000000'], tr/val_loss:  0.943580/  2.575862, val:  35.42%, val_best:  43.33%, tr:  91.32%, tr_best:  98.98%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 73.7166%\n",
      "layer   3  Sparsity: 98.9022%\n",
      "total_backward_count 704880 real_backward_count 138491  19.647%\n",
      "epoch-72  lr=['4.0000000'], tr/val_loss:  0.937594/  2.320132, val:  34.17%, val_best:  43.33%, tr:  92.85%, tr_best:  98.98%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 72.7891%\n",
      "layer   3  Sparsity: 98.8586%\n",
      "total_backward_count 714670 real_backward_count 140469  19.655%\n",
      "epoch-73  lr=['4.0000000'], tr/val_loss:  0.949562/  3.819412, val:  25.42%, val_best:  43.33%, tr:  92.85%, tr_best:  98.98%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 73.2461%\n",
      "layer   3  Sparsity: 98.8634%\n",
      "total_backward_count 724460 real_backward_count 142475  19.666%\n",
      "epoch-74  lr=['4.0000000'], tr/val_loss:  1.137208/  3.350964, val:  28.75%, val_best:  43.33%, tr:  93.97%, tr_best:  98.98%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 73.0682%\n",
      "layer   3  Sparsity: 98.7571%\n",
      "total_backward_count 734250 real_backward_count 144620  19.696%\n",
      "epoch-75  lr=['4.0000000'], tr/val_loss:  0.840687/  2.955173, val:  40.00%, val_best:  43.33%, tr:  96.22%, tr_best:  98.98%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 73.1726%\n",
      "layer   3  Sparsity: 98.6900%\n",
      "total_backward_count 744040 real_backward_count 146541  19.695%\n",
      "epoch-76  lr=['4.0000000'], tr/val_loss:  0.827462/  2.563792, val:  35.42%, val_best:  43.33%, tr:  95.30%, tr_best:  98.98%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 73.3694%\n",
      "layer   3  Sparsity: 98.7160%\n",
      "total_backward_count 753830 real_backward_count 148366  19.682%\n",
      "fc layer 2 self.abs_max_out: 1376.0\n",
      "epoch-77  lr=['4.0000000'], tr/val_loss:  0.880521/  3.677855, val:  22.92%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 73.5214%\n",
      "layer   3  Sparsity: 98.7910%\n",
      "total_backward_count 763620 real_backward_count 150158  19.664%\n",
      "fc layer 2 self.abs_max_out: 1396.0\n",
      "epoch-78  lr=['4.0000000'], tr/val_loss:  0.875703/  2.878197, val:  25.83%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 80.98 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 73.5266%\n",
      "layer   3  Sparsity: 98.8053%\n",
      "total_backward_count 773410 real_backward_count 151908  19.641%\n",
      "fc layer 2 self.abs_max_out: 1436.0\n",
      "lif layer 2 self.abs_max_v: 2648.0\n",
      "fc layer 2 self.abs_max_out: 1456.0\n",
      "lif layer 2 self.abs_max_v: 2650.5\n",
      "epoch-79  lr=['4.0000000'], tr/val_loss:  0.872832/  2.812436, val:  33.33%, val_best:  43.33%, tr:  94.48%, tr_best:  98.98%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 73.6317%\n",
      "layer   3  Sparsity: 98.7988%\n",
      "total_backward_count 783200 real_backward_count 153705  19.625%\n",
      "lif layer 2 self.abs_max_v: 2676.5\n",
      "epoch-80  lr=['4.0000000'], tr/val_loss:  0.953880/  3.088924, val:  18.33%, val_best:  43.33%, tr:  92.95%, tr_best:  98.98%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 73.9509%\n",
      "layer   3  Sparsity: 98.8183%\n",
      "total_backward_count 792990 real_backward_count 155541  19.614%\n",
      "epoch-81  lr=['4.0000000'], tr/val_loss:  0.870337/  2.970032, val:  15.00%, val_best:  43.33%, tr:  92.24%, tr_best:  98.98%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 73.8526%\n",
      "layer   3  Sparsity: 98.8132%\n",
      "total_backward_count 802780 real_backward_count 157339  19.599%\n",
      "epoch-82  lr=['4.0000000'], tr/val_loss:  0.885307/  2.391070, val:  31.67%, val_best:  43.33%, tr:  93.67%, tr_best:  98.98%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 73.9674%\n",
      "layer   3  Sparsity: 98.7982%\n",
      "total_backward_count 812570 real_backward_count 159223  19.595%\n",
      "epoch-83  lr=['4.0000000'], tr/val_loss:  0.871732/  2.587222, val:  29.58%, val_best:  43.33%, tr:  94.08%, tr_best:  98.98%, epoch time: 80.58 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 74.0849%\n",
      "layer   3  Sparsity: 98.7951%\n",
      "total_backward_count 822360 real_backward_count 161104  19.590%\n",
      "lif layer 2 self.abs_max_v: 2686.5\n",
      "epoch-84  lr=['4.0000000'], tr/val_loss:  0.890826/  2.820047, val:  25.83%, val_best:  43.33%, tr:  94.48%, tr_best:  98.98%, epoch time: 80.72 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 74.2256%\n",
      "layer   3  Sparsity: 98.8169%\n",
      "total_backward_count 832150 real_backward_count 162980  19.585%\n",
      "epoch-85  lr=['4.0000000'], tr/val_loss:  0.885065/  2.384005, val:  30.42%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 74.3296%\n",
      "layer   3  Sparsity: 98.8100%\n",
      "total_backward_count 841940 real_backward_count 164814  19.576%\n",
      "lif layer 2 self.abs_max_v: 2700.5\n",
      "lif layer 2 self.abs_max_v: 2747.5\n",
      "epoch-86  lr=['4.0000000'], tr/val_loss:  0.869493/  2.768588, val:  31.25%, val_best:  43.33%, tr:  94.59%, tr_best:  98.98%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 73.9556%\n",
      "layer   3  Sparsity: 98.7998%\n",
      "total_backward_count 851730 real_backward_count 166644  19.565%\n",
      "epoch-87  lr=['4.0000000'], tr/val_loss:  0.892268/  2.634182, val:  22.08%, val_best:  43.33%, tr:  94.08%, tr_best:  98.98%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 74.1274%\n",
      "layer   3  Sparsity: 98.7364%\n",
      "total_backward_count 861520 real_backward_count 168529  19.562%\n",
      "epoch-88  lr=['4.0000000'], tr/val_loss:  0.874863/  2.855075, val:  29.17%, val_best:  43.33%, tr:  93.97%, tr_best:  98.98%, epoch time: 80.61 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   2  Sparsity: 74.5539%\n",
      "layer   3  Sparsity: 98.7431%\n",
      "total_backward_count 871310 real_backward_count 170339  19.550%\n",
      "epoch-89  lr=['4.0000000'], tr/val_loss:  0.881132/  2.458115, val:  29.17%, val_best:  43.33%, tr:  94.38%, tr_best:  98.98%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 74.2304%\n",
      "layer   3  Sparsity: 98.7349%\n",
      "total_backward_count 881100 real_backward_count 172203  19.544%\n",
      "epoch-90  lr=['4.0000000'], tr/val_loss:  0.876102/  3.104211, val:  30.42%, val_best:  43.33%, tr:  94.08%, tr_best:  98.98%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 73.9658%\n",
      "layer   3  Sparsity: 98.7886%\n",
      "total_backward_count 890890 real_backward_count 173980  19.529%\n",
      "epoch-91  lr=['4.0000000'], tr/val_loss:  0.868575/  2.549792, val:  33.75%, val_best:  43.33%, tr:  92.65%, tr_best:  98.98%, epoch time: 80.54 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 74.1603%\n",
      "layer   3  Sparsity: 98.8132%\n",
      "total_backward_count 900680 real_backward_count 175767  19.515%\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "fc layer 2 self.abs_max_out: 1514.0\n",
      "epoch-92  lr=['4.0000000'], tr/val_loss:  0.913212/  2.608342, val:  29.58%, val_best:  43.33%, tr:  93.56%, tr_best:  98.98%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 74.3466%\n",
      "layer   3  Sparsity: 98.8311%\n",
      "total_backward_count 910470 real_backward_count 177640  19.511%\n",
      "lif layer 2 self.abs_max_v: 2771.0\n",
      "epoch-93  lr=['4.0000000'], tr/val_loss:  0.868236/  2.624864, val:  28.75%, val_best:  43.33%, tr:  94.69%, tr_best:  98.98%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 74.3978%\n",
      "layer   3  Sparsity: 98.7889%\n",
      "total_backward_count 920260 real_backward_count 179427  19.497%\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "lif layer 2 self.abs_max_v: 2894.0\n",
      "epoch-94  lr=['4.0000000'], tr/val_loss:  0.877616/  2.288903, val:  39.17%, val_best:  43.33%, tr:  92.34%, tr_best:  98.98%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 74.2909%\n",
      "layer   3  Sparsity: 98.7893%\n",
      "total_backward_count 930050 real_backward_count 181188  19.482%\n",
      "epoch-95  lr=['4.0000000'], tr/val_loss:  0.876975/  2.202343, val:  40.00%, val_best:  43.33%, tr:  92.34%, tr_best:  98.98%, epoch time: 81.00 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 74.2846%\n",
      "layer   3  Sparsity: 98.7901%\n",
      "total_backward_count 939840 real_backward_count 182977  19.469%\n",
      "epoch-96  lr=['4.0000000'], tr/val_loss:  0.858551/  2.693210, val:  32.50%, val_best:  43.33%, tr:  94.28%, tr_best:  98.98%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 74.2705%\n",
      "layer   3  Sparsity: 98.7861%\n",
      "total_backward_count 949630 real_backward_count 184767  19.457%\n",
      "lif layer 2 self.abs_max_v: 2954.5\n",
      "epoch-97  lr=['4.0000000'], tr/val_loss:  0.877045/  2.743972, val:  30.00%, val_best:  43.33%, tr:  93.05%, tr_best:  98.98%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 74.2005%\n",
      "layer   3  Sparsity: 98.7543%\n",
      "total_backward_count 959420 real_backward_count 186579  19.447%\n",
      "epoch-98  lr=['4.0000000'], tr/val_loss:  0.853121/  3.293422, val:  26.25%, val_best:  43.33%, tr:  94.48%, tr_best:  98.98%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 74.0795%\n",
      "layer   3  Sparsity: 98.7868%\n",
      "total_backward_count 969210 real_backward_count 188442  19.443%\n",
      "epoch-99  lr=['4.0000000'], tr/val_loss:  0.856663/  3.080017, val:  35.83%, val_best:  43.33%, tr:  94.18%, tr_best:  98.98%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 73.8520%\n",
      "layer   3  Sparsity: 98.7980%\n",
      "total_backward_count 979000 real_backward_count 190188  19.427%\n",
      "lif layer 2 self.abs_max_v: 2982.5\n",
      "epoch-100 lr=['4.0000000'], tr/val_loss:  0.872597/  3.238711, val:  25.42%, val_best:  43.33%, tr:  92.85%, tr_best:  98.98%, epoch time: 80.42 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 74.0879%\n",
      "layer   3  Sparsity: 98.8194%\n",
      "total_backward_count 988790 real_backward_count 191969  19.415%\n",
      "lif layer 2 self.abs_max_v: 2995.0\n",
      "lif layer 2 self.abs_max_v: 2996.5\n",
      "epoch-101 lr=['4.0000000'], tr/val_loss:  0.889864/  2.506445, val:  34.17%, val_best:  43.33%, tr:  93.77%, tr_best:  98.98%, epoch time: 80.58 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 73.6776%\n",
      "layer   3  Sparsity: 98.7681%\n",
      "total_backward_count 998580 real_backward_count 193839  19.411%\n",
      "epoch-102 lr=['4.0000000'], tr/val_loss:  0.882780/  3.057231, val:  25.83%, val_best:  43.33%, tr:  94.59%, tr_best:  98.98%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 74.0196%\n",
      "layer   3  Sparsity: 98.8072%\n",
      "total_backward_count 1008370 real_backward_count 195675  19.405%\n",
      "lif layer 2 self.abs_max_v: 3015.0\n",
      "epoch-103 lr=['4.0000000'], tr/val_loss:  0.888125/  2.704406, val:  32.50%, val_best:  43.33%, tr:  92.85%, tr_best:  98.98%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 73.7158%\n",
      "layer   3  Sparsity: 98.8409%\n",
      "total_backward_count 1018160 real_backward_count 197496  19.397%\n",
      "epoch-104 lr=['4.0000000'], tr/val_loss:  0.881522/  2.540806, val:  32.92%, val_best:  43.33%, tr:  93.56%, tr_best:  98.98%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 73.6065%\n",
      "layer   3  Sparsity: 98.8122%\n",
      "total_backward_count 1027950 real_backward_count 199299  19.388%\n",
      "epoch-105 lr=['4.0000000'], tr/val_loss:  0.871607/  2.022988, val:  33.33%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 73.8076%\n",
      "layer   3  Sparsity: 98.8029%\n",
      "total_backward_count 1037740 real_backward_count 201137  19.382%\n",
      "epoch-106 lr=['4.0000000'], tr/val_loss:  0.916646/  2.766497, val:  24.58%, val_best:  43.33%, tr:  93.36%, tr_best:  98.98%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 73.5035%\n",
      "layer   3  Sparsity: 98.9222%\n",
      "total_backward_count 1047530 real_backward_count 202925  19.372%\n",
      "epoch-107 lr=['4.0000000'], tr/val_loss:  0.979845/  2.796201, val:  27.08%, val_best:  43.33%, tr:  91.93%, tr_best:  98.98%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 73.9119%\n",
      "layer   3  Sparsity: 98.9806%\n",
      "total_backward_count 1057320 real_backward_count 204827  19.372%\n",
      "epoch-108 lr=['4.0000000'], tr/val_loss:  0.978012/  2.299668, val:  28.33%, val_best:  43.33%, tr:  91.22%, tr_best:  98.98%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 73.7963%\n",
      "layer   3  Sparsity: 98.9974%\n",
      "total_backward_count 1067110 real_backward_count 206769  19.377%\n",
      "epoch-109 lr=['4.0000000'], tr/val_loss:  0.932266/  2.314815, val:  24.17%, val_best:  43.33%, tr:  93.36%, tr_best:  98.98%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 73.7391%\n",
      "layer   3  Sparsity: 98.9720%\n",
      "total_backward_count 1076900 real_backward_count 208606  19.371%\n",
      "fc layer 1 self.abs_max_out: 31897.0\n",
      "lif layer 1 self.abs_max_v: 59408.5\n",
      "epoch-110 lr=['4.0000000'], tr/val_loss:  0.955863/  2.573770, val:  23.75%, val_best:  43.33%, tr:  92.03%, tr_best:  98.98%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 73.5056%\n",
      "layer   3  Sparsity: 98.9780%\n",
      "total_backward_count 1086690 real_backward_count 210476  19.369%\n",
      "lif layer 1 self.abs_max_v: 60345.0\n",
      "epoch-111 lr=['4.0000000'], tr/val_loss:  0.960424/  3.374276, val:  20.00%, val_best:  43.33%, tr:  92.34%, tr_best:  98.98%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 73.3816%\n",
      "layer   3  Sparsity: 98.9760%\n",
      "total_backward_count 1096480 real_backward_count 212371  19.368%\n",
      "epoch-112 lr=['4.0000000'], tr/val_loss:  0.969984/  2.574084, val:  32.92%, val_best:  43.33%, tr:  91.42%, tr_best:  98.98%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 73.1977%\n",
      "layer   3  Sparsity: 98.9754%\n",
      "total_backward_count 1106270 real_backward_count 214285  19.370%\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "lif layer 1 self.abs_max_v: 60757.0\n",
      "epoch-113 lr=['4.0000000'], tr/val_loss:  0.946188/  3.673697, val:  22.50%, val_best:  43.33%, tr:  91.22%, tr_best:  98.98%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 73.5547%\n",
      "layer   3  Sparsity: 99.0267%\n",
      "total_backward_count 1116060 real_backward_count 216201  19.372%\n",
      "fc layer 1 self.abs_max_out: 32490.0\n",
      "epoch-114 lr=['4.0000000'], tr/val_loss:  0.954429/  2.798040, val:  22.50%, val_best:  43.33%, tr:  92.34%, tr_best:  98.98%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 73.5562%\n",
      "layer   3  Sparsity: 99.0429%\n",
      "total_backward_count 1125850 real_backward_count 218141  19.376%\n",
      "epoch-115 lr=['4.0000000'], tr/val_loss:  0.956322/  2.438011, val:  31.25%, val_best:  43.33%, tr:  92.54%, tr_best:  98.98%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 73.6099%\n",
      "layer   3  Sparsity: 99.0207%\n",
      "total_backward_count 1135640 real_backward_count 220077  19.379%\n",
      "lif layer 1 self.abs_max_v: 61237.0\n",
      "epoch-116 lr=['4.0000000'], tr/val_loss:  0.981619/  2.307531, val:  29.17%, val_best:  43.33%, tr:  92.75%, tr_best:  98.98%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 73.3329%\n",
      "layer   3  Sparsity: 99.0664%\n",
      "total_backward_count 1145430 real_backward_count 222021  19.383%\n",
      "epoch-117 lr=['4.0000000'], tr/val_loss:  0.966589/  2.229248, val:  27.92%, val_best:  43.33%, tr:  92.13%, tr_best:  98.98%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 73.0847%\n",
      "layer   3  Sparsity: 99.0936%\n",
      "total_backward_count 1155220 real_backward_count 223905  19.382%\n",
      "epoch-118 lr=['4.0000000'], tr/val_loss:  0.947203/  2.125503, val:  34.17%, val_best:  43.33%, tr:  92.24%, tr_best:  98.98%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 73.0331%\n",
      "layer   3  Sparsity: 99.0539%\n",
      "total_backward_count 1165010 real_backward_count 225843  19.385%\n",
      "lif layer 2 self.abs_max_v: 3017.5\n",
      "lif layer 2 self.abs_max_v: 3043.0\n",
      "epoch-119 lr=['4.0000000'], tr/val_loss:  0.863219/  2.553405, val:  27.50%, val_best:  43.33%, tr:  94.38%, tr_best:  98.98%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 72.9850%\n",
      "layer   3  Sparsity: 98.9608%\n",
      "total_backward_count 1174800 real_backward_count 227729  19.384%\n",
      "epoch-120 lr=['4.0000000'], tr/val_loss:  0.830876/  2.540424, val:  34.17%, val_best:  43.33%, tr:  94.69%, tr_best:  98.98%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 73.0020%\n",
      "layer   3  Sparsity: 98.9635%\n",
      "total_backward_count 1184590 real_backward_count 229574  19.380%\n",
      "epoch-121 lr=['4.0000000'], tr/val_loss:  0.851541/  2.780687, val:  24.17%, val_best:  43.33%, tr:  95.20%, tr_best:  98.98%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 72.9274%\n",
      "layer   3  Sparsity: 98.9891%\n",
      "total_backward_count 1194380 real_backward_count 231509  19.383%\n",
      "epoch-122 lr=['4.0000000'], tr/val_loss:  0.921594/  2.153865, val:  24.17%, val_best:  43.33%, tr:  92.95%, tr_best:  98.98%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 72.8564%\n",
      "layer   3  Sparsity: 99.0399%\n",
      "total_backward_count 1204170 real_backward_count 233438  19.386%\n",
      "epoch-123 lr=['4.0000000'], tr/val_loss:  0.957870/  2.287668, val:  18.75%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 72.7697%\n",
      "layer   3  Sparsity: 99.0812%\n",
      "total_backward_count 1213960 real_backward_count 235346  19.387%\n",
      "epoch-124 lr=['4.0000000'], tr/val_loss:  0.979470/  1.978368, val:  38.33%, val_best:  43.33%, tr:  91.01%, tr_best:  98.98%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 72.9608%\n",
      "layer   3  Sparsity: 99.0723%\n",
      "total_backward_count 1223750 real_backward_count 237260  19.388%\n",
      "epoch-125 lr=['4.0000000'], tr/val_loss:  0.994564/  2.203714, val:  41.25%, val_best:  43.33%, tr:  92.34%, tr_best:  98.98%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 73.1795%\n",
      "layer   3  Sparsity: 99.0715%\n",
      "total_backward_count 1233540 real_backward_count 239151  19.387%\n",
      "epoch-126 lr=['4.0000000'], tr/val_loss:  0.965157/  2.914296, val:  20.00%, val_best:  43.33%, tr:  91.93%, tr_best:  98.98%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 73.0421%\n",
      "layer   3  Sparsity: 99.0447%\n",
      "total_backward_count 1243330 real_backward_count 241029  19.386%\n",
      "epoch-127 lr=['4.0000000'], tr/val_loss:  0.973711/  2.338540, val:  28.33%, val_best:  43.33%, tr:  91.32%, tr_best:  98.98%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 73.3280%\n",
      "layer   3  Sparsity: 99.0481%\n",
      "total_backward_count 1253120 real_backward_count 242980  19.390%\n",
      "epoch-128 lr=['4.0000000'], tr/val_loss:  0.985098/  2.339904, val:  22.08%, val_best:  43.33%, tr:  89.89%, tr_best:  98.98%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 73.5094%\n",
      "layer   3  Sparsity: 99.0492%\n",
      "total_backward_count 1262910 real_backward_count 244838  19.387%\n",
      "epoch-129 lr=['4.0000000'], tr/val_loss:  0.977653/  2.912366, val:  31.67%, val_best:  43.33%, tr:  92.03%, tr_best:  98.98%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 73.8756%\n",
      "layer   3  Sparsity: 99.0392%\n",
      "total_backward_count 1272700 real_backward_count 246785  19.391%\n",
      "epoch-130 lr=['4.0000000'], tr/val_loss:  0.968934/  2.614176, val:  25.42%, val_best:  43.33%, tr:  91.11%, tr_best:  98.98%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 73.4737%\n",
      "layer   3  Sparsity: 99.0570%\n",
      "total_backward_count 1282490 real_backward_count 248732  19.394%\n",
      "epoch-131 lr=['4.0000000'], tr/val_loss:  0.987391/  1.982325, val:  33.33%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 73.1670%\n",
      "layer   3  Sparsity: 99.0606%\n",
      "total_backward_count 1292280 real_backward_count 250593  19.392%\n",
      "epoch-132 lr=['4.0000000'], tr/val_loss:  0.976016/  2.215005, val:  32.50%, val_best:  43.33%, tr:  91.83%, tr_best:  98.98%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 73.2833%\n",
      "layer   3  Sparsity: 99.0473%\n",
      "total_backward_count 1302070 real_backward_count 252489  19.391%\n",
      "epoch-133 lr=['4.0000000'], tr/val_loss:  0.975257/  2.511867, val:  27.50%, val_best:  43.33%, tr:  91.73%, tr_best:  98.98%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 72.9822%\n",
      "layer   3  Sparsity: 99.0470%\n",
      "total_backward_count 1311860 real_backward_count 254429  19.395%\n",
      "lif layer 2 self.abs_max_v: 3071.5\n",
      "epoch-134 lr=['4.0000000'], tr/val_loss:  0.951701/  2.343784, val:  28.33%, val_best:  43.33%, tr:  92.44%, tr_best:  98.98%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 72.7296%\n",
      "layer   3  Sparsity: 99.0468%\n",
      "total_backward_count 1321650 real_backward_count 256315  19.394%\n",
      "lif layer 2 self.abs_max_v: 3077.0\n",
      "lif layer 2 self.abs_max_v: 3103.5\n",
      "epoch-135 lr=['4.0000000'], tr/val_loss:  0.972727/  2.131094, val:  27.92%, val_best:  43.33%, tr:  91.42%, tr_best:  98.98%, epoch time: 80.61 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 72.8281%\n",
      "layer   3  Sparsity: 99.0365%\n",
      "total_backward_count 1331440 real_backward_count 258265  19.397%\n",
      "epoch-136 lr=['4.0000000'], tr/val_loss:  0.859253/  2.559261, val:  32.50%, val_best:  43.33%, tr:  95.30%, tr_best:  98.98%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 72.7582%\n",
      "layer   3  Sparsity: 98.9499%\n",
      "total_backward_count 1341230 real_backward_count 260159  19.397%\n",
      "lif layer 2 self.abs_max_v: 3106.0\n",
      "lif layer 2 self.abs_max_v: 3118.0\n",
      "epoch-137 lr=['4.0000000'], tr/val_loss:  0.841867/  2.519724, val:  35.83%, val_best:  43.33%, tr:  95.10%, tr_best:  98.98%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 72.8513%\n",
      "layer   3  Sparsity: 98.9595%\n",
      "total_backward_count 1351020 real_backward_count 262082  19.399%\n",
      "epoch-138 lr=['4.0000000'], tr/val_loss:  0.854567/  2.425437, val:  32.92%, val_best:  43.33%, tr:  95.10%, tr_best:  98.98%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 73.0607%\n",
      "layer   3  Sparsity: 98.9078%\n",
      "total_backward_count 1360810 real_backward_count 264034  19.403%\n",
      "epoch-139 lr=['4.0000000'], tr/val_loss:  0.833145/  3.078840, val:  22.08%, val_best:  43.33%, tr:  95.30%, tr_best:  98.98%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 73.2333%\n",
      "layer   3  Sparsity: 98.8622%\n",
      "total_backward_count 1370600 real_backward_count 265906  19.401%\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "epoch-140 lr=['4.0000000'], tr/val_loss:  0.819494/  2.594182, val:  26.67%, val_best:  43.33%, tr:  96.12%, tr_best:  98.98%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 72.5778%\n",
      "layer   3  Sparsity: 98.9065%\n",
      "total_backward_count 1380390 real_backward_count 267751  19.397%\n",
      "epoch-141 lr=['4.0000000'], tr/val_loss:  0.822429/  2.596147, val:  37.50%, val_best:  43.33%, tr:  95.30%, tr_best:  98.98%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 72.4528%\n",
      "layer   3  Sparsity: 98.9550%\n",
      "total_backward_count 1390180 real_backward_count 269559  19.390%\n",
      "epoch-142 lr=['4.0000000'], tr/val_loss:  0.895689/  3.130538, val:  33.75%, val_best:  43.33%, tr:  92.24%, tr_best:  98.98%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 72.7331%\n",
      "layer   3  Sparsity: 98.9792%\n",
      "total_backward_count 1399970 real_backward_count 271408  19.387%\n",
      "epoch-143 lr=['4.0000000'], tr/val_loss:  0.951525/  2.112839, val:  34.58%, val_best:  43.33%, tr:  90.70%, tr_best:  98.98%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 72.9330%\n",
      "layer   3  Sparsity: 99.0215%\n",
      "total_backward_count 1409760 real_backward_count 273277  19.385%\n",
      "epoch-144 lr=['4.0000000'], tr/val_loss:  0.943462/  2.352138, val:  20.00%, val_best:  43.33%, tr:  91.11%, tr_best:  98.98%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 73.0777%\n",
      "layer   3  Sparsity: 99.0345%\n",
      "total_backward_count 1419550 real_backward_count 275125  19.381%\n",
      "epoch-145 lr=['4.0000000'], tr/val_loss:  0.954503/  2.659147, val:  29.58%, val_best:  43.33%, tr:  92.13%, tr_best:  98.98%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 73.3459%\n",
      "layer   3  Sparsity: 98.9961%\n",
      "total_backward_count 1429340 real_backward_count 277037  19.382%\n",
      "epoch-146 lr=['4.0000000'], tr/val_loss:  0.941524/  2.451669, val:  34.17%, val_best:  43.33%, tr:  92.65%, tr_best:  98.98%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 73.2798%\n",
      "layer   3  Sparsity: 98.9880%\n",
      "total_backward_count 1439130 real_backward_count 279004  19.387%\n",
      "epoch-147 lr=['4.0000000'], tr/val_loss:  0.863329/  2.579750, val:  24.58%, val_best:  43.33%, tr:  94.99%, tr_best:  98.98%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 73.3609%\n",
      "layer   3  Sparsity: 98.9152%\n",
      "total_backward_count 1448920 real_backward_count 280925  19.389%\n",
      "epoch-148 lr=['4.0000000'], tr/val_loss:  0.833564/  3.171834, val:  26.25%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 72.7210%\n",
      "layer   3  Sparsity: 98.8723%\n",
      "total_backward_count 1458710 real_backward_count 282861  19.391%\n",
      "epoch-149 lr=['4.0000000'], tr/val_loss:  0.848034/  2.380095, val:  25.83%, val_best:  43.33%, tr:  95.81%, tr_best:  98.98%, epoch time: 80.55 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 72.8892%\n",
      "layer   3  Sparsity: 98.9376%\n",
      "total_backward_count 1468500 real_backward_count 284820  19.395%\n",
      "lif layer 1 self.abs_max_v: 61561.5\n",
      "epoch-150 lr=['4.0000000'], tr/val_loss:  0.914599/  3.037894, val:  15.42%, val_best:  43.33%, tr:  94.69%, tr_best:  98.98%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 72.8121%\n",
      "layer   3  Sparsity: 99.0279%\n",
      "total_backward_count 1478290 real_backward_count 286707  19.395%\n",
      "fc layer 1 self.abs_max_out: 33603.0\n",
      "epoch-151 lr=['4.0000000'], tr/val_loss:  0.964967/  2.091352, val:  35.83%, val_best:  43.33%, tr:  91.52%, tr_best:  98.98%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 73.0060%\n",
      "layer   3  Sparsity: 99.0380%\n",
      "total_backward_count 1488080 real_backward_count 288610  19.395%\n",
      "lif layer 2 self.abs_max_v: 3122.5\n",
      "lif layer 2 self.abs_max_v: 3142.5\n",
      "lif layer 1 self.abs_max_v: 61668.0\n",
      "lif layer 2 self.abs_max_v: 3143.5\n",
      "lif layer 2 self.abs_max_v: 3153.0\n",
      "lif layer 2 self.abs_max_v: 3157.5\n",
      "epoch-152 lr=['4.0000000'], tr/val_loss:  0.923937/  2.786052, val:  25.83%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 72.8116%\n",
      "layer   3  Sparsity: 98.9763%\n",
      "total_backward_count 1497870 real_backward_count 290598  19.401%\n",
      "lif layer 1 self.abs_max_v: 62421.5\n",
      "epoch-153 lr=['4.0000000'], tr/val_loss:  0.881896/  2.799041, val:  28.33%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 72.7936%\n",
      "layer   3  Sparsity: 98.9682%\n",
      "total_backward_count 1507660 real_backward_count 292559  19.405%\n",
      "fc layer 2 self.abs_max_out: 1704.0\n",
      "lif layer 2 self.abs_max_v: 3177.5\n",
      "lif layer 2 self.abs_max_v: 3225.5\n",
      "lif layer 2 self.abs_max_v: 3245.0\n",
      "epoch-154 lr=['4.0000000'], tr/val_loss:  0.829808/  2.099115, val:  40.83%, val_best:  43.33%, tr:  95.20%, tr_best:  98.98%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 73.0881%\n",
      "layer   3  Sparsity: 98.9835%\n",
      "total_backward_count 1517450 real_backward_count 294501  19.408%\n",
      "epoch-155 lr=['4.0000000'], tr/val_loss:  0.828098/  2.014964, val:  31.25%, val_best:  43.33%, tr:  95.30%, tr_best:  98.98%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 72.7432%\n",
      "layer   3  Sparsity: 99.0149%\n",
      "total_backward_count 1527240 real_backward_count 296359  19.405%\n",
      "epoch-156 lr=['4.0000000'], tr/val_loss:  0.832245/  2.145283, val:  39.17%, val_best:  43.33%, tr:  94.99%, tr_best:  98.98%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 72.6315%\n",
      "layer   3  Sparsity: 99.0082%\n",
      "total_backward_count 1537030 real_backward_count 298162  19.399%\n",
      "lif layer 2 self.abs_max_v: 3257.5\n",
      "lif layer 2 self.abs_max_v: 3270.0\n",
      "lif layer 2 self.abs_max_v: 3276.0\n",
      "lif layer 2 self.abs_max_v: 3289.0\n",
      "lif layer 2 self.abs_max_v: 3305.5\n",
      "lif layer 2 self.abs_max_v: 3306.0\n",
      "lif layer 2 self.abs_max_v: 3314.0\n",
      "epoch-157 lr=['4.0000000'], tr/val_loss:  0.805536/  2.510863, val:  25.83%, val_best:  43.33%, tr:  95.81%, tr_best:  98.98%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 72.3416%\n",
      "layer   3  Sparsity: 98.9726%\n",
      "total_backward_count 1546820 real_backward_count 299942  19.391%\n",
      "lif layer 2 self.abs_max_v: 3334.0\n",
      "lif layer 2 self.abs_max_v: 3349.0\n",
      "lif layer 2 self.abs_max_v: 3375.5\n",
      "lif layer 2 self.abs_max_v: 3389.0\n",
      "lif layer 2 self.abs_max_v: 3395.5\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "epoch-158 lr=['4.0000000'], tr/val_loss:  0.847999/  2.391042, val:  27.92%, val_best:  43.33%, tr:  94.79%, tr_best:  98.98%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 72.4592%\n",
      "layer   3  Sparsity: 99.0207%\n",
      "total_backward_count 1556610 real_backward_count 301799  19.388%\n",
      "epoch-159 lr=['4.0000000'], tr/val_loss:  0.833999/  2.481246, val:  24.17%, val_best:  43.33%, tr:  95.91%, tr_best:  98.98%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 72.9002%\n",
      "layer   3  Sparsity: 98.9864%\n",
      "total_backward_count 1566400 real_backward_count 303613  19.383%\n",
      "epoch-160 lr=['4.0000000'], tr/val_loss:  0.841140/  2.167683, val:  40.42%, val_best:  43.33%, tr:  94.69%, tr_best:  98.98%, epoch time: 80.80 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 72.9677%\n",
      "layer   3  Sparsity: 98.9800%\n",
      "total_backward_count 1576190 real_backward_count 305413  19.377%\n",
      "epoch-161 lr=['4.0000000'], tr/val_loss:  0.847263/  2.102232, val:  36.67%, val_best:  43.33%, tr:  94.38%, tr_best:  98.98%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 72.7739%\n",
      "layer   3  Sparsity: 98.9760%\n",
      "total_backward_count 1585980 real_backward_count 307283  19.375%\n",
      "epoch-162 lr=['4.0000000'], tr/val_loss:  0.820939/  2.464493, val:  27.50%, val_best:  43.33%, tr:  95.51%, tr_best:  98.98%, epoch time: 80.64 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 72.5324%\n",
      "layer   3  Sparsity: 98.9715%\n",
      "total_backward_count 1595770 real_backward_count 309065  19.368%\n",
      "epoch-163 lr=['4.0000000'], tr/val_loss:  0.875008/  2.653461, val:  28.75%, val_best:  43.33%, tr:  94.48%, tr_best:  98.98%, epoch time: 80.52 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 73.0327%\n",
      "layer   3  Sparsity: 99.0032%\n",
      "total_backward_count 1605560 real_backward_count 310826  19.359%\n",
      "lif layer 2 self.abs_max_v: 3399.0\n",
      "epoch-164 lr=['4.0000000'], tr/val_loss:  0.923437/  2.172299, val:  34.17%, val_best:  43.33%, tr:  92.54%, tr_best:  98.98%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 72.6374%\n",
      "layer   3  Sparsity: 99.0297%\n",
      "total_backward_count 1615350 real_backward_count 312747  19.361%\n",
      "fc layer 2 self.abs_max_out: 1765.0\n",
      "lif layer 1 self.abs_max_v: 63380.5\n",
      "epoch-165 lr=['4.0000000'], tr/val_loss:  0.962792/  2.355879, val:  28.75%, val_best:  43.33%, tr:  93.16%, tr_best:  98.98%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 71.6731%\n",
      "layer   3  Sparsity: 99.0425%\n",
      "total_backward_count 1625140 real_backward_count 314656  19.362%\n",
      "lif layer 2 self.abs_max_v: 3401.0\n",
      "fc layer 2 self.abs_max_out: 1785.0\n",
      "lif layer 2 self.abs_max_v: 3415.5\n",
      "lif layer 2 self.abs_max_v: 3429.0\n",
      "lif layer 2 self.abs_max_v: 3435.5\n",
      "lif layer 2 self.abs_max_v: 3439.0\n",
      "lif layer 2 self.abs_max_v: 3481.5\n",
      "fc layer 1 self.abs_max_out: 34967.0\n",
      "fc layer 1 self.abs_max_out: 36191.0\n",
      "fc layer 1 self.abs_max_out: 37367.0\n",
      "lif layer 1 self.abs_max_v: 67969.5\n",
      "lif layer 1 self.abs_max_v: 69108.0\n",
      "lif layer 1 self.abs_max_v: 69937.0\n",
      "fc layer 1 self.abs_max_out: 38422.0\n",
      "lif layer 1 self.abs_max_v: 71190.0\n",
      "epoch-166 lr=['4.0000000'], tr/val_loss:  1.105292/  2.360094, val:  24.17%, val_best:  43.33%, tr:  83.96%, tr_best:  98.98%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 67.3907%\n",
      "layer   3  Sparsity: 99.1555%\n",
      "total_backward_count 1634930 real_backward_count 317566  19.424%\n",
      "lif layer 2 self.abs_max_v: 3503.0\n",
      "epoch-167 lr=['4.0000000'], tr/val_loss:  1.083898/  2.692729, val:  25.42%, val_best:  43.33%, tr:  88.15%, tr_best:  98.98%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 66.1057%\n",
      "layer   3  Sparsity: 99.1838%\n",
      "total_backward_count 1644720 real_backward_count 320003  19.456%\n",
      "lif layer 2 self.abs_max_v: 3510.5\n",
      "epoch-168 lr=['4.0000000'], tr/val_loss:  1.109496/  2.207766, val:  20.00%, val_best:  43.33%, tr:  89.07%, tr_best:  98.98%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 65.7603%\n",
      "layer   3  Sparsity: 99.2030%\n",
      "total_backward_count 1654510 real_backward_count 322265  19.478%\n",
      "lif layer 2 self.abs_max_v: 3511.5\n",
      "epoch-169 lr=['4.0000000'], tr/val_loss:  1.152836/  2.317427, val:  25.83%, val_best:  43.33%, tr:  85.50%, tr_best:  98.98%, epoch time: 80.73 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 65.2626%\n",
      "layer   3  Sparsity: 99.2274%\n",
      "total_backward_count 1664300 real_backward_count 324567  19.502%\n",
      "fc layer 2 self.abs_max_out: 1805.0\n",
      "lif layer 2 self.abs_max_v: 3545.0\n",
      "lif layer 2 self.abs_max_v: 3562.0\n",
      "lif layer 2 self.abs_max_v: 3581.0\n",
      "epoch-170 lr=['4.0000000'], tr/val_loss:  1.197504/  2.117821, val:  25.83%, val_best:  43.33%, tr:  84.47%, tr_best:  98.98%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 64.5867%\n",
      "layer   3  Sparsity: 99.2435%\n",
      "total_backward_count 1674090 real_backward_count 326991  19.532%\n",
      "lif layer 2 self.abs_max_v: 3594.0\n",
      "epoch-171 lr=['4.0000000'], tr/val_loss:  1.097214/  2.421643, val:  24.58%, val_best:  43.33%, tr:  85.39%, tr_best:  98.98%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 66.5673%\n",
      "layer   3  Sparsity: 99.2025%\n",
      "total_backward_count 1683880 real_backward_count 328868  19.530%\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "epoch-172 lr=['4.0000000'], tr/val_loss:  0.990806/  1.838699, val:  33.75%, val_best:  43.33%, tr:  91.32%, tr_best:  98.98%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 66.8037%\n",
      "layer   3  Sparsity: 99.1550%\n",
      "total_backward_count 1693670 real_backward_count 330811  19.532%\n",
      "fc layer 1 self.abs_max_out: 40197.0\n",
      "lif layer 2 self.abs_max_v: 3600.5\n",
      "lif layer 2 self.abs_max_v: 3602.5\n",
      "lif layer 2 self.abs_max_v: 3626.5\n",
      "lif layer 2 self.abs_max_v: 3638.5\n",
      "lif layer 2 self.abs_max_v: 3644.5\n",
      "epoch-173 lr=['4.0000000'], tr/val_loss:  0.928815/  2.343442, val:  30.42%, val_best:  43.33%, tr:  93.67%, tr_best:  98.98%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 65.2106%\n",
      "layer   3  Sparsity: 99.1226%\n",
      "total_backward_count 1703460 real_backward_count 332665  19.529%\n",
      "epoch-174 lr=['4.0000000'], tr/val_loss:  1.017865/  2.170694, val:  36.67%, val_best:  43.33%, tr:  92.54%, tr_best:  98.98%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 65.6992%\n",
      "layer   3  Sparsity: 99.1558%\n",
      "total_backward_count 1713250 real_backward_count 334661  19.534%\n",
      "fc layer 2 self.abs_max_out: 1865.0\n",
      "epoch-175 lr=['4.0000000'], tr/val_loss:  1.091968/  2.202440, val:  28.75%, val_best:  43.33%, tr:  89.17%, tr_best:  98.98%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 66.2356%\n",
      "layer   3  Sparsity: 99.1813%\n",
      "total_backward_count 1723040 real_backward_count 336643  19.538%\n",
      "epoch-176 lr=['4.0000000'], tr/val_loss:  1.085020/  2.407032, val:  34.17%, val_best:  43.33%, tr:  88.56%, tr_best:  98.98%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 66.3904%\n",
      "layer   3  Sparsity: 99.1875%\n",
      "total_backward_count 1732830 real_backward_count 338614  19.541%\n",
      "epoch-177 lr=['4.0000000'], tr/val_loss:  1.035939/  2.157315, val:  30.83%, val_best:  43.33%, tr:  89.89%, tr_best:  98.98%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 66.4353%\n",
      "layer   3  Sparsity: 99.1669%\n",
      "total_backward_count 1742620 real_backward_count 340534  19.541%\n",
      "epoch-178 lr=['4.0000000'], tr/val_loss:  1.096665/  2.680754, val:  28.75%, val_best:  43.33%, tr:  88.66%, tr_best:  98.98%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 65.5553%\n",
      "layer   3  Sparsity: 99.2235%\n",
      "total_backward_count 1752410 real_backward_count 342654  19.553%\n",
      "lif layer 1 self.abs_max_v: 71204.0\n",
      "epoch-179 lr=['4.0000000'], tr/val_loss:  1.062699/  2.131770, val:  32.08%, val_best:  43.33%, tr:  90.19%, tr_best:  98.98%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 63.7859%\n",
      "layer   3  Sparsity: 99.2297%\n",
      "total_backward_count 1762200 real_backward_count 344975  19.576%\n",
      "epoch-180 lr=['4.0000000'], tr/val_loss:  1.060843/  2.242611, val:  35.42%, val_best:  43.33%, tr:  91.01%, tr_best:  98.98%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 63.7779%\n",
      "layer   3  Sparsity: 99.2255%\n",
      "total_backward_count 1771990 real_backward_count 347169  19.592%\n",
      "lif layer 1 self.abs_max_v: 71290.0\n",
      "epoch-181 lr=['4.0000000'], tr/val_loss:  1.101185/  2.266774, val:  27.92%, val_best:  43.33%, tr:  88.97%, tr_best:  98.98%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 64.1514%\n",
      "layer   3  Sparsity: 99.2784%\n",
      "total_backward_count 1781780 real_backward_count 349306  19.604%\n",
      "lif layer 1 self.abs_max_v: 71292.0\n",
      "epoch-182 lr=['4.0000000'], tr/val_loss:  1.151448/  2.385493, val:  30.83%, val_best:  43.33%, tr:  87.03%, tr_best:  98.98%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 62.8839%\n",
      "layer   3  Sparsity: 99.2794%\n",
      "total_backward_count 1791570 real_backward_count 351621  19.626%\n",
      "epoch-183 lr=['4.0000000'], tr/val_loss:  1.128611/  2.180529, val:  20.42%, val_best:  43.33%, tr:  88.46%, tr_best:  98.98%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 62.9041%\n",
      "layer   3  Sparsity: 99.2886%\n",
      "total_backward_count 1801360 real_backward_count 353987  19.651%\n",
      "epoch-184 lr=['4.0000000'], tr/val_loss:  1.130673/  2.085569, val:  34.17%, val_best:  43.33%, tr:  87.74%, tr_best:  98.98%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 62.8411%\n",
      "layer   3  Sparsity: 99.2818%\n",
      "total_backward_count 1811150 real_backward_count 356341  19.675%\n",
      "epoch-185 lr=['4.0000000'], tr/val_loss:  1.134680/  2.347916, val:  20.83%, val_best:  43.33%, tr:  89.07%, tr_best:  98.98%, epoch time: 81.34 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 61.8305%\n",
      "layer   3  Sparsity: 99.2998%\n",
      "total_backward_count 1820940 real_backward_count 358793  19.704%\n",
      "epoch-186 lr=['4.0000000'], tr/val_loss:  1.231820/  2.425797, val:  33.75%, val_best:  43.33%, tr:  85.19%, tr_best:  98.98%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 61.4380%\n",
      "layer   3  Sparsity: 99.3186%\n",
      "total_backward_count 1830730 real_backward_count 361278  19.734%\n",
      "epoch-187 lr=['4.0000000'], tr/val_loss:  1.264279/  2.364138, val:  28.33%, val_best:  43.33%, tr:  81.82%, tr_best:  98.98%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 61.1635%\n",
      "layer   3  Sparsity: 99.3262%\n",
      "total_backward_count 1840520 real_backward_count 364210  19.788%\n",
      "epoch-188 lr=['4.0000000'], tr/val_loss:  1.231644/  2.149957, val:  32.92%, val_best:  43.33%, tr:  84.17%, tr_best:  98.98%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 61.3792%\n",
      "layer   3  Sparsity: 99.3097%\n",
      "total_backward_count 1850310 real_backward_count 367216  19.846%\n",
      "epoch-189 lr=['4.0000000'], tr/val_loss:  1.270033/  2.168371, val:  32.08%, val_best:  43.33%, tr:  82.64%, tr_best:  98.98%, epoch time: 80.56 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 61.0155%\n",
      "layer   3  Sparsity: 99.3578%\n",
      "total_backward_count 1860100 real_backward_count 370174  19.901%\n",
      "epoch-190 lr=['4.0000000'], tr/val_loss:  1.263600/  2.090899, val:  32.92%, val_best:  43.33%, tr:  82.84%, tr_best:  98.98%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 60.4087%\n",
      "layer   3  Sparsity: 99.3484%\n",
      "total_backward_count 1869890 real_backward_count 372995  19.947%\n",
      "epoch-191 lr=['4.0000000'], tr/val_loss:  1.261967/  2.552982, val:  25.00%, val_best:  43.33%, tr:  82.84%, tr_best:  98.98%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 60.8325%\n",
      "layer   3  Sparsity: 99.3522%\n",
      "total_backward_count 1879680 real_backward_count 375925  19.999%\n",
      "epoch-192 lr=['4.0000000'], tr/val_loss:  1.266941/  2.327801, val:  24.17%, val_best:  43.33%, tr:  84.58%, tr_best:  98.98%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 60.4296%\n",
      "layer   3  Sparsity: 99.3513%\n",
      "total_backward_count 1889470 real_backward_count 378804  20.048%\n",
      "epoch-193 lr=['4.0000000'], tr/val_loss:  1.228890/  2.261835, val:  21.25%, val_best:  43.33%, tr:  84.37%, tr_best:  98.98%, epoch time: 81.30 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 60.2731%\n",
      "layer   3  Sparsity: 99.3428%\n",
      "total_backward_count 1899260 real_backward_count 381663  20.095%\n",
      "epoch-194 lr=['4.0000000'], tr/val_loss:  1.273711/  2.160773, val:  26.67%, val_best:  43.33%, tr:  82.12%, tr_best:  98.98%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 60.4491%\n",
      "layer   3  Sparsity: 99.3507%\n",
      "total_backward_count 1909050 real_backward_count 384461  20.139%\n",
      "fc layer 1 self.abs_max_out: 41896.0\n",
      "lif layer 1 self.abs_max_v: 72476.5\n",
      "epoch-195 lr=['4.0000000'], tr/val_loss:  1.224047/  2.029570, val:  32.08%, val_best:  43.33%, tr:  84.78%, tr_best:  98.98%, epoch time: 80.62 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 60.6006%\n",
      "layer   3  Sparsity: 99.3351%\n",
      "total_backward_count 1918840 real_backward_count 387322  20.185%\n",
      "fc layer 1 self.abs_max_out: 41899.0\n",
      "lif layer 1 self.abs_max_v: 72511.0\n",
      "epoch-196 lr=['4.0000000'], tr/val_loss:  1.212808/  2.120196, val:  24.17%, val_best:  43.33%, tr:  86.93%, tr_best:  98.98%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 60.2881%\n",
      "layer   3  Sparsity: 99.3292%\n",
      "total_backward_count 1928630 real_backward_count 390231  20.234%\n",
      "epoch-197 lr=['4.0000000'], tr/val_loss:  1.207249/  2.142799, val:  28.33%, val_best:  43.33%, tr:  87.44%, tr_best:  98.98%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 60.5651%\n",
      "layer   3  Sparsity: 99.3427%\n",
      "total_backward_count 1938420 real_backward_count 393111  20.280%\n",
      "epoch-198 lr=['4.0000000'], tr/val_loss:  1.231861/  2.102150, val:  29.17%, val_best:  43.33%, tr:  86.41%, tr_best:  98.98%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 61.1297%\n",
      "layer   3  Sparsity: 99.3681%\n",
      "total_backward_count 1948210 real_backward_count 395993  20.326%\n",
      "epoch-199 lr=['4.0000000'], tr/val_loss:  1.259013/  2.095724, val:  30.42%, val_best:  43.33%, tr:  83.66%, tr_best:  98.98%, epoch time: 80.62 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 60.9601%\n",
      "layer   3  Sparsity: 99.3705%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0a72b505b440ae9eb32c8a31e22e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.83657</td></tr><tr><td>tr_epoch_loss</td><td>1.25901</td></tr><tr><td>val_acc_best</td><td>0.43333</td></tr><tr><td>val_acc_now</td><td>0.30417</td></tr><tr><td>val_loss</td><td>2.09572</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-sweep-845</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/10p6wqda' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/10p6wqda</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251211_214200-10p6wqda/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mzhubrot with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_020828-mzhubrot</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mzhubrot' target=\"_blank\">sage-sweep-855</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mzhubrot' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mzhubrot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251212_020837_752', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 32, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 32, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.25, 'lif_layer_v_threshold2': 128, 'init_scaling': [0.25, 1, 1], 'learning_rate': 4, 'learning_rate2': 8} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 32, self.v_threshold 32\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.25, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 1, 1])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=32, v_reset=10000, sg_width=32, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 1, 1])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=0.25, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 1, 1])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 4\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 373.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1696.0\n",
      "lif layer 2 self.abs_max_v: 1696.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 1204.0\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 457.5\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "lif layer 2 self.abs_max_v: 2903.0\n",
      "fc layer 3 self.abs_max_out: 1833.0\n",
      "fc layer 1 self.abs_max_out: 410.0\n",
      "lif layer 1 self.abs_max_v: 542.0\n",
      "lif layer 2 self.abs_max_v: 2960.5\n",
      "fc layer 1 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 560.0\n",
      "lif layer 1 self.abs_max_v: 654.0\n",
      "lif layer 2 self.abs_max_v: 3047.0\n",
      "fc layer 2 self.abs_max_out: 2647.0\n",
      "lif layer 2 self.abs_max_v: 3857.5\n",
      "fc layer 1 self.abs_max_out: 528.0\n",
      "lif layer 1 self.abs_max_v: 832.0\n",
      "fc layer 2 self.abs_max_out: 2986.0\n",
      "lif layer 2 self.abs_max_v: 4324.0\n",
      "fc layer 1 self.abs_max_out: 536.0\n",
      "lif layer 2 self.abs_max_v: 4866.0\n",
      "fc layer 1 self.abs_max_out: 713.0\n",
      "fc layer 2 self.abs_max_out: 3144.0\n",
      "lif layer 1 self.abs_max_v: 851.0\n",
      "fc layer 2 self.abs_max_out: 3225.0\n",
      "fc layer 2 self.abs_max_out: 3236.0\n",
      "fc layer 1 self.abs_max_out: 799.0\n",
      "lif layer 1 self.abs_max_v: 882.0\n",
      "fc layer 2 self.abs_max_out: 4478.0\n",
      "lif layer 2 self.abs_max_v: 5178.5\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "lif layer 1 self.abs_max_v: 1079.0\n",
      "lif layer 2 self.abs_max_v: 5531.0\n",
      "lif layer 2 self.abs_max_v: 5680.5\n",
      "lif layer 2 self.abs_max_v: 5887.5\n",
      "lif layer 2 self.abs_max_v: 6003.0\n",
      "fc layer 1 self.abs_max_out: 1078.0\n",
      "lif layer 1 self.abs_max_v: 1104.5\n",
      "lif layer 1 self.abs_max_v: 1394.5\n",
      "lif layer 2 self.abs_max_v: 6324.5\n",
      "lif layer 2 self.abs_max_v: 6337.0\n",
      "lif layer 2 self.abs_max_v: 7643.5\n",
      "fc layer 2 self.abs_max_out: 4693.0\n",
      "lif layer 2 self.abs_max_v: 8515.0\n",
      "lif layer 2 self.abs_max_v: 8558.5\n",
      "lif layer 2 self.abs_max_v: 8653.0\n",
      "fc layer 2 self.abs_max_out: 4889.0\n",
      "fc layer 3 self.abs_max_out: 1850.0\n",
      "lif layer 2 self.abs_max_v: 8655.0\n",
      "lif layer 1 self.abs_max_v: 1686.5\n",
      "lif layer 2 self.abs_max_v: 8775.0\n",
      "lif layer 2 self.abs_max_v: 9163.5\n",
      "fc layer 1 self.abs_max_out: 1135.0\n",
      "fc layer 1 self.abs_max_out: 1169.0\n",
      "fc layer 3 self.abs_max_out: 1913.0\n",
      "fc layer 3 self.abs_max_out: 1983.0\n",
      "fc layer 3 self.abs_max_out: 2200.0\n",
      "fc layer 1 self.abs_max_out: 1171.0\n",
      "fc layer 2 self.abs_max_out: 4921.0\n",
      "lif layer 2 self.abs_max_v: 9284.0\n",
      "fc layer 2 self.abs_max_out: 5285.0\n",
      "fc layer 2 self.abs_max_out: 5660.0\n",
      "fc layer 2 self.abs_max_out: 6054.0\n",
      "fc layer 2 self.abs_max_out: 6245.0\n",
      "lif layer 2 self.abs_max_v: 9352.0\n",
      "lif layer 2 self.abs_max_v: 9992.0\n",
      "lif layer 2 self.abs_max_v: 10146.0\n",
      "lif layer 2 self.abs_max_v: 10517.0\n",
      "fc layer 2 self.abs_max_out: 6640.0\n",
      "lif layer 2 self.abs_max_v: 11224.5\n",
      "lif layer 2 self.abs_max_v: 11242.5\n",
      "fc layer 2 self.abs_max_out: 6732.0\n",
      "fc layer 2 self.abs_max_out: 6811.0\n",
      "lif layer 2 self.abs_max_v: 11308.0\n",
      "fc layer 2 self.abs_max_out: 7186.0\n",
      "lif layer 2 self.abs_max_v: 12375.0\n",
      "fc layer 1 self.abs_max_out: 1201.0\n",
      "fc layer 1 self.abs_max_out: 1245.0\n",
      "lif layer 1 self.abs_max_v: 1721.5\n",
      "fc layer 1 self.abs_max_out: 1370.0\n",
      "lif layer 1 self.abs_max_v: 2129.0\n",
      "lif layer 1 self.abs_max_v: 2138.5\n",
      "fc layer 1 self.abs_max_out: 1559.0\n",
      "fc layer 1 self.abs_max_out: 1815.0\n",
      "fc layer 1 self.abs_max_out: 2163.0\n",
      "lif layer 1 self.abs_max_v: 2201.0\n",
      "lif layer 1 self.abs_max_v: 2459.5\n",
      "lif layer 2 self.abs_max_v: 12462.0\n",
      "lif layer 2 self.abs_max_v: 12795.0\n",
      "fc layer 1 self.abs_max_out: 2202.0\n",
      "lif layer 2 self.abs_max_v: 12901.5\n",
      "fc layer 2 self.abs_max_out: 7285.0\n",
      "lif layer 2 self.abs_max_v: 13223.5\n",
      "fc layer 2 self.abs_max_out: 7339.0\n",
      "lif layer 2 self.abs_max_v: 13951.0\n",
      "fc layer 2 self.abs_max_out: 7923.0\n",
      "lif layer 2 self.abs_max_v: 14535.0\n",
      "lif layer 2 self.abs_max_v: 14747.5\n",
      "lif layer 1 self.abs_max_v: 2525.5\n",
      "lif layer 1 self.abs_max_v: 2533.0\n",
      "lif layer 1 self.abs_max_v: 2807.0\n",
      "lif layer 1 self.abs_max_v: 2821.0\n",
      "lif layer 1 self.abs_max_v: 2885.0\n",
      "lif layer 1 self.abs_max_v: 3264.0\n",
      "lif layer 1 self.abs_max_v: 3346.5\n",
      "lif layer 1 self.abs_max_v: 3502.0\n",
      "fc layer 1 self.abs_max_out: 2238.0\n",
      "lif layer 1 self.abs_max_v: 3651.0\n",
      "lif layer 1 self.abs_max_v: 3902.5\n",
      "fc layer 1 self.abs_max_out: 2249.0\n",
      "fc layer 1 self.abs_max_out: 2254.0\n",
      "fc layer 1 self.abs_max_out: 2395.0\n",
      "lif layer 1 self.abs_max_v: 4033.5\n",
      "fc layer 1 self.abs_max_out: 2587.0\n",
      "lif layer 1 self.abs_max_v: 4604.0\n",
      "fc layer 1 self.abs_max_out: 2710.0\n",
      "lif layer 1 self.abs_max_v: 4732.0\n",
      "lif layer 1 self.abs_max_v: 4782.0\n",
      "lif layer 1 self.abs_max_v: 4831.5\n",
      "fc layer 1 self.abs_max_out: 2765.0\n",
      "fc layer 1 self.abs_max_out: 2786.0\n",
      "lif layer 1 self.abs_max_v: 5050.0\n",
      "lif layer 1 self.abs_max_v: 5176.0\n",
      "fc layer 1 self.abs_max_out: 2854.0\n",
      "fc layer 1 self.abs_max_out: 2897.0\n",
      "lif layer 1 self.abs_max_v: 5383.0\n",
      "lif layer 1 self.abs_max_v: 5537.5\n",
      "fc layer 1 self.abs_max_out: 3068.0\n",
      "lif layer 1 self.abs_max_v: 5837.0\n",
      "fc layer 1 self.abs_max_out: 3335.0\n",
      "lif layer 1 self.abs_max_v: 6253.5\n",
      "fc layer 1 self.abs_max_out: 3352.0\n",
      "lif layer 1 self.abs_max_v: 6479.0\n",
      "epoch-0   lr=['4.0000000'], tr/val_loss:110.394089/228.441742, val:  32.92%, val_best:  32.92%, tr:  71.30%, tr_best:  71.30%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 73.0120%\n",
      "layer   3  Sparsity: 70.7955%\n",
      "total_backward_count 9790 real_backward_count 4026  41.124%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 3522.0\n",
      "fc layer 1 self.abs_max_out: 3557.0\n",
      "lif layer 1 self.abs_max_v: 6559.5\n",
      "lif layer 1 self.abs_max_v: 6787.0\n",
      "epoch-1   lr=['4.0000000'], tr/val_loss: 53.519379/125.459953, val:  43.75%, val_best:  43.75%, tr:  85.09%, tr_best:  85.09%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 78.0393%\n",
      "layer   3  Sparsity: 74.1244%\n",
      "total_backward_count 19580 real_backward_count 7083  36.175%\n",
      "fc layer 1 self.abs_max_out: 3665.0\n",
      "fc layer 1 self.abs_max_out: 3674.0\n",
      "fc layer 1 self.abs_max_out: 3687.0\n",
      "lif layer 1 self.abs_max_v: 6984.0\n",
      "fc layer 1 self.abs_max_out: 3887.0\n",
      "lif layer 1 self.abs_max_v: 7288.0\n",
      "epoch-2   lr=['4.0000000'], tr/val_loss: 39.227928/113.906403, val:  43.33%, val_best:  43.75%, tr:  87.54%, tr_best:  87.54%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 78.9674%\n",
      "layer   3  Sparsity: 77.4627%\n",
      "total_backward_count 29370 real_backward_count 9905  33.725%\n",
      "fc layer 1 self.abs_max_out: 4082.0\n",
      "fc layer 1 self.abs_max_out: 4232.0\n",
      "lif layer 1 self.abs_max_v: 7856.5\n",
      "fc layer 1 self.abs_max_out: 4387.0\n",
      "lif layer 1 self.abs_max_v: 7870.0\n",
      "fc layer 1 self.abs_max_out: 4608.0\n",
      "lif layer 1 self.abs_max_v: 8543.0\n",
      "lif layer 1 self.abs_max_v: 8598.5\n",
      "epoch-3   lr=['4.0000000'], tr/val_loss: 32.463066/ 77.342346, val:  41.67%, val_best:  43.75%, tr:  89.38%, tr_best:  89.38%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 78.4865%\n",
      "layer   3  Sparsity: 77.6321%\n",
      "total_backward_count 39160 real_backward_count 12421  31.719%\n",
      "epoch-4   lr=['4.0000000'], tr/val_loss: 26.735260/ 71.200462, val:  39.58%, val_best:  43.75%, tr:  89.89%, tr_best:  89.89%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 79.1607%\n",
      "layer   3  Sparsity: 79.5877%\n",
      "total_backward_count 48950 real_backward_count 14844  30.325%\n",
      "epoch-5   lr=['4.0000000'], tr/val_loss: 20.983912/ 68.478111, val:  37.50%, val_best:  43.75%, tr:  91.73%, tr_best:  91.73%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 80.0579%\n",
      "layer   3  Sparsity: 80.2651%\n",
      "total_backward_count 58740 real_backward_count 17075  29.069%\n",
      "epoch-6   lr=['4.0000000'], tr/val_loss: 20.191704/ 70.032310, val:  49.58%, val_best:  49.58%, tr:  92.54%, tr_best:  92.54%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 80.0226%\n",
      "layer   3  Sparsity: 79.2313%\n",
      "total_backward_count 68530 real_backward_count 19195  28.010%\n",
      "epoch-7   lr=['4.0000000'], tr/val_loss: 17.679283/ 51.647358, val:  53.33%, val_best:  53.33%, tr:  93.97%, tr_best:  93.97%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 79.3365%\n",
      "layer   3  Sparsity: 79.3161%\n",
      "total_backward_count 78320 real_backward_count 21087  26.924%\n",
      "epoch-8   lr=['4.0000000'], tr/val_loss: 14.311379/ 40.685497, val:  47.08%, val_best:  53.33%, tr:  94.99%, tr_best:  94.99%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 79.5595%\n",
      "layer   3  Sparsity: 80.2757%\n",
      "total_backward_count 88110 real_backward_count 22945  26.041%\n",
      "fc layer 1 self.abs_max_out: 4653.0\n",
      "lif layer 1 self.abs_max_v: 8896.0\n",
      "epoch-9   lr=['4.0000000'], tr/val_loss: 13.513930/ 54.393219, val:  41.25%, val_best:  53.33%, tr:  95.51%, tr_best:  95.51%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 80.3696%\n",
      "layer   3  Sparsity: 81.0948%\n",
      "total_backward_count 97900 real_backward_count 24770  25.301%\n",
      "epoch-10  lr=['4.0000000'], tr/val_loss: 13.279938/ 58.445267, val:  39.17%, val_best:  53.33%, tr:  95.51%, tr_best:  95.51%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 80.6777%\n",
      "layer   3  Sparsity: 81.2035%\n",
      "total_backward_count 107690 real_backward_count 26564  24.667%\n",
      "fc layer 1 self.abs_max_out: 4782.0\n",
      "epoch-11  lr=['4.0000000'], tr/val_loss: 12.465993/ 38.383186, val:  50.42%, val_best:  53.33%, tr:  94.48%, tr_best:  95.51%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 80.4404%\n",
      "layer   3  Sparsity: 81.3857%\n",
      "total_backward_count 117480 real_backward_count 28330  24.115%\n",
      "epoch-12  lr=['4.0000000'], tr/val_loss: 11.308935/ 47.653931, val:  48.33%, val_best:  53.33%, tr:  95.40%, tr_best:  95.51%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 80.3140%\n",
      "layer   3  Sparsity: 81.4356%\n",
      "total_backward_count 127270 real_backward_count 30004  23.575%\n",
      "epoch-13  lr=['4.0000000'], tr/val_loss: 13.083748/ 50.157089, val:  37.50%, val_best:  53.33%, tr:  95.40%, tr_best:  95.51%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 80.6022%\n",
      "layer   3  Sparsity: 81.7177%\n",
      "total_backward_count 137060 real_backward_count 31822  23.218%\n",
      "epoch-14  lr=['4.0000000'], tr/val_loss: 10.832888/ 37.212334, val:  49.17%, val_best:  53.33%, tr:  94.69%, tr_best:  95.51%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 80.1646%\n",
      "layer   3  Sparsity: 82.6824%\n",
      "total_backward_count 146850 real_backward_count 33536  22.837%\n",
      "fc layer 1 self.abs_max_out: 4799.0\n",
      "epoch-15  lr=['4.0000000'], tr/val_loss: 10.603004/ 64.467300, val:  40.83%, val_best:  53.33%, tr:  94.59%, tr_best:  95.51%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 79.0893%\n",
      "layer   3  Sparsity: 82.1466%\n",
      "total_backward_count 156640 real_backward_count 35192  22.467%\n",
      "fc layer 1 self.abs_max_out: 4986.0\n",
      "fc layer 1 self.abs_max_out: 5039.0\n",
      "epoch-16  lr=['4.0000000'], tr/val_loss: 10.280090/ 40.489491, val:  44.17%, val_best:  53.33%, tr:  95.20%, tr_best:  95.51%, epoch time: 80.38 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 78.7285%\n",
      "layer   3  Sparsity: 83.3525%\n",
      "total_backward_count 166430 real_backward_count 36835  22.132%\n",
      "epoch-17  lr=['4.0000000'], tr/val_loss: 11.424690/ 35.756527, val:  51.67%, val_best:  53.33%, tr:  93.97%, tr_best:  95.51%, epoch time: 80.74 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 78.6285%\n",
      "layer   3  Sparsity: 83.3256%\n",
      "total_backward_count 176220 real_backward_count 38610  21.910%\n",
      "fc layer 1 self.abs_max_out: 5159.0\n",
      "epoch-18  lr=['4.0000000'], tr/val_loss: 10.863879/ 39.011040, val:  45.83%, val_best:  53.33%, tr:  95.81%, tr_best:  95.81%, epoch time: 80.83 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 78.8210%\n",
      "layer   3  Sparsity: 82.1313%\n",
      "total_backward_count 186010 real_backward_count 40384  21.711%\n",
      "epoch-19  lr=['4.0000000'], tr/val_loss: 10.332204/ 47.821434, val:  40.00%, val_best:  53.33%, tr:  95.10%, tr_best:  95.81%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 78.4534%\n",
      "layer   3  Sparsity: 82.1730%\n",
      "total_backward_count 195800 real_backward_count 42013  21.457%\n",
      "fc layer 1 self.abs_max_out: 5337.0\n",
      "lif layer 1 self.abs_max_v: 9147.5\n",
      "epoch-20  lr=['4.0000000'], tr/val_loss:  8.861400/ 39.395046, val:  45.00%, val_best:  53.33%, tr:  95.20%, tr_best:  95.81%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 78.4466%\n",
      "layer   3  Sparsity: 82.5555%\n",
      "total_backward_count 205590 real_backward_count 43532  21.174%\n",
      "fc layer 1 self.abs_max_out: 5783.0\n",
      "lif layer 1 self.abs_max_v: 9612.5\n",
      "lif layer 1 self.abs_max_v: 10271.5\n",
      "lif layer 1 self.abs_max_v: 10479.5\n",
      "lif layer 1 self.abs_max_v: 10806.0\n",
      "epoch-21  lr=['4.0000000'], tr/val_loss:  8.373386/ 25.034573, val:  44.17%, val_best:  53.33%, tr:  94.79%, tr_best:  95.81%, epoch time: 80.56 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 78.5820%\n",
      "layer   3  Sparsity: 83.5567%\n",
      "total_backward_count 215380 real_backward_count 45090  20.935%\n",
      "epoch-22  lr=['4.0000000'], tr/val_loss:  9.651163/ 33.702282, val:  47.50%, val_best:  53.33%, tr:  93.67%, tr_best:  95.81%, epoch time: 80.52 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 78.7507%\n",
      "layer   3  Sparsity: 83.1403%\n",
      "total_backward_count 225170 real_backward_count 46680  20.731%\n",
      "epoch-23  lr=['4.0000000'], tr/val_loss:  7.870355/ 30.289425, val:  51.25%, val_best:  53.33%, tr:  93.56%, tr_best:  95.81%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 78.6817%\n",
      "layer   3  Sparsity: 82.5782%\n",
      "total_backward_count 234960 real_backward_count 48107  20.475%\n",
      "epoch-24  lr=['4.0000000'], tr/val_loss:  8.611643/ 34.752563, val:  40.83%, val_best:  53.33%, tr:  94.28%, tr_best:  95.81%, epoch time: 81.38 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 79.6525%\n",
      "layer   3  Sparsity: 82.4361%\n",
      "total_backward_count 244750 real_backward_count 49611  20.270%\n",
      "epoch-25  lr=['4.0000000'], tr/val_loss:  8.174659/ 25.931849, val:  51.67%, val_best:  53.33%, tr:  93.05%, tr_best:  95.81%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 79.6474%\n",
      "layer   3  Sparsity: 83.3501%\n",
      "total_backward_count 254540 real_backward_count 51162  20.100%\n",
      "epoch-26  lr=['4.0000000'], tr/val_loss:  7.697031/ 31.888927, val:  50.00%, val_best:  53.33%, tr:  94.69%, tr_best:  95.81%, epoch time: 81.26 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 79.2927%\n",
      "layer   3  Sparsity: 82.6255%\n",
      "total_backward_count 264330 real_backward_count 52620  19.907%\n",
      "fc layer 2 self.abs_max_out: 8402.0\n",
      "epoch-27  lr=['4.0000000'], tr/val_loss:  8.183657/ 32.172749, val:  46.25%, val_best:  53.33%, tr:  93.87%, tr_best:  95.81%, epoch time: 81.35 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 79.7873%\n",
      "layer   3  Sparsity: 82.7519%\n",
      "total_backward_count 274120 real_backward_count 54160  19.758%\n",
      "epoch-28  lr=['4.0000000'], tr/val_loss:  7.790107/ 68.568825, val:  30.42%, val_best:  53.33%, tr:  93.77%, tr_best:  95.81%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 79.4480%\n",
      "layer   3  Sparsity: 81.9698%\n",
      "total_backward_count 283910 real_backward_count 55625  19.592%\n",
      "epoch-29  lr=['4.0000000'], tr/val_loss:  7.931576/ 57.628044, val:  37.08%, val_best:  53.33%, tr:  94.28%, tr_best:  95.81%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 79.1162%\n",
      "layer   3  Sparsity: 82.0340%\n",
      "total_backward_count 293700 real_backward_count 57066  19.430%\n",
      "epoch-30  lr=['4.0000000'], tr/val_loss:  7.877879/ 34.215874, val:  47.92%, val_best:  53.33%, tr:  93.77%, tr_best:  95.81%, epoch time: 81.23 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 79.4081%\n",
      "layer   3  Sparsity: 82.3103%\n",
      "total_backward_count 303490 real_backward_count 58587  19.304%\n",
      "epoch-31  lr=['4.0000000'], tr/val_loss:  7.205631/ 32.782623, val:  41.25%, val_best:  53.33%, tr:  94.69%, tr_best:  95.81%, epoch time: 81.08 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 79.6390%\n",
      "layer   3  Sparsity: 83.3138%\n",
      "total_backward_count 313280 real_backward_count 59995  19.151%\n",
      "fc layer 1 self.abs_max_out: 5958.0\n",
      "epoch-32  lr=['4.0000000'], tr/val_loss:  7.368546/ 39.925991, val:  41.25%, val_best:  53.33%, tr:  95.30%, tr_best:  95.81%, epoch time: 81.45 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 79.8394%\n",
      "layer   3  Sparsity: 82.9836%\n",
      "total_backward_count 323070 real_backward_count 61398  19.005%\n",
      "epoch-33  lr=['4.0000000'], tr/val_loss:  7.679740/ 27.268612, val:  39.17%, val_best:  53.33%, tr:  93.77%, tr_best:  95.81%, epoch time: 80.98 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 79.9802%\n",
      "layer   3  Sparsity: 83.2260%\n",
      "total_backward_count 332860 real_backward_count 62881  18.891%\n",
      "lif layer 2 self.abs_max_v: 14762.5\n",
      "lif layer 2 self.abs_max_v: 15693.0\n",
      "lif layer 2 self.abs_max_v: 16056.5\n",
      "lif layer 2 self.abs_max_v: 16384.5\n",
      "fc layer 2 self.abs_max_out: 8475.0\n",
      "lif layer 2 self.abs_max_v: 16667.5\n",
      "fc layer 1 self.abs_max_out: 6134.0\n",
      "epoch-34  lr=['4.0000000'], tr/val_loss:  6.562075/ 45.961533, val:  42.50%, val_best:  53.33%, tr:  94.59%, tr_best:  95.81%, epoch time: 81.82 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 79.6089%\n",
      "layer   3  Sparsity: 83.3838%\n",
      "total_backward_count 342650 real_backward_count 64239  18.748%\n",
      "fc layer 2 self.abs_max_out: 9065.0\n",
      "lif layer 2 self.abs_max_v: 16804.5\n",
      "fc layer 1 self.abs_max_out: 6414.0\n",
      "lif layer 1 self.abs_max_v: 11017.5\n",
      "lif layer 1 self.abs_max_v: 11151.5\n",
      "epoch-35  lr=['4.0000000'], tr/val_loss:  7.674176/ 36.758373, val:  39.58%, val_best:  53.33%, tr:  93.87%, tr_best:  95.81%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 79.3587%\n",
      "layer   3  Sparsity: 84.2669%\n",
      "total_backward_count 352440 real_backward_count 65655  18.629%\n",
      "fc layer 1 self.abs_max_out: 6427.0\n",
      "lif layer 1 self.abs_max_v: 11273.5\n",
      "epoch-36  lr=['4.0000000'], tr/val_loss:  7.049574/ 31.607422, val:  53.33%, val_best:  53.33%, tr:  94.69%, tr_best:  95.81%, epoch time: 81.03 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 79.1903%\n",
      "layer   3  Sparsity: 83.5638%\n",
      "total_backward_count 362230 real_backward_count 67048  18.510%\n",
      "epoch-37  lr=['4.0000000'], tr/val_loss:  6.443144/ 27.878876, val:  48.75%, val_best:  53.33%, tr:  94.08%, tr_best:  95.81%, epoch time: 80.84 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 79.6080%\n",
      "layer   3  Sparsity: 84.0753%\n",
      "total_backward_count 372020 real_backward_count 68316  18.364%\n",
      "fc layer 2 self.abs_max_out: 9475.0\n",
      "epoch-38  lr=['4.0000000'], tr/val_loss:  6.760586/ 25.258110, val:  44.17%, val_best:  53.33%, tr:  93.77%, tr_best:  95.81%, epoch time: 81.72 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 78.9686%\n",
      "layer   3  Sparsity: 84.0638%\n",
      "total_backward_count 381810 real_backward_count 69711  18.258%\n",
      "epoch-39  lr=['4.0000000'], tr/val_loss:  6.481181/ 29.052633, val:  49.17%, val_best:  53.33%, tr:  93.77%, tr_best:  95.81%, epoch time: 80.85 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 78.7041%\n",
      "layer   3  Sparsity: 84.2820%\n",
      "total_backward_count 391600 real_backward_count 71087  18.153%\n",
      "fc layer 2 self.abs_max_out: 9663.0\n",
      "epoch-40  lr=['4.0000000'], tr/val_loss:  6.462010/ 28.136707, val:  44.58%, val_best:  53.33%, tr:  94.38%, tr_best:  95.81%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 78.7649%\n",
      "layer   3  Sparsity: 83.4037%\n",
      "total_backward_count 401390 real_backward_count 72428  18.044%\n",
      "fc layer 1 self.abs_max_out: 6441.0\n",
      "epoch-41  lr=['4.0000000'], tr/val_loss:  6.822979/ 22.908241, val:  50.83%, val_best:  53.33%, tr:  94.18%, tr_best:  95.81%, epoch time: 81.19 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 78.9856%\n",
      "layer   3  Sparsity: 83.5843%\n",
      "total_backward_count 411180 real_backward_count 73792  17.946%\n",
      "epoch-42  lr=['4.0000000'], tr/val_loss:  6.113636/ 36.665337, val:  42.92%, val_best:  53.33%, tr:  95.10%, tr_best:  95.81%, epoch time: 81.01 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 79.1368%\n",
      "layer   3  Sparsity: 83.4793%\n",
      "total_backward_count 420970 real_backward_count 75103  17.840%\n",
      "fc layer 1 self.abs_max_out: 6557.0\n",
      "fc layer 1 self.abs_max_out: 6853.0\n",
      "lif layer 1 self.abs_max_v: 11445.5\n",
      "lif layer 1 self.abs_max_v: 11850.0\n",
      "epoch-43  lr=['4.0000000'], tr/val_loss:  6.187802/ 23.997053, val:  48.33%, val_best:  53.33%, tr:  93.97%, tr_best:  95.81%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 79.9808%\n",
      "layer   3  Sparsity: 84.2432%\n",
      "total_backward_count 430760 real_backward_count 76420  17.741%\n",
      "epoch-44  lr=['4.0000000'], tr/val_loss:  5.811204/ 28.409121, val:  48.33%, val_best:  53.33%, tr:  93.46%, tr_best:  95.81%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 80.3235%\n",
      "layer   3  Sparsity: 84.1338%\n",
      "total_backward_count 440550 real_backward_count 77721  17.642%\n",
      "epoch-45  lr=['4.0000000'], tr/val_loss:  5.437513/ 33.863899, val:  50.00%, val_best:  53.33%, tr:  94.79%, tr_best:  95.81%, epoch time: 80.60 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 80.3051%\n",
      "layer   3  Sparsity: 84.4757%\n",
      "total_backward_count 450340 real_backward_count 79031  17.549%\n",
      "epoch-46  lr=['4.0000000'], tr/val_loss:  6.032167/ 22.620770, val:  54.17%, val_best:  54.17%, tr:  94.69%, tr_best:  95.81%, epoch time: 80.76 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 80.3852%\n",
      "layer   3  Sparsity: 84.7132%\n",
      "total_backward_count 460130 real_backward_count 80367  17.466%\n",
      "fc layer 2 self.abs_max_out: 9709.0\n",
      "fc layer 1 self.abs_max_out: 7059.0\n",
      "lif layer 1 self.abs_max_v: 12217.0\n",
      "epoch-47  lr=['4.0000000'], tr/val_loss:  5.763689/ 33.529854, val:  43.75%, val_best:  54.17%, tr:  95.30%, tr_best:  95.81%, epoch time: 81.21 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 80.3971%\n",
      "layer   3  Sparsity: 84.3277%\n",
      "total_backward_count 469920 real_backward_count 81685  17.383%\n",
      "epoch-48  lr=['4.0000000'], tr/val_loss:  5.604750/ 31.159964, val:  44.17%, val_best:  54.17%, tr:  94.59%, tr_best:  95.81%, epoch time: 81.14 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 79.9740%\n",
      "layer   3  Sparsity: 84.0399%\n",
      "total_backward_count 479710 real_backward_count 82908  17.283%\n",
      "lif layer 2 self.abs_max_v: 17605.0\n",
      "epoch-49  lr=['4.0000000'], tr/val_loss:  5.635939/ 27.605780, val:  49.17%, val_best:  54.17%, tr:  95.10%, tr_best:  95.81%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 79.9425%\n",
      "layer   3  Sparsity: 83.8194%\n",
      "total_backward_count 489500 real_backward_count 84165  17.194%\n",
      "lif layer 2 self.abs_max_v: 17810.0\n",
      "epoch-50  lr=['4.0000000'], tr/val_loss:  5.250179/ 40.013821, val:  37.08%, val_best:  54.17%, tr:  94.28%, tr_best:  95.81%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 80.3725%\n",
      "layer   3  Sparsity: 84.2278%\n",
      "total_backward_count 499290 real_backward_count 85404  17.105%\n",
      "epoch-51  lr=['4.0000000'], tr/val_loss:  5.171731/ 28.948662, val:  37.92%, val_best:  54.17%, tr:  94.69%, tr_best:  95.81%, epoch time: 81.52 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 79.7034%\n",
      "layer   3  Sparsity: 84.2178%\n",
      "total_backward_count 509080 real_backward_count 86642  17.019%\n",
      "fc layer 2 self.abs_max_out: 9748.0\n",
      "epoch-52  lr=['4.0000000'], tr/val_loss:  5.058093/ 21.929628, val:  52.50%, val_best:  54.17%, tr:  95.40%, tr_best:  95.81%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 79.1301%\n",
      "layer   3  Sparsity: 84.5084%\n",
      "total_backward_count 518870 real_backward_count 87884  16.938%\n",
      "epoch-53  lr=['4.0000000'], tr/val_loss:  5.181538/ 27.784275, val:  44.17%, val_best:  54.17%, tr:  95.10%, tr_best:  95.81%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 79.1468%\n",
      "layer   3  Sparsity: 84.5501%\n",
      "total_backward_count 528660 real_backward_count 89130  16.860%\n",
      "fc layer 2 self.abs_max_out: 9836.0\n",
      "epoch-54  lr=['4.0000000'], tr/val_loss:  5.880115/ 30.248373, val:  45.00%, val_best:  54.17%, tr:  95.10%, tr_best:  95.81%, epoch time: 80.91 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 79.0118%\n",
      "layer   3  Sparsity: 83.6348%\n",
      "total_backward_count 538450 real_backward_count 90464  16.801%\n",
      "epoch-55  lr=['4.0000000'], tr/val_loss:  5.719990/ 28.410498, val:  47.08%, val_best:  54.17%, tr:  94.18%, tr_best:  95.81%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 79.2943%\n",
      "layer   3  Sparsity: 84.0406%\n",
      "total_backward_count 548240 real_backward_count 91778  16.740%\n",
      "fc layer 2 self.abs_max_out: 10094.0\n",
      "lif layer 2 self.abs_max_v: 18163.0\n",
      "lif layer 2 self.abs_max_v: 18254.0\n",
      "epoch-56  lr=['4.0000000'], tr/val_loss:  5.298123/ 29.724407, val:  37.08%, val_best:  54.17%, tr:  94.89%, tr_best:  95.81%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 79.6007%\n",
      "layer   3  Sparsity: 83.9463%\n",
      "total_backward_count 558030 real_backward_count 93039  16.673%\n",
      "epoch-57  lr=['4.0000000'], tr/val_loss:  5.168156/ 30.509020, val:  41.25%, val_best:  54.17%, tr:  93.87%, tr_best:  95.81%, epoch time: 81.33 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 79.3663%\n",
      "layer   3  Sparsity: 84.4539%\n",
      "total_backward_count 567820 real_backward_count 94313  16.610%\n",
      "epoch-58  lr=['4.0000000'], tr/val_loss:  4.969821/ 24.254162, val:  47.50%, val_best:  54.17%, tr:  95.20%, tr_best:  95.81%, epoch time: 80.85 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 79.4595%\n",
      "layer   3  Sparsity: 84.5205%\n",
      "total_backward_count 577610 real_backward_count 95511  16.536%\n",
      "epoch-59  lr=['4.0000000'], tr/val_loss:  4.850473/ 32.545567, val:  42.08%, val_best:  54.17%, tr:  95.51%, tr_best:  95.81%, epoch time: 81.39 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 79.5716%\n",
      "layer   3  Sparsity: 83.6902%\n",
      "total_backward_count 587400 real_backward_count 96676  16.458%\n",
      "fc layer 2 self.abs_max_out: 10297.0\n",
      "lif layer 2 self.abs_max_v: 18709.5\n",
      "lif layer 2 self.abs_max_v: 18851.0\n",
      "lif layer 2 self.abs_max_v: 18932.5\n",
      "epoch-60  lr=['4.0000000'], tr/val_loss:  5.228832/ 23.533590, val:  47.08%, val_best:  54.17%, tr:  94.79%, tr_best:  95.81%, epoch time: 81.25 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 79.6293%\n",
      "layer   3  Sparsity: 84.0029%\n",
      "total_backward_count 597190 real_backward_count 97896  16.393%\n",
      "epoch-61  lr=['4.0000000'], tr/val_loss:  4.956030/ 17.562239, val:  49.17%, val_best:  54.17%, tr:  95.20%, tr_best:  95.81%, epoch time: 81.30 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 79.8661%\n",
      "layer   3  Sparsity: 84.2625%\n",
      "total_backward_count 606980 real_backward_count 99106  16.328%\n",
      "epoch-62  lr=['4.0000000'], tr/val_loss:  5.179321/ 30.360331, val:  38.75%, val_best:  54.17%, tr:  94.69%, tr_best:  95.81%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 79.6570%\n",
      "layer   3  Sparsity: 84.4933%\n",
      "total_backward_count 616770 real_backward_count 100384  16.276%\n",
      "lif layer 2 self.abs_max_v: 19041.0\n",
      "lif layer 2 self.abs_max_v: 19352.5\n",
      "fc layer 1 self.abs_max_out: 7137.0\n",
      "lif layer 1 self.abs_max_v: 12310.5\n",
      "epoch-63  lr=['4.0000000'], tr/val_loss:  5.121047/ 28.726711, val:  35.42%, val_best:  54.17%, tr:  94.48%, tr_best:  95.81%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 79.4609%\n",
      "layer   3  Sparsity: 84.2757%\n",
      "total_backward_count 626560 real_backward_count 101603  16.216%\n",
      "epoch-64  lr=['4.0000000'], tr/val_loss:  5.129469/ 21.703211, val:  47.92%, val_best:  54.17%, tr:  94.38%, tr_best:  95.81%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 79.0410%\n",
      "layer   3  Sparsity: 84.1166%\n",
      "total_backward_count 636350 real_backward_count 102804  16.155%\n",
      "epoch-65  lr=['4.0000000'], tr/val_loss:  4.883576/ 25.777853, val:  42.08%, val_best:  54.17%, tr:  94.69%, tr_best:  95.81%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 79.0633%\n",
      "layer   3  Sparsity: 84.6654%\n",
      "total_backward_count 646140 real_backward_count 103973  16.091%\n",
      "epoch-66  lr=['4.0000000'], tr/val_loss:  5.530752/ 31.868528, val:  46.67%, val_best:  54.17%, tr:  94.38%, tr_best:  95.81%, epoch time: 80.80 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 79.1209%\n",
      "layer   3  Sparsity: 84.4984%\n",
      "total_backward_count 655930 real_backward_count 105237  16.044%\n",
      "fc layer 2 self.abs_max_out: 10365.0\n",
      "fc layer 1 self.abs_max_out: 7201.0\n",
      "lif layer 1 self.abs_max_v: 12506.5\n",
      "epoch-67  lr=['4.0000000'], tr/val_loss:  5.787640/ 27.133650, val:  42.50%, val_best:  54.17%, tr:  94.18%, tr_best:  95.81%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 78.8319%\n",
      "layer   3  Sparsity: 84.5780%\n",
      "total_backward_count 665720 real_backward_count 106490  15.996%\n",
      "epoch-68  lr=['4.0000000'], tr/val_loss:  5.401889/ 22.508131, val:  56.25%, val_best:  56.25%, tr:  95.20%, tr_best:  95.81%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 78.8484%\n",
      "layer   3  Sparsity: 84.3907%\n",
      "total_backward_count 675510 real_backward_count 107699  15.943%\n",
      "epoch-69  lr=['4.0000000'], tr/val_loss:  5.079649/ 34.987465, val:  35.83%, val_best:  56.25%, tr:  94.69%, tr_best:  95.81%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 79.0282%\n",
      "layer   3  Sparsity: 84.4282%\n",
      "total_backward_count 685300 real_backward_count 108891  15.890%\n",
      "lif layer 2 self.abs_max_v: 19377.5\n",
      "lif layer 2 self.abs_max_v: 19436.0\n",
      "fc layer 1 self.abs_max_out: 7354.0\n",
      "lif layer 1 self.abs_max_v: 12895.5\n",
      "epoch-70  lr=['4.0000000'], tr/val_loss:  5.239088/ 25.882658, val:  41.25%, val_best:  56.25%, tr:  94.48%, tr_best:  95.81%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 78.9759%\n",
      "layer   3  Sparsity: 84.4347%\n",
      "total_backward_count 695090 real_backward_count 110112  15.841%\n",
      "lif layer 2 self.abs_max_v: 19624.5\n",
      "epoch-71  lr=['4.0000000'], tr/val_loss:  4.794807/ 29.973097, val:  41.67%, val_best:  56.25%, tr:  94.38%, tr_best:  95.81%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 79.0208%\n",
      "layer   3  Sparsity: 85.0876%\n",
      "total_backward_count 704880 real_backward_count 111296  15.789%\n",
      "fc layer 2 self.abs_max_out: 10965.0\n",
      "lif layer 2 self.abs_max_v: 19867.0\n",
      "lif layer 2 self.abs_max_v: 20394.5\n",
      "epoch-72  lr=['4.0000000'], tr/val_loss:  4.494694/ 24.215801, val:  47.92%, val_best:  56.25%, tr:  94.79%, tr_best:  95.81%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 78.8181%\n",
      "layer   3  Sparsity: 84.8459%\n",
      "total_backward_count 714670 real_backward_count 112440  15.733%\n",
      "epoch-73  lr=['4.0000000'], tr/val_loss:  5.203349/ 34.126320, val:  39.17%, val_best:  56.25%, tr:  95.10%, tr_best:  95.81%, epoch time: 80.74 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 78.8646%\n",
      "layer   3  Sparsity: 84.5345%\n",
      "total_backward_count 724460 real_backward_count 113646  15.687%\n",
      "epoch-74  lr=['4.0000000'], tr/val_loss:  5.221979/ 18.456539, val:  54.17%, val_best:  56.25%, tr:  94.89%, tr_best:  95.81%, epoch time: 81.65 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 78.6399%\n",
      "layer   3  Sparsity: 84.6443%\n",
      "total_backward_count 734250 real_backward_count 114870  15.645%\n",
      "epoch-75  lr=['4.0000000'], tr/val_loss:  4.696537/ 16.012968, val:  61.67%, val_best:  61.67%, tr:  93.97%, tr_best:  95.81%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 78.6479%\n",
      "layer   3  Sparsity: 84.8091%\n",
      "total_backward_count 744040 real_backward_count 116012  15.592%\n",
      "epoch-76  lr=['4.0000000'], tr/val_loss:  5.081186/ 27.396404, val:  50.42%, val_best:  61.67%, tr:  94.79%, tr_best:  95.81%, epoch time: 80.74 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 78.7726%\n",
      "layer   3  Sparsity: 84.4829%\n",
      "total_backward_count 753830 real_backward_count 117264  15.556%\n",
      "epoch-77  lr=['4.0000000'], tr/val_loss:  5.008241/ 31.615614, val:  40.42%, val_best:  61.67%, tr:  95.30%, tr_best:  95.81%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 78.6896%\n",
      "layer   3  Sparsity: 83.6414%\n",
      "total_backward_count 763620 real_backward_count 118448  15.511%\n",
      "fc layer 1 self.abs_max_out: 7680.0\n",
      "lif layer 1 self.abs_max_v: 13486.5\n",
      "epoch-78  lr=['4.0000000'], tr/val_loss:  4.921453/ 30.787752, val:  42.92%, val_best:  61.67%, tr:  93.77%, tr_best:  95.81%, epoch time: 81.02 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 79.1186%\n",
      "layer   3  Sparsity: 83.9106%\n",
      "total_backward_count 773410 real_backward_count 119642  15.469%\n",
      "epoch-79  lr=['4.0000000'], tr/val_loss:  4.616439/ 23.876747, val:  56.67%, val_best:  61.67%, tr:  94.59%, tr_best:  95.81%, epoch time: 81.34 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 79.2627%\n",
      "layer   3  Sparsity: 84.7700%\n",
      "total_backward_count 783200 real_backward_count 120780  15.421%\n",
      "epoch-80  lr=['4.0000000'], tr/val_loss:  4.417789/ 22.992573, val:  50.42%, val_best:  61.67%, tr:  94.59%, tr_best:  95.81%, epoch time: 81.01 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 79.1685%\n",
      "layer   3  Sparsity: 85.3476%\n",
      "total_backward_count 792990 real_backward_count 121906  15.373%\n",
      "epoch-81  lr=['4.0000000'], tr/val_loss:  4.594698/ 36.174973, val:  39.58%, val_best:  61.67%, tr:  94.79%, tr_best:  95.81%, epoch time: 81.27 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 79.0443%\n",
      "layer   3  Sparsity: 84.5201%\n",
      "total_backward_count 802780 real_backward_count 123050  15.328%\n",
      "epoch-82  lr=['4.0000000'], tr/val_loss:  5.076762/ 26.106323, val:  48.75%, val_best:  61.67%, tr:  93.77%, tr_best:  95.81%, epoch time: 81.20 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 79.2407%\n",
      "layer   3  Sparsity: 84.8352%\n",
      "total_backward_count 812570 real_backward_count 124300  15.297%\n",
      "epoch-83  lr=['4.0000000'], tr/val_loss:  5.059663/ 27.044891, val:  48.33%, val_best:  61.67%, tr:  94.69%, tr_best:  95.81%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 79.2082%\n",
      "layer   3  Sparsity: 84.1959%\n",
      "total_backward_count 822360 real_backward_count 125487  15.259%\n",
      "epoch-84  lr=['4.0000000'], tr/val_loss:  4.921853/ 24.810688, val:  42.92%, val_best:  61.67%, tr:  94.79%, tr_best:  95.81%, epoch time: 80.38 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 79.1689%\n",
      "layer   3  Sparsity: 85.1281%\n",
      "total_backward_count 832150 real_backward_count 126715  15.227%\n",
      "epoch-85  lr=['4.0000000'], tr/val_loss:  4.738851/ 29.452049, val:  45.00%, val_best:  61.67%, tr:  95.20%, tr_best:  95.81%, epoch time: 81.29 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 79.3162%\n",
      "layer   3  Sparsity: 84.3319%\n",
      "total_backward_count 841940 real_backward_count 127883  15.189%\n",
      "epoch-86  lr=['4.0000000'], tr/val_loss:  4.541646/ 24.273144, val:  52.50%, val_best:  61.67%, tr:  95.40%, tr_best:  95.81%, epoch time: 80.72 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 78.9011%\n",
      "layer   3  Sparsity: 84.5871%\n",
      "total_backward_count 851730 real_backward_count 129006  15.146%\n",
      "epoch-87  lr=['4.0000000'], tr/val_loss:  4.964071/ 25.001680, val:  47.08%, val_best:  61.67%, tr:  95.20%, tr_best:  95.81%, epoch time: 81.28 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 79.5736%\n",
      "layer   3  Sparsity: 84.7484%\n",
      "total_backward_count 861520 real_backward_count 130222  15.115%\n",
      "epoch-88  lr=['4.0000000'], tr/val_loss:  4.932783/ 36.847721, val:  44.58%, val_best:  61.67%, tr:  95.81%, tr_best:  95.81%, epoch time: 81.38 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 79.3389%\n",
      "layer   3  Sparsity: 84.4221%\n",
      "total_backward_count 871310 real_backward_count 131419  15.083%\n",
      "epoch-89  lr=['4.0000000'], tr/val_loss:  4.928418/ 18.951929, val:  56.67%, val_best:  61.67%, tr:  93.97%, tr_best:  95.81%, epoch time: 80.89 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 79.3341%\n",
      "layer   3  Sparsity: 84.3194%\n",
      "total_backward_count 881100 real_backward_count 132643  15.054%\n",
      "epoch-90  lr=['4.0000000'], tr/val_loss:  5.049759/ 32.188988, val:  50.00%, val_best:  61.67%, tr:  94.79%, tr_best:  95.81%, epoch time: 81.62 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 79.4778%\n",
      "layer   3  Sparsity: 84.4626%\n",
      "total_backward_count 890890 real_backward_count 133874  15.027%\n",
      "epoch-91  lr=['4.0000000'], tr/val_loss:  4.509612/ 18.012836, val:  53.33%, val_best:  61.67%, tr:  95.10%, tr_best:  95.81%, epoch time: 81.48 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 79.7085%\n",
      "layer   3  Sparsity: 84.9524%\n",
      "total_backward_count 900680 real_backward_count 134989  14.987%\n",
      "lif layer 2 self.abs_max_v: 20934.5\n",
      "lif layer 2 self.abs_max_v: 21051.0\n",
      "epoch-92  lr=['4.0000000'], tr/val_loss:  4.447797/ 34.416348, val:  42.50%, val_best:  61.67%, tr:  95.20%, tr_best:  95.81%, epoch time: 81.08 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 79.3740%\n",
      "layer   3  Sparsity: 84.9357%\n",
      "total_backward_count 910470 real_backward_count 136099  14.948%\n",
      "epoch-93  lr=['4.0000000'], tr/val_loss:  4.456998/ 20.368181, val:  54.17%, val_best:  61.67%, tr:  94.59%, tr_best:  95.81%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 79.4525%\n",
      "layer   3  Sparsity: 84.7993%\n",
      "total_backward_count 920260 real_backward_count 137228  14.912%\n",
      "epoch-94  lr=['4.0000000'], tr/val_loss:  4.533548/ 33.972557, val:  42.92%, val_best:  61.67%, tr:  94.69%, tr_best:  95.81%, epoch time: 80.72 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 79.6212%\n",
      "layer   3  Sparsity: 85.1734%\n",
      "total_backward_count 930050 real_backward_count 138386  14.879%\n",
      "fc layer 1 self.abs_max_out: 7809.0\n",
      "lif layer 1 self.abs_max_v: 13703.5\n",
      "epoch-95  lr=['4.0000000'], tr/val_loss:  4.452473/ 27.235315, val:  54.17%, val_best:  61.67%, tr:  94.99%, tr_best:  95.81%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 79.4591%\n",
      "layer   3  Sparsity: 85.1604%\n",
      "total_backward_count 939840 real_backward_count 139526  14.846%\n",
      "fc layer 1 self.abs_max_out: 7873.0\n",
      "lif layer 1 self.abs_max_v: 13907.0\n",
      "epoch-96  lr=['4.0000000'], tr/val_loss:  4.356390/ 28.344948, val:  49.58%, val_best:  61.67%, tr:  96.12%, tr_best:  96.12%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 79.5669%\n",
      "layer   3  Sparsity: 85.3777%\n",
      "total_backward_count 949630 real_backward_count 140671  14.813%\n",
      "epoch-97  lr=['4.0000000'], tr/val_loss:  4.236238/ 23.257048, val:  46.25%, val_best:  61.67%, tr:  95.81%, tr_best:  96.12%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 79.7076%\n",
      "layer   3  Sparsity: 85.3824%\n",
      "total_backward_count 959420 real_backward_count 141770  14.777%\n",
      "fc layer 2 self.abs_max_out: 11506.0\n",
      "fc layer 1 self.abs_max_out: 7909.0\n",
      "epoch-98  lr=['4.0000000'], tr/val_loss:  3.990371/ 17.072199, val:  50.00%, val_best:  61.67%, tr:  95.51%, tr_best:  96.12%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 79.6504%\n",
      "layer   3  Sparsity: 85.6747%\n",
      "total_backward_count 969210 real_backward_count 142823  14.736%\n",
      "epoch-99  lr=['4.0000000'], tr/val_loss:  4.177764/ 20.863419, val:  60.42%, val_best:  61.67%, tr:  96.02%, tr_best:  96.12%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 79.4784%\n",
      "layer   3  Sparsity: 85.4195%\n",
      "total_backward_count 979000 real_backward_count 143895  14.698%\n",
      "lif layer 2 self.abs_max_v: 21184.5\n",
      "lif layer 2 self.abs_max_v: 21280.5\n",
      "lif layer 2 self.abs_max_v: 21509.5\n",
      "epoch-100 lr=['4.0000000'], tr/val_loss:  4.258317/ 27.184973, val:  43.75%, val_best:  61.67%, tr:  95.81%, tr_best:  96.12%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 79.2124%\n",
      "layer   3  Sparsity: 84.7382%\n",
      "total_backward_count 988790 real_backward_count 144930  14.657%\n",
      "epoch-101 lr=['4.0000000'], tr/val_loss:  4.498002/ 23.003462, val:  51.67%, val_best:  61.67%, tr:  95.51%, tr_best:  96.12%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 79.6434%\n",
      "layer   3  Sparsity: 85.2773%\n",
      "total_backward_count 998580 real_backward_count 146063  14.627%\n",
      "epoch-102 lr=['4.0000000'], tr/val_loss:  4.198868/ 25.548246, val:  43.75%, val_best:  61.67%, tr:  94.69%, tr_best:  96.12%, epoch time: 80.52 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 79.7782%\n",
      "layer   3  Sparsity: 85.2499%\n",
      "total_backward_count 1008370 real_backward_count 147173  14.595%\n",
      "epoch-103 lr=['4.0000000'], tr/val_loss:  4.497176/ 33.476898, val:  42.08%, val_best:  61.67%, tr:  94.48%, tr_best:  96.12%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 79.8219%\n",
      "layer   3  Sparsity: 84.5396%\n",
      "total_backward_count 1018160 real_backward_count 148291  14.565%\n",
      "epoch-104 lr=['4.0000000'], tr/val_loss:  4.187275/ 19.977608, val:  65.83%, val_best:  65.83%, tr:  96.12%, tr_best:  96.12%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 79.5041%\n",
      "layer   3  Sparsity: 85.4654%\n",
      "total_backward_count 1027950 real_backward_count 149377  14.532%\n",
      "lif layer 2 self.abs_max_v: 21539.5\n",
      "lif layer 2 self.abs_max_v: 21793.0\n",
      "lif layer 2 self.abs_max_v: 21842.0\n",
      "epoch-105 lr=['4.0000000'], tr/val_loss:  4.446089/ 20.711134, val:  52.08%, val_best:  65.83%, tr:  96.32%, tr_best:  96.32%, epoch time: 80.44 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 79.0617%\n",
      "layer   3  Sparsity: 85.2068%\n",
      "total_backward_count 1037740 real_backward_count 150505  14.503%\n",
      "epoch-106 lr=['4.0000000'], tr/val_loss:  4.224962/ 40.363297, val:  26.67%, val_best:  65.83%, tr:  94.48%, tr_best:  96.32%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 79.1933%\n",
      "layer   3  Sparsity: 85.1576%\n",
      "total_backward_count 1047530 real_backward_count 151609  14.473%\n",
      "epoch-107 lr=['4.0000000'], tr/val_loss:  4.200038/ 22.552029, val:  54.58%, val_best:  65.83%, tr:  94.99%, tr_best:  96.32%, epoch time: 80.44 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 79.0774%\n",
      "layer   3  Sparsity: 84.9416%\n",
      "total_backward_count 1057320 real_backward_count 152725  14.445%\n",
      "epoch-108 lr=['4.0000000'], tr/val_loss:  4.166198/ 19.645651, val:  57.92%, val_best:  65.83%, tr:  95.91%, tr_best:  96.32%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 79.4534%\n",
      "layer   3  Sparsity: 85.4546%\n",
      "total_backward_count 1067110 real_backward_count 153833  14.416%\n",
      "epoch-109 lr=['4.0000000'], tr/val_loss:  4.230844/ 24.582729, val:  50.42%, val_best:  65.83%, tr:  94.79%, tr_best:  96.32%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 79.2481%\n",
      "layer   3  Sparsity: 85.1610%\n",
      "total_backward_count 1076900 real_backward_count 154937  14.387%\n",
      "epoch-110 lr=['4.0000000'], tr/val_loss:  3.984095/ 23.499653, val:  51.25%, val_best:  65.83%, tr:  95.81%, tr_best:  96.32%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 79.0834%\n",
      "layer   3  Sparsity: 85.4442%\n",
      "total_backward_count 1086690 real_backward_count 155983  14.354%\n",
      "fc layer 2 self.abs_max_out: 11542.0\n",
      "epoch-111 lr=['4.0000000'], tr/val_loss:  4.075140/ 28.751415, val:  44.58%, val_best:  65.83%, tr:  95.30%, tr_best:  96.32%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 79.8237%\n",
      "layer   3  Sparsity: 84.9108%\n",
      "total_backward_count 1096480 real_backward_count 157073  14.325%\n",
      "lif layer 2 self.abs_max_v: 21937.5\n",
      "lif layer 2 self.abs_max_v: 22106.5\n",
      "epoch-112 lr=['4.0000000'], tr/val_loss:  4.162384/ 19.170954, val:  62.50%, val_best:  65.83%, tr:  96.63%, tr_best:  96.63%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 79.5662%\n",
      "layer   3  Sparsity: 85.3015%\n",
      "total_backward_count 1106270 real_backward_count 158172  14.298%\n",
      "fc layer 2 self.abs_max_out: 11631.0\n",
      "epoch-113 lr=['4.0000000'], tr/val_loss:  4.343550/ 28.913719, val:  45.42%, val_best:  65.83%, tr:  96.42%, tr_best:  96.63%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 79.1219%\n",
      "layer   3  Sparsity: 85.3412%\n",
      "total_backward_count 1116060 real_backward_count 159299  14.273%\n",
      "epoch-114 lr=['4.0000000'], tr/val_loss:  3.975684/ 21.616297, val:  41.25%, val_best:  65.83%, tr:  94.28%, tr_best:  96.63%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 79.5953%\n",
      "layer   3  Sparsity: 85.7064%\n",
      "total_backward_count 1125850 real_backward_count 160417  14.249%\n",
      "epoch-115 lr=['4.0000000'], tr/val_loss:  4.078610/ 19.230230, val:  47.50%, val_best:  65.83%, tr:  95.71%, tr_best:  96.63%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 79.3306%\n",
      "layer   3  Sparsity: 85.5974%\n",
      "total_backward_count 1135640 real_backward_count 161519  14.223%\n",
      "epoch-116 lr=['4.0000000'], tr/val_loss:  4.240447/ 16.667271, val:  62.92%, val_best:  65.83%, tr:  96.02%, tr_best:  96.63%, epoch time: 81.16 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 79.1998%\n",
      "layer   3  Sparsity: 84.9281%\n",
      "total_backward_count 1145430 real_backward_count 162581  14.194%\n",
      "epoch-117 lr=['4.0000000'], tr/val_loss:  4.297905/ 27.713768, val:  46.25%, val_best:  65.83%, tr:  94.59%, tr_best:  96.63%, epoch time: 80.90 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 79.6271%\n",
      "layer   3  Sparsity: 84.9634%\n",
      "total_backward_count 1155220 real_backward_count 163699  14.170%\n",
      "epoch-118 lr=['4.0000000'], tr/val_loss:  4.439150/ 19.929628, val:  41.67%, val_best:  65.83%, tr:  95.91%, tr_best:  96.63%, epoch time: 80.93 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 79.8526%\n",
      "layer   3  Sparsity: 85.0626%\n",
      "total_backward_count 1165010 real_backward_count 164843  14.149%\n",
      "fc layer 1 self.abs_max_out: 7969.0\n",
      "lif layer 1 self.abs_max_v: 14049.5\n",
      "epoch-119 lr=['4.0000000'], tr/val_loss:  4.424784/ 21.197231, val:  45.83%, val_best:  65.83%, tr:  94.69%, tr_best:  96.63%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 79.5580%\n",
      "layer   3  Sparsity: 84.7715%\n",
      "total_backward_count 1174800 real_backward_count 165971  14.128%\n",
      "fc layer 1 self.abs_max_out: 8160.0\n",
      "lif layer 1 self.abs_max_v: 14446.0\n",
      "epoch-120 lr=['4.0000000'], tr/val_loss:  3.952614/ 20.998198, val:  45.42%, val_best:  65.83%, tr:  95.20%, tr_best:  96.63%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 79.6171%\n",
      "layer   3  Sparsity: 85.1458%\n",
      "total_backward_count 1184590 real_backward_count 167018  14.099%\n",
      "epoch-121 lr=['4.0000000'], tr/val_loss:  4.126925/ 23.566784, val:  42.50%, val_best:  65.83%, tr:  95.30%, tr_best:  96.63%, epoch time: 80.64 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 79.3899%\n",
      "layer   3  Sparsity: 85.4949%\n",
      "total_backward_count 1194380 real_backward_count 168108  14.075%\n",
      "epoch-122 lr=['4.0000000'], tr/val_loss:  4.175898/ 18.880053, val:  56.25%, val_best:  65.83%, tr:  95.71%, tr_best:  96.63%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 79.4815%\n",
      "layer   3  Sparsity: 85.5685%\n",
      "total_backward_count 1204170 real_backward_count 169220  14.053%\n",
      "fc layer 1 self.abs_max_out: 8177.0\n",
      "lif layer 1 self.abs_max_v: 14489.5\n",
      "epoch-123 lr=['4.0000000'], tr/val_loss:  4.624496/ 27.272614, val:  52.08%, val_best:  65.83%, tr:  95.81%, tr_best:  96.63%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 79.2268%\n",
      "layer   3  Sparsity: 84.2326%\n",
      "total_backward_count 1213960 real_backward_count 170370  14.034%\n",
      "epoch-124 lr=['4.0000000'], tr/val_loss:  4.483791/ 15.082109, val:  64.17%, val_best:  65.83%, tr:  95.51%, tr_best:  96.63%, epoch time: 80.42 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 79.6410%\n",
      "layer   3  Sparsity: 85.1863%\n",
      "total_backward_count 1223750 real_backward_count 171531  14.017%\n",
      "fc layer 1 self.abs_max_out: 8493.0\n",
      "lif layer 1 self.abs_max_v: 14898.5\n",
      "epoch-125 lr=['4.0000000'], tr/val_loss:  4.298060/ 22.217871, val:  56.25%, val_best:  65.83%, tr:  94.28%, tr_best:  96.63%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 79.4861%\n",
      "layer   3  Sparsity: 85.1437%\n",
      "total_backward_count 1233540 real_backward_count 172655  13.997%\n",
      "epoch-126 lr=['4.0000000'], tr/val_loss:  4.051833/ 30.450356, val:  50.42%, val_best:  65.83%, tr:  95.71%, tr_best:  96.63%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 79.5134%\n",
      "layer   3  Sparsity: 85.0466%\n",
      "total_backward_count 1243330 real_backward_count 173713  13.972%\n",
      "epoch-127 lr=['4.0000000'], tr/val_loss:  3.601281/ 16.274542, val:  55.83%, val_best:  65.83%, tr:  96.12%, tr_best:  96.63%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 79.5985%\n",
      "layer   3  Sparsity: 85.4355%\n",
      "total_backward_count 1253120 real_backward_count 174701  13.941%\n",
      "epoch-128 lr=['4.0000000'], tr/val_loss:  4.225516/ 28.756214, val:  41.25%, val_best:  65.83%, tr:  94.48%, tr_best:  96.63%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 79.8612%\n",
      "layer   3  Sparsity: 85.1897%\n",
      "total_backward_count 1262910 real_backward_count 175808  13.921%\n",
      "epoch-129 lr=['4.0000000'], tr/val_loss:  4.079247/ 19.106619, val:  58.75%, val_best:  65.83%, tr:  94.79%, tr_best:  96.63%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 79.6443%\n",
      "layer   3  Sparsity: 85.2163%\n",
      "total_backward_count 1272700 real_backward_count 176884  13.898%\n",
      "fc layer 2 self.abs_max_out: 11654.0\n",
      "epoch-130 lr=['4.0000000'], tr/val_loss:  4.140837/ 36.986370, val:  37.08%, val_best:  65.83%, tr:  96.22%, tr_best:  96.63%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 79.4402%\n",
      "layer   3  Sparsity: 84.8618%\n",
      "total_backward_count 1282490 real_backward_count 177950  13.875%\n",
      "fc layer 1 self.abs_max_out: 8507.0\n",
      "lif layer 1 self.abs_max_v: 14990.0\n",
      "epoch-131 lr=['4.0000000'], tr/val_loss:  4.167897/ 15.973713, val:  52.92%, val_best:  65.83%, tr:  95.81%, tr_best:  96.63%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 79.4758%\n",
      "layer   3  Sparsity: 84.8671%\n",
      "total_backward_count 1292280 real_backward_count 179048  13.855%\n",
      "epoch-132 lr=['4.0000000'], tr/val_loss:  4.008943/ 28.231668, val:  48.75%, val_best:  65.83%, tr:  95.61%, tr_best:  96.63%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 79.4589%\n",
      "layer   3  Sparsity: 85.2779%\n",
      "total_backward_count 1302070 real_backward_count 180120  13.833%\n",
      "epoch-133 lr=['4.0000000'], tr/val_loss:  4.128241/ 19.706234, val:  56.25%, val_best:  65.83%, tr:  94.38%, tr_best:  96.63%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 79.4317%\n",
      "layer   3  Sparsity: 84.7854%\n",
      "total_backward_count 1311860 real_backward_count 181184  13.811%\n",
      "epoch-134 lr=['4.0000000'], tr/val_loss:  4.092939/ 33.453629, val:  44.17%, val_best:  65.83%, tr:  96.12%, tr_best:  96.63%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 79.8225%\n",
      "layer   3  Sparsity: 84.5934%\n",
      "total_backward_count 1321650 real_backward_count 182214  13.787%\n",
      "epoch-135 lr=['4.0000000'], tr/val_loss:  4.429382/ 22.444170, val:  52.08%, val_best:  65.83%, tr:  95.61%, tr_best:  96.63%, epoch time: 81.18 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 79.8002%\n",
      "layer   3  Sparsity: 85.2456%\n",
      "total_backward_count 1331440 real_backward_count 183305  13.767%\n",
      "epoch-136 lr=['4.0000000'], tr/val_loss:  4.075253/ 20.913715, val:  58.33%, val_best:  65.83%, tr:  95.30%, tr_best:  96.63%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 79.5089%\n",
      "layer   3  Sparsity: 84.7692%\n",
      "total_backward_count 1341230 real_backward_count 184324  13.743%\n",
      "fc layer 2 self.abs_max_out: 11662.0\n",
      "lif layer 2 self.abs_max_v: 22532.5\n",
      "lif layer 2 self.abs_max_v: 22804.5\n",
      "lif layer 2 self.abs_max_v: 22894.5\n",
      "epoch-137 lr=['4.0000000'], tr/val_loss:  3.959166/ 21.749418, val:  50.42%, val_best:  65.83%, tr:  95.20%, tr_best:  96.63%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 79.5728%\n",
      "layer   3  Sparsity: 84.9094%\n",
      "total_backward_count 1351020 real_backward_count 185333  13.718%\n",
      "epoch-138 lr=['4.0000000'], tr/val_loss:  4.166220/ 24.304321, val:  52.08%, val_best:  65.83%, tr:  95.20%, tr_best:  96.63%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 79.8379%\n",
      "layer   3  Sparsity: 84.9601%\n",
      "total_backward_count 1360810 real_backward_count 186402  13.698%\n",
      "epoch-139 lr=['4.0000000'], tr/val_loss:  4.059199/ 31.876225, val:  44.58%, val_best:  65.83%, tr:  96.73%, tr_best:  96.73%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 80.0094%\n",
      "layer   3  Sparsity: 84.7171%\n",
      "total_backward_count 1370600 real_backward_count 187462  13.677%\n",
      "epoch-140 lr=['4.0000000'], tr/val_loss:  4.056261/ 25.226816, val:  45.00%, val_best:  65.83%, tr:  95.71%, tr_best:  96.73%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 80.0264%\n",
      "layer   3  Sparsity: 85.4578%\n",
      "total_backward_count 1380390 real_backward_count 188510  13.656%\n",
      "epoch-141 lr=['4.0000000'], tr/val_loss:  3.527682/ 22.809904, val:  45.00%, val_best:  65.83%, tr:  95.40%, tr_best:  96.73%, epoch time: 81.00 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 79.8726%\n",
      "layer   3  Sparsity: 85.6015%\n",
      "total_backward_count 1390180 real_backward_count 189516  13.632%\n",
      "lif layer 1 self.abs_max_v: 15042.5\n",
      "epoch-142 lr=['4.0000000'], tr/val_loss:  4.102609/ 26.571770, val:  55.42%, val_best:  65.83%, tr:  96.12%, tr_best:  96.73%, epoch time: 80.58 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 79.8201%\n",
      "layer   3  Sparsity: 85.2574%\n",
      "total_backward_count 1399970 real_backward_count 190610  13.615%\n",
      "epoch-143 lr=['4.0000000'], tr/val_loss:  3.986112/ 22.199699, val:  46.25%, val_best:  65.83%, tr:  96.42%, tr_best:  96.73%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 79.8101%\n",
      "layer   3  Sparsity: 85.4466%\n",
      "total_backward_count 1409760 real_backward_count 191676  13.596%\n",
      "epoch-144 lr=['4.0000000'], tr/val_loss:  3.969033/ 24.856157, val:  49.58%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 79.3883%\n",
      "layer   3  Sparsity: 85.1763%\n",
      "total_backward_count 1419550 real_backward_count 192752  13.578%\n",
      "epoch-145 lr=['4.0000000'], tr/val_loss:  3.831458/ 31.688759, val:  49.17%, val_best:  65.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 79.5656%\n",
      "layer   3  Sparsity: 85.2436%\n",
      "total_backward_count 1429340 real_backward_count 193815  13.560%\n",
      "epoch-146 lr=['4.0000000'], tr/val_loss:  3.998386/ 35.398270, val:  41.25%, val_best:  65.83%, tr:  95.30%, tr_best:  96.73%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 79.6550%\n",
      "layer   3  Sparsity: 85.1539%\n",
      "total_backward_count 1439130 real_backward_count 194883  13.542%\n",
      "epoch-147 lr=['4.0000000'], tr/val_loss:  3.981840/ 18.256685, val:  38.75%, val_best:  65.83%, tr:  94.69%, tr_best:  96.73%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 80.1434%\n",
      "layer   3  Sparsity: 85.0009%\n",
      "total_backward_count 1448920 real_backward_count 195927  13.522%\n",
      "epoch-148 lr=['4.0000000'], tr/val_loss:  3.830997/ 26.248816, val:  49.58%, val_best:  65.83%, tr:  96.12%, tr_best:  96.73%, epoch time: 80.52 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 79.9772%\n",
      "layer   3  Sparsity: 85.6370%\n",
      "total_backward_count 1458710 real_backward_count 196954  13.502%\n",
      "epoch-149 lr=['4.0000000'], tr/val_loss:  3.868354/ 24.385834, val:  51.25%, val_best:  65.83%, tr:  96.02%, tr_best:  96.73%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 80.0685%\n",
      "layer   3  Sparsity: 85.3324%\n",
      "total_backward_count 1468500 real_backward_count 197977  13.482%\n",
      "epoch-150 lr=['4.0000000'], tr/val_loss:  3.955695/ 29.981167, val:  55.00%, val_best:  65.83%, tr:  94.18%, tr_best:  96.73%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 79.9501%\n",
      "layer   3  Sparsity: 84.9662%\n",
      "total_backward_count 1478290 real_backward_count 199016  13.463%\n",
      "fc layer 2 self.abs_max_out: 11743.0\n",
      "epoch-151 lr=['4.0000000'], tr/val_loss:  4.175407/ 21.905964, val:  42.08%, val_best:  65.83%, tr:  95.71%, tr_best:  96.73%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 79.8328%\n",
      "layer   3  Sparsity: 85.7059%\n",
      "total_backward_count 1488080 real_backward_count 200096  13.447%\n",
      "epoch-152 lr=['4.0000000'], tr/val_loss:  3.817088/ 31.757818, val:  51.67%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 79.8549%\n",
      "layer   3  Sparsity: 85.2568%\n",
      "total_backward_count 1497870 real_backward_count 201122  13.427%\n",
      "fc layer 2 self.abs_max_out: 11801.0\n",
      "epoch-153 lr=['4.0000000'], tr/val_loss:  4.263746/ 23.080492, val:  53.75%, val_best:  65.83%, tr:  96.02%, tr_best:  96.73%, epoch time: 80.70 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 79.8806%\n",
      "layer   3  Sparsity: 86.0305%\n",
      "total_backward_count 1507660 real_backward_count 202215  13.413%\n",
      "epoch-154 lr=['4.0000000'], tr/val_loss:  3.757788/ 25.931646, val:  42.92%, val_best:  65.83%, tr:  94.28%, tr_best:  96.73%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 79.8310%\n",
      "layer   3  Sparsity: 86.1001%\n",
      "total_backward_count 1517450 real_backward_count 203248  13.394%\n",
      "fc layer 2 self.abs_max_out: 11844.0\n",
      "epoch-155 lr=['4.0000000'], tr/val_loss:  4.199389/ 29.314379, val:  41.25%, val_best:  65.83%, tr:  94.99%, tr_best:  96.73%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 79.6141%\n",
      "layer   3  Sparsity: 85.2892%\n",
      "total_backward_count 1527240 real_backward_count 204325  13.379%\n",
      "epoch-156 lr=['4.0000000'], tr/val_loss:  4.016114/ 18.111977, val:  47.92%, val_best:  65.83%, tr:  95.20%, tr_best:  96.73%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 79.9514%\n",
      "layer   3  Sparsity: 85.5538%\n",
      "total_backward_count 1537030 real_backward_count 205376  13.362%\n",
      "epoch-157 lr=['4.0000000'], tr/val_loss:  3.933994/ 24.397913, val:  54.17%, val_best:  65.83%, tr:  95.40%, tr_best:  96.73%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 79.7654%\n",
      "layer   3  Sparsity: 85.5191%\n",
      "total_backward_count 1546820 real_backward_count 206404  13.344%\n",
      "epoch-158 lr=['4.0000000'], tr/val_loss:  4.068769/ 24.945410, val:  45.83%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 79.9398%\n",
      "layer   3  Sparsity: 85.6956%\n",
      "total_backward_count 1556610 real_backward_count 207454  13.327%\n",
      "epoch-159 lr=['4.0000000'], tr/val_loss:  4.083314/ 28.008129, val:  44.58%, val_best:  65.83%, tr:  96.02%, tr_best:  96.73%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 80.1574%\n",
      "layer   3  Sparsity: 85.7610%\n",
      "total_backward_count 1566400 real_backward_count 208518  13.312%\n",
      "epoch-160 lr=['4.0000000'], tr/val_loss:  4.188766/ 20.957857, val:  56.25%, val_best:  65.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 80.1449%\n",
      "layer   3  Sparsity: 85.3903%\n",
      "total_backward_count 1576190 real_backward_count 209561  13.295%\n",
      "epoch-161 lr=['4.0000000'], tr/val_loss:  3.877764/ 20.581772, val:  57.08%, val_best:  65.83%, tr:  95.91%, tr_best:  96.73%, epoch time: 81.01 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 79.9735%\n",
      "layer   3  Sparsity: 85.8636%\n",
      "total_backward_count 1585980 real_backward_count 210583  13.278%\n",
      "epoch-162 lr=['4.0000000'], tr/val_loss:  3.691065/ 21.050640, val:  51.67%, val_best:  65.83%, tr:  96.53%, tr_best:  96.73%, epoch time: 80.88 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 80.2331%\n",
      "layer   3  Sparsity: 86.1913%\n",
      "total_backward_count 1595770 real_backward_count 211594  13.260%\n",
      "epoch-163 lr=['4.0000000'], tr/val_loss:  3.840841/ 20.589283, val:  52.50%, val_best:  65.83%, tr:  94.69%, tr_best:  96.73%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 80.3497%\n",
      "layer   3  Sparsity: 85.8247%\n",
      "total_backward_count 1605560 real_backward_count 212638  13.244%\n",
      "epoch-164 lr=['4.0000000'], tr/val_loss:  3.802370/ 26.463217, val:  47.92%, val_best:  65.83%, tr:  94.79%, tr_best:  96.73%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 80.4746%\n",
      "layer   3  Sparsity: 85.7942%\n",
      "total_backward_count 1615350 real_backward_count 213682  13.228%\n",
      "epoch-165 lr=['4.0000000'], tr/val_loss:  3.948172/ 23.393789, val:  50.00%, val_best:  65.83%, tr:  95.30%, tr_best:  96.73%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 80.4379%\n",
      "layer   3  Sparsity: 85.6912%\n",
      "total_backward_count 1625140 real_backward_count 214724  13.213%\n",
      "epoch-166 lr=['4.0000000'], tr/val_loss:  3.927820/ 21.682581, val:  45.00%, val_best:  65.83%, tr:  95.10%, tr_best:  96.73%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 80.6487%\n",
      "layer   3  Sparsity: 85.5240%\n",
      "total_backward_count 1634930 real_backward_count 215758  13.197%\n",
      "epoch-167 lr=['4.0000000'], tr/val_loss:  3.664865/ 30.063120, val:  46.25%, val_best:  65.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 80.4913%\n",
      "layer   3  Sparsity: 85.9893%\n",
      "total_backward_count 1644720 real_backward_count 216768  13.180%\n",
      "epoch-168 lr=['4.0000000'], tr/val_loss:  3.466332/ 23.009823, val:  45.00%, val_best:  65.83%, tr:  95.30%, tr_best:  96.73%, epoch time: 80.55 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 80.7656%\n",
      "layer   3  Sparsity: 86.4396%\n",
      "total_backward_count 1654510 real_backward_count 217746  13.161%\n",
      "epoch-169 lr=['4.0000000'], tr/val_loss:  3.676196/ 22.165535, val:  46.67%, val_best:  65.83%, tr:  96.63%, tr_best:  96.73%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 80.6934%\n",
      "layer   3  Sparsity: 85.8595%\n",
      "total_backward_count 1664300 real_backward_count 218743  13.143%\n",
      "epoch-170 lr=['4.0000000'], tr/val_loss:  3.784173/ 21.745592, val:  50.83%, val_best:  65.83%, tr:  95.20%, tr_best:  96.73%, epoch time: 81.21 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 80.7530%\n",
      "layer   3  Sparsity: 85.8519%\n",
      "total_backward_count 1674090 real_backward_count 219774  13.128%\n",
      "epoch-171 lr=['4.0000000'], tr/val_loss:  3.758519/ 25.915386, val:  44.17%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 80.6554%\n",
      "layer   3  Sparsity: 85.7496%\n",
      "total_backward_count 1683880 real_backward_count 220821  13.114%\n",
      "epoch-172 lr=['4.0000000'], tr/val_loss:  3.938802/ 21.608135, val:  54.58%, val_best:  65.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 80.7346%\n",
      "layer   3  Sparsity: 85.3118%\n",
      "total_backward_count 1693670 real_backward_count 221885  13.101%\n",
      "epoch-173 lr=['4.0000000'], tr/val_loss:  3.591188/ 25.336477, val:  42.50%, val_best:  65.83%, tr:  96.22%, tr_best:  96.73%, epoch time: 80.60 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 80.5359%\n",
      "layer   3  Sparsity: 86.1076%\n",
      "total_backward_count 1703460 real_backward_count 222876  13.084%\n",
      "epoch-174 lr=['4.0000000'], tr/val_loss:  3.510555/ 21.487740, val:  52.50%, val_best:  65.83%, tr:  95.30%, tr_best:  96.73%, epoch time: 80.83 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 80.5694%\n",
      "layer   3  Sparsity: 85.7144%\n",
      "total_backward_count 1713250 real_backward_count 223849  13.066%\n",
      "epoch-175 lr=['4.0000000'], tr/val_loss:  3.778140/ 23.665958, val:  48.33%, val_best:  65.83%, tr:  94.59%, tr_best:  96.73%, epoch time: 80.82 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 80.5595%\n",
      "layer   3  Sparsity: 85.7552%\n",
      "total_backward_count 1723040 real_backward_count 224881  13.051%\n",
      "epoch-176 lr=['4.0000000'], tr/val_loss:  3.705573/ 18.424425, val:  57.08%, val_best:  65.83%, tr:  94.99%, tr_best:  96.73%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 80.1039%\n",
      "layer   3  Sparsity: 85.7722%\n",
      "total_backward_count 1732830 real_backward_count 225901  13.037%\n",
      "epoch-177 lr=['4.0000000'], tr/val_loss:  3.862562/ 19.640518, val:  53.75%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 80.3525%\n",
      "layer   3  Sparsity: 85.7659%\n",
      "total_backward_count 1742620 real_backward_count 226933  13.023%\n",
      "epoch-178 lr=['4.0000000'], tr/val_loss:  3.584631/ 31.586815, val:  43.33%, val_best:  65.83%, tr:  95.91%, tr_best:  96.73%, epoch time: 80.78 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 80.5687%\n",
      "layer   3  Sparsity: 85.5640%\n",
      "total_backward_count 1752410 real_backward_count 227895  13.005%\n",
      "epoch-179 lr=['4.0000000'], tr/val_loss:  3.935164/ 27.791370, val:  36.67%, val_best:  65.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 81.05 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 80.5109%\n",
      "layer   3  Sparsity: 85.7700%\n",
      "total_backward_count 1762200 real_backward_count 228954  12.993%\n",
      "epoch-180 lr=['4.0000000'], tr/val_loss:  3.980446/ 23.685436, val:  45.42%, val_best:  65.83%, tr:  93.97%, tr_best:  96.73%, epoch time: 81.15 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 80.6577%\n",
      "layer   3  Sparsity: 85.7619%\n",
      "total_backward_count 1771990 real_backward_count 230026  12.981%\n",
      "epoch-181 lr=['4.0000000'], tr/val_loss:  3.536337/ 21.189827, val:  48.33%, val_best:  65.83%, tr:  96.22%, tr_best:  96.73%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 81.1379%\n",
      "layer   3  Sparsity: 86.1928%\n",
      "total_backward_count 1781780 real_backward_count 230955  12.962%\n",
      "epoch-182 lr=['4.0000000'], tr/val_loss:  3.577211/ 23.619709, val:  57.92%, val_best:  65.83%, tr:  95.91%, tr_best:  96.73%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 80.4857%\n",
      "layer   3  Sparsity: 85.4347%\n",
      "total_backward_count 1791570 real_backward_count 231929  12.946%\n",
      "epoch-183 lr=['4.0000000'], tr/val_loss:  4.046209/ 35.394833, val:  41.67%, val_best:  65.83%, tr:  95.20%, tr_best:  96.73%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 80.4477%\n",
      "layer   3  Sparsity: 84.7426%\n",
      "total_backward_count 1801360 real_backward_count 232946  12.932%\n",
      "epoch-184 lr=['4.0000000'], tr/val_loss:  4.110297/ 29.190735, val:  50.42%, val_best:  65.83%, tr:  96.32%, tr_best:  96.73%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 80.5205%\n",
      "layer   3  Sparsity: 85.1290%\n",
      "total_backward_count 1811150 real_backward_count 233965  12.918%\n",
      "epoch-185 lr=['4.0000000'], tr/val_loss:  4.023429/ 33.335644, val:  43.75%, val_best:  65.83%, tr:  95.51%, tr_best:  96.73%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 80.4009%\n",
      "layer   3  Sparsity: 85.2153%\n",
      "total_backward_count 1820940 real_backward_count 235001  12.905%\n",
      "epoch-186 lr=['4.0000000'], tr/val_loss:  4.077667/ 22.642492, val:  57.08%, val_best:  65.83%, tr:  96.12%, tr_best:  96.73%, epoch time: 80.91 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 80.3635%\n",
      "layer   3  Sparsity: 85.3373%\n",
      "total_backward_count 1830730 real_backward_count 236059  12.894%\n",
      "epoch-187 lr=['4.0000000'], tr/val_loss:  3.928893/ 21.355967, val:  43.75%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 80.6669%\n",
      "layer   3  Sparsity: 85.8665%\n",
      "total_backward_count 1840520 real_backward_count 237100  12.882%\n",
      "epoch-188 lr=['4.0000000'], tr/val_loss:  3.742244/ 22.276464, val:  58.33%, val_best:  65.83%, tr:  95.10%, tr_best:  96.73%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 80.5118%\n",
      "layer   3  Sparsity: 85.9747%\n",
      "total_backward_count 1850310 real_backward_count 238112  12.869%\n",
      "epoch-189 lr=['4.0000000'], tr/val_loss:  3.876871/ 18.203400, val:  55.42%, val_best:  65.83%, tr:  94.89%, tr_best:  96.73%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 80.6671%\n",
      "layer   3  Sparsity: 85.0964%\n",
      "total_backward_count 1860100 real_backward_count 239134  12.856%\n",
      "epoch-190 lr=['4.0000000'], tr/val_loss:  3.707644/ 22.308790, val:  53.33%, val_best:  65.83%, tr:  95.81%, tr_best:  96.73%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 80.5443%\n",
      "layer   3  Sparsity: 85.2897%\n",
      "total_backward_count 1869890 real_backward_count 240089  12.840%\n",
      "epoch-191 lr=['4.0000000'], tr/val_loss:  3.808541/ 29.489222, val:  38.33%, val_best:  65.83%, tr:  95.20%, tr_best:  96.73%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 80.3536%\n",
      "layer   3  Sparsity: 85.6076%\n",
      "total_backward_count 1879680 real_backward_count 241099  12.827%\n",
      "epoch-192 lr=['4.0000000'], tr/val_loss:  3.909050/ 25.945244, val:  55.83%, val_best:  65.83%, tr:  95.91%, tr_best:  96.73%, epoch time: 80.92 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 80.5478%\n",
      "layer   3  Sparsity: 85.0649%\n",
      "total_backward_count 1889470 real_backward_count 242138  12.815%\n",
      "epoch-193 lr=['4.0000000'], tr/val_loss:  3.773746/ 23.172850, val:  43.33%, val_best:  65.83%, tr:  95.40%, tr_best:  96.73%, epoch time: 80.82 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 80.4259%\n",
      "layer   3  Sparsity: 85.4192%\n",
      "total_backward_count 1899260 real_backward_count 243146  12.802%\n",
      "epoch-194 lr=['4.0000000'], tr/val_loss:  3.876576/ 27.354185, val:  50.83%, val_best:  65.83%, tr:  95.10%, tr_best:  96.73%, epoch time: 80.99 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 80.5181%\n",
      "layer   3  Sparsity: 85.4023%\n",
      "total_backward_count 1909050 real_backward_count 244154  12.789%\n",
      "epoch-195 lr=['4.0000000'], tr/val_loss:  3.759085/ 25.270370, val:  51.25%, val_best:  65.83%, tr:  95.91%, tr_best:  96.73%, epoch time: 81.04 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 80.8897%\n",
      "layer   3  Sparsity: 85.7892%\n",
      "total_backward_count 1918840 real_backward_count 245171  12.777%\n",
      "epoch-196 lr=['4.0000000'], tr/val_loss:  3.746165/ 22.007368, val:  54.17%, val_best:  65.83%, tr:  95.30%, tr_best:  96.73%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 80.9731%\n",
      "layer   3  Sparsity: 85.4830%\n",
      "total_backward_count 1928630 real_backward_count 246152  12.763%\n",
      "fc layer 2 self.abs_max_out: 12069.0\n",
      "epoch-197 lr=['4.0000000'], tr/val_loss:  3.720545/ 29.076565, val:  45.00%, val_best:  65.83%, tr:  95.61%, tr_best:  96.73%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 80.5394%\n",
      "layer   3  Sparsity: 85.8934%\n",
      "total_backward_count 1938420 real_backward_count 247167  12.751%\n",
      "epoch-198 lr=['4.0000000'], tr/val_loss:  3.649722/ 19.527987, val:  48.33%, val_best:  65.83%, tr:  94.48%, tr_best:  96.73%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 80.4355%\n",
      "layer   3  Sparsity: 85.5677%\n",
      "total_backward_count 1948210 real_backward_count 248161  12.738%\n",
      "epoch-199 lr=['4.0000000'], tr/val_loss:  3.692135/ 24.289780, val:  49.17%, val_best:  65.83%, tr:  96.12%, tr_best:  96.73%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 80.0639%\n",
      "layer   3  Sparsity: 85.7550%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3f4834177f44f3bcaabb083aacda68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñá‚ñá‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñá‚ñá‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.96118</td></tr><tr><td>tr_epoch_loss</td><td>3.69213</td></tr><tr><td>val_acc_best</td><td>0.65833</td></tr><tr><td>val_acc_now</td><td>0.49167</td></tr><tr><td>val_loss</td><td>24.28978</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-sweep-855</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mzhubrot' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mzhubrot</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251212_020828-mzhubrot/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xd12qc31 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.015625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_063744-xd12qc31</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xd12qc31' target=\"_blank\">devoted-sweep-863</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xd12qc31' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xd12qc31</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251212_063754_186', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 512, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 64, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.015625, 'lif_layer_v_threshold2': 256, 'init_scaling': [0.5, 0.0625, 0.5], 'learning_rate': 1, 'learning_rate2': 2} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 64, self.v_threshold 512\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.015625, self.v_threshold 256\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.0625, 0.5])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=512, v_reset=10000, sg_width=64, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.0625, 0.5])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=256, v_reset=10000, sg_width=0.015625, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.5, 0.0625, 0.5])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 742.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 8.0\n",
      "lif layer 2 self.abs_max_v: 8.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 791.0\n",
      "lif layer 1 self.abs_max_v: 1023.0\n",
      "fc layer 2 self.abs_max_out: 48.0\n",
      "lif layer 2 self.abs_max_v: 51.0\n",
      "fc layer 1 self.abs_max_out: 815.0\n",
      "lif layer 1 self.abs_max_v: 1078.0\n",
      "fc layer 2 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 66.5\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 1123.0\n",
      "fc layer 2 self.abs_max_out: 61.0\n",
      "lif layer 2 self.abs_max_v: 81.0\n",
      "lif layer 1 self.abs_max_v: 1316.5\n",
      "fc layer 2 self.abs_max_out: 65.0\n",
      "lif layer 2 self.abs_max_v: 93.0\n",
      "fc layer 2 self.abs_max_out: 76.0\n",
      "lif layer 2 self.abs_max_v: 109.5\n",
      "fc layer 1 self.abs_max_out: 1051.0\n",
      "lif layer 1 self.abs_max_v: 1671.5\n",
      "fc layer 2 self.abs_max_out: 127.0\n",
      "lif layer 2 self.abs_max_v: 175.0\n",
      "lif layer 2 self.abs_max_v: 196.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 220.0\n",
      "fc layer 1 self.abs_max_out: 1418.0\n",
      "fc layer 2 self.abs_max_out: 164.0\n",
      "lif layer 1 self.abs_max_v: 1701.5\n",
      "lif layer 2 self.abs_max_v: 237.5\n",
      "fc layer 2 self.abs_max_out: 194.0\n",
      "lif layer 2 self.abs_max_v: 270.5\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 245.0\n",
      "lif layer 2 self.abs_max_v: 364.5\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "fc layer 1 self.abs_max_out: 1478.0\n",
      "fc layer 2 self.abs_max_out: 322.0\n",
      "lif layer 2 self.abs_max_v: 466.5\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "lif layer 1 self.abs_max_v: 2008.5\n",
      "fc layer 2 self.abs_max_out: 355.0\n",
      "lif layer 2 self.abs_max_v: 528.5\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 2 self.abs_max_out: 397.0\n",
      "fc layer 1 self.abs_max_out: 1525.0\n",
      "fc layer 1 self.abs_max_out: 1533.0\n",
      "lif layer 1 self.abs_max_v: 2205.0\n",
      "fc layer 1 self.abs_max_out: 1849.0\n",
      "lif layer 1 self.abs_max_v: 2769.5\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 629.5\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 704.0\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "lif layer 2 self.abs_max_v: 745.0\n",
      "fc layer 2 self.abs_max_out: 486.0\n",
      "lif layer 2 self.abs_max_v: 851.0\n",
      "fc layer 1 self.abs_max_out: 1951.0\n",
      "fc layer 2 self.abs_max_out: 567.0\n",
      "lif layer 2 self.abs_max_v: 884.0\n",
      "lif layer 2 self.abs_max_v: 991.0\n",
      "fc layer 3 self.abs_max_out: 520.0\n",
      "fc layer 2 self.abs_max_out: 636.0\n",
      "lif layer 2 self.abs_max_v: 1121.5\n",
      "lif layer 1 self.abs_max_v: 2771.5\n",
      "lif layer 1 self.abs_max_v: 2816.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "lif layer 2 self.abs_max_v: 1126.0\n",
      "lif layer 1 self.abs_max_v: 2852.5\n",
      "lif layer 2 self.abs_max_v: 1209.5\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "lif layer 2 self.abs_max_v: 1287.5\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1740.0\n",
      "lif layer 2 self.abs_max_v: 1775.5\n",
      "fc layer 1 self.abs_max_out: 2141.0\n",
      "lif layer 1 self.abs_max_v: 3099.0\n",
      "lif layer 2 self.abs_max_v: 1934.0\n",
      "lif layer 2 self.abs_max_v: 1949.5\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "lif layer 2 self.abs_max_v: 1967.0\n",
      "fc layer 3 self.abs_max_out: 718.0\n",
      "fc layer 3 self.abs_max_out: 946.0\n",
      "fc layer 2 self.abs_max_out: 1162.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "lif layer 2 self.abs_max_v: 2054.5\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "lif layer 2 self.abs_max_v: 2218.5\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "lif layer 2 self.abs_max_v: 2413.5\n",
      "fc layer 2 self.abs_max_out: 1521.0\n",
      "lif layer 2 self.abs_max_v: 2632.5\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "lif layer 2 self.abs_max_v: 2824.0\n",
      "fc layer 2 self.abs_max_out: 1622.0\n",
      "lif layer 2 self.abs_max_v: 3021.0\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "fc layer 2 self.abs_max_out: 1649.0\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "lif layer 2 self.abs_max_v: 3161.5\n",
      "lif layer 2 self.abs_max_v: 3170.0\n",
      "fc layer 2 self.abs_max_out: 1747.0\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "lif layer 2 self.abs_max_v: 3237.0\n",
      "fc layer 2 self.abs_max_out: 1996.0\n",
      "lif layer 2 self.abs_max_v: 3280.0\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "fc layer 2 self.abs_max_out: 2940.0\n",
      "lif layer 2 self.abs_max_v: 4502.0\n",
      "lif layer 2 self.abs_max_v: 4706.5\n",
      "lif layer 1 self.abs_max_v: 3238.5\n",
      "fc layer 2 self.abs_max_out: 3044.0\n",
      "lif layer 2 self.abs_max_v: 5268.0\n",
      "lif layer 2 self.abs_max_v: 5271.5\n",
      "lif layer 2 self.abs_max_v: 5489.0\n",
      "fc layer 2 self.abs_max_out: 3072.0\n",
      "fc layer 1 self.abs_max_out: 2165.0\n",
      "fc layer 2 self.abs_max_out: 3277.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "fc layer 1 self.abs_max_out: 2221.0\n",
      "lif layer 2 self.abs_max_v: 5552.0\n",
      "lif layer 2 self.abs_max_v: 5720.5\n",
      "lif layer 2 self.abs_max_v: 5878.0\n",
      "fc layer 2 self.abs_max_out: 3808.0\n",
      "lif layer 2 self.abs_max_v: 6496.5\n",
      "lif layer 1 self.abs_max_v: 3350.5\n",
      "lif layer 2 self.abs_max_v: 6628.0\n",
      "fc layer 2 self.abs_max_out: 4367.0\n",
      "lif layer 2 self.abs_max_v: 7681.0\n",
      "fc layer 1 self.abs_max_out: 2240.0\n",
      "fc layer 2 self.abs_max_out: 5104.0\n",
      "lif layer 2 self.abs_max_v: 8016.5\n",
      "fc layer 1 self.abs_max_out: 2267.0\n",
      "fc layer 1 self.abs_max_out: 2293.0\n",
      "fc layer 1 self.abs_max_out: 2298.0\n",
      "lif layer 1 self.abs_max_v: 3742.5\n",
      "lif layer 2 self.abs_max_v: 8114.5\n",
      "lif layer 2 self.abs_max_v: 8493.5\n",
      "lif layer 2 self.abs_max_v: 8854.0\n",
      "lif layer 2 self.abs_max_v: 9375.0\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "fc layer 1 self.abs_max_out: 2482.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 51.776386/119.675720, val:  34.58%, val_best:  34.58%, tr:  79.37%, tr_best:  79.37%, epoch time: 81.19 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 88.7697%\n",
      "layer   3  Sparsity: 71.2015%\n",
      "total_backward_count 9790 real_backward_count 4465  45.608%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 5277.0\n",
      "fc layer 1 self.abs_max_out: 2598.0\n",
      "fc layer 2 self.abs_max_out: 5311.0\n",
      "lif layer 2 self.abs_max_v: 9444.5\n",
      "lif layer 2 self.abs_max_v: 9557.5\n",
      "lif layer 1 self.abs_max_v: 3745.0\n",
      "lif layer 2 self.abs_max_v: 9616.0\n",
      "lif layer 2 self.abs_max_v: 9864.0\n",
      "lif layer 2 self.abs_max_v: 10080.0\n",
      "lif layer 2 self.abs_max_v: 10284.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 28.483582/123.032173, val:  27.50%, val_best:  34.58%, tr:  94.89%, tr_best:  94.89%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 87.6166%\n",
      "layer   3  Sparsity: 67.6477%\n",
      "total_backward_count 19580 real_backward_count 7465  38.126%\n",
      "fc layer 2 self.abs_max_out: 5389.0\n",
      "fc layer 2 self.abs_max_out: 5636.0\n",
      "lif layer 2 self.abs_max_v: 10418.5\n",
      "lif layer 1 self.abs_max_v: 3858.0\n",
      "fc layer 1 self.abs_max_out: 2631.0\n",
      "fc layer 2 self.abs_max_out: 5672.0\n",
      "lif layer 2 self.abs_max_v: 10752.0\n",
      "lif layer 2 self.abs_max_v: 10797.5\n",
      "fc layer 1 self.abs_max_out: 2763.0\n",
      "fc layer 2 self.abs_max_out: 5764.0\n",
      "fc layer 1 self.abs_max_out: 2778.0\n",
      "fc layer 1 self.abs_max_out: 2890.0\n",
      "fc layer 2 self.abs_max_out: 5775.0\n",
      "fc layer 1 self.abs_max_out: 2904.0\n",
      "fc layer 2 self.abs_max_out: 6557.0\n",
      "lif layer 2 self.abs_max_v: 11006.5\n",
      "lif layer 2 self.abs_max_v: 11300.5\n",
      "lif layer 2 self.abs_max_v: 11323.0\n",
      "lif layer 2 self.abs_max_v: 11404.5\n",
      "lif layer 2 self.abs_max_v: 11491.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 20.286003/ 87.084541, val:  32.50%, val_best:  34.58%, tr:  97.04%, tr_best:  97.04%, epoch time: 81.66 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 86.6947%\n",
      "layer   3  Sparsity: 67.0032%\n",
      "total_backward_count 29370 real_backward_count 9943  33.854%\n",
      "lif layer 2 self.abs_max_v: 11500.0\n",
      "lif layer 2 self.abs_max_v: 11839.5\n",
      "fc layer 1 self.abs_max_out: 2981.0\n",
      "fc layer 2 self.abs_max_out: 6809.0\n",
      "lif layer 2 self.abs_max_v: 11949.5\n",
      "lif layer 2 self.abs_max_v: 12151.0\n",
      "fc layer 2 self.abs_max_out: 6825.0\n",
      "lif layer 2 self.abs_max_v: 12199.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 17.033751/ 86.597031, val:  31.67%, val_best:  34.58%, tr:  98.47%, tr_best:  98.47%, epoch time: 81.37 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 86.0184%\n",
      "layer   3  Sparsity: 67.3340%\n",
      "total_backward_count 39160 real_backward_count 12087  30.866%\n",
      "lif layer 2 self.abs_max_v: 12235.5\n",
      "fc layer 2 self.abs_max_out: 6895.0\n",
      "fc layer 1 self.abs_max_out: 3061.0\n",
      "lif layer 2 self.abs_max_v: 12426.5\n",
      "lif layer 2 self.abs_max_v: 12945.0\n",
      "lif layer 2 self.abs_max_v: 13266.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 14.693197/ 81.364098, val:  28.33%, val_best:  34.58%, tr:  97.04%, tr_best:  98.47%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 85.9274%\n",
      "layer   3  Sparsity: 67.6889%\n",
      "total_backward_count 48950 real_backward_count 14026  28.654%\n",
      "fc layer 1 self.abs_max_out: 3122.0\n",
      "fc layer 1 self.abs_max_out: 3295.0\n",
      "fc layer 2 self.abs_max_out: 7029.0\n",
      "fc layer 2 self.abs_max_out: 7150.0\n",
      "lif layer 2 self.abs_max_v: 13347.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 13.031927/ 98.250259, val:  25.83%, val_best:  34.58%, tr:  96.53%, tr_best:  98.47%, epoch time: 81.30 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 85.8016%\n",
      "layer   3  Sparsity: 68.4039%\n",
      "total_backward_count 58740 real_backward_count 15828  26.946%\n",
      "lif layer 2 self.abs_max_v: 13397.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 12.821525/ 94.929527, val:  20.83%, val_best:  34.58%, tr:  97.45%, tr_best:  98.47%, epoch time: 81.03 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 85.4636%\n",
      "layer   3  Sparsity: 69.0364%\n",
      "total_backward_count 68530 real_backward_count 17677  25.795%\n",
      "fc layer 2 self.abs_max_out: 7195.0\n",
      "fc layer 2 self.abs_max_out: 7611.0\n",
      "lif layer 2 self.abs_max_v: 13639.5\n",
      "lif layer 2 self.abs_max_v: 13647.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 12.074079/ 87.906219, val:  31.25%, val_best:  34.58%, tr:  97.96%, tr_best:  98.47%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 84.9877%\n",
      "layer   3  Sparsity: 68.0043%\n",
      "total_backward_count 78320 real_backward_count 19396  24.765%\n",
      "lif layer 2 self.abs_max_v: 13681.5\n",
      "lif layer 2 self.abs_max_v: 13923.0\n",
      "lif layer 2 self.abs_max_v: 13962.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 12.332982/ 57.595375, val:  33.33%, val_best:  34.58%, tr:  97.75%, tr_best:  98.47%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 84.5715%\n",
      "layer   3  Sparsity: 67.9419%\n",
      "total_backward_count 88110 real_backward_count 21138  23.990%\n",
      "fc layer 1 self.abs_max_out: 3630.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 11.939413/ 71.097229, val:  28.33%, val_best:  34.58%, tr:  97.75%, tr_best:  98.47%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 84.5841%\n",
      "layer   3  Sparsity: 68.4479%\n",
      "total_backward_count 97900 real_backward_count 22882  23.373%\n",
      "lif layer 2 self.abs_max_v: 14084.5\n",
      "fc layer 2 self.abs_max_out: 7686.0\n",
      "lif layer 2 self.abs_max_v: 14722.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 12.063070/ 57.429932, val:  32.50%, val_best:  34.58%, tr:  98.57%, tr_best:  98.57%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 84.4539%\n",
      "layer   3  Sparsity: 68.4522%\n",
      "total_backward_count 107690 real_backward_count 24600  22.843%\n",
      "fc layer 2 self.abs_max_out: 7852.0\n",
      "fc layer 1 self.abs_max_out: 3673.0\n",
      "lif layer 2 self.abs_max_v: 14725.0\n",
      "fc layer 2 self.abs_max_out: 7853.0\n",
      "lif layer 2 self.abs_max_v: 14981.0\n",
      "fc layer 2 self.abs_max_out: 7993.0\n",
      "lif layer 2 self.abs_max_v: 15483.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 11.474896/ 90.634605, val:  15.83%, val_best:  34.58%, tr:  98.16%, tr_best:  98.57%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 83.9813%\n",
      "layer   3  Sparsity: 68.6176%\n",
      "total_backward_count 117480 real_backward_count 26271  22.362%\n",
      "fc layer 2 self.abs_max_out: 8138.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 11.115172/ 76.521263, val:  27.92%, val_best:  34.58%, tr:  97.14%, tr_best:  98.57%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 83.7404%\n",
      "layer   3  Sparsity: 68.3655%\n",
      "total_backward_count 127270 real_backward_count 27886  21.911%\n",
      "fc layer 2 self.abs_max_out: 8197.0\n",
      "fc layer 2 self.abs_max_out: 8366.0\n",
      "fc layer 2 self.abs_max_out: 8388.0\n",
      "lif layer 2 self.abs_max_v: 15598.0\n",
      "lif layer 2 self.abs_max_v: 15849.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 11.657364/108.229912, val:  15.00%, val_best:  34.58%, tr:  97.96%, tr_best:  98.57%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 83.4601%\n",
      "layer   3  Sparsity: 68.6340%\n",
      "total_backward_count 137060 real_backward_count 29586  21.586%\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 11.234563/117.759079, val:  20.83%, val_best:  34.58%, tr:  97.45%, tr_best:  98.57%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 83.5815%\n",
      "layer   3  Sparsity: 68.8521%\n",
      "total_backward_count 146850 real_backward_count 31209  21.252%\n",
      "fc layer 2 self.abs_max_out: 8391.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 10.580890/ 68.796921, val:  30.83%, val_best:  34.58%, tr:  99.08%, tr_best:  99.08%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 83.8420%\n",
      "layer   3  Sparsity: 69.7401%\n",
      "total_backward_count 156640 real_backward_count 32807  20.944%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 10.292087/ 50.840252, val:  33.33%, val_best:  34.58%, tr:  97.85%, tr_best:  99.08%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 83.9082%\n",
      "layer   3  Sparsity: 70.1283%\n",
      "total_backward_count 166430 real_backward_count 34353  20.641%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 10.472765/ 74.040733, val:  21.25%, val_best:  34.58%, tr:  97.85%, tr_best:  99.08%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 83.6914%\n",
      "layer   3  Sparsity: 69.9404%\n",
      "total_backward_count 176220 real_backward_count 35958  20.405%\n",
      "lif layer 1 self.abs_max_v: 3992.5\n",
      "fc layer 2 self.abs_max_out: 8484.0\n",
      "lif layer 2 self.abs_max_v: 16203.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 10.124839/ 55.779476, val:  36.67%, val_best:  36.67%, tr:  97.96%, tr_best:  99.08%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 83.5680%\n",
      "layer   3  Sparsity: 69.6663%\n",
      "total_backward_count 186010 real_backward_count 37545  20.184%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  9.783618/ 49.610973, val:  30.83%, val_best:  36.67%, tr:  98.06%, tr_best:  99.08%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 83.3966%\n",
      "layer   3  Sparsity: 69.7842%\n",
      "total_backward_count 195800 real_backward_count 39048  19.943%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  9.902925/ 87.163086, val:  36.25%, val_best:  36.67%, tr:  97.85%, tr_best:  99.08%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 83.6069%\n",
      "layer   3  Sparsity: 69.8461%\n",
      "total_backward_count 205590 real_backward_count 40569  19.733%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 10.178648/ 61.627945, val:  33.33%, val_best:  36.67%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.89 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 84.0791%\n",
      "layer   3  Sparsity: 70.4165%\n",
      "total_backward_count 215380 real_backward_count 42154  19.572%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  9.566528/ 66.270439, val:  32.92%, val_best:  36.67%, tr:  97.55%, tr_best:  99.08%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 84.0854%\n",
      "layer   3  Sparsity: 71.2617%\n",
      "total_backward_count 225170 real_backward_count 43681  19.399%\n",
      "fc layer 1 self.abs_max_out: 3692.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  9.785961/ 36.525013, val:  42.92%, val_best:  42.92%, tr:  97.55%, tr_best:  99.08%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 84.0340%\n",
      "layer   3  Sparsity: 71.5021%\n",
      "total_backward_count 234960 real_backward_count 45239  19.254%\n",
      "fc layer 1 self.abs_max_out: 3760.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  9.155454/ 50.641537, val:  38.33%, val_best:  42.92%, tr:  97.14%, tr_best:  99.08%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 84.2502%\n",
      "layer   3  Sparsity: 72.4015%\n",
      "total_backward_count 244750 real_backward_count 46764  19.107%\n",
      "fc layer 1 self.abs_max_out: 3851.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  9.218727/ 39.942154, val:  38.75%, val_best:  42.92%, tr:  97.75%, tr_best:  99.08%, epoch time: 80.51 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 84.2963%\n",
      "layer   3  Sparsity: 72.8608%\n",
      "total_backward_count 254540 real_backward_count 48348  18.994%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  8.888589/ 51.973980, val:  32.50%, val_best:  42.92%, tr:  96.32%, tr_best:  99.08%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 84.3627%\n",
      "layer   3  Sparsity: 72.9494%\n",
      "total_backward_count 264330 real_backward_count 49871  18.867%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  9.219299/ 30.537666, val:  34.17%, val_best:  42.92%, tr:  96.32%, tr_best:  99.08%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 84.4792%\n",
      "layer   3  Sparsity: 73.3386%\n",
      "total_backward_count 274120 real_backward_count 51422  18.759%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  8.554364/ 40.543625, val:  36.67%, val_best:  42.92%, tr:  96.32%, tr_best:  99.08%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 84.3432%\n",
      "layer   3  Sparsity: 73.8730%\n",
      "total_backward_count 283910 real_backward_count 52932  18.644%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  8.589266/ 48.064739, val:  36.67%, val_best:  42.92%, tr:  97.75%, tr_best:  99.08%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 84.4425%\n",
      "layer   3  Sparsity: 73.9925%\n",
      "total_backward_count 293700 real_backward_count 54430  18.533%\n",
      "lif layer 1 self.abs_max_v: 4051.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  8.345299/ 54.504646, val:  39.58%, val_best:  42.92%, tr:  97.24%, tr_best:  99.08%, epoch time: 81.09 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 84.4272%\n",
      "layer   3  Sparsity: 73.9757%\n",
      "total_backward_count 303490 real_backward_count 55944  18.434%\n",
      "fc layer 1 self.abs_max_out: 3863.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  8.629029/ 34.279732, val:  41.25%, val_best:  42.92%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 84.4848%\n",
      "layer   3  Sparsity: 74.1895%\n",
      "total_backward_count 313280 real_backward_count 57473  18.346%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  8.638845/ 38.317856, val:  36.67%, val_best:  42.92%, tr:  97.96%, tr_best:  99.08%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 84.7179%\n",
      "layer   3  Sparsity: 75.2178%\n",
      "total_backward_count 323070 real_backward_count 59027  18.271%\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "lif layer 1 self.abs_max_v: 4190.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  8.061665/ 51.048382, val:  29.58%, val_best:  42.92%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 84.9539%\n",
      "layer   3  Sparsity: 75.4733%\n",
      "total_backward_count 332860 real_backward_count 60564  18.195%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  8.390862/ 42.112896, val:  40.00%, val_best:  42.92%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 84.9390%\n",
      "layer   3  Sparsity: 75.7174%\n",
      "total_backward_count 342650 real_backward_count 62115  18.128%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  8.325298/ 37.605137, val:  45.83%, val_best:  45.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 84.8826%\n",
      "layer   3  Sparsity: 76.0685%\n",
      "total_backward_count 352440 real_backward_count 63699  18.074%\n",
      "fc layer 1 self.abs_max_out: 4198.0\n",
      "lif layer 1 self.abs_max_v: 4198.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  7.739170/ 30.165264, val:  42.50%, val_best:  45.83%, tr:  97.24%, tr_best:  99.08%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 84.7716%\n",
      "layer   3  Sparsity: 76.3802%\n",
      "total_backward_count 362230 real_backward_count 65209  18.002%\n",
      "fc layer 2 self.abs_max_out: 8705.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  7.547570/ 48.769386, val:  35.00%, val_best:  45.83%, tr:  96.83%, tr_best:  99.08%, epoch time: 81.03 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 84.9533%\n",
      "layer   3  Sparsity: 76.7701%\n",
      "total_backward_count 372020 real_backward_count 66749  17.942%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  7.799018/ 37.264622, val:  42.08%, val_best:  45.83%, tr:  97.34%, tr_best:  99.08%, epoch time: 81.45 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 84.7495%\n",
      "layer   3  Sparsity: 76.9129%\n",
      "total_backward_count 381810 real_backward_count 68327  17.896%\n",
      "fc layer 1 self.abs_max_out: 4269.0\n",
      "lif layer 1 self.abs_max_v: 4269.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  7.523472/ 42.216705, val:  34.58%, val_best:  45.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 84.7408%\n",
      "layer   3  Sparsity: 76.8919%\n",
      "total_backward_count 391600 real_backward_count 69827  17.831%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  7.344436/ 37.108479, val:  44.17%, val_best:  45.83%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 85.3916%\n",
      "layer   3  Sparsity: 77.6402%\n",
      "total_backward_count 401390 real_backward_count 71351  17.776%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  7.497103/ 47.963417, val:  28.75%, val_best:  45.83%, tr:  96.42%, tr_best:  99.08%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 85.2301%\n",
      "layer   3  Sparsity: 77.5635%\n",
      "total_backward_count 411180 real_backward_count 72897  17.729%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  6.887781/ 36.530872, val:  42.08%, val_best:  45.83%, tr:  97.96%, tr_best:  99.08%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 85.2541%\n",
      "layer   3  Sparsity: 78.1539%\n",
      "total_backward_count 420970 real_backward_count 74374  17.667%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  6.944103/ 40.386692, val:  40.42%, val_best:  45.83%, tr:  97.04%, tr_best:  99.08%, epoch time: 80.54 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 85.3045%\n",
      "layer   3  Sparsity: 78.3982%\n",
      "total_backward_count 430760 real_backward_count 75860  17.611%\n",
      "lif layer 1 self.abs_max_v: 4270.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  7.280564/ 36.307613, val:  35.00%, val_best:  45.83%, tr:  96.83%, tr_best:  99.08%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 85.5651%\n",
      "layer   3  Sparsity: 78.4018%\n",
      "total_backward_count 440550 real_backward_count 77419  17.573%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  7.232817/ 28.405136, val:  43.75%, val_best:  45.83%, tr:  95.40%, tr_best:  99.08%, epoch time: 80.51 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 85.6644%\n",
      "layer   3  Sparsity: 78.3025%\n",
      "total_backward_count 450340 real_backward_count 78967  17.535%\n",
      "lif layer 1 self.abs_max_v: 4323.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  7.166083/ 37.497593, val:  37.92%, val_best:  45.83%, tr:  97.75%, tr_best:  99.08%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 85.4238%\n",
      "layer   3  Sparsity: 78.2223%\n",
      "total_backward_count 460130 real_backward_count 80493  17.494%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  7.014803/ 36.750187, val:  38.33%, val_best:  45.83%, tr:  95.61%, tr_best:  99.08%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 85.2163%\n",
      "layer   3  Sparsity: 78.3581%\n",
      "total_backward_count 469920 real_backward_count 82003  17.450%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  6.961847/ 42.632488, val:  33.75%, val_best:  45.83%, tr:  96.42%, tr_best:  99.08%, epoch time: 80.71 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 85.2173%\n",
      "layer   3  Sparsity: 78.4216%\n",
      "total_backward_count 479710 real_backward_count 83496  17.406%\n",
      "lif layer 1 self.abs_max_v: 4621.5\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  7.036545/ 28.019096, val:  47.92%, val_best:  47.92%, tr:  97.65%, tr_best:  99.08%, epoch time: 80.78 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 85.1675%\n",
      "layer   3  Sparsity: 78.6642%\n",
      "total_backward_count 489500 real_backward_count 85036  17.372%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  6.838560/ 40.774357, val:  36.67%, val_best:  47.92%, tr:  95.71%, tr_best:  99.08%, epoch time: 80.84 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 85.2616%\n",
      "layer   3  Sparsity: 78.7555%\n",
      "total_backward_count 499290 real_backward_count 86546  17.334%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  6.975456/ 30.718441, val:  38.75%, val_best:  47.92%, tr:  97.24%, tr_best:  99.08%, epoch time: 80.58 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 85.0176%\n",
      "layer   3  Sparsity: 78.1744%\n",
      "total_backward_count 509080 real_backward_count 88035  17.293%\n",
      "lif layer 1 self.abs_max_v: 5012.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  7.090766/ 32.251286, val:  40.42%, val_best:  47.92%, tr:  97.85%, tr_best:  99.08%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 85.0541%\n",
      "layer   3  Sparsity: 78.0626%\n",
      "total_backward_count 518870 real_backward_count 89560  17.261%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  7.075069/ 47.556789, val:  39.58%, val_best:  47.92%, tr:  96.12%, tr_best:  99.08%, epoch time: 80.42 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 85.3594%\n",
      "layer   3  Sparsity: 78.4040%\n",
      "total_backward_count 528660 real_backward_count 91073  17.227%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  6.897323/ 31.357838, val:  45.42%, val_best:  47.92%, tr:  97.34%, tr_best:  99.08%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 85.5547%\n",
      "layer   3  Sparsity: 78.8889%\n",
      "total_backward_count 538450 real_backward_count 92599  17.197%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  6.577388/ 31.580084, val:  39.58%, val_best:  47.92%, tr:  95.10%, tr_best:  99.08%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 85.6251%\n",
      "layer   3  Sparsity: 79.1044%\n",
      "total_backward_count 548240 real_backward_count 94113  17.166%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  6.963749/ 32.816925, val:  33.33%, val_best:  47.92%, tr:  96.02%, tr_best:  99.08%, epoch time: 80.58 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 85.3559%\n",
      "layer   3  Sparsity: 79.0272%\n",
      "total_backward_count 558030 real_backward_count 95673  17.145%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  6.292356/ 27.853893, val:  50.83%, val_best:  50.83%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.98 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 85.6044%\n",
      "layer   3  Sparsity: 79.3910%\n",
      "total_backward_count 567820 real_backward_count 97152  17.110%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  6.632964/ 32.468723, val:  43.75%, val_best:  50.83%, tr:  97.14%, tr_best:  99.08%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 85.4803%\n",
      "layer   3  Sparsity: 79.3723%\n",
      "total_backward_count 577610 real_backward_count 98660  17.081%\n",
      "fc layer 1 self.abs_max_out: 4313.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  6.068991/ 39.591354, val:  40.42%, val_best:  50.83%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 85.2296%\n",
      "layer   3  Sparsity: 79.0577%\n",
      "total_backward_count 587400 real_backward_count 100078  17.037%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  6.344245/ 34.152893, val:  40.83%, val_best:  50.83%, tr:  97.34%, tr_best:  99.08%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 85.2361%\n",
      "layer   3  Sparsity: 79.3046%\n",
      "total_backward_count 597190 real_backward_count 101518  16.999%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  6.145464/ 22.834967, val:  43.33%, val_best:  50.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 84.9424%\n",
      "layer   3  Sparsity: 79.6380%\n",
      "total_backward_count 606980 real_backward_count 102969  16.964%\n",
      "fc layer 1 self.abs_max_out: 4327.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  6.124330/ 25.082548, val:  37.92%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 84.9076%\n",
      "layer   3  Sparsity: 79.5529%\n",
      "total_backward_count 616770 real_backward_count 104377  16.923%\n",
      "fc layer 1 self.abs_max_out: 4333.0\n",
      "fc layer 1 self.abs_max_out: 4345.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  6.027149/ 34.516903, val:  32.92%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 85.1244%\n",
      "layer   3  Sparsity: 79.8893%\n",
      "total_backward_count 626560 real_backward_count 105829  16.890%\n",
      "fc layer 1 self.abs_max_out: 4515.0\n",
      "lif layer 1 self.abs_max_v: 5082.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  6.244680/ 36.319595, val:  31.25%, val_best:  50.83%, tr:  97.14%, tr_best:  99.08%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 85.3462%\n",
      "layer   3  Sparsity: 79.3698%\n",
      "total_backward_count 636350 real_backward_count 107293  16.861%\n",
      "lif layer 1 self.abs_max_v: 5250.5\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  6.436919/ 37.250252, val:  44.17%, val_best:  50.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 85.1944%\n",
      "layer   3  Sparsity: 79.1045%\n",
      "total_backward_count 646140 real_backward_count 108772  16.834%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  6.393291/ 30.134706, val:  37.50%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 84.9592%\n",
      "layer   3  Sparsity: 79.2645%\n",
      "total_backward_count 655930 real_backward_count 110199  16.800%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  6.052552/ 20.828783, val:  45.83%, val_best:  50.83%, tr:  96.42%, tr_best:  99.08%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 84.7326%\n",
      "layer   3  Sparsity: 79.2043%\n",
      "total_backward_count 665720 real_backward_count 111635  16.769%\n",
      "lif layer 1 self.abs_max_v: 5371.5\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  6.227899/ 33.858246, val:  49.58%, val_best:  50.83%, tr:  97.45%, tr_best:  99.08%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 85.1740%\n",
      "layer   3  Sparsity: 79.0183%\n",
      "total_backward_count 675510 real_backward_count 113102  16.743%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  6.214710/ 55.164055, val:  42.50%, val_best:  50.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 85.3550%\n",
      "layer   3  Sparsity: 78.6554%\n",
      "total_backward_count 685300 real_backward_count 114532  16.713%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  6.508859/ 38.804714, val:  34.58%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 85.3106%\n",
      "layer   3  Sparsity: 78.8320%\n",
      "total_backward_count 695090 real_backward_count 115994  16.688%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  6.435315/ 37.756443, val:  40.42%, val_best:  50.83%, tr:  96.22%, tr_best:  99.08%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 84.9985%\n",
      "layer   3  Sparsity: 78.5589%\n",
      "total_backward_count 704880 real_backward_count 117441  16.661%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  6.444461/ 49.447269, val:  35.00%, val_best:  50.83%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 85.0395%\n",
      "layer   3  Sparsity: 78.7426%\n",
      "total_backward_count 714670 real_backward_count 118909  16.638%\n",
      "lif layer 1 self.abs_max_v: 5505.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  6.340412/ 49.500137, val:  37.08%, val_best:  50.83%, tr:  97.96%, tr_best:  99.08%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 85.2581%\n",
      "layer   3  Sparsity: 79.2250%\n",
      "total_backward_count 724460 real_backward_count 120392  16.618%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  6.370157/ 44.924545, val:  34.17%, val_best:  50.83%, tr:  95.71%, tr_best:  99.08%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 85.3446%\n",
      "layer   3  Sparsity: 79.6275%\n",
      "total_backward_count 734250 real_backward_count 121885  16.600%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  5.968470/ 28.078011, val:  42.50%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 85.2840%\n",
      "layer   3  Sparsity: 80.0587%\n",
      "total_backward_count 744040 real_backward_count 123328  16.575%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  6.246469/ 22.673992, val:  44.58%, val_best:  50.83%, tr:  95.61%, tr_best:  99.08%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 85.3395%\n",
      "layer   3  Sparsity: 80.3642%\n",
      "total_backward_count 753830 real_backward_count 124854  16.563%\n",
      "lif layer 1 self.abs_max_v: 5535.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  5.718174/ 31.684572, val:  38.75%, val_best:  50.83%, tr:  98.06%, tr_best:  99.08%, epoch time: 80.51 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 85.3477%\n",
      "layer   3  Sparsity: 80.3329%\n",
      "total_backward_count 763620 real_backward_count 126290  16.538%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  6.015382/ 26.084795, val:  42.92%, val_best:  50.83%, tr:  96.73%, tr_best:  99.08%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 85.3156%\n",
      "layer   3  Sparsity: 80.2630%\n",
      "total_backward_count 773410 real_backward_count 127788  16.523%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  5.915936/ 24.697035, val:  50.00%, val_best:  50.83%, tr:  97.04%, tr_best:  99.08%, epoch time: 80.55 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 85.2632%\n",
      "layer   3  Sparsity: 79.8068%\n",
      "total_backward_count 783200 real_backward_count 129222  16.499%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  5.831108/ 29.094898, val:  40.83%, val_best:  50.83%, tr:  96.32%, tr_best:  99.08%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 85.4548%\n",
      "layer   3  Sparsity: 80.2647%\n",
      "total_backward_count 792990 real_backward_count 130667  16.478%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  5.798368/ 29.087086, val:  34.58%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 85.4509%\n",
      "layer   3  Sparsity: 80.5084%\n",
      "total_backward_count 802780 real_backward_count 132132  16.459%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  5.734075/ 41.103569, val:  36.67%, val_best:  50.83%, tr:  94.89%, tr_best:  99.08%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 85.2634%\n",
      "layer   3  Sparsity: 80.4845%\n",
      "total_backward_count 812570 real_backward_count 133613  16.443%\n",
      "lif layer 1 self.abs_max_v: 5616.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  6.027038/ 27.020065, val:  44.58%, val_best:  50.83%, tr:  96.42%, tr_best:  99.08%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 85.2707%\n",
      "layer   3  Sparsity: 80.0009%\n",
      "total_backward_count 822360 real_backward_count 135116  16.430%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  6.095001/ 38.107933, val:  32.50%, val_best:  50.83%, tr:  97.34%, tr_best:  99.08%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 85.1723%\n",
      "layer   3  Sparsity: 79.8624%\n",
      "total_backward_count 832150 real_backward_count 136592  16.414%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  6.160650/ 23.679274, val:  37.08%, val_best:  50.83%, tr:  95.61%, tr_best:  99.08%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 85.2674%\n",
      "layer   3  Sparsity: 80.3953%\n",
      "total_backward_count 841940 real_backward_count 138097  16.402%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  5.468473/ 29.188810, val:  41.67%, val_best:  50.83%, tr:  95.30%, tr_best:  99.08%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 85.1602%\n",
      "layer   3  Sparsity: 80.7530%\n",
      "total_backward_count 851730 real_backward_count 139537  16.383%\n",
      "lif layer 1 self.abs_max_v: 5779.5\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  5.822793/ 21.486658, val:  42.50%, val_best:  50.83%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 85.1170%\n",
      "layer   3  Sparsity: 80.4916%\n",
      "total_backward_count 861520 real_backward_count 140999  16.366%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  5.722540/ 30.560211, val:  41.67%, val_best:  50.83%, tr:  97.34%, tr_best:  99.08%, epoch time: 80.70 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 84.8667%\n",
      "layer   3  Sparsity: 79.7347%\n",
      "total_backward_count 871310 real_backward_count 142394  16.343%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  6.209878/ 38.033936, val:  33.75%, val_best:  50.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.55 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 84.8598%\n",
      "layer   3  Sparsity: 79.4764%\n",
      "total_backward_count 881100 real_backward_count 143840  16.325%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  6.055878/ 23.171076, val:  41.25%, val_best:  50.83%, tr:  96.12%, tr_best:  99.08%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 84.9858%\n",
      "layer   3  Sparsity: 80.1636%\n",
      "total_backward_count 890890 real_backward_count 145314  16.311%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  5.730636/ 16.890121, val:  41.67%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.62 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 85.2278%\n",
      "layer   3  Sparsity: 80.2151%\n",
      "total_backward_count 900680 real_backward_count 146741  16.292%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  5.794934/ 32.067635, val:  36.25%, val_best:  50.83%, tr:  95.30%, tr_best:  99.08%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 85.3397%\n",
      "layer   3  Sparsity: 80.9776%\n",
      "total_backward_count 910470 real_backward_count 148212  16.279%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  5.650899/ 32.426754, val:  35.83%, val_best:  50.83%, tr:  95.61%, tr_best:  99.08%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 85.3402%\n",
      "layer   3  Sparsity: 81.2994%\n",
      "total_backward_count 920260 real_backward_count 149689  16.266%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  5.675685/ 19.091383, val:  42.50%, val_best:  50.83%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 85.4438%\n",
      "layer   3  Sparsity: 81.1096%\n",
      "total_backward_count 930050 real_backward_count 151165  16.253%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  5.617401/ 23.148212, val:  37.92%, val_best:  50.83%, tr:  97.24%, tr_best:  99.08%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 85.6950%\n",
      "layer   3  Sparsity: 80.6025%\n",
      "total_backward_count 939840 real_backward_count 152606  16.237%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  5.734781/ 48.872105, val:  35.83%, val_best:  50.83%, tr:  96.12%, tr_best:  99.08%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 85.6231%\n",
      "layer   3  Sparsity: 80.7943%\n",
      "total_backward_count 949630 real_backward_count 154081  16.225%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  5.597333/ 23.842670, val:  45.42%, val_best:  50.83%, tr:  97.45%, tr_best:  99.08%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 85.6507%\n",
      "layer   3  Sparsity: 80.9910%\n",
      "total_backward_count 959420 real_backward_count 155536  16.211%\n",
      "lif layer 1 self.abs_max_v: 5829.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  5.271275/ 39.008324, val:  37.08%, val_best:  50.83%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 85.5973%\n",
      "layer   3  Sparsity: 81.7968%\n",
      "total_backward_count 969210 real_backward_count 156975  16.196%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  5.179778/ 30.445808, val:  39.58%, val_best:  50.83%, tr:  96.73%, tr_best:  99.08%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 85.7030%\n",
      "layer   3  Sparsity: 81.7922%\n",
      "total_backward_count 979000 real_backward_count 158382  16.178%\n",
      "fc layer 1 self.abs_max_out: 4556.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  5.493467/ 29.799171, val:  37.92%, val_best:  50.83%, tr:  97.34%, tr_best:  99.08%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 85.4118%\n",
      "layer   3  Sparsity: 81.2480%\n",
      "total_backward_count 988790 real_backward_count 159789  16.160%\n",
      "fc layer 1 self.abs_max_out: 4557.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  5.496488/ 27.228758, val:  44.58%, val_best:  50.83%, tr:  96.94%, tr_best:  99.08%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 85.3880%\n",
      "layer   3  Sparsity: 81.4979%\n",
      "total_backward_count 998580 real_backward_count 161282  16.151%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  5.301638/ 26.613022, val:  39.17%, val_best:  50.83%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 85.2525%\n",
      "layer   3  Sparsity: 81.6715%\n",
      "total_backward_count 1008370 real_backward_count 162748  16.140%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  5.374537/ 22.904371, val:  43.33%, val_best:  50.83%, tr:  97.04%, tr_best:  99.08%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 85.2228%\n",
      "layer   3  Sparsity: 81.8401%\n",
      "total_backward_count 1018160 real_backward_count 164215  16.129%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  5.226548/ 22.365400, val:  41.25%, val_best:  50.83%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 85.5158%\n",
      "layer   3  Sparsity: 82.0870%\n",
      "total_backward_count 1027950 real_backward_count 165661  16.116%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  5.080091/ 37.312523, val:  27.08%, val_best:  50.83%, tr:  96.73%, tr_best:  99.08%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 85.4854%\n",
      "layer   3  Sparsity: 82.3817%\n",
      "total_backward_count 1037740 real_backward_count 167121  16.104%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  5.060207/ 44.092567, val:  30.00%, val_best:  50.83%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 85.2896%\n",
      "layer   3  Sparsity: 82.0161%\n",
      "total_backward_count 1047530 real_backward_count 168531  16.088%\n",
      "fc layer 1 self.abs_max_out: 4558.0\n",
      "lif layer 1 self.abs_max_v: 5832.5\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  5.249382/ 22.986433, val:  37.08%, val_best:  50.83%, tr:  95.71%, tr_best:  99.08%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 85.4844%\n",
      "layer   3  Sparsity: 82.3233%\n",
      "total_backward_count 1057320 real_backward_count 170000  16.078%\n",
      "fc layer 1 self.abs_max_out: 4595.0\n",
      "lif layer 1 self.abs_max_v: 5940.5\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  5.130723/ 22.982878, val:  42.92%, val_best:  50.83%, tr:  96.02%, tr_best:  99.08%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 85.3725%\n",
      "layer   3  Sparsity: 82.7958%\n",
      "total_backward_count 1067110 real_backward_count 171499  16.071%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  5.003332/ 33.773136, val:  32.08%, val_best:  50.83%, tr:  95.51%, tr_best:  99.08%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 85.4002%\n",
      "layer   3  Sparsity: 82.9417%\n",
      "total_backward_count 1076900 real_backward_count 172937  16.059%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  4.847425/ 33.504375, val:  35.00%, val_best:  50.83%, tr:  94.79%, tr_best:  99.08%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 85.7495%\n",
      "layer   3  Sparsity: 82.8453%\n",
      "total_backward_count 1086690 real_backward_count 174326  16.042%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  4.964691/ 20.661270, val:  50.83%, val_best:  50.83%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 85.6611%\n",
      "layer   3  Sparsity: 82.6511%\n",
      "total_backward_count 1096480 real_backward_count 175719  16.026%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  5.009649/ 32.651691, val:  51.25%, val_best:  51.25%, tr:  96.12%, tr_best:  99.08%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 85.4886%\n",
      "layer   3  Sparsity: 82.5380%\n",
      "total_backward_count 1106270 real_backward_count 177168  16.015%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  5.305990/ 29.962706, val:  41.25%, val_best:  51.25%, tr:  96.12%, tr_best:  99.08%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 85.3039%\n",
      "layer   3  Sparsity: 82.3444%\n",
      "total_backward_count 1116060 real_backward_count 178617  16.004%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  5.149088/ 40.179909, val:  33.33%, val_best:  51.25%, tr:  95.81%, tr_best:  99.08%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 85.5704%\n",
      "layer   3  Sparsity: 82.5227%\n",
      "total_backward_count 1125850 real_backward_count 180054  15.993%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  5.210221/ 33.146431, val:  39.17%, val_best:  51.25%, tr:  96.32%, tr_best:  99.08%, epoch time: 80.98 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 85.7242%\n",
      "layer   3  Sparsity: 82.6056%\n",
      "total_backward_count 1135640 real_backward_count 181501  15.982%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  5.199274/ 24.916813, val:  48.33%, val_best:  51.25%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 85.5133%\n",
      "layer   3  Sparsity: 81.9722%\n",
      "total_backward_count 1145430 real_backward_count 182934  15.971%\n",
      "lif layer 1 self.abs_max_v: 6110.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  5.182581/ 30.752975, val:  37.08%, val_best:  51.25%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 85.2947%\n",
      "layer   3  Sparsity: 81.8114%\n",
      "total_backward_count 1155220 real_backward_count 184342  15.957%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  5.270799/ 42.239471, val:  32.08%, val_best:  51.25%, tr:  96.73%, tr_best:  99.08%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 85.4547%\n",
      "layer   3  Sparsity: 82.0307%\n",
      "total_backward_count 1165010 real_backward_count 185788  15.947%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  5.112092/ 23.913685, val:  48.75%, val_best:  51.25%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.38 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 85.4937%\n",
      "layer   3  Sparsity: 82.1699%\n",
      "total_backward_count 1174800 real_backward_count 187196  15.934%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  5.115884/ 23.919744, val:  45.42%, val_best:  51.25%, tr:  96.22%, tr_best:  99.08%, epoch time: 81.03 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 85.7800%\n",
      "layer   3  Sparsity: 82.1703%\n",
      "total_backward_count 1184590 real_backward_count 188605  15.922%\n",
      "lif layer 1 self.abs_max_v: 6194.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  5.024271/ 33.915909, val:  41.67%, val_best:  51.25%, tr:  97.04%, tr_best:  99.08%, epoch time: 81.16 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 85.6384%\n",
      "layer   3  Sparsity: 81.7902%\n",
      "total_backward_count 1194380 real_backward_count 190004  15.908%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  5.041899/ 27.536467, val:  40.83%, val_best:  51.25%, tr:  96.22%, tr_best:  99.08%, epoch time: 81.30 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 85.7208%\n",
      "layer   3  Sparsity: 82.1679%\n",
      "total_backward_count 1204170 real_backward_count 191435  15.898%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  5.243177/ 25.427032, val:  42.08%, val_best:  51.25%, tr:  96.63%, tr_best:  99.08%, epoch time: 81.06 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 85.6922%\n",
      "layer   3  Sparsity: 82.0398%\n",
      "total_backward_count 1213960 real_backward_count 192882  15.889%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  5.135923/ 24.421879, val:  46.25%, val_best:  51.25%, tr:  96.73%, tr_best:  99.08%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 85.6951%\n",
      "layer   3  Sparsity: 82.9205%\n",
      "total_backward_count 1223750 real_backward_count 194344  15.881%\n",
      "lif layer 1 self.abs_max_v: 6244.5\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  4.800251/ 16.865501, val:  45.42%, val_best:  51.25%, tr:  97.04%, tr_best:  99.08%, epoch time: 80.88 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 85.5609%\n",
      "layer   3  Sparsity: 82.9451%\n",
      "total_backward_count 1233540 real_backward_count 195764  15.870%\n",
      "lif layer 1 self.abs_max_v: 6431.5\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  4.819448/ 28.429535, val:  39.58%, val_best:  51.25%, tr:  96.63%, tr_best:  99.08%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 85.7103%\n",
      "layer   3  Sparsity: 82.8049%\n",
      "total_backward_count 1243330 real_backward_count 197136  15.855%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  4.885378/ 29.286173, val:  30.42%, val_best:  51.25%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 85.7731%\n",
      "layer   3  Sparsity: 82.6481%\n",
      "total_backward_count 1253120 real_backward_count 198508  15.841%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  4.821504/ 25.215385, val:  42.08%, val_best:  51.25%, tr:  96.83%, tr_best:  99.08%, epoch time: 81.25 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 85.7521%\n",
      "layer   3  Sparsity: 83.1005%\n",
      "total_backward_count 1262910 real_backward_count 199893  15.828%\n",
      "lif layer 1 self.abs_max_v: 6511.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  4.750333/ 16.246241, val:  56.67%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 81.23 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 85.5533%\n",
      "layer   3  Sparsity: 83.0618%\n",
      "total_backward_count 1272700 real_backward_count 201322  15.818%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  4.892192/ 34.649792, val:  42.50%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 81.29 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 85.6963%\n",
      "layer   3  Sparsity: 82.7643%\n",
      "total_backward_count 1282490 real_backward_count 202743  15.809%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  4.960536/ 20.736443, val:  47.50%, val_best:  56.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 85.8596%\n",
      "layer   3  Sparsity: 82.7262%\n",
      "total_backward_count 1292280 real_backward_count 204127  15.796%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  4.652804/ 26.136055, val:  35.42%, val_best:  56.67%, tr:  96.42%, tr_best:  99.08%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 85.9801%\n",
      "layer   3  Sparsity: 83.0138%\n",
      "total_backward_count 1302070 real_backward_count 205458  15.779%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  4.643320/ 21.394304, val:  49.17%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 85.9059%\n",
      "layer   3  Sparsity: 83.0173%\n",
      "total_backward_count 1311860 real_backward_count 206844  15.767%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  4.771229/ 39.325047, val:  32.92%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 85.7657%\n",
      "layer   3  Sparsity: 83.1996%\n",
      "total_backward_count 1321650 real_backward_count 208264  15.758%\n",
      "lif layer 1 self.abs_max_v: 6625.5\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  5.049941/ 22.099262, val:  42.92%, val_best:  56.67%, tr:  95.51%, tr_best:  99.08%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 85.5475%\n",
      "layer   3  Sparsity: 83.0789%\n",
      "total_backward_count 1331440 real_backward_count 209710  15.751%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  4.779490/ 21.477655, val:  49.58%, val_best:  56.67%, tr:  95.71%, tr_best:  99.08%, epoch time: 81.73 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   2  Sparsity: 85.6063%\n",
      "layer   3  Sparsity: 83.0639%\n",
      "total_backward_count 1341230 real_backward_count 211096  15.739%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  4.853892/ 33.883865, val:  35.83%, val_best:  56.67%, tr:  95.40%, tr_best:  99.08%, epoch time: 81.51 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 85.5046%\n",
      "layer   3  Sparsity: 82.6379%\n",
      "total_backward_count 1351020 real_backward_count 212459  15.726%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  5.002721/ 20.415564, val:  43.33%, val_best:  56.67%, tr:  96.63%, tr_best:  99.08%, epoch time: 81.08 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 85.6093%\n",
      "layer   3  Sparsity: 82.9117%\n",
      "total_backward_count 1360810 real_backward_count 213857  15.715%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  4.783624/ 34.759823, val:  33.75%, val_best:  56.67%, tr:  95.51%, tr_best:  99.08%, epoch time: 82.36 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   2  Sparsity: 85.6053%\n",
      "layer   3  Sparsity: 83.0721%\n",
      "total_backward_count 1370600 real_backward_count 215272  15.706%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  5.112383/ 27.870466, val:  37.92%, val_best:  56.67%, tr:  95.30%, tr_best:  99.08%, epoch time: 81.28 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 85.3962%\n",
      "layer   3  Sparsity: 82.6788%\n",
      "total_backward_count 1380390 real_backward_count 216675  15.697%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  4.856469/ 17.948147, val:  47.50%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.51 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 85.6438%\n",
      "layer   3  Sparsity: 83.2544%\n",
      "total_backward_count 1390180 real_backward_count 218051  15.685%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  4.643724/ 29.996311, val:  37.92%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 81.14 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 85.9538%\n",
      "layer   3  Sparsity: 83.7609%\n",
      "total_backward_count 1399970 real_backward_count 219473  15.677%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  4.743188/ 22.024872, val:  42.08%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.85 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 85.8659%\n",
      "layer   3  Sparsity: 83.6566%\n",
      "total_backward_count 1409760 real_backward_count 220886  15.668%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  4.678410/ 25.250200, val:  38.75%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 85.7513%\n",
      "layer   3  Sparsity: 83.4772%\n",
      "total_backward_count 1419550 real_backward_count 222265  15.657%\n",
      "lif layer 1 self.abs_max_v: 6696.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  4.719051/ 23.045643, val:  39.58%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 85.5957%\n",
      "layer   3  Sparsity: 83.8262%\n",
      "total_backward_count 1429340 real_backward_count 223711  15.651%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  4.565329/ 25.275047, val:  37.50%, val_best:  56.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 85.7418%\n",
      "layer   3  Sparsity: 84.1658%\n",
      "total_backward_count 1439130 real_backward_count 225129  15.643%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  4.654329/ 25.472656, val:  48.33%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.82 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 85.8322%\n",
      "layer   3  Sparsity: 83.9075%\n",
      "total_backward_count 1448920 real_backward_count 226590  15.639%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  4.888052/ 28.574717, val:  44.17%, val_best:  56.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 85.7904%\n",
      "layer   3  Sparsity: 83.6901%\n",
      "total_backward_count 1458710 real_backward_count 228056  15.634%\n",
      "fc layer 1 self.abs_max_out: 4608.0\n",
      "lif layer 1 self.abs_max_v: 6777.5\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  4.812620/ 20.656858, val:  42.08%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 85.7220%\n",
      "layer   3  Sparsity: 83.9290%\n",
      "total_backward_count 1468500 real_backward_count 229534  15.631%\n",
      "lif layer 1 self.abs_max_v: 7056.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  4.631598/ 33.446110, val:  29.58%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 80.89 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 85.7998%\n",
      "layer   3  Sparsity: 83.9170%\n",
      "total_backward_count 1478290 real_backward_count 230989  15.625%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  4.710773/ 20.783131, val:  38.75%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 85.7230%\n",
      "layer   3  Sparsity: 83.6963%\n",
      "total_backward_count 1488080 real_backward_count 232426  15.619%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  4.732343/ 17.515596, val:  46.67%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 85.4980%\n",
      "layer   3  Sparsity: 83.7015%\n",
      "total_backward_count 1497870 real_backward_count 233866  15.613%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  4.742412/ 22.196115, val:  40.00%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 85.3080%\n",
      "layer   3  Sparsity: 83.6877%\n",
      "total_backward_count 1507660 real_backward_count 235311  15.608%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  4.495680/ 18.070871, val:  45.42%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 85.5250%\n",
      "layer   3  Sparsity: 83.8027%\n",
      "total_backward_count 1517450 real_backward_count 236670  15.597%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  4.658248/ 21.355734, val:  42.08%, val_best:  56.67%, tr:  95.71%, tr_best:  99.08%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 85.4780%\n",
      "layer   3  Sparsity: 83.7411%\n",
      "total_backward_count 1527240 real_backward_count 238097  15.590%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  4.722123/ 35.632000, val:  34.58%, val_best:  56.67%, tr:  94.89%, tr_best:  99.08%, epoch time: 80.56 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   2  Sparsity: 85.4752%\n",
      "layer   3  Sparsity: 83.4265%\n",
      "total_backward_count 1537030 real_backward_count 239496  15.582%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  4.681300/ 25.561054, val:  42.50%, val_best:  56.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 85.7360%\n",
      "layer   3  Sparsity: 83.3293%\n",
      "total_backward_count 1546820 real_backward_count 240862  15.571%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  4.781634/ 23.983391, val:  39.58%, val_best:  56.67%, tr:  94.79%, tr_best:  99.08%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   2  Sparsity: 85.6455%\n",
      "layer   3  Sparsity: 82.9685%\n",
      "total_backward_count 1556610 real_backward_count 242267  15.564%\n",
      "fc layer 1 self.abs_max_out: 4644.0\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  4.892488/ 22.283215, val:  42.92%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1178%\n",
      "layer   2  Sparsity: 85.7996%\n",
      "layer   3  Sparsity: 83.0558%\n",
      "total_backward_count 1566400 real_backward_count 243701  15.558%\n",
      "fc layer 1 self.abs_max_out: 4692.0\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  4.599647/ 28.545374, val:  37.92%, val_best:  56.67%, tr:  96.12%, tr_best:  99.08%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 85.5211%\n",
      "layer   3  Sparsity: 83.2076%\n",
      "total_backward_count 1576190 real_backward_count 245056  15.547%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  4.945921/ 28.132793, val:  40.42%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 85.4951%\n",
      "layer   3  Sparsity: 83.4564%\n",
      "total_backward_count 1585980 real_backward_count 246496  15.542%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  4.647532/ 18.388163, val:  42.50%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 85.5151%\n",
      "layer   3  Sparsity: 83.5365%\n",
      "total_backward_count 1595770 real_backward_count 247843  15.531%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  4.827754/ 27.715071, val:  42.50%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.51 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 85.5017%\n",
      "layer   3  Sparsity: 83.3248%\n",
      "total_backward_count 1605560 real_backward_count 249242  15.524%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  4.630681/ 20.729321, val:  49.58%, val_best:  56.67%, tr:  94.79%, tr_best:  99.08%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 85.3392%\n",
      "layer   3  Sparsity: 83.5561%\n",
      "total_backward_count 1615350 real_backward_count 250630  15.516%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  4.714514/ 24.524090, val:  40.42%, val_best:  56.67%, tr:  95.30%, tr_best:  99.08%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 85.2430%\n",
      "layer   3  Sparsity: 83.5216%\n",
      "total_backward_count 1625140 real_backward_count 251986  15.505%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  4.588802/ 28.828306, val:  39.58%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 85.2894%\n",
      "layer   3  Sparsity: 83.6625%\n",
      "total_backward_count 1634930 real_backward_count 253377  15.498%\n",
      "lif layer 1 self.abs_max_v: 7089.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  4.699623/ 27.422705, val:  45.42%, val_best:  56.67%, tr:  95.71%, tr_best:  99.08%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 85.5661%\n",
      "layer   3  Sparsity: 83.4487%\n",
      "total_backward_count 1644720 real_backward_count 254748  15.489%\n",
      "fc layer 1 self.abs_max_out: 4700.0\n",
      "lif layer 1 self.abs_max_v: 7220.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  4.620831/ 41.983353, val:  34.58%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 85.6917%\n",
      "layer   3  Sparsity: 83.6128%\n",
      "total_backward_count 1654510 real_backward_count 256102  15.479%\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  4.678151/ 20.164181, val:  44.58%, val_best:  56.67%, tr:  96.73%, tr_best:  99.08%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 85.5914%\n",
      "layer   3  Sparsity: 83.5134%\n",
      "total_backward_count 1664300 real_backward_count 257469  15.470%\n",
      "lif layer 1 self.abs_max_v: 7318.5\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  4.867521/ 36.466999, val:  24.58%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 85.5620%\n",
      "layer   3  Sparsity: 83.5401%\n",
      "total_backward_count 1674090 real_backward_count 258913  15.466%\n",
      "fc layer 1 self.abs_max_out: 4789.0\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  4.529147/ 20.299784, val:  49.58%, val_best:  56.67%, tr:  96.83%, tr_best:  99.08%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0424%\n",
      "layer   2  Sparsity: 85.3677%\n",
      "layer   3  Sparsity: 83.6674%\n",
      "total_backward_count 1683880 real_backward_count 260288  15.458%\n",
      "fc layer 1 self.abs_max_out: 4811.0\n",
      "lif layer 1 self.abs_max_v: 7666.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  4.745753/ 26.114660, val:  44.58%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 85.4323%\n",
      "layer   3  Sparsity: 83.5695%\n",
      "total_backward_count 1693670 real_backward_count 261700  15.452%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  4.658360/ 23.612717, val:  40.00%, val_best:  56.67%, tr:  96.53%, tr_best:  99.08%, epoch time: 80.90 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 85.4072%\n",
      "layer   3  Sparsity: 83.0907%\n",
      "total_backward_count 1703460 real_backward_count 263022  15.440%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  4.711749/ 31.254934, val:  47.92%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 85.4525%\n",
      "layer   3  Sparsity: 83.7676%\n",
      "total_backward_count 1713250 real_backward_count 264435  15.435%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  4.651612/ 23.313618, val:  45.00%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 85.5100%\n",
      "layer   3  Sparsity: 83.4836%\n",
      "total_backward_count 1723040 real_backward_count 265832  15.428%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  4.824456/ 30.107378, val:  41.67%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   2  Sparsity: 85.2779%\n",
      "layer   3  Sparsity: 83.1511%\n",
      "total_backward_count 1732830 real_backward_count 267222  15.421%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  5.021098/ 28.313303, val:  47.08%, val_best:  56.67%, tr:  96.22%, tr_best:  99.08%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0312%\n",
      "layer   2  Sparsity: 85.2318%\n",
      "layer   3  Sparsity: 82.8817%\n",
      "total_backward_count 1742620 real_backward_count 268626  15.415%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  4.911074/ 26.317419, val:  44.17%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.44 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   2  Sparsity: 85.2669%\n",
      "layer   3  Sparsity: 83.0774%\n",
      "total_backward_count 1752410 real_backward_count 270012  15.408%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  4.875837/ 27.303324, val:  33.33%, val_best:  56.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   2  Sparsity: 85.3354%\n",
      "layer   3  Sparsity: 83.4707%\n",
      "total_backward_count 1762200 real_backward_count 271421  15.402%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  5.067132/ 25.035297, val:  47.50%, val_best:  56.67%, tr:  97.24%, tr_best:  99.08%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 85.1460%\n",
      "layer   3  Sparsity: 83.2764%\n",
      "total_backward_count 1771990 real_backward_count 272848  15.398%\n",
      "lif layer 1 self.abs_max_v: 7683.5\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  4.671474/ 36.889008, val:  34.58%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 85.0800%\n",
      "layer   3  Sparsity: 83.5175%\n",
      "total_backward_count 1781780 real_backward_count 274182  15.388%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  4.646395/ 22.331228, val:  45.83%, val_best:  56.67%, tr:  95.91%, tr_best:  99.08%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   2  Sparsity: 85.0051%\n",
      "layer   3  Sparsity: 83.4119%\n",
      "total_backward_count 1791570 real_backward_count 275541  15.380%\n",
      "lif layer 1 self.abs_max_v: 7701.5\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  4.758710/ 21.899557, val:  42.92%, val_best:  56.67%, tr:  96.32%, tr_best:  99.08%, epoch time: 81.02 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   2  Sparsity: 85.1831%\n",
      "layer   3  Sparsity: 84.0151%\n",
      "total_backward_count 1801360 real_backward_count 276957  15.375%\n",
      "lif layer 1 self.abs_max_v: 7776.0\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  4.517400/ 29.797112, val:  37.92%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   2  Sparsity: 85.0580%\n",
      "layer   3  Sparsity: 83.4626%\n",
      "total_backward_count 1811150 real_backward_count 278279  15.365%\n",
      "lif layer 1 self.abs_max_v: 7955.5\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  4.934904/ 22.791498, val:  36.67%, val_best:  56.67%, tr:  93.97%, tr_best:  99.08%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 85.1559%\n",
      "layer   3  Sparsity: 83.0091%\n",
      "total_backward_count 1820940 real_backward_count 279670  15.359%\n",
      "lif layer 1 self.abs_max_v: 8148.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  5.104313/ 19.316042, val:  53.33%, val_best:  56.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 85.2424%\n",
      "layer   3  Sparsity: 82.7064%\n",
      "total_backward_count 1830730 real_backward_count 281120  15.356%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  4.935713/ 29.218687, val:  34.58%, val_best:  56.67%, tr:  95.20%, tr_best:  99.08%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 85.3070%\n",
      "layer   3  Sparsity: 83.0503%\n",
      "total_backward_count 1840520 real_backward_count 282551  15.352%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  4.546521/ 23.462317, val:  51.67%, val_best:  56.67%, tr:  95.30%, tr_best:  99.08%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 85.3807%\n",
      "layer   3  Sparsity: 83.3116%\n",
      "total_backward_count 1850310 real_backward_count 283932  15.345%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  4.707015/ 20.518847, val:  47.50%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 85.3900%\n",
      "layer   3  Sparsity: 83.1638%\n",
      "total_backward_count 1860100 real_backward_count 285279  15.337%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  4.388779/ 26.635706, val:  45.00%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 85.3660%\n",
      "layer   3  Sparsity: 83.1906%\n",
      "total_backward_count 1869890 real_backward_count 286602  15.327%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  4.658320/ 36.709591, val:  29.58%, val_best:  56.67%, tr:  96.02%, tr_best:  99.08%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 85.1265%\n",
      "layer   3  Sparsity: 83.3216%\n",
      "total_backward_count 1879680 real_backward_count 288007  15.322%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  4.796947/ 31.453609, val:  33.75%, val_best:  56.67%, tr:  94.59%, tr_best:  99.08%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 84.8381%\n",
      "layer   3  Sparsity: 83.0986%\n",
      "total_backward_count 1889470 real_backward_count 289411  15.317%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  4.575575/ 23.575731, val:  39.17%, val_best:  56.67%, tr:  95.51%, tr_best:  99.08%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 84.8771%\n",
      "layer   3  Sparsity: 82.9188%\n",
      "total_backward_count 1899260 real_backward_count 290758  15.309%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  4.657437/ 27.779688, val:  40.42%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.50 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   2  Sparsity: 84.9726%\n",
      "layer   3  Sparsity: 83.0343%\n",
      "total_backward_count 1909050 real_backward_count 292103  15.301%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  4.503477/ 28.475954, val:  49.58%, val_best:  56.67%, tr:  94.69%, tr_best:  99.08%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 85.2415%\n",
      "layer   3  Sparsity: 83.3292%\n",
      "total_backward_count 1918840 real_backward_count 293455  15.293%\n",
      "lif layer 1 self.abs_max_v: 8153.0\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  4.731941/ 30.649475, val:  38.33%, val_best:  56.67%, tr:  95.81%, tr_best:  99.08%, epoch time: 80.55 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 85.2312%\n",
      "layer   3  Sparsity: 82.9946%\n",
      "total_backward_count 1928630 real_backward_count 294853  15.288%\n",
      "lif layer 1 self.abs_max_v: 8158.5\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  4.725361/ 25.930115, val:  52.08%, val_best:  56.67%, tr:  94.28%, tr_best:  99.08%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 85.1103%\n",
      "layer   3  Sparsity: 82.8950%\n",
      "total_backward_count 1938420 real_backward_count 296259  15.284%\n",
      "fc layer 1 self.abs_max_out: 4875.0\n",
      "lif layer 1 self.abs_max_v: 8402.0\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  4.466784/ 18.744143, val:  41.67%, val_best:  56.67%, tr:  95.61%, tr_best:  99.08%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 85.2530%\n",
      "layer   3  Sparsity: 83.0015%\n",
      "total_backward_count 1948210 real_backward_count 297566  15.274%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  4.365493/ 30.779617, val:  42.92%, val_best:  56.67%, tr:  96.32%, tr_best:  99.08%, epoch time: 80.29 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   2  Sparsity: 85.1222%\n",
      "layer   3  Sparsity: 83.3680%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239ab0b357fc44d1a25624cb612893aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.96323</td></tr><tr><td>tr_epoch_loss</td><td>4.36549</td></tr><tr><td>val_acc_best</td><td>0.56667</td></tr><tr><td>val_acc_now</td><td>0.42917</td></tr><tr><td>val_loss</td><td>30.77962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-sweep-863</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xd12qc31' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xd12qc31</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251212_063744-xd12qc31/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vvv6g7u7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_110627-vvv6g7u7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vvv6g7u7' target=\"_blank\">noble-sweep-872</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/ngjk7mes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vvv6g7u7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vvv6g7u7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251212_110636_579', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 64, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 0.25, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 0.5, 'lif_layer_v_threshold2': 64, 'init_scaling': [0.25, 0.03125, 0.03125], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 0.25, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 0.5, self.v_threshold 64\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.03125, 0.03125])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.25, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.03125, 0.03125])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=64, v_reset=10000, sg_width=0.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[0.25, 0.03125, 0.03125])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 373.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 39.0\n",
      "lif layer 2 self.abs_max_v: 39.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 457.5\n",
      "fc layer 2 self.abs_max_out: 56.0\n",
      "lif layer 2 self.abs_max_v: 70.5\n",
      "fc layer 1 self.abs_max_out: 410.0\n",
      "lif layer 1 self.abs_max_v: 542.0\n",
      "fc layer 2 self.abs_max_out: 66.0\n",
      "lif layer 2 self.abs_max_v: 101.5\n",
      "fc layer 1 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 571.0\n",
      "fc layer 2 self.abs_max_out: 101.0\n",
      "lif layer 2 self.abs_max_v: 134.5\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "lif layer 1 self.abs_max_v: 654.0\n",
      "fc layer 2 self.abs_max_out: 119.0\n",
      "lif layer 2 self.abs_max_v: 171.0\n",
      "fc layer 3 self.abs_max_out: 24.0\n",
      "lif layer 1 self.abs_max_v: 669.5\n",
      "fc layer 2 self.abs_max_out: 174.0\n",
      "lif layer 2 self.abs_max_v: 228.5\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "lif layer 1 self.abs_max_v: 867.0\n",
      "fc layer 2 self.abs_max_out: 199.0\n",
      "lif layer 2 self.abs_max_v: 273.5\n",
      "lif layer 2 self.abs_max_v: 279.0\n",
      "fc layer 1 self.abs_max_out: 675.0\n",
      "fc layer 1 self.abs_max_out: 744.0\n",
      "lif layer 1 self.abs_max_v: 880.5\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "lif layer 1 self.abs_max_v: 995.5\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "fc layer 1 self.abs_max_out: 756.0\n",
      "fc layer 3 self.abs_max_out: 27.0\n",
      "lif layer 1 self.abs_max_v: 1055.5\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 1 self.abs_max_out: 765.0\n",
      "lif layer 1 self.abs_max_v: 1104.5\n",
      "lif layer 2 self.abs_max_v: 306.0\n",
      "fc layer 3 self.abs_max_out: 29.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "lif layer 1 self.abs_max_v: 1394.5\n",
      "lif layer 2 self.abs_max_v: 328.0\n",
      "lif layer 2 self.abs_max_v: 330.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 345.0\n",
      "fc layer 2 self.abs_max_out: 207.0\n",
      "fc layer 2 self.abs_max_out: 213.0\n",
      "lif layer 2 self.abs_max_v: 357.5\n",
      "lif layer 2 self.abs_max_v: 358.5\n",
      "fc layer 3 self.abs_max_out: 38.0\n",
      "fc layer 1 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "lif layer 2 self.abs_max_v: 373.5\n",
      "lif layer 2 self.abs_max_v: 401.5\n",
      "lif layer 2 self.abs_max_v: 413.0\n",
      "lif layer 2 self.abs_max_v: 425.5\n",
      "lif layer 1 self.abs_max_v: 1417.0\n",
      "lif layer 2 self.abs_max_v: 433.0\n",
      "fc layer 3 self.abs_max_out: 44.0\n",
      "lif layer 1 self.abs_max_v: 1419.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "fc layer 2 self.abs_max_out: 234.0\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "fc layer 1 self.abs_max_out: 1118.0\n",
      "lif layer 1 self.abs_max_v: 1646.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 2 self.abs_max_out: 249.0\n",
      "lif layer 2 self.abs_max_v: 440.5\n",
      "fc layer 2 self.abs_max_out: 299.0\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 441.0\n",
      "lif layer 2 self.abs_max_v: 457.0\n",
      "lif layer 2 self.abs_max_v: 474.5\n",
      "lif layer 2 self.abs_max_v: 477.5\n",
      "lif layer 2 self.abs_max_v: 529.5\n",
      "lif layer 2 self.abs_max_v: 534.0\n",
      "lif layer 2 self.abs_max_v: 536.0\n",
      "lif layer 2 self.abs_max_v: 545.5\n",
      "lif layer 2 self.abs_max_v: 547.0\n",
      "fc layer 2 self.abs_max_out: 326.0\n",
      "lif layer 2 self.abs_max_v: 599.5\n",
      "fc layer 2 self.abs_max_out: 339.0\n",
      "lif layer 2 self.abs_max_v: 639.0\n",
      "lif layer 2 self.abs_max_v: 655.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "fc layer 1 self.abs_max_out: 1144.0\n",
      "lif layer 1 self.abs_max_v: 1731.5\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "lif layer 1 self.abs_max_v: 1746.5\n",
      "fc layer 2 self.abs_max_out: 374.0\n",
      "fc layer 3 self.abs_max_out: 180.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "fc layer 1 self.abs_max_out: 1308.0\n",
      "fc layer 2 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 397.0\n",
      "fc layer 1 self.abs_max_out: 1349.0\n",
      "fc layer 1 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 1 self.abs_max_out: 1460.0\n",
      "lif layer 1 self.abs_max_v: 2059.0\n",
      "fc layer 1 self.abs_max_out: 1463.0\n",
      "lif layer 1 self.abs_max_v: 2154.0\n",
      "fc layer 2 self.abs_max_out: 440.0\n",
      "fc layer 2 self.abs_max_out: 446.0\n",
      "fc layer 1 self.abs_max_out: 1549.0\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "fc layer 2 self.abs_max_out: 517.0\n",
      "fc layer 2 self.abs_max_out: 536.0\n",
      "fc layer 2 self.abs_max_out: 541.0\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "fc layer 1 self.abs_max_out: 1566.0\n",
      "fc layer 1 self.abs_max_out: 1606.0\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "fc layer 1 self.abs_max_out: 1634.0\n",
      "fc layer 1 self.abs_max_out: 1705.0\n",
      "fc layer 1 self.abs_max_out: 1710.0\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 1 self.abs_max_v: 2206.5\n",
      "fc layer 1 self.abs_max_out: 1890.0\n",
      "fc layer 1 self.abs_max_out: 2022.0\n",
      "fc layer 1 self.abs_max_out: 2059.0\n",
      "fc layer 1 self.abs_max_out: 2098.0\n",
      "fc layer 1 self.abs_max_out: 2246.0\n",
      "lif layer 1 self.abs_max_v: 2246.0\n",
      "lif layer 1 self.abs_max_v: 2606.5\n",
      "lif layer 1 self.abs_max_v: 2692.5\n",
      "fc layer 1 self.abs_max_out: 2277.0\n",
      "fc layer 1 self.abs_max_out: 2403.0\n",
      "lif layer 2 self.abs_max_v: 712.5\n",
      "lif layer 2 self.abs_max_v: 754.5\n",
      "lif layer 2 self.abs_max_v: 802.5\n",
      "fc layer 2 self.abs_max_out: 598.0\n",
      "fc layer 2 self.abs_max_out: 615.0\n",
      "fc layer 2 self.abs_max_out: 652.0\n",
      "lif layer 1 self.abs_max_v: 2707.5\n",
      "fc layer 2 self.abs_max_out: 671.0\n",
      "fc layer 2 self.abs_max_out: 740.0\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "lif layer 2 self.abs_max_v: 817.5\n",
      "lif layer 2 self.abs_max_v: 838.0\n",
      "fc layer 1 self.abs_max_out: 2455.0\n",
      "lif layer 1 self.abs_max_v: 2740.0\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 1 self.abs_max_out: 2459.0\n",
      "fc layer 1 self.abs_max_out: 2626.0\n",
      "lif layer 1 self.abs_max_v: 2876.5\n",
      "fc layer 1 self.abs_max_out: 2721.0\n",
      "fc layer 1 self.abs_max_out: 2886.0\n",
      "lif layer 1 self.abs_max_v: 2886.0\n",
      "lif layer 1 self.abs_max_v: 2986.0\n",
      "lif layer 1 self.abs_max_v: 3085.0\n",
      "lif layer 2 self.abs_max_v: 841.0\n",
      "lif layer 2 self.abs_max_v: 864.5\n",
      "lif layer 2 self.abs_max_v: 873.5\n",
      "lif layer 2 self.abs_max_v: 889.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 1 self.abs_max_v: 3292.0\n",
      "lif layer 2 self.abs_max_v: 892.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "lif layer 2 self.abs_max_v: 928.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "lif layer 2 self.abs_max_v: 929.5\n",
      "lif layer 2 self.abs_max_v: 960.0\n",
      "fc layer 2 self.abs_max_out: 766.0\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 1 self.abs_max_out: 3025.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 850.0\n",
      "lif layer 2 self.abs_max_v: 961.0\n",
      "lif layer 2 self.abs_max_v: 968.5\n",
      "fc layer 1 self.abs_max_out: 3041.0\n",
      "fc layer 1 self.abs_max_out: 3185.0\n",
      "lif layer 2 self.abs_max_v: 979.0\n",
      "lif layer 2 self.abs_max_v: 979.5\n",
      "fc layer 3 self.abs_max_out: 293.0\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "lif layer 1 self.abs_max_v: 3408.0\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "lif layer 1 self.abs_max_v: 3431.0\n",
      "fc layer 1 self.abs_max_out: 3463.0\n",
      "lif layer 1 self.abs_max_v: 3839.5\n",
      "lif layer 2 self.abs_max_v: 996.0\n",
      "lif layer 2 self.abs_max_v: 1012.0\n",
      "fc layer 1 self.abs_max_out: 3532.0\n",
      "lif layer 2 self.abs_max_v: 1062.5\n",
      "lif layer 2 self.abs_max_v: 1063.5\n",
      "lif layer 2 self.abs_max_v: 1072.0\n",
      "lif layer 2 self.abs_max_v: 1088.0\n",
      "lif layer 2 self.abs_max_v: 1107.0\n",
      "lif layer 2 self.abs_max_v: 1129.5\n",
      "lif layer 2 self.abs_max_v: 1134.0\n",
      "lif layer 2 self.abs_max_v: 1141.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "lif layer 1 self.abs_max_v: 4141.0\n",
      "lif layer 1 self.abs_max_v: 4849.5\n",
      "fc layer 1 self.abs_max_out: 3568.0\n",
      "lif layer 2 self.abs_max_v: 1174.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "lif layer 2 self.abs_max_v: 1338.0\n",
      "lif layer 2 self.abs_max_v: 1468.5\n",
      "lif layer 2 self.abs_max_v: 1578.5\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 2 self.abs_max_out: 923.0\n",
      "fc layer 2 self.abs_max_out: 982.0\n",
      "fc layer 2 self.abs_max_out: 1006.0\n",
      "fc layer 2 self.abs_max_out: 1011.0\n",
      "fc layer 2 self.abs_max_out: 1052.0\n",
      "fc layer 1 self.abs_max_out: 3643.0\n",
      "fc layer 1 self.abs_max_out: 3732.0\n",
      "fc layer 1 self.abs_max_out: 3808.0\n",
      "fc layer 1 self.abs_max_out: 3943.0\n",
      "fc layer 1 self.abs_max_out: 4216.0\n",
      "fc layer 1 self.abs_max_out: 4236.0\n",
      "fc layer 1 self.abs_max_out: 4374.0\n",
      "fc layer 1 self.abs_max_out: 4379.0\n",
      "fc layer 1 self.abs_max_out: 4383.0\n",
      "fc layer 1 self.abs_max_out: 4396.0\n",
      "lif layer 1 self.abs_max_v: 4931.5\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "fc layer 2 self.abs_max_out: 1409.0\n",
      "lif layer 2 self.abs_max_v: 1635.0\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "lif layer 2 self.abs_max_v: 1636.0\n",
      "fc layer 1 self.abs_max_out: 4481.0\n",
      "fc layer 1 self.abs_max_out: 4496.0\n",
      "fc layer 1 self.abs_max_out: 4639.0\n",
      "fc layer 2 self.abs_max_out: 1437.0\n",
      "lif layer 2 self.abs_max_v: 1673.5\n",
      "lif layer 2 self.abs_max_v: 1682.0\n",
      "lif layer 2 self.abs_max_v: 1698.0\n",
      "lif layer 1 self.abs_max_v: 5535.5\n",
      "fc layer 1 self.abs_max_out: 4662.0\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "lif layer 2 self.abs_max_v: 1753.0\n",
      "lif layer 2 self.abs_max_v: 1833.5\n",
      "lif layer 2 self.abs_max_v: 1834.5\n",
      "lif layer 2 self.abs_max_v: 1871.5\n",
      "lif layer 2 self.abs_max_v: 1884.0\n",
      "lif layer 1 self.abs_max_v: 5892.0\n",
      "lif layer 1 self.abs_max_v: 6340.0\n",
      "lif layer 1 self.abs_max_v: 6507.0\n",
      "lif layer 2 self.abs_max_v: 1982.0\n",
      "lif layer 2 self.abs_max_v: 2028.0\n",
      "lif layer 2 self.abs_max_v: 2051.0\n",
      "fc layer 1 self.abs_max_out: 4814.0\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 1 self.abs_max_out: 4946.0\n",
      "fc layer 1 self.abs_max_out: 5125.0\n",
      "fc layer 1 self.abs_max_out: 5248.0\n",
      "fc layer 2 self.abs_max_out: 1602.0\n",
      "lif layer 1 self.abs_max_v: 6640.0\n",
      "lif layer 2 self.abs_max_v: 2074.0\n",
      "lif layer 1 self.abs_max_v: 6835.5\n",
      "lif layer 1 self.abs_max_v: 6928.0\n",
      "fc layer 1 self.abs_max_out: 5314.0\n",
      "fc layer 1 self.abs_max_out: 5402.0\n",
      "fc layer 1 self.abs_max_out: 5520.0\n",
      "fc layer 1 self.abs_max_out: 5547.0\n",
      "lif layer 1 self.abs_max_v: 7325.0\n",
      "lif layer 1 self.abs_max_v: 7584.5\n",
      "lif layer 1 self.abs_max_v: 7902.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 10.900624/135.271652, val:  16.67%, val_best:  16.67%, tr:  98.67%, tr_best:  98.67%, epoch time: 81.78 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 61.8471%\n",
      "layer   3  Sparsity: 62.6944%\n",
      "total_backward_count 9790 real_backward_count 1621  16.558%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1621.0\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "lif layer 2 self.abs_max_v: 2216.5\n",
      "lif layer 2 self.abs_max_v: 2260.5\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "fc layer 2 self.abs_max_out: 1792.0\n",
      "lif layer 2 self.abs_max_v: 2311.0\n",
      "fc layer 2 self.abs_max_out: 1834.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 1 self.abs_max_out: 5606.0\n",
      "fc layer 1 self.abs_max_out: 5754.0\n",
      "lif layer 2 self.abs_max_v: 2320.5\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "fc layer 2 self.abs_max_out: 2099.0\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "fc layer 1 self.abs_max_out: 6033.0\n",
      "fc layer 1 self.abs_max_out: 6187.0\n",
      "fc layer 1 self.abs_max_out: 6488.0\n",
      "lif layer 1 self.abs_max_v: 7941.5\n",
      "lif layer 2 self.abs_max_v: 2324.5\n",
      "lif layer 2 self.abs_max_v: 2363.5\n",
      "lif layer 2 self.abs_max_v: 2401.0\n",
      "fc layer 2 self.abs_max_out: 2145.0\n",
      "lif layer 2 self.abs_max_v: 2406.0\n",
      "lif layer 1 self.abs_max_v: 7995.5\n",
      "lif layer 2 self.abs_max_v: 2443.0\n",
      "lif layer 2 self.abs_max_v: 2481.5\n",
      "lif layer 2 self.abs_max_v: 2501.0\n",
      "lif layer 2 self.abs_max_v: 2504.5\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "fc layer 1 self.abs_max_out: 6527.0\n",
      "lif layer 2 self.abs_max_v: 2520.5\n",
      "lif layer 2 self.abs_max_v: 2588.5\n",
      "lif layer 1 self.abs_max_v: 8016.5\n",
      "lif layer 1 self.abs_max_v: 8146.5\n",
      "lif layer 2 self.abs_max_v: 2606.0\n",
      "lif layer 2 self.abs_max_v: 2609.0\n",
      "lif layer 2 self.abs_max_v: 2621.5\n",
      "fc layer 1 self.abs_max_out: 6759.0\n",
      "fc layer 1 self.abs_max_out: 6787.0\n",
      "fc layer 1 self.abs_max_out: 6853.0\n",
      "fc layer 1 self.abs_max_out: 6926.0\n",
      "fc layer 1 self.abs_max_out: 7088.0\n",
      "lif layer 2 self.abs_max_v: 2665.5\n",
      "lif layer 2 self.abs_max_v: 2672.5\n",
      "lif layer 2 self.abs_max_v: 2680.5\n",
      "lif layer 2 self.abs_max_v: 2721.5\n",
      "lif layer 1 self.abs_max_v: 8374.5\n",
      "lif layer 1 self.abs_max_v: 8435.5\n",
      "fc layer 2 self.abs_max_out: 2221.0\n",
      "fc layer 2 self.abs_max_out: 2264.0\n",
      "fc layer 2 self.abs_max_out: 2368.0\n",
      "fc layer 2 self.abs_max_out: 2468.0\n",
      "lif layer 1 self.abs_max_v: 8900.5\n",
      "lif layer 2 self.abs_max_v: 2742.5\n",
      "lif layer 2 self.abs_max_v: 2753.5\n",
      "lif layer 2 self.abs_max_v: 2759.0\n",
      "lif layer 2 self.abs_max_v: 2761.5\n",
      "lif layer 2 self.abs_max_v: 2764.0\n",
      "lif layer 2 self.abs_max_v: 2829.0\n",
      "lif layer 2 self.abs_max_v: 2841.5\n",
      "lif layer 2 self.abs_max_v: 2859.0\n",
      "lif layer 2 self.abs_max_v: 2885.5\n",
      "lif layer 2 self.abs_max_v: 2921.0\n",
      "lif layer 1 self.abs_max_v: 8917.0\n",
      "lif layer 1 self.abs_max_v: 9693.5\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "lif layer 1 self.abs_max_v: 10069.5\n",
      "lif layer 1 self.abs_max_v: 10570.5\n",
      "lif layer 1 self.abs_max_v: 10988.5\n",
      "lif layer 1 self.abs_max_v: 11228.5\n",
      "fc layer 2 self.abs_max_out: 2528.0\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "fc layer 2 self.abs_max_out: 2618.0\n",
      "fc layer 2 self.abs_max_out: 2696.0\n",
      "fc layer 2 self.abs_max_out: 2700.0\n",
      "lif layer 2 self.abs_max_v: 2928.0\n",
      "lif layer 2 self.abs_max_v: 2940.5\n",
      "lif layer 2 self.abs_max_v: 2962.5\n",
      "lif layer 2 self.abs_max_v: 2966.0\n",
      "fc layer 1 self.abs_max_out: 7207.0\n",
      "fc layer 1 self.abs_max_out: 7238.0\n",
      "fc layer 1 self.abs_max_out: 7321.0\n",
      "lif layer 2 self.abs_max_v: 3053.5\n",
      "lif layer 2 self.abs_max_v: 3061.5\n",
      "lif layer 2 self.abs_max_v: 3160.0\n",
      "lif layer 2 self.abs_max_v: 3188.0\n",
      "lif layer 2 self.abs_max_v: 3247.5\n",
      "lif layer 2 self.abs_max_v: 3255.5\n",
      "lif layer 2 self.abs_max_v: 3281.0\n",
      "fc layer 1 self.abs_max_out: 7804.0\n",
      "fc layer 2 self.abs_max_out: 2719.0\n",
      "fc layer 2 self.abs_max_out: 2779.0\n",
      "fc layer 2 self.abs_max_out: 2804.0\n",
      "lif layer 2 self.abs_max_v: 3282.0\n",
      "lif layer 2 self.abs_max_v: 3295.0\n",
      "lif layer 2 self.abs_max_v: 3301.5\n",
      "lif layer 2 self.abs_max_v: 3317.0\n",
      "lif layer 2 self.abs_max_v: 3319.5\n",
      "lif layer 2 self.abs_max_v: 3338.0\n",
      "lif layer 2 self.abs_max_v: 3341.5\n",
      "lif layer 2 self.abs_max_v: 3350.0\n",
      "lif layer 2 self.abs_max_v: 3358.5\n",
      "lif layer 2 self.abs_max_v: 3367.0\n",
      "lif layer 2 self.abs_max_v: 3382.5\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "lif layer 2 self.abs_max_v: 3395.5\n",
      "lif layer 2 self.abs_max_v: 3431.0\n",
      "lif layer 2 self.abs_max_v: 3436.5\n",
      "fc layer 2 self.abs_max_out: 2923.0\n",
      "fc layer 2 self.abs_max_out: 3029.0\n",
      "lif layer 2 self.abs_max_v: 3438.5\n",
      "lif layer 2 self.abs_max_v: 3465.5\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 8245.0\n",
      "lif layer 1 self.abs_max_v: 11917.5\n",
      "fc layer 1 self.abs_max_out: 8267.0\n",
      "fc layer 1 self.abs_max_out: 8438.0\n",
      "fc layer 1 self.abs_max_out: 8527.0\n",
      "fc layer 1 self.abs_max_out: 8542.0\n",
      "fc layer 1 self.abs_max_out: 8559.0\n",
      "lif layer 1 self.abs_max_v: 12152.0\n",
      "lif layer 1 self.abs_max_v: 12438.0\n",
      "lif layer 1 self.abs_max_v: 12977.0\n",
      "lif layer 1 self.abs_max_v: 13837.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 13.025411/ 88.611374, val:  27.92%, val_best:  27.92%, tr:  98.77%, tr_best:  98.77%, epoch time: 81.01 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 58.5760%\n",
      "layer   3  Sparsity: 56.4727%\n",
      "total_backward_count 19580 real_backward_count 3085  15.756%\n",
      "lif layer 2 self.abs_max_v: 3469.5\n",
      "lif layer 2 self.abs_max_v: 3474.5\n",
      "lif layer 2 self.abs_max_v: 3514.0\n",
      "fc layer 2 self.abs_max_out: 3128.0\n",
      "fc layer 2 self.abs_max_out: 3140.0\n",
      "lif layer 2 self.abs_max_v: 3553.0\n",
      "lif layer 2 self.abs_max_v: 3573.5\n",
      "lif layer 2 self.abs_max_v: 3584.0\n",
      "lif layer 2 self.abs_max_v: 3589.0\n",
      "lif layer 2 self.abs_max_v: 3597.5\n",
      "lif layer 2 self.abs_max_v: 3614.5\n",
      "lif layer 2 self.abs_max_v: 3656.5\n",
      "lif layer 2 self.abs_max_v: 3688.5\n",
      "fc layer 2 self.abs_max_out: 3297.0\n",
      "lif layer 2 self.abs_max_v: 3751.5\n",
      "fc layer 2 self.abs_max_out: 3351.0\n",
      "fc layer 2 self.abs_max_out: 3373.0\n",
      "fc layer 2 self.abs_max_out: 3570.0\n",
      "fc layer 2 self.abs_max_out: 3650.0\n",
      "fc layer 2 self.abs_max_out: 3719.0\n",
      "fc layer 1 self.abs_max_out: 8990.0\n",
      "lif layer 1 self.abs_max_v: 13972.0\n",
      "fc layer 1 self.abs_max_out: 9039.0\n",
      "fc layer 2 self.abs_max_out: 3747.0\n",
      "lif layer 2 self.abs_max_v: 3780.5\n",
      "lif layer 2 self.abs_max_v: 3803.5\n",
      "lif layer 2 self.abs_max_v: 3807.0\n",
      "fc layer 2 self.abs_max_out: 3828.0\n",
      "lif layer 2 self.abs_max_v: 3828.0\n",
      "lif layer 2 self.abs_max_v: 3837.0\n",
      "lif layer 2 self.abs_max_v: 3870.5\n",
      "lif layer 2 self.abs_max_v: 3887.5\n",
      "lif layer 2 self.abs_max_v: 3888.0\n",
      "lif layer 2 self.abs_max_v: 3890.5\n",
      "fc layer 1 self.abs_max_out: 9734.0\n",
      "lif layer 2 self.abs_max_v: 3891.5\n",
      "lif layer 2 self.abs_max_v: 3910.0\n",
      "lif layer 2 self.abs_max_v: 4049.0\n",
      "lif layer 2 self.abs_max_v: 4100.5\n",
      "lif layer 2 self.abs_max_v: 4162.0\n",
      "fc layer 2 self.abs_max_out: 3901.0\n",
      "fc layer 2 self.abs_max_out: 3962.0\n",
      "fc layer 2 self.abs_max_out: 4043.0\n",
      "lif layer 2 self.abs_max_v: 4171.0\n",
      "lif layer 2 self.abs_max_v: 4173.0\n",
      "fc layer 1 self.abs_max_out: 9794.0\n",
      "lif layer 1 self.abs_max_v: 13980.0\n",
      "lif layer 1 self.abs_max_v: 15097.5\n",
      "lif layer 1 self.abs_max_v: 15251.0\n",
      "lif layer 1 self.abs_max_v: 16644.5\n",
      "lif layer 1 self.abs_max_v: 17712.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 14.035214/ 93.399437, val:  28.33%, val_best:  28.33%, tr:  98.06%, tr_best:  98.77%, epoch time: 80.94 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 58.3894%\n",
      "layer   3  Sparsity: 56.6188%\n",
      "total_backward_count 29370 real_backward_count 4595  15.645%\n",
      "lif layer 2 self.abs_max_v: 4178.0\n",
      "lif layer 2 self.abs_max_v: 4183.5\n",
      "lif layer 2 self.abs_max_v: 4209.0\n",
      "fc layer 1 self.abs_max_out: 9848.0\n",
      "fc layer 1 self.abs_max_out: 10170.0\n",
      "lif layer 2 self.abs_max_v: 4224.5\n",
      "lif layer 2 self.abs_max_v: 4285.0\n",
      "lif layer 2 self.abs_max_v: 4288.0\n",
      "lif layer 2 self.abs_max_v: 4303.0\n",
      "lif layer 2 self.abs_max_v: 4605.0\n",
      "lif layer 2 self.abs_max_v: 4746.5\n",
      "lif layer 2 self.abs_max_v: 4812.5\n",
      "lif layer 2 self.abs_max_v: 4833.0\n",
      "lif layer 2 self.abs_max_v: 4849.5\n",
      "fc layer 1 self.abs_max_out: 10243.0\n",
      "fc layer 1 self.abs_max_out: 10259.0\n",
      "fc layer 1 self.abs_max_out: 11471.0\n",
      "fc layer 1 self.abs_max_out: 11573.0\n",
      "fc layer 2 self.abs_max_out: 4210.0\n",
      "lif layer 2 self.abs_max_v: 4862.0\n",
      "lif layer 2 self.abs_max_v: 4864.0\n",
      "lif layer 2 self.abs_max_v: 4917.5\n",
      "lif layer 2 self.abs_max_v: 4958.0\n",
      "lif layer 2 self.abs_max_v: 4968.0\n",
      "lif layer 2 self.abs_max_v: 4973.0\n",
      "lif layer 2 self.abs_max_v: 4989.5\n",
      "lif layer 2 self.abs_max_v: 5036.0\n",
      "lif layer 2 self.abs_max_v: 5059.0\n",
      "lif layer 2 self.abs_max_v: 5070.5\n",
      "lif layer 2 self.abs_max_v: 5076.5\n",
      "fc layer 1 self.abs_max_out: 11733.0\n",
      "fc layer 1 self.abs_max_out: 11763.0\n",
      "lif layer 1 self.abs_max_v: 18633.0\n",
      "lif layer 1 self.abs_max_v: 19638.0\n",
      "lif layer 1 self.abs_max_v: 21014.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 12.851306/ 66.758682, val:  33.33%, val_best:  33.33%, tr:  97.85%, tr_best:  98.77%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 59.7721%\n",
      "layer   3  Sparsity: 58.6097%\n",
      "total_backward_count 39160 real_backward_count 6083  15.534%\n",
      "lif layer 2 self.abs_max_v: 5101.5\n",
      "lif layer 2 self.abs_max_v: 5263.0\n",
      "lif layer 2 self.abs_max_v: 5312.5\n",
      "lif layer 2 self.abs_max_v: 5401.5\n",
      "lif layer 2 self.abs_max_v: 5432.0\n",
      "lif layer 2 self.abs_max_v: 5448.0\n",
      "lif layer 2 self.abs_max_v: 5454.0\n",
      "fc layer 1 self.abs_max_out: 11926.0\n",
      "fc layer 1 self.abs_max_out: 12176.0\n",
      "lif layer 2 self.abs_max_v: 5522.5\n",
      "fc layer 1 self.abs_max_out: 13959.0\n",
      "lif layer 1 self.abs_max_v: 21349.5\n",
      "lif layer 1 self.abs_max_v: 23569.5\n",
      "lif layer 1 self.abs_max_v: 25130.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss: 12.154382/104.907722, val:  19.17%, val_best:  33.33%, tr:  98.77%, tr_best:  98.77%, epoch time: 80.59 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 60.7570%\n",
      "layer   3  Sparsity: 60.5432%\n",
      "total_backward_count 48950 real_backward_count 7558  15.440%\n",
      "lif layer 2 self.abs_max_v: 5807.0\n",
      "lif layer 2 self.abs_max_v: 6198.5\n",
      "lif layer 2 self.abs_max_v: 6314.5\n",
      "lif layer 2 self.abs_max_v: 6353.5\n",
      "fc layer 1 self.abs_max_out: 14094.0\n",
      "lif layer 1 self.abs_max_v: 26129.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss: 11.679516/ 89.106453, val:  27.50%, val_best:  33.33%, tr:  98.57%, tr_best:  98.77%, epoch time: 80.91 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   2  Sparsity: 61.8595%\n",
      "layer   3  Sparsity: 61.5146%\n",
      "total_backward_count 58740 real_backward_count 9001  15.323%\n",
      "lif layer 2 self.abs_max_v: 6705.0\n",
      "lif layer 2 self.abs_max_v: 7041.5\n",
      "lif layer 2 self.abs_max_v: 7154.0\n",
      "lif layer 2 self.abs_max_v: 7468.0\n",
      "fc layer 1 self.abs_max_out: 14509.0\n",
      "fc layer 1 self.abs_max_out: 15170.0\n",
      "lif layer 1 self.abs_max_v: 27780.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 11.850535/ 96.320366, val:  30.83%, val_best:  33.33%, tr:  98.57%, tr_best:  98.77%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 62.9650%\n",
      "layer   3  Sparsity: 60.2848%\n",
      "total_backward_count 68530 real_backward_count 10495  15.314%\n",
      "fc layer 2 self.abs_max_out: 4281.0\n",
      "fc layer 2 self.abs_max_out: 4499.0\n",
      "lif layer 2 self.abs_max_v: 7873.5\n",
      "lif layer 2 self.abs_max_v: 8436.0\n",
      "lif layer 2 self.abs_max_v: 8717.0\n",
      "lif layer 2 self.abs_max_v: 8802.5\n",
      "fc layer 1 self.abs_max_out: 15705.0\n",
      "lif layer 1 self.abs_max_v: 28593.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss: 12.006989/ 80.211044, val:  22.50%, val_best:  33.33%, tr:  98.98%, tr_best:  98.98%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 63.6456%\n",
      "layer   3  Sparsity: 60.4431%\n",
      "total_backward_count 78320 real_backward_count 11990  15.309%\n",
      "fc layer 2 self.abs_max_out: 4567.0\n",
      "fc layer 2 self.abs_max_out: 4596.0\n",
      "fc layer 2 self.abs_max_out: 4606.0\n",
      "fc layer 1 self.abs_max_out: 15850.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss: 12.595793/ 72.412636, val:  32.92%, val_best:  33.33%, tr:  98.16%, tr_best:  98.98%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0400%\n",
      "layer   2  Sparsity: 63.7022%\n",
      "layer   3  Sparsity: 60.5639%\n",
      "total_backward_count 88110 real_backward_count 13559  15.389%\n",
      "fc layer 2 self.abs_max_out: 4663.0\n",
      "fc layer 1 self.abs_max_out: 16239.0\n",
      "lif layer 1 self.abs_max_v: 29179.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss: 12.100830/ 72.393280, val:  37.50%, val_best:  37.50%, tr:  98.98%, tr_best:  98.98%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 64.3738%\n",
      "layer   3  Sparsity: 60.6336%\n",
      "total_backward_count 97900 real_backward_count 15059  15.382%\n",
      "fc layer 2 self.abs_max_out: 4780.0\n",
      "lif layer 2 self.abs_max_v: 8832.5\n",
      "lif layer 2 self.abs_max_v: 9196.5\n",
      "fc layer 2 self.abs_max_out: 4811.0\n",
      "fc layer 2 self.abs_max_out: 5000.0\n",
      "fc layer 2 self.abs_max_out: 5212.0\n",
      "fc layer 1 self.abs_max_out: 16839.0\n",
      "lif layer 1 self.abs_max_v: 30414.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss: 12.434195/ 88.521179, val:  29.58%, val_best:  37.50%, tr:  98.57%, tr_best:  98.98%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 65.1467%\n",
      "layer   3  Sparsity: 61.1074%\n",
      "total_backward_count 107690 real_backward_count 16606  15.420%\n",
      "fc layer 2 self.abs_max_out: 5227.0\n",
      "lif layer 2 self.abs_max_v: 9549.0\n",
      "lif layer 2 self.abs_max_v: 10001.5\n",
      "fc layer 2 self.abs_max_out: 5432.0\n",
      "lif layer 2 self.abs_max_v: 10132.5\n",
      "lif layer 2 self.abs_max_v: 10482.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss: 12.658423/ 91.423416, val:  26.25%, val_best:  37.50%, tr:  98.47%, tr_best:  98.98%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 65.8388%\n",
      "layer   3  Sparsity: 60.6969%\n",
      "total_backward_count 117480 real_backward_count 18160  15.458%\n",
      "fc layer 2 self.abs_max_out: 5501.0\n",
      "fc layer 2 self.abs_max_out: 5521.0\n",
      "lif layer 2 self.abs_max_v: 10530.0\n",
      "fc layer 2 self.abs_max_out: 5623.0\n",
      "fc layer 1 self.abs_max_out: 17234.0\n",
      "lif layer 1 self.abs_max_v: 30945.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss: 11.752892/ 81.752640, val:  33.75%, val_best:  37.50%, tr:  99.18%, tr_best:  99.18%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 66.2552%\n",
      "layer   3  Sparsity: 62.1886%\n",
      "total_backward_count 127270 real_backward_count 19648  15.438%\n",
      "lif layer 2 self.abs_max_v: 10794.5\n",
      "fc layer 2 self.abs_max_out: 5625.0\n",
      "fc layer 2 self.abs_max_out: 5670.0\n",
      "fc layer 1 self.abs_max_out: 17626.0\n",
      "lif layer 1 self.abs_max_v: 31567.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss: 11.813786/102.831985, val:  26.25%, val_best:  37.50%, tr:  97.96%, tr_best:  99.18%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 66.3192%\n",
      "layer   3  Sparsity: 62.9621%\n",
      "total_backward_count 137060 real_backward_count 21174  15.449%\n",
      "fc layer 1 self.abs_max_out: 18339.0\n",
      "lif layer 1 self.abs_max_v: 32865.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss: 12.080613/ 97.332954, val:  22.50%, val_best:  37.50%, tr:  98.26%, tr_best:  99.18%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   2  Sparsity: 66.5841%\n",
      "layer   3  Sparsity: 63.0362%\n",
      "total_backward_count 146850 real_backward_count 22707  15.463%\n",
      "fc layer 1 self.abs_max_out: 18489.0\n",
      "lif layer 1 self.abs_max_v: 33106.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss: 11.902603/ 74.036339, val:  37.92%, val_best:  37.92%, tr:  98.77%, tr_best:  99.18%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 66.8270%\n",
      "layer   3  Sparsity: 61.9906%\n",
      "total_backward_count 156640 real_backward_count 24199  15.449%\n",
      "fc layer 1 self.abs_max_out: 18522.0\n",
      "lif layer 1 self.abs_max_v: 33134.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss: 11.570681/ 53.166004, val:  39.17%, val_best:  39.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 80.54 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 67.1283%\n",
      "layer   3  Sparsity: 62.2342%\n",
      "total_backward_count 166430 real_backward_count 25646  15.409%\n",
      "fc layer 2 self.abs_max_out: 5790.0\n",
      "fc layer 1 self.abs_max_out: 19279.0\n",
      "lif layer 1 self.abs_max_v: 34398.0\n",
      "lif layer 1 self.abs_max_v: 34570.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss: 12.012644/ 51.257355, val:  41.25%, val_best:  41.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 81.43 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 67.1930%\n",
      "layer   3  Sparsity: 63.3411%\n",
      "total_backward_count 176220 real_backward_count 27183  15.426%\n",
      "fc layer 1 self.abs_max_out: 19332.0\n",
      "lif layer 1 self.abs_max_v: 34664.0\n",
      "lif layer 1 self.abs_max_v: 34751.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss: 11.908695/ 85.530334, val:  32.50%, val_best:  41.25%, tr:  98.37%, tr_best:  99.18%, epoch time: 80.82 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 67.4900%\n",
      "layer   3  Sparsity: 62.7894%\n",
      "total_backward_count 186010 real_backward_count 28685  15.421%\n",
      "fc layer 1 self.abs_max_out: 19514.0\n",
      "lif layer 1 self.abs_max_v: 34832.5\n",
      "lif layer 1 self.abs_max_v: 34930.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss: 11.299638/ 77.150139, val:  22.92%, val_best:  41.25%, tr:  98.16%, tr_best:  99.18%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 67.5838%\n",
      "layer   3  Sparsity: 62.7462%\n",
      "total_backward_count 195800 real_backward_count 30129  15.388%\n",
      "fc layer 1 self.abs_max_out: 20117.0\n",
      "lif layer 1 self.abs_max_v: 35867.0\n",
      "lif layer 1 self.abs_max_v: 35957.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss: 11.533257/ 94.168175, val:  23.75%, val_best:  41.25%, tr:  97.96%, tr_best:  99.18%, epoch time: 81.23 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 67.2645%\n",
      "layer   3  Sparsity: 63.2653%\n",
      "total_backward_count 205590 real_backward_count 31608  15.374%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss: 11.836679/ 52.508331, val:  29.58%, val_best:  41.25%, tr:  98.98%, tr_best:  99.18%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 67.5390%\n",
      "layer   3  Sparsity: 62.9032%\n",
      "total_backward_count 215380 real_backward_count 33121  15.378%\n",
      "fc layer 2 self.abs_max_out: 5936.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss: 11.451055/ 55.078850, val:  46.67%, val_best:  46.67%, tr:  99.08%, tr_best:  99.18%, epoch time: 81.39 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 67.5838%\n",
      "layer   3  Sparsity: 62.7261%\n",
      "total_backward_count 225170 real_backward_count 34580  15.357%\n",
      "fc layer 1 self.abs_max_out: 20317.0\n",
      "lif layer 1 self.abs_max_v: 35999.0\n",
      "lif layer 1 self.abs_max_v: 36245.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss: 11.259931/ 49.438423, val:  46.67%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 81.14 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 67.5064%\n",
      "layer   3  Sparsity: 63.1610%\n",
      "total_backward_count 234960 real_backward_count 36013  15.327%\n",
      "fc layer 2 self.abs_max_out: 6001.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss: 11.873248/ 60.365101, val:  42.92%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 81.18 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 67.4538%\n",
      "layer   3  Sparsity: 63.1430%\n",
      "total_backward_count 244750 real_backward_count 37485  15.316%\n",
      "fc layer 2 self.abs_max_out: 6009.0\n",
      "fc layer 2 self.abs_max_out: 6060.0\n",
      "fc layer 1 self.abs_max_out: 20563.0\n",
      "lif layer 1 self.abs_max_v: 36293.0\n",
      "lif layer 1 self.abs_max_v: 36589.5\n",
      "lif layer 2 self.abs_max_v: 10821.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss: 12.072811/ 49.046204, val:  40.83%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 67.8062%\n",
      "layer   3  Sparsity: 63.2578%\n",
      "total_backward_count 254540 real_backward_count 39021  15.330%\n",
      "fc layer 2 self.abs_max_out: 6382.0\n",
      "fc layer 2 self.abs_max_out: 6751.0\n",
      "lif layer 2 self.abs_max_v: 11340.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss: 11.876024/ 43.882610, val:  40.42%, val_best:  46.67%, tr:  99.18%, tr_best:  99.18%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 67.6882%\n",
      "layer   3  Sparsity: 63.2514%\n",
      "total_backward_count 264330 real_backward_count 40529  15.333%\n",
      "fc layer 1 self.abs_max_out: 20675.0\n",
      "lif layer 1 self.abs_max_v: 36912.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss: 11.935448/ 67.646065, val:  33.75%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.97 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 67.6972%\n",
      "layer   3  Sparsity: 63.2667%\n",
      "total_backward_count 274120 real_backward_count 42014  15.327%\n",
      "fc layer 1 self.abs_max_out: 21128.0\n",
      "lif layer 1 self.abs_max_v: 37519.5\n",
      "lif layer 1 self.abs_max_v: 37777.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss: 10.979956/ 88.764969, val:  25.42%, val_best:  46.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 80.48 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 68.0564%\n",
      "layer   3  Sparsity: 64.5574%\n",
      "total_backward_count 283910 real_backward_count 43463  15.309%\n",
      "fc layer 2 self.abs_max_out: 6860.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss: 11.755219/ 85.808418, val:  26.25%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.25 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 68.0158%\n",
      "layer   3  Sparsity: 64.6902%\n",
      "total_backward_count 293700 real_backward_count 44984  15.316%\n",
      "fc layer 2 self.abs_max_out: 6909.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss: 11.017248/ 61.094814, val:  32.08%, val_best:  46.67%, tr:  97.34%, tr_best:  99.18%, epoch time: 81.21 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 68.1460%\n",
      "layer   3  Sparsity: 64.4330%\n",
      "total_backward_count 303490 real_backward_count 46439  15.302%\n",
      "fc layer 2 self.abs_max_out: 6988.0\n",
      "fc layer 2 self.abs_max_out: 7341.0\n",
      "lif layer 2 self.abs_max_v: 11409.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss: 11.918954/ 74.821442, val:  32.92%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.38 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 68.1952%\n",
      "layer   3  Sparsity: 64.4862%\n",
      "total_backward_count 313280 real_backward_count 47997  15.321%\n",
      "fc layer 2 self.abs_max_out: 7707.0\n",
      "lif layer 2 self.abs_max_v: 11987.5\n",
      "lif layer 2 self.abs_max_v: 12035.0\n",
      "fc layer 1 self.abs_max_out: 21381.0\n",
      "lif layer 1 self.abs_max_v: 37810.5\n",
      "lif layer 1 self.abs_max_v: 37959.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss: 11.491257/ 42.091202, val:  42.08%, val_best:  46.67%, tr:  97.96%, tr_best:  99.18%, epoch time: 81.09 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 67.8737%\n",
      "layer   3  Sparsity: 64.8969%\n",
      "total_backward_count 323070 real_backward_count 49491  15.319%\n",
      "fc layer 1 self.abs_max_out: 21785.0\n",
      "lif layer 1 self.abs_max_v: 38733.5\n",
      "lif layer 1 self.abs_max_v: 38805.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss: 11.446360/ 63.320255, val:  37.92%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 80.91 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 68.0697%\n",
      "layer   3  Sparsity: 64.6934%\n",
      "total_backward_count 332860 real_backward_count 50996  15.321%\n",
      "fc layer 1 self.abs_max_out: 21809.0\n",
      "lif layer 1 self.abs_max_v: 38846.0\n",
      "lif layer 1 self.abs_max_v: 38934.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss: 11.469487/103.426308, val:  29.17%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 81.25 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   2  Sparsity: 67.9044%\n",
      "layer   3  Sparsity: 64.4733%\n",
      "total_backward_count 342650 real_backward_count 52495  15.320%\n",
      "lif layer 2 self.abs_max_v: 12142.5\n",
      "lif layer 2 self.abs_max_v: 12751.5\n",
      "lif layer 2 self.abs_max_v: 12775.0\n",
      "fc layer 1 self.abs_max_out: 21988.0\n",
      "lif layer 1 self.abs_max_v: 39186.5\n",
      "lif layer 1 self.abs_max_v: 39267.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss: 11.813782/ 61.680225, val:  38.33%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.04 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 68.2933%\n",
      "layer   3  Sparsity: 64.8989%\n",
      "total_backward_count 352440 real_backward_count 54059  15.338%\n",
      "fc layer 1 self.abs_max_out: 22245.0\n",
      "lif layer 1 self.abs_max_v: 39624.0\n",
      "lif layer 1 self.abs_max_v: 39780.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss: 11.987526/ 49.552750, val:  44.17%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.77 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 68.3122%\n",
      "layer   3  Sparsity: 65.1033%\n",
      "total_backward_count 362230 real_backward_count 55647  15.362%\n",
      "fc layer 1 self.abs_max_out: 22429.0\n",
      "lif layer 1 self.abs_max_v: 40044.5\n",
      "lif layer 1 self.abs_max_v: 40101.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss: 10.855135/ 74.178497, val:  32.50%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.27 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 68.2211%\n",
      "layer   3  Sparsity: 65.4759%\n",
      "total_backward_count 372020 real_backward_count 57099  15.348%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss: 11.316207/ 78.665634, val:  35.00%, val_best:  46.67%, tr:  97.96%, tr_best:  99.18%, epoch time: 80.93 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 68.4678%\n",
      "layer   3  Sparsity: 65.5971%\n",
      "total_backward_count 381810 real_backward_count 58636  15.357%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss: 11.716125/ 49.064312, val:  37.92%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.19 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 68.4683%\n",
      "layer   3  Sparsity: 65.1319%\n",
      "total_backward_count 391600 real_backward_count 60215  15.377%\n",
      "fc layer 2 self.abs_max_out: 8003.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss: 11.022317/ 67.786697, val:  37.08%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.56 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 68.3042%\n",
      "layer   3  Sparsity: 65.4983%\n",
      "total_backward_count 401390 real_backward_count 61700  15.372%\n",
      "fc layer 1 self.abs_max_out: 22507.0\n",
      "lif layer 1 self.abs_max_v: 40287.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss: 11.339451/ 65.490852, val:  36.25%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 81.22 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 68.5085%\n",
      "layer   3  Sparsity: 66.1080%\n",
      "total_backward_count 411180 real_backward_count 63246  15.382%\n",
      "lif layer 2 self.abs_max_v: 12839.5\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss: 10.919611/ 66.679001, val:  39.58%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.49 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 68.6379%\n",
      "layer   3  Sparsity: 66.8919%\n",
      "total_backward_count 420970 real_backward_count 64762  15.384%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss: 10.985602/ 70.590828, val:  33.33%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 80.74 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   2  Sparsity: 68.4694%\n",
      "layer   3  Sparsity: 66.8171%\n",
      "total_backward_count 430760 real_backward_count 66322  15.397%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss: 11.026917/ 67.307823, val:  32.92%, val_best:  46.67%, tr:  97.96%, tr_best:  99.18%, epoch time: 82.07 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 68.5306%\n",
      "layer   3  Sparsity: 66.6546%\n",
      "total_backward_count 440550 real_backward_count 67848  15.401%\n",
      "lif layer 2 self.abs_max_v: 13239.5\n",
      "lif layer 2 self.abs_max_v: 13996.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss: 11.261518/ 63.374912, val:  37.08%, val_best:  46.67%, tr:  97.45%, tr_best:  99.18%, epoch time: 80.89 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 68.2644%\n",
      "layer   3  Sparsity: 66.7954%\n",
      "total_backward_count 450340 real_backward_count 69463  15.425%\n",
      "fc layer 2 self.abs_max_out: 8225.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss: 11.007342/ 76.425781, val:  27.92%, val_best:  46.67%, tr:  98.88%, tr_best:  99.18%, epoch time: 80.85 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 68.5002%\n",
      "layer   3  Sparsity: 66.6304%\n",
      "total_backward_count 460130 real_backward_count 71018  15.434%\n",
      "lif layer 2 self.abs_max_v: 14160.0\n",
      "fc layer 1 self.abs_max_out: 22655.0\n",
      "lif layer 1 self.abs_max_v: 40613.5\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss: 11.107386/ 79.110748, val:  34.58%, val_best:  46.67%, tr:  97.85%, tr_best:  99.18%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 68.6316%\n",
      "layer   3  Sparsity: 66.6878%\n",
      "total_backward_count 469920 real_backward_count 72583  15.446%\n",
      "lif layer 2 self.abs_max_v: 14449.0\n",
      "fc layer 1 self.abs_max_out: 22748.0\n",
      "lif layer 1 self.abs_max_v: 40689.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss: 10.751596/ 70.635452, val:  35.42%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 80.77 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 68.2648%\n",
      "layer   3  Sparsity: 66.6991%\n",
      "total_backward_count 479710 real_backward_count 74095  15.446%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss: 11.247227/ 66.862694, val:  37.08%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 80.92 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   2  Sparsity: 68.4759%\n",
      "layer   3  Sparsity: 66.6549%\n",
      "total_backward_count 489500 real_backward_count 75666  15.458%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss: 11.357691/ 89.303963, val:  31.25%, val_best:  46.67%, tr:  99.08%, tr_best:  99.18%, epoch time: 80.82 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 68.9406%\n",
      "layer   3  Sparsity: 67.1329%\n",
      "total_backward_count 499290 real_backward_count 77267  15.475%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss: 11.455138/ 43.990372, val:  38.33%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 81.11 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 68.9458%\n",
      "layer   3  Sparsity: 67.4716%\n",
      "total_backward_count 509080 real_backward_count 78864  15.491%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss: 10.689960/ 70.141319, val:  35.83%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.07 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 69.0326%\n",
      "layer   3  Sparsity: 67.2023%\n",
      "total_backward_count 518870 real_backward_count 80379  15.491%\n",
      "fc layer 2 self.abs_max_out: 8430.0\n",
      "fc layer 2 self.abs_max_out: 8453.0\n",
      "lif layer 2 self.abs_max_v: 14953.5\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss: 11.103310/ 73.822723, val:  30.83%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 80.86 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 68.9573%\n",
      "layer   3  Sparsity: 67.6488%\n",
      "total_backward_count 528660 real_backward_count 81907  15.493%\n",
      "lif layer 2 self.abs_max_v: 15139.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss: 10.827475/ 45.588219, val:  45.00%, val_best:  46.67%, tr:  97.96%, tr_best:  99.18%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 68.9153%\n",
      "layer   3  Sparsity: 67.9255%\n",
      "total_backward_count 538450 real_backward_count 83475  15.503%\n",
      "lif layer 2 self.abs_max_v: 15431.5\n",
      "fc layer 1 self.abs_max_out: 23090.0\n",
      "lif layer 1 self.abs_max_v: 41048.0\n",
      "lif layer 1 self.abs_max_v: 41291.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss: 11.075884/ 83.880630, val:  29.17%, val_best:  46.67%, tr:  97.96%, tr_best:  99.18%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1056%\n",
      "layer   2  Sparsity: 68.9224%\n",
      "layer   3  Sparsity: 67.4364%\n",
      "total_backward_count 548240 real_backward_count 85033  15.510%\n",
      "lif layer 2 self.abs_max_v: 15874.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss: 11.041141/ 81.381737, val:  29.58%, val_best:  46.67%, tr:  97.55%, tr_best:  99.18%, epoch time: 80.93 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 69.3738%\n",
      "layer   3  Sparsity: 67.4777%\n",
      "total_backward_count 558030 real_backward_count 86587  15.517%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss: 10.892517/ 77.595764, val:  34.58%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.85 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 69.1364%\n",
      "layer   3  Sparsity: 67.4174%\n",
      "total_backward_count 567820 real_backward_count 88144  15.523%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss: 10.952354/ 69.483528, val:  30.00%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 80.99 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 69.1355%\n",
      "layer   3  Sparsity: 66.6422%\n",
      "total_backward_count 577610 real_backward_count 89678  15.526%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss: 11.275066/ 61.421955, val:  32.92%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.08 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 69.1515%\n",
      "layer   3  Sparsity: 65.7030%\n",
      "total_backward_count 587400 real_backward_count 91186  15.524%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss: 11.642677/ 67.462433, val:  36.25%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 81.50 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 69.2844%\n",
      "layer   3  Sparsity: 65.7095%\n",
      "total_backward_count 597190 real_backward_count 92765  15.534%\n",
      "fc layer 2 self.abs_max_out: 8538.0\n",
      "lif layer 2 self.abs_max_v: 16204.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss: 11.224624/ 68.562531, val:  32.08%, val_best:  46.67%, tr:  98.06%, tr_best:  99.18%, epoch time: 81.21 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0350%\n",
      "layer   2  Sparsity: 69.3870%\n",
      "layer   3  Sparsity: 66.1487%\n",
      "total_backward_count 606980 real_backward_count 94296  15.535%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss: 10.867834/ 61.946682, val:  32.50%, val_best:  46.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 69.4082%\n",
      "layer   3  Sparsity: 66.5708%\n",
      "total_backward_count 616770 real_backward_count 95806  15.534%\n",
      "fc layer 2 self.abs_max_out: 8649.0\n",
      "fc layer 2 self.abs_max_out: 8658.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss: 10.975550/ 77.453812, val:  30.83%, val_best:  46.67%, tr:  98.06%, tr_best:  99.18%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 69.0564%\n",
      "layer   3  Sparsity: 67.5070%\n",
      "total_backward_count 626560 real_backward_count 97356  15.538%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss: 11.012817/ 77.076691, val:  36.25%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   2  Sparsity: 69.0916%\n",
      "layer   3  Sparsity: 68.6527%\n",
      "total_backward_count 636350 real_backward_count 98937  15.548%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss: 10.667621/ 68.481758, val:  31.67%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.18 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   2  Sparsity: 68.9776%\n",
      "layer   3  Sparsity: 68.6019%\n",
      "total_backward_count 646140 real_backward_count 100483  15.551%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss: 10.968641/ 65.358444, val:  35.42%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 69.1152%\n",
      "layer   3  Sparsity: 68.0699%\n",
      "total_backward_count 655930 real_backward_count 102093  15.565%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss: 11.348004/ 45.432617, val:  39.17%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 80.81 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   2  Sparsity: 69.2836%\n",
      "layer   3  Sparsity: 67.5112%\n",
      "total_backward_count 665720 real_backward_count 103683  15.575%\n",
      "lif layer 2 self.abs_max_v: 16300.5\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss: 10.595318/ 50.270863, val:  45.83%, val_best:  46.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 68.8789%\n",
      "layer   3  Sparsity: 68.5306%\n",
      "total_backward_count 675510 real_backward_count 105251  15.581%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss: 10.707873/ 89.177269, val:  31.25%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.38 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 68.8539%\n",
      "layer   3  Sparsity: 67.8417%\n",
      "total_backward_count 685300 real_backward_count 106818  15.587%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss: 10.662911/ 57.377998, val:  40.83%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 81.59 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 68.8619%\n",
      "layer   3  Sparsity: 68.6930%\n",
      "total_backward_count 695090 real_backward_count 108403  15.596%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss: 10.811215/ 90.893394, val:  25.83%, val_best:  46.67%, tr:  97.24%, tr_best:  99.18%, epoch time: 81.75 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 68.9184%\n",
      "layer   3  Sparsity: 69.1485%\n",
      "total_backward_count 704880 real_backward_count 110051  15.613%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss: 10.857302/ 57.555946, val:  40.42%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 81.31 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 68.6502%\n",
      "layer   3  Sparsity: 68.8333%\n",
      "total_backward_count 714670 real_backward_count 111672  15.626%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss: 10.816389/ 58.746811, val:  39.17%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.95 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 68.6603%\n",
      "layer   3  Sparsity: 68.6649%\n",
      "total_backward_count 724460 real_backward_count 113281  15.637%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss: 11.128683/ 57.647388, val:  37.08%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.70 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 68.5705%\n",
      "layer   3  Sparsity: 67.7940%\n",
      "total_backward_count 734250 real_backward_count 114878  15.646%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss: 10.926692/ 77.027931, val:  41.25%, val_best:  46.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 80.82 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 68.4733%\n",
      "layer   3  Sparsity: 67.3364%\n",
      "total_backward_count 744040 real_backward_count 116455  15.652%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss: 11.255739/ 51.681656, val:  40.42%, val_best:  46.67%, tr:  97.96%, tr_best:  99.18%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 68.6381%\n",
      "layer   3  Sparsity: 68.2568%\n",
      "total_backward_count 753830 real_backward_count 118109  15.668%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss: 10.975358/ 80.828239, val:  25.83%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 68.8133%\n",
      "layer   3  Sparsity: 68.7092%\n",
      "total_backward_count 763620 real_backward_count 119719  15.678%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss: 10.725805/ 91.339569, val:  36.25%, val_best:  46.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 81.09 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 68.5414%\n",
      "layer   3  Sparsity: 69.4591%\n",
      "total_backward_count 773410 real_backward_count 121338  15.689%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss: 11.023326/ 66.181290, val:  38.75%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.89 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 68.6863%\n",
      "layer   3  Sparsity: 69.2484%\n",
      "total_backward_count 783200 real_backward_count 122962  15.700%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss: 11.045456/ 58.247299, val:  37.08%, val_best:  46.67%, tr:  98.88%, tr_best:  99.18%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   2  Sparsity: 68.6661%\n",
      "layer   3  Sparsity: 68.0338%\n",
      "total_backward_count 792990 real_backward_count 124552  15.707%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss: 11.392017/116.410378, val:  25.42%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 68.8942%\n",
      "layer   3  Sparsity: 66.8303%\n",
      "total_backward_count 802780 real_backward_count 126116  15.710%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss: 12.132176/ 51.706810, val:  46.25%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 69.1118%\n",
      "layer   3  Sparsity: 65.8648%\n",
      "total_backward_count 812570 real_backward_count 127745  15.721%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss: 11.926508/ 98.993576, val:  35.00%, val_best:  46.67%, tr:  98.06%, tr_best:  99.18%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0299%\n",
      "layer   2  Sparsity: 68.8396%\n",
      "layer   3  Sparsity: 65.5596%\n",
      "total_backward_count 822360 real_backward_count 129339  15.728%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss: 12.156059/ 56.085606, val:  38.75%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.32 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 68.6595%\n",
      "layer   3  Sparsity: 65.4926%\n",
      "total_backward_count 832150 real_backward_count 130975  15.739%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss: 11.570575/ 84.723022, val:  23.75%, val_best:  46.67%, tr:  98.06%, tr_best:  99.18%, epoch time: 80.90 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 68.8165%\n",
      "layer   3  Sparsity: 65.6741%\n",
      "total_backward_count 841940 real_backward_count 132565  15.745%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss: 11.722216/ 84.955803, val:  25.83%, val_best:  46.67%, tr:  98.88%, tr_best:  99.18%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   2  Sparsity: 68.7261%\n",
      "layer   3  Sparsity: 65.2847%\n",
      "total_backward_count 851730 real_backward_count 134143  15.749%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss: 12.202909/ 69.092209, val:  38.33%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 68.7060%\n",
      "layer   3  Sparsity: 65.5874%\n",
      "total_backward_count 861520 real_backward_count 135768  15.759%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss: 11.515000/ 71.615036, val:  41.25%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 80.90 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   2  Sparsity: 69.0006%\n",
      "layer   3  Sparsity: 64.6643%\n",
      "total_backward_count 871310 real_backward_count 137295  15.757%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss: 11.756525/ 72.764641, val:  35.83%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 69.0004%\n",
      "layer   3  Sparsity: 63.6482%\n",
      "total_backward_count 881100 real_backward_count 138776  15.750%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss: 11.909874/ 63.711746, val:  42.50%, val_best:  46.67%, tr:  97.55%, tr_best:  99.18%, epoch time: 81.26 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 68.8222%\n",
      "layer   3  Sparsity: 64.8048%\n",
      "total_backward_count 890890 real_backward_count 140331  15.752%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss: 11.291247/ 65.013107, val:  35.83%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 81.35 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 69.0980%\n",
      "layer   3  Sparsity: 64.2452%\n",
      "total_backward_count 900680 real_backward_count 141804  15.744%\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss: 12.298962/ 70.276184, val:  34.17%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 81.00 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 69.0472%\n",
      "layer   3  Sparsity: 62.8343%\n",
      "total_backward_count 910470 real_backward_count 143344  15.744%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss: 12.123114/ 80.959930, val:  29.58%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 81.24 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 69.3744%\n",
      "layer   3  Sparsity: 62.9856%\n",
      "total_backward_count 920260 real_backward_count 144874  15.743%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss: 12.149355/ 54.405251, val:  41.25%, val_best:  46.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 81.22 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 69.2192%\n",
      "layer   3  Sparsity: 62.8815%\n",
      "total_backward_count 930050 real_backward_count 146383  15.739%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss: 12.115460/ 80.193947, val:  37.92%, val_best:  46.67%, tr:  99.08%, tr_best:  99.18%, epoch time: 81.24 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 69.1276%\n",
      "layer   3  Sparsity: 64.0575%\n",
      "total_backward_count 939840 real_backward_count 147922  15.739%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss: 11.356483/ 73.137924, val:  42.92%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 68.8923%\n",
      "layer   3  Sparsity: 64.3048%\n",
      "total_backward_count 949630 real_backward_count 149373  15.730%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss: 11.895858/ 76.259087, val:  33.75%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 80.84 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 68.9976%\n",
      "layer   3  Sparsity: 63.8153%\n",
      "total_backward_count 959420 real_backward_count 150871  15.725%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss: 11.794266/ 87.907104, val:  32.92%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.97 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 69.0878%\n",
      "layer   3  Sparsity: 64.1439%\n",
      "total_backward_count 969210 real_backward_count 152373  15.721%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss: 11.887987/ 54.606380, val:  37.50%, val_best:  46.67%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.34 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 69.0508%\n",
      "layer   3  Sparsity: 64.2776%\n",
      "total_backward_count 979000 real_backward_count 153889  15.719%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss: 11.543451/112.132462, val:  25.00%, val_best:  46.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 81.26 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 68.8272%\n",
      "layer   3  Sparsity: 64.1443%\n",
      "total_backward_count 988790 real_backward_count 155388  15.715%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss: 12.019786/ 79.565521, val:  31.67%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 68.9537%\n",
      "layer   3  Sparsity: 64.7529%\n",
      "total_backward_count 998580 real_backward_count 156972  15.720%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss: 11.587832/ 63.960987, val:  35.00%, val_best:  46.67%, tr:  98.98%, tr_best:  99.18%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 69.4169%\n",
      "layer   3  Sparsity: 65.1078%\n",
      "total_backward_count 1008370 real_backward_count 158493  15.718%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-103 lr=['1.0000000'], tr/val_loss: 11.892639/ 77.374916, val:  30.42%, val_best:  46.67%, tr:  98.16%, tr_best:  99.18%, epoch time: 69.95 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   2  Sparsity: 69.5622%\n",
      "layer   3  Sparsity: 64.3472%\n",
      "total_backward_count 1018160 real_backward_count 160036  15.718%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss: 11.549074/ 56.736786, val:  34.17%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 69.4846%\n",
      "layer   3  Sparsity: 65.2129%\n",
      "total_backward_count 1027950 real_backward_count 161554  15.716%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss: 11.494286/ 66.604668, val:  38.33%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 69.74 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 69.5646%\n",
      "layer   3  Sparsity: 65.2504%\n",
      "total_backward_count 1037740 real_backward_count 163063  15.713%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss: 11.364089/ 87.848152, val:  29.58%, val_best:  46.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 69.10 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 69.6150%\n",
      "layer   3  Sparsity: 64.7334%\n",
      "total_backward_count 1047530 real_backward_count 164524  15.706%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss: 11.254567/ 79.206619, val:  39.17%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 69.92 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 69.5245%\n",
      "layer   3  Sparsity: 65.4699%\n",
      "total_backward_count 1057320 real_backward_count 166014  15.701%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss: 11.065146/ 67.553802, val:  38.33%, val_best:  46.67%, tr:  98.06%, tr_best:  99.18%, epoch time: 69.78 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0757%\n",
      "layer   2  Sparsity: 69.6897%\n",
      "layer   3  Sparsity: 66.3474%\n",
      "total_backward_count 1067110 real_backward_count 167507  15.697%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss: 10.679632/ 68.357765, val:  30.00%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 69.73 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0448%\n",
      "layer   2  Sparsity: 69.5895%\n",
      "layer   3  Sparsity: 66.2206%\n",
      "total_backward_count 1076900 real_backward_count 168951  15.689%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss: 10.821356/ 94.644859, val:  27.50%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 69.3744%\n",
      "layer   3  Sparsity: 67.2037%\n",
      "total_backward_count 1086690 real_backward_count 170454  15.686%\n",
      "fc layer 2 self.abs_max_out: 8701.0\n",
      "lif layer 2 self.abs_max_v: 16314.5\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss: 11.033942/ 79.775864, val:  31.25%, val_best:  46.67%, tr:  98.47%, tr_best:  99.18%, epoch time: 69.34 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 69.0391%\n",
      "layer   3  Sparsity: 66.7758%\n",
      "total_backward_count 1096480 real_backward_count 171963  15.683%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss: 11.468359/ 49.408169, val:  42.50%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 69.67 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 69.0515%\n",
      "layer   3  Sparsity: 67.2451%\n",
      "total_backward_count 1106270 real_backward_count 173530  15.686%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss: 11.145349/ 88.471130, val:  28.33%, val_best:  46.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 69.1045%\n",
      "layer   3  Sparsity: 68.1376%\n",
      "total_backward_count 1116060 real_backward_count 175123  15.691%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss: 11.028336/ 67.614906, val:  24.17%, val_best:  46.67%, tr:  98.37%, tr_best:  99.18%, epoch time: 70.34 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 69.1187%\n",
      "layer   3  Sparsity: 67.7466%\n",
      "total_backward_count 1125850 real_backward_count 176649  15.690%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss: 11.431763/ 72.768684, val:  28.33%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 69.2813%\n",
      "layer   3  Sparsity: 66.9750%\n",
      "total_backward_count 1135640 real_backward_count 178222  15.694%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss: 11.797928/ 70.087563, val:  37.50%, val_best:  46.67%, tr:  98.57%, tr_best:  99.18%, epoch time: 81.97 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 69.1418%\n",
      "layer   3  Sparsity: 66.3033%\n",
      "total_backward_count 1145430 real_backward_count 179808  15.698%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss: 11.533230/ 79.879105, val:  37.92%, val_best:  46.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 81.18 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 69.0520%\n",
      "layer   3  Sparsity: 65.8876%\n",
      "total_backward_count 1155220 real_backward_count 181352  15.698%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss: 11.808771/ 59.478722, val:  41.25%, val_best:  46.67%, tr:  98.77%, tr_best:  99.18%, epoch time: 81.62 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 69.2508%\n",
      "layer   3  Sparsity: 65.4635%\n",
      "total_backward_count 1165010 real_backward_count 182914  15.701%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss: 11.546172/ 54.945190, val:  47.08%, val_best:  47.08%, tr:  98.77%, tr_best:  99.18%, epoch time: 81.77 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 69.1692%\n",
      "layer   3  Sparsity: 65.2393%\n",
      "total_backward_count 1174800 real_backward_count 184418  15.698%\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss: 11.796905/ 72.463425, val:  37.08%, val_best:  47.08%, tr:  98.57%, tr_best:  99.18%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 69.3978%\n",
      "layer   3  Sparsity: 64.3614%\n",
      "total_backward_count 1184590 real_backward_count 185926  15.695%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss: 11.996188/ 69.031837, val:  41.25%, val_best:  47.08%, tr:  98.47%, tr_best:  99.18%, epoch time: 81.75 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 68.9944%\n",
      "layer   3  Sparsity: 64.6498%\n",
      "total_backward_count 1194380 real_backward_count 187473  15.696%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss: 12.135609/ 55.268307, val:  34.17%, val_best:  47.08%, tr:  98.88%, tr_best:  99.18%, epoch time: 82.01 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 69.3004%\n",
      "layer   3  Sparsity: 64.4282%\n",
      "total_backward_count 1204170 real_backward_count 189018  15.697%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss: 11.520094/ 75.954391, val:  34.58%, val_best:  47.08%, tr:  98.47%, tr_best:  99.18%, epoch time: 82.60 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 69.0302%\n",
      "layer   3  Sparsity: 63.3183%\n",
      "total_backward_count 1213960 real_backward_count 190499  15.692%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss: 12.321186/ 78.205803, val:  40.83%, val_best:  47.08%, tr:  98.88%, tr_best:  99.18%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 69.0483%\n",
      "layer   3  Sparsity: 63.0550%\n",
      "total_backward_count 1223750 real_backward_count 192016  15.691%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss: 11.893743/ 83.806854, val:  31.25%, val_best:  47.08%, tr:  98.37%, tr_best:  99.18%, epoch time: 81.34 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0327%\n",
      "layer   2  Sparsity: 69.2112%\n",
      "layer   3  Sparsity: 63.8982%\n",
      "total_backward_count 1233540 real_backward_count 193559  15.691%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss: 11.801209/ 92.312981, val:  27.92%, val_best:  47.08%, tr:  99.08%, tr_best:  99.18%, epoch time: 81.41 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 69.0458%\n",
      "layer   3  Sparsity: 65.2619%\n",
      "total_backward_count 1243330 real_backward_count 195122  15.694%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss: 11.806158/ 80.909195, val:  24.58%, val_best:  47.08%, tr:  98.77%, tr_best:  99.18%, epoch time: 81.48 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   2  Sparsity: 69.1930%\n",
      "layer   3  Sparsity: 63.7863%\n",
      "total_backward_count 1253120 real_backward_count 196624  15.691%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss: 11.981215/ 76.269707, val:  35.00%, val_best:  47.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 81.65 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 69.4941%\n",
      "layer   3  Sparsity: 62.9034%\n",
      "total_backward_count 1262910 real_backward_count 198112  15.687%\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss: 11.972737/ 74.838882, val:  32.50%, val_best:  47.08%, tr:  98.37%, tr_best:  99.18%, epoch time: 81.31 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 69.6471%\n",
      "layer   3  Sparsity: 62.0856%\n",
      "total_backward_count 1272700 real_backward_count 199584  15.682%\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss: 12.331105/123.137688, val:  28.33%, val_best:  47.08%, tr:  98.67%, tr_best:  99.18%, epoch time: 82.13 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 69.5141%\n",
      "layer   3  Sparsity: 60.9915%\n",
      "total_backward_count 1282490 real_backward_count 201082  15.679%\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss: 12.304924/ 65.285370, val:  37.92%, val_best:  47.08%, tr:  98.67%, tr_best:  99.18%, epoch time: 82.37 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 69.5605%\n",
      "layer   3  Sparsity: 61.7376%\n",
      "total_backward_count 1292280 real_backward_count 202536  15.673%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss: 11.615234/ 75.933838, val:  37.08%, val_best:  47.08%, tr:  99.08%, tr_best:  99.18%, epoch time: 81.33 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 69.8789%\n",
      "layer   3  Sparsity: 62.9813%\n",
      "total_backward_count 1302070 real_backward_count 203991  15.667%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss: 12.016479/ 72.364601, val:  42.50%, val_best:  47.08%, tr:  98.37%, tr_best:  99.18%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   2  Sparsity: 69.2902%\n",
      "layer   3  Sparsity: 64.1073%\n",
      "total_backward_count 1311860 real_backward_count 205563  15.670%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss: 11.980615/ 77.236115, val:  35.42%, val_best:  47.08%, tr:  98.47%, tr_best:  99.18%, epoch time: 80.74 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 69.3827%\n",
      "layer   3  Sparsity: 64.1743%\n",
      "total_backward_count 1321650 real_backward_count 207082  15.668%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss: 11.985435/ 62.731106, val:  38.75%, val_best:  47.08%, tr:  98.67%, tr_best:  99.18%, epoch time: 81.68 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 69.3297%\n",
      "layer   3  Sparsity: 65.3269%\n",
      "total_backward_count 1331440 real_backward_count 208669  15.672%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss: 11.274047/ 71.261642, val:  38.33%, val_best:  47.08%, tr:  98.77%, tr_best:  99.18%, epoch time: 80.96 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 69.2363%\n",
      "layer   3  Sparsity: 64.9719%\n",
      "total_backward_count 1341230 real_backward_count 210169  15.670%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss: 11.422304/ 99.770592, val:  28.33%, val_best:  47.08%, tr:  97.96%, tr_best:  99.18%, epoch time: 82.00 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 69.2066%\n",
      "layer   3  Sparsity: 64.5177%\n",
      "total_backward_count 1351020 real_backward_count 211666  15.667%\n",
      "fc layer 2 self.abs_max_out: 8718.0\n",
      "lif layer 2 self.abs_max_v: 16346.5\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss: 11.576121/ 47.958355, val:  37.92%, val_best:  47.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 82.43 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 69.2189%\n",
      "layer   3  Sparsity: 64.2914%\n",
      "total_backward_count 1360810 real_backward_count 213170  15.665%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss: 12.090445/ 87.565109, val:  27.50%, val_best:  47.08%, tr:  99.18%, tr_best:  99.28%, epoch time: 81.56 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 69.1857%\n",
      "layer   3  Sparsity: 64.1635%\n",
      "total_backward_count 1370600 real_backward_count 214696  15.664%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss: 11.507663/ 62.945305, val:  36.25%, val_best:  47.08%, tr:  98.57%, tr_best:  99.28%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 69.2742%\n",
      "layer   3  Sparsity: 65.1706%\n",
      "total_backward_count 1380390 real_backward_count 216200  15.662%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss: 11.521698/ 56.182167, val:  34.58%, val_best:  47.08%, tr:  98.98%, tr_best:  99.28%, epoch time: 81.06 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 69.3611%\n",
      "layer   3  Sparsity: 65.5200%\n",
      "total_backward_count 1390180 real_backward_count 217696  15.660%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss: 11.484495/ 63.858994, val:  40.00%, val_best:  47.08%, tr:  97.75%, tr_best:  99.28%, epoch time: 82.01 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.1195%\n",
      "layer   2  Sparsity: 69.7031%\n",
      "layer   3  Sparsity: 66.4579%\n",
      "total_backward_count 1399970 real_backward_count 219274  15.663%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss: 10.844633/ 52.309010, val:  45.83%, val_best:  47.08%, tr:  97.45%, tr_best:  99.28%, epoch time: 81.42 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 69.6215%\n",
      "layer   3  Sparsity: 67.0544%\n",
      "total_backward_count 1409760 real_backward_count 220806  15.663%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss: 11.419193/ 59.564735, val:  40.00%, val_best:  47.08%, tr:  98.88%, tr_best:  99.28%, epoch time: 81.52 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 69.6207%\n",
      "layer   3  Sparsity: 66.6556%\n",
      "total_backward_count 1419550 real_backward_count 222368  15.665%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss: 11.120623/ 70.374283, val:  30.83%, val_best:  47.08%, tr:  98.37%, tr_best:  99.28%, epoch time: 81.95 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 69.3476%\n",
      "layer   3  Sparsity: 66.4875%\n",
      "total_backward_count 1429340 real_backward_count 223854  15.661%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss: 11.621958/ 72.017014, val:  37.50%, val_best:  47.08%, tr:  97.96%, tr_best:  99.28%, epoch time: 81.23 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 69.3665%\n",
      "layer   3  Sparsity: 66.0318%\n",
      "total_backward_count 1439130 real_backward_count 225392  15.662%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss: 11.196629/118.742554, val:  21.25%, val_best:  47.08%, tr:  98.88%, tr_best:  99.28%, epoch time: 81.89 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   2  Sparsity: 69.3470%\n",
      "layer   3  Sparsity: 65.4929%\n",
      "total_backward_count 1448920 real_backward_count 226865  15.658%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss: 11.251139/ 83.495316, val:  35.00%, val_best:  47.08%, tr:  98.57%, tr_best:  99.28%, epoch time: 81.42 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 69.0358%\n",
      "layer   3  Sparsity: 65.7189%\n",
      "total_backward_count 1458710 real_backward_count 228385  15.657%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss: 11.398316/ 67.886185, val:  40.42%, val_best:  47.08%, tr:  98.37%, tr_best:  99.28%, epoch time: 81.70 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 68.5410%\n",
      "layer   3  Sparsity: 65.4519%\n",
      "total_backward_count 1468500 real_backward_count 229909  15.656%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss: 11.715536/ 85.945839, val:  32.92%, val_best:  47.08%, tr:  97.96%, tr_best:  99.28%, epoch time: 81.57 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 69.0114%\n",
      "layer   3  Sparsity: 65.3746%\n",
      "total_backward_count 1478290 real_backward_count 231427  15.655%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [4096.0,2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0, 2.0, 4.0, 8.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [4096.0,2048.0,1024.0,512.0,256.0,128.0,64.0,32.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0, 2.0, 4.0, 8.0]},\n",
    "        \"init_scaling_0\": {\"values\": [1.0, 1/2, 1/4, 1/8, 1/16, 1/32]},\n",
    "        \"init_scaling_1\": {\"values\": [1.0, 1/2, 1/4, 1/8, 1/16, 1/32]},\n",
    "        \"init_scaling_2\": {\"values\": [1.0, 1/2, 1/4, 1/8, 1/16, 1/32]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"1\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'ngjk7mes'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
