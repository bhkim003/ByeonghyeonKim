{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30968/2809884579.py:45: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from spikingjelly.datasets import split_to_train_test_set\n",
    "from spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dvs 데이터 시각화 코드\n",
    " ##############################################################################################\n",
    "            # mapping = {\n",
    "            #     0: 'Hand Clapping',\n",
    "            #     1: 'Right Hand Wave',\n",
    "            #     2: 'Left Hand Wave',\n",
    "            #     3: 'Right Arm CW',\n",
    "            #     4: 'Right Arm CCW',\n",
    "            #     5: 'Left Arm CW',\n",
    "            #     6: 'Left Arm CCW',\n",
    "            #     7: 'Arm Roll',\n",
    "            #     8: 'Air Drums',\n",
    "            #     9: 'Air Guitar',\n",
    "            #     10: 'Other'\n",
    "            # }\n",
    "def dvs_visualization(inputs, labels, TIME, BATCH):\n",
    "            \n",
    "    what_input = random.randint(0, BATCH - 1)\n",
    "    inputs_for_view = inputs.permute(1, 0, 2, 3, 4)\n",
    "    for i in range(TIME):\n",
    "        # 예시 데이터 생성\n",
    "        data1 = inputs_for_view[what_input][i][0].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "        data2 = inputs_for_view[what_input][i][1].numpy()  # torch tensor를 numpy 배열로 변환\n",
    "\n",
    "        # 데이터 플로팅\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1행 2열의 subplot 생성\n",
    "\n",
    "        # 첫 번째 subplot에 데이터1 플로팅\n",
    "        im1 = axs[0].imshow(data1, cmap='viridis', interpolation='nearest')\n",
    "        axs[0].set_title(f'Channel 0\\nLabel: {labels[what_input]}  Time: {i}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[0].set_xlabel('X axis')\n",
    "        axs[0].set_ylabel('Y axis')\n",
    "        axs[0].grid(False)\n",
    "        fig.colorbar(im1, ax=axs[0])  # Color bar 추가\n",
    "\n",
    "        # 두 번째 subplot에 데이터2 플로팅\n",
    "        im2 = axs[1].imshow(data2, cmap='viridis', interpolation='nearest')\n",
    "        axs[1].set_title(f'Channel 1\\nLabel: {labels[what_input]}  Time: {i}')  # 라벨값 맵핑하여 제목에 추가\n",
    "        axs[1].set_xlabel('X axis')\n",
    "        axs[1].set_ylabel('Y axis')\n",
    "        axs[1].grid(False)\n",
    "        fig.colorbar(im2, ax=axs[1])  # Color bar 추가\n",
    "\n",
    "        plt.tight_layout()  # subplot 간 간격 조정\n",
    "        plt.show()\n",
    "    sys.exit(\"종료\")\n",
    "\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    weight_count_print = False, # True # False\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    verbose_interval = 100, #숫자 크게 하면 꺼짐\n",
    "                    validation_interval = 10, #숫자 크게 하면 꺼짐\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    gradient_verbose = False,\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = True,\n",
    "\n",
    "                    nda_net = False,\n",
    "                    \n",
    "                    domain_il_epoch = 0, # over 0, then domain il mode on\n",
    "\n",
    "                    dvs_clipping = True, \n",
    "                    dvs_duration = 1000000,\n",
    "\n",
    "                    OTTT_sWS_on = True, # True # False\n",
    "                  ):\n",
    "    if OTTT_sWS_on == True:\n",
    "        assert BPTT_on == False and tdBN_on == False and convTrue_fcFalse == True\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False\n",
    "\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "    \n",
    "    \n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    torch.manual_seed(my_seed)\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME,\n",
    "            dvs_clipping,\n",
    "            dvs_duration)\n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "    \n",
    "    ## parameter number calculator (안 중요함) ##################################################################################################################\n",
    "    params_num = 0\n",
    "    img_size = IMAGE_SIZE \n",
    "    bias_param = 1 # 1 or 0\n",
    "    classifier_making = False\n",
    "    if (convTrue_fcFalse == True):\n",
    "        past_kernel = synapse_conv_in_channels\n",
    "        for kernel in cfg:\n",
    "            if (classifier_making == False):\n",
    "                if (type(kernel) == list):\n",
    "                    for residual_kernel in kernel:\n",
    "                        if (residual_kernel >= 10000 and residual_kernel < 20000): # separable\n",
    "                            residual_kernel -= 10000\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            params_num += (1**2 * past_kernel + bias_param) * residual_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        elif (residual_kernel >= 20000 and residual_kernel < 30000): # depthwise\n",
    "                            residual_kernel -= 20000\n",
    "                            # 'past_kernel' should be same with 'kernel'\n",
    "                            params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                            past_kernel = residual_kernel  \n",
    "                        else:\n",
    "                            params_num += residual_kernel * ((synapse_conv_kernel_size**2) * past_kernel + bias_param)\n",
    "                            past_kernel = residual_kernel\n",
    "                elif (kernel == 'P' or kernel == 'M'):\n",
    "                    img_size = img_size // 2\n",
    "                elif (kernel == 'D'):\n",
    "                    img_size = 1\n",
    "                elif (kernel == 'L'):\n",
    "                    classifier_making = True\n",
    "                    past_kernel = past_kernel * (img_size**2)\n",
    "                else:\n",
    "                    if (kernel >= 10000 and kernel < 20000): # separable\n",
    "                        kernel -= 10000\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        params_num += (1**2 * past_kernel + bias_param) * kernel\n",
    "                        past_kernel = kernel  \n",
    "                    elif (kernel >= 20000 and kernel < 30000): # depthwise\n",
    "                        kernel -= 20000\n",
    "                        # 'past_kernel' should be same with 'kernel'\n",
    "                        params_num += (synapse_conv_kernel_size**2 + bias_param) * past_kernel\n",
    "                        past_kernel = kernel  \n",
    "                    else:\n",
    "                        params_num += kernel * (synapse_conv_kernel_size**2 * past_kernel + bias_param)\n",
    "                        past_kernel = kernel    \n",
    "            else: # classifier making\n",
    "                params_num += (past_kernel + bias_param) * kernel\n",
    "                past_kernel = kernel\n",
    "        \n",
    "        \n",
    "        if classifier_making == False:\n",
    "            past_kernel = past_kernel*img_size*img_size\n",
    "\n",
    "        params_num += (past_kernel + bias_param) * synapse_fc_out_features\n",
    "    else:\n",
    "        past_in_channel = synapse_conv_in_channels*img_size*img_size\n",
    "        for in_channel in cfg:\n",
    "            if (type(in_channel) == list):\n",
    "                for residual_in_channel in in_channel:\n",
    "                    params_num += (past_in_channel + bias_param) * residual_in_channel\n",
    "                    past_in_channel = residual_in_channel\n",
    "            # elif (in_channel == 'M'): #it's a holy FC layer!\n",
    "            #     img_size = img_size // 2\n",
    "            else:\n",
    "                print('past_in_channel', past_in_channel)\n",
    "                print('bias_param', bias_param)\n",
    "                print('in_channel', in_channel)\n",
    "                params_num += (past_in_channel + bias_param) * in_channel\n",
    "                past_in_channel = in_channel\n",
    "        params_num += (past_in_channel + bias_param) * synapse_fc_out_features\n",
    "    ###########################################################################################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            if (single_step == False):\n",
    "                net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                            synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                            lif_layer_v_init, lif_layer_v_decay, \n",
    "                            lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                            lif_layer_sg_width,\n",
    "                            tdBN_on,\n",
    "                            BN_on, TIME,\n",
    "                            surrogate,\n",
    "                            BPTT_on).to(device)\n",
    "        else:\n",
    "            if (single_step == False):\n",
    "                net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                            synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                            synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                            synapse_conv_trace_const2, \n",
    "                            lif_layer_v_init, lif_layer_v_decay, \n",
    "                            lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                            lif_layer_sg_width,\n",
    "                            synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                            tdBN_on,\n",
    "                            BN_on, TIME,\n",
    "                            surrogate,\n",
    "                            BPTT_on,\n",
    "                            OTTT_sWS_on).to(device)\n",
    "            else:\n",
    "                net = MY_SNN_CONV_ottt_sstep(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                            synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                            synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                            synapse_conv_trace_const2, \n",
    "                            lif_layer_v_init, lif_layer_v_decay, \n",
    "                            lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                            lif_layer_sg_width,\n",
    "                            synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                            tdBN_on,\n",
    "                            BN_on, TIME,\n",
    "                            surrogate,\n",
    "                            BPTT_on,\n",
    "                            OTTT_sWS_on).to(device)\n",
    "        if (nda_net == True):\n",
    "            net = VGG(cfg = cfg, num_classes=10, batch_norm = tdBN_on, in_c = synapse_conv_in_channels, \n",
    "                      lif_layer_v_threshold=lif_layer_v_threshold, lif_layer_v_decay=lif_layer_v_decay, lif_layer_sg_width=lif_layer_sg_width)\n",
    "            net.T = TIME\n",
    "        net = torch.nn.DataParallel(net) #나중에풀어줘\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)        \n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "    ## param num and memory estimation except BN with MY own calculation some lines above ##########################################\n",
    "    real_param_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    if (weight_count_print == True):\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f'Layer: {name} | Number of parameters: {param.numel()}')\n",
    "    # Batch norm 있으면 아래 두 개 서로 다를 수 있음.\n",
    "    # assert real_param_num == params_num, f'parameter number is not same. real_param_num: {real_param_num}, params_num: {params_num}'    \n",
    "    print('='*50)\n",
    "    print(f\"My Num of PARAMS: {params_num:,}, system's param_num : {real_param_num:,}\")\n",
    "    memory = params_num / 8 / 1024 / 1024 # MB\n",
    "    precision = 32\n",
    "    memory = memory * precision \n",
    "    print(f\"Memory: {memory:.2f}MiB at {precision}-bit\")\n",
    "    print('='*50)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    if (OTTT_sWS_on == True):\n",
    "        # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    val_acc = 0\n",
    "    val_acc_now = 0\n",
    "    elapsed_time_val = 0\n",
    "    iter_acc_array = np.array([])\n",
    "    tr_acc_array = np.array([])\n",
    "    val_acc_now_array = np.array([])\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # if (domain_il_epoch>0 and which_data == 'PMNIST'):\n",
    "        #     k = epoch // domain_il_epoch\n",
    "        #     xtrain=data[k]['train']['x']\n",
    "        #     ytrain=data[k]['train']['y']\n",
    "        #     xtest =data[k]['test']['x']\n",
    "        #     ytest =data[k]['test']['y']\n",
    "\n",
    "        \n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        if (ddp_on == True):\n",
    "            if torch.distributed.get_rank() == 0:   \n",
    "                iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        else:\n",
    "            iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "        \n",
    "        #### validation_interval이 batch size보다 작을 시 validation_interval을 batch size로 맞춰줌#############\n",
    "        validation_interval2 = validation_interval\n",
    "        if (validation_interval > len(iterator)):\n",
    "            validation_interval2 = len(iterator)\n",
    "        ##################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            iter_one_train_time_start = time.time()\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                # print('x_len',x_len)\n",
    "                # mask = padded_sequence_mask(x_len)\n",
    "                # max_time_step = x_len.max()\n",
    "                # min_time_step = x_len.min()\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "                    \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH)\n",
    "            # ######################################################################################################\n",
    "\n",
    "\n",
    "            ## device로 보내주기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "\n",
    "            \n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                \n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            if i % 10 == 0:\n",
    "                print(iter_acc)\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            if i % verbose_interval == verbose_interval-1:\n",
    "                print(f'{epoch}-{i} training acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}, val_acc: {100 * val_acc_now:.2f}%')\n",
    "            iter_acc_string = f'{epoch}-{i}/{len(train_loader)} iter_acc: {100 * iter_acc:.2f}%, lr={[f\"{lr}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "\n",
    "            if (i % verbose_interval == verbose_interval-1):\n",
    "                print(f\"iter_one_train_time: {elapsed_time} seconds, last one_val_time: {elapsed_time_val} seconds\\n\")\n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i % validation_interval2 == validation_interval2-1:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data in test_loader:\n",
    "                        ## data loading & semi-pre-processing ##########################################################\n",
    "                        if len(data) == 2:\n",
    "                            inputs, labels = data\n",
    "                            # 처리 로직 작성\n",
    "                        elif len(data) == 3:\n",
    "                            inputs, labels, x_len = data\n",
    "                            # print('x_len',x_len)\n",
    "                            # mask = padded_sequence_mask(x_len)\n",
    "                            # max_time_step = x_len.max()\n",
    "                            # min_time_step = x_len.min()\n",
    "                            # B, T, *spatial_dims = inputs.shape\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        real_batch = labels.size(0)\n",
    "\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs.permute(1, 0, 2, 3, 4)) #inputs: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss = criterion(outputs, labels)\n",
    "                        else:\n",
    "                            val_loss=0\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs[t])\n",
    "                                loss = criterion(outputs, labels)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += loss.data\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_acc_now = correct / total\n",
    "                    # print(f'{epoch}-{i} validation acc: {100 * val_acc_now:.2f}%, lr={[f\"{lr:.10f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}')\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time_val = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                # print(f\"iter_one_val_time: {elapsed_time_val} seconds\")\n",
    "\n",
    "                # network save\n",
    "                if val_acc < val_acc_now:\n",
    "                    val_acc = val_acc_now\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "                    # torch.save(net, f\"net_save/save_now_net_{unique_name}.pth\")\n",
    "                    # torch.save(net.module.state_dict(), f\"net_save/save_now_net_weights2_{unique_name}.pth\")\n",
    "                    # torch.save(net.module, f\"net_save/save_now_net2_{unique_name}.pth\")\n",
    "            ####################################################################################################################################################\n",
    "            iterator.set_description(f\"iter_acc: {iter_acc_string}, iter_loss: {loss}, val_acc: {100 * val_acc_now:.2f}%\")  \n",
    "            wandb.log({\"iter_acc\": iter_acc}, step=i+epoch*len(train_loader))\n",
    "            wandb.log({\"tr_acc\": tr_acc}, step=i+epoch*len(train_loader))\n",
    "            wandb.log({\"val_acc_now\": val_acc_now}, step=i+epoch*len(train_loader))\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            iter_acc_array = np.append(iter_acc_array, iter_acc)\n",
    "            tr_acc_array = np.append(tr_acc_array, tr_acc)\n",
    "            val_acc_now_array = np.append(val_acc_now_array, val_acc_now)\n",
    "            base_name = f'{current_time}'\n",
    "            iter_acc_file_name_time = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "            tr_acc_file_name_time = f'result_save/{base_name}_tr_acc_array_{unique_name}.npy'\n",
    "            val_acc_file_name_time = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "            hyperparameters_file_name_time = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "            hyperparameters['current epoch'] = epoch\n",
    "\n",
    "            ### 모듈 세이브: 덮어쓰기 하기 싫으면 주석 풀어서 사용 (시간마다 새로 쓰기) 비추천 ########################\n",
    "            # np.save(iter_acc_file_name_time, iter_acc_array)\n",
    "            # np.save(tr_acc_file_name_time, iter_acc_array)\n",
    "            # np.save(val_acc_file_name_time, val_acc_now_array)\n",
    "            # with open(hyperparameters_file_name_time, 'w') as f:\n",
    "            #     json.dump(hyperparameters, f, indent=4)\n",
    "            #########################################################################################################\n",
    "\n",
    "            ## 모듈 세이브 ###########################################################################################\n",
    "            # np.save(f'result_save/iter_acc_array_{unique_name}.npy', iter_acc_array)\n",
    "            # np.save(f'result_save/tr_acc_array_{unique_name}.npy', tr_acc_array)\n",
    "            # np.save(f'result_save/val_acc_now_array_{unique_name}.npy', val_acc_now_array)\n",
    "            # with open(f'result_save/hyperparameters_{unique_name}.json', 'w') as f:\n",
    "            #     json.dump(hyperparameters, f, indent=4)\n",
    "            ##########################################################################################################\n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "                \n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "        # 실행 시간 계산\n",
    "        epoch_time_end = time.time()\n",
    "        print(f\"epoch_time: {epoch_time_end - epoch_start_time} seconds\\n\") \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240724_192255-duocpdnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/duocpdnb' target=\"_blank\">rich-paper-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/duocpdnb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/duocpdnb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "DataParallel(\n",
      "  (module): MY_SNN_CONV_ottt_sstep(\n",
      "    (layers): OTTTSequential(\n",
      "      (0): SYNAPSE_CONV_trace_sstep()\n",
      "      (1): LIF_layer_trace_sstep()\n",
      "      (2): Scale()\n",
      "      (3): SYNAPSE_CONV_trace_sstep()\n",
      "      (4): LIF_layer_trace_sstep()\n",
      "      (5): Scale()\n",
      "      (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (7): SYNAPSE_CONV_trace_sstep()\n",
      "      (8): LIF_layer_trace_sstep()\n",
      "      (9): Scale()\n",
      "      (10): SYNAPSE_CONV_trace_sstep()\n",
      "      (11): LIF_layer_trace_sstep()\n",
      "      (12): Scale()\n",
      "      (13): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (14): SYNAPSE_CONV_trace_sstep()\n",
      "      (15): LIF_layer_trace_sstep()\n",
      "      (16): Scale()\n",
      "      (17): SYNAPSE_CONV_trace_sstep()\n",
      "      (18): LIF_layer_trace_sstep()\n",
      "      (19): Scale()\n",
      "      (20): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (21): SYNAPSE_CONV_trace_sstep()\n",
      "      (22): LIF_layer_trace_sstep()\n",
      "      (23): Scale()\n",
      "      (24): SYNAPSE_CONV_trace_sstep()\n",
      "      (25): LIF_layer_trace_sstep()\n",
      "      (26): Scale()\n",
      "      (27): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (28): DimChanger_for_FC_sstep()\n",
      "      (29): SYNAPSE_FC_trace_sstep()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==================================================\n",
      "My Num of PARAMS: 9,225,610, system's param_num : 9,228,362\n",
      "Memory: 35.19MiB at 32-bit\n",
      "==================================================\n",
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-0/391 iter_acc: 11.72%, lr=['0.1'], iter_loss: 2.1897177696228027, val_acc: 0.00%:   0%|          | 1/391 [00:01<08:11,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-10/391 iter_acc: 11.72%, lr=['0.1'], iter_loss: 2.1963155269622803, val_acc: 0.00%:   3%|▎         | 11/391 [00:05<02:23,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-20/391 iter_acc: 9.38%, lr=['0.1'], iter_loss: 2.181701898574829, val_acc: 0.00%:   5%|▌         | 21/391 [00:08<02:15,  2.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-30/391 iter_acc: 13.28%, lr=['0.1'], iter_loss: 2.170358657836914, val_acc: 0.00%:   8%|▊         | 31/391 [00:13<02:41,  2.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-40/391 iter_acc: 21.88%, lr=['0.1'], iter_loss: 2.0783309936523438, val_acc: 0.00%:  10%|█         | 41/391 [00:18<03:22,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-50/391 iter_acc: 23.44%, lr=['0.1'], iter_loss: 2.0126986503601074, val_acc: 0.00%:  13%|█▎        | 51/391 [00:22<02:13,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-60/391 iter_acc: 25.00%, lr=['0.1'], iter_loss: 1.9056624174118042, val_acc: 0.00%:  16%|█▌        | 61/391 [00:26<02:06,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-70/391 iter_acc: 19.53%, lr=['0.1'], iter_loss: 1.9967660903930664, val_acc: 0.00%:  18%|█▊        | 71/391 [00:30<02:05,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-80/391 iter_acc: 25.00%, lr=['0.1'], iter_loss: 1.9172331094741821, val_acc: 0.00%:  21%|██        | 81/391 [00:35<02:27,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-90/391 iter_acc: 24.22%, lr=['0.1'], iter_loss: 1.8886576890945435, val_acc: 0.00%:  23%|██▎       | 91/391 [00:43<03:52,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-100/391 iter_acc: 26.56%, lr=['0.1'], iter_loss: 1.8702223300933838, val_acc: 0.00%:  26%|██▌       | 101/391 [00:49<02:01,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-110/391 iter_acc: 25.00%, lr=['0.1'], iter_loss: 1.8777198791503906, val_acc: 0.00%:  28%|██▊       | 111/391 [00:53<01:45,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter_acc: 0-118/391 iter_acc: 32.03%, lr=['0.1'], iter_loss: 1.8524060249328613, val_acc: 0.00%:  30%|███       | 119/391 [00:56<01:42,  2.65it/s]"
     ]
    }
   ],
   "source": [
    "### my_snn control board ########################\n",
    "decay = 0.5 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "wandb.init(project= f'my_snn {unique_name}')\n",
    "my_snn_system(  devices = \"3\",\n",
    "                single_step = True, # True # False\n",
    "                unique_name = unique_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 6 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 128, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "                #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'CIFAR10',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = [64],\n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], #ottt\n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [], \n",
    "                \n",
    "                net_print = True, # True # False\n",
    "                weight_count_print = False, # True # False\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_{unique_name}.pth\",\n",
    "                learning_rate = 0.1, # default 0.001  # ottt 0.1 0.00001 # nda 0.001 \n",
    "                epoch_num = 300,\n",
    "                verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "                validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "                gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False,   # True # False\n",
    "\n",
    "                nda_net = False,   # True # False\n",
    "\n",
    "                domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "                dvs_clipping = True, # dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                dvs_duration = 1000000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                #있는 데이터들 #gesture 1000000 #nmnist 10000\n",
    "\n",
    "                OTTT_sWS_on = True, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "                \n",
    "                ) \n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling\n",
    "# 이 낫다. \n",
    " \n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes',\n",
    "#     'name': 'my_snn_sweep',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_now'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         \"learning_rate\": {\"values\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0]},\n",
    "#         \"batch_size\": {\"values\": [64, 96, 128]},\n",
    "#         \"decay\": {\"values\": [0.3,0.4,0.5,0.6,0.7,0.8,0.875,0.9]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     unique_name = unique_name_hyper ## 이거 설정하면 새로운 경로에 모두 save\n",
    "    \n",
    "#     wandb.init()\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     batch_size  =  wandb.config.batch_size\n",
    "#     decay  =  wandb.config.decay\n",
    "\n",
    "#     my_snn_system(  devices = \"3\",\n",
    "#                     single_step = True, # True # False\n",
    "#                     unique_name = unique_name,\n",
    "#                     my_seed = 42,\n",
    "#                     TIME = 6 , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                     BATCH = batch_size, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                     IMAGE_SIZE = 32, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28\n",
    "#                     # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                     #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                     # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                     which_data = 'CIFAR10',\n",
    "#     # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "#     # 'DVS_GESTURE','DVS_CIFAR10_2','NMNIST','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                     # CLASS_NUM = 10,\n",
    "#                     data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                     rate_coding = False, # True # False\n",
    "\n",
    "#                     lif_layer_v_init = 0.0,\n",
    "#                     lif_layer_v_decay = decay,\n",
    "#                     lif_layer_v_threshold = 1.0,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                     lif_layer_v_reset = 0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                     lif_layer_sg_width = 1.0, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                     # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                     synapse_conv_kernel_size = 3,\n",
    "#                     synapse_conv_stride = 1,\n",
    "#                     synapse_conv_padding = 1,\n",
    "#                     synapse_conv_trace_const1 = 1,\n",
    "#                     synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                     # synapse_fc_out_features = CLASS_NUM,\n",
    "#                     synapse_fc_trace_const1 = 1,\n",
    "#                     synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "#                     pre_trained = False, # True # False\n",
    "#                     convTrue_fcFalse = True, # True # False\n",
    "\n",
    "#                     # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                     # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                     # cfg = [64],\n",
    "#                     # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                     cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                     # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], #ottt\n",
    "#                     # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], # ottt \n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                     # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                     # cfg = [20001,10001], # depthwise, separable\n",
    "#                     # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                     # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                     # cfg = [], \n",
    "                    \n",
    "#                     net_print = True, # True # False\n",
    "#                     weight_count_print = False, # True # False\n",
    "                    \n",
    "#                     pre_trained_path = f\"net_save/save_now_net_{unique_name}.pth\",\n",
    "#                     learning_rate = learning_rate, # default 0.001  # ottt 0.1 0.00001 # nda 0.001 \n",
    "#                     epoch_num = 4,\n",
    "#                     verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                     validation_interval = 999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                     tdBN_on = False,  # True # False\n",
    "#                     BN_on = False,  # True # False\n",
    "                    \n",
    "#                     surrogate = 'sigmoid', # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                    \n",
    "#                     gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                     BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                     optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                     scheduler_name = 'CosineAnnealingLR', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                    \n",
    "#                     ddp_on = False,   # True # False\n",
    "\n",
    "#                     nda_net = False,   # True # False\n",
    "\n",
    "#                     domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                    \n",
    "#                     dvs_clipping = True, # dvs zero&one  # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                     dvs_duration = 1000000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                     #있는 데이터들 #gesture 1000000 #nmnist 10000\n",
    "\n",
    "#                     OTTT_sWS_on = True, # True # False # BPTT끄고, CONV에만 적용됨.\n",
    "                    \n",
    "#                     ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def pad_array_to_match_length(array1, array2):\n",
    "#     if len(array1) > len(array2):\n",
    "#         padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "#         return array1, padded_array2\n",
    "#     elif len(array2) > len(array1):\n",
    "#         padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "#         return padded_array1, array2\n",
    "#     else:\n",
    "#         return array1, array2\n",
    "# def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# current_time = '20240628_110116'\n",
    "# base_name = f'{current_time}'\n",
    "# iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "# ### if you want to just see most recent train and val acc###########################\n",
    "# iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "# tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "# loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "# loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "# loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "# hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "# loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "# loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "# loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "# top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "# top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "# top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "# which_data = hyperparameters['which_data']\n",
    "# BPTT_on = hyperparameters['BPTT_on']\n",
    "# current_epoch = hyperparameters['current epoch']\n",
    "# surrogate = hyperparameters['surrogate']\n",
    "# cfg = hyperparameters['cfg']\n",
    "# tdBN_on = hyperparameters['tdBN_on']\n",
    "# BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "# iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# # 그래프 그리기\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "# plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "# plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # # 텍스트 추가\n",
    "# # plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# # plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "# plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# # 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "# title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "# plt.title(title)\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlim(0)  # x축을 0부터 시작\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
