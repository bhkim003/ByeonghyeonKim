{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17145/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    \n",
    "                    random_select_ratio = 4,\n",
    "                    leaky_temporal_filter= 1.0,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "\n",
    "                now_T = inputs.shape[1]\n",
    "                window_T = temporal_filter * TIME  # Ìïú Î∞ïÏä§(time window) Í∏∏Ïù¥\n",
    "\n",
    "                # 1) Î∞ïÏä§ Í∞úÏàò Í≥ÑÏÇ∞\n",
    "                num_windows = now_T // window_T   # Îî± Îñ®Ïñ¥ÏßÄÎäî Î∞ïÏä§ Í∞úÏàò\n",
    "\n",
    "                assert num_windows >= 1\n",
    "\n",
    "                # 2) ÎÇ®Îäî ÎÇòÎ®∏ÏßÄ timestepÏùÄ ÏûòÎùºÎ≤ÑÎ¶¨Í≥†, Îî± Î∞ïÏä§ Îã®ÏúÑÍπåÏßÄÎßå ÏÇ¨Ïö©\n",
    "                valid_T = num_windows * window_T\n",
    "                inputs = inputs[:, :valid_T]   # shape: [1, valid_T, C, H, W]\n",
    "\n",
    "                # 3) (B=1, num_windows, window_T, ...) ÌòïÌÉúÎ°ú reshape\n",
    "                B, T, *rest = inputs.shape  # B=1\n",
    "                inputs_reshaped = inputs.view(B, num_windows, window_T, *rest)\n",
    "                # inputs_reshaped: [1, num_windows, window_T, C, H, W] Í∞ÄÏ†ï\n",
    "\n",
    "                # 4) Í∞Å Î∞ïÏä§Î≥Ñ Ìï© (Ïä§ÌååÏù¥ÌÅ¨ Í∞úÏàò) Í≥ÑÏÇ∞\n",
    "                #    dim=(2,3,4,5): window_T, C, H, W Ï†ÑÏ≤¥ Ìï©\n",
    "                window_sums = inputs_reshaped.sum(dim=tuple(range(2, inputs_reshaped.dim())))  # shape: [1, num_windows]\n",
    "                window_sums = window_sums[0]  # [num_windows]\n",
    "\n",
    "                # 5) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ ÏÑ†ÌÉù\n",
    "                #    random_select_ratio ÎπÑÏú®ÎßåÌÅº ÏÉÅÏúÑ Î∞ïÏä§Îßå ÌõÑÎ≥¥Î°ú ÏîÄ\n",
    "                # print(f'num_windows: {num_windows}, random_select_ratio: {random_select_ratio}')\n",
    "                N = min(num_windows, round(random_select_ratio))\n",
    "                # N = max(1, min(num_windows, round(num_windows * random_select_ratio)))\n",
    "                topk_vals, topk_idx = torch.topk(window_sums, k=N)  # top-N Î∞ïÏä§ Ïù∏Îç±Ïä§\n",
    "                # print(f'N: {N}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 6) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ Ï§ëÏóêÏÑú ÎûúÎç§ÌïòÍ≤å ÌïòÎÇò ÏÑ†ÌÉù\n",
    "                #    (python random ÎòêÎäî torch.randperm Îëò Îã§ Í∞ÄÎä•)\n",
    "                rand_pos = random.randint(0, N - 1)\n",
    "                chosen_win_idx = topk_idx[rand_pos].item()  # 0 ~ num_windows-1 Ï§ë ÌïòÎÇò\n",
    "                # print(f'chosen_win_idx: {chosen_win_idx}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 7) ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í∑∏ Î∞ïÏä§ Íµ¨Í∞ÑÎßå ÏÇ¨Ïö©\n",
    "                start_idx = chosen_win_idx * window_T\n",
    "                end_idx = start_idx + window_T\n",
    "                inputs = inputs[:, start_idx:end_idx]\n",
    "                # shape: [1, window_T, C, H, W]\n",
    "\n",
    "                # print(f'inputs.shape after random select: {inputs.shape}')\n",
    "\n",
    "\n",
    "                if temporal_filter_accumulation == False:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # print(f'inputs.shape: {inputs.shape}')\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if labels.item() == 6:\n",
    "            #     # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            #     ##############################################################################################\n",
    "            #     dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            #     #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if temporal_filter_accumulation == False:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 random_select_ratio = 4,\n",
    "#                 leaky_temporal_filter= 0.0,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m36rifiv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_144418-m36rifiv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m36rifiv' target=\"_blank\">balmy-sweep-40</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m36rifiv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m36rifiv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251114_144425_268', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 467.0\n",
      "lif layer 1 self.abs_max_v: 467.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 588.0\n",
      "lif layer 1 self.abs_max_v: 747.5\n",
      "fc layer 1 self.abs_max_out: 846.0\n",
      "lif layer 1 self.abs_max_v: 1220.0\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "lif layer 2 self.abs_max_v: 192.0\n",
      "lif layer 1 self.abs_max_v: 1278.5\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 387.5\n",
      "fc layer 1 self.abs_max_out: 896.0\n",
      "lif layer 2 self.abs_max_v: 394.5\n",
      "fc layer 1 self.abs_max_out: 956.0\n",
      "lif layer 1 self.abs_max_v: 1282.0\n",
      "fc layer 1 self.abs_max_out: 1039.0\n",
      "lif layer 2 self.abs_max_v: 422.5\n",
      "lif layer 1 self.abs_max_v: 1309.0\n",
      "fc layer 1 self.abs_max_out: 1049.0\n",
      "lif layer 1 self.abs_max_v: 1456.0\n",
      "fc layer 2 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 430.0\n",
      "fc layer 1 self.abs_max_out: 1063.0\n",
      "lif layer 1 self.abs_max_v: 1739.5\n",
      "fc layer 1 self.abs_max_out: 1402.0\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 718.0\n",
      "fc layer 1 self.abs_max_out: 1901.0\n",
      "lif layer 1 self.abs_max_v: 2403.0\n",
      "fc layer 2 self.abs_max_out: 1152.0\n",
      "lif layer 2 self.abs_max_v: 1359.0\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 1 self.abs_max_out: 2649.0\n",
      "lif layer 1 self.abs_max_v: 2649.0\n",
      "lif layer 2 self.abs_max_v: 1626.5\n",
      "fc layer 3 self.abs_max_out: 100.0\n",
      "fc layer 1 self.abs_max_out: 2775.0\n",
      "lif layer 1 self.abs_max_v: 2775.0\n",
      "fc layer 3 self.abs_max_out: 126.0\n",
      "fc layer 1 self.abs_max_out: 2928.0\n",
      "lif layer 1 self.abs_max_v: 2928.0\n",
      "fc layer 2 self.abs_max_out: 1163.0\n",
      "lif layer 2 self.abs_max_v: 1894.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "lif layer 2 self.abs_max_v: 2095.5\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "lif layer 2 self.abs_max_v: 2149.5\n",
      "fc layer 2 self.abs_max_out: 1880.0\n",
      "lif layer 2 self.abs_max_v: 2299.0\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "fc layer 1 self.abs_max_out: 3210.0\n",
      "lif layer 1 self.abs_max_v: 3210.0\n",
      "fc layer 1 self.abs_max_out: 3346.0\n",
      "lif layer 1 self.abs_max_v: 3346.0\n",
      "lif layer 2 self.abs_max_v: 2390.5\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "fc layer 2 self.abs_max_out: 2219.0\n",
      "lif layer 2 self.abs_max_v: 2500.5\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "lif layer 2 self.abs_max_v: 2544.0\n",
      "fc layer 1 self.abs_max_out: 3610.0\n",
      "lif layer 1 self.abs_max_v: 3610.0\n",
      "fc layer 1 self.abs_max_out: 5031.0\n",
      "lif layer 1 self.abs_max_v: 5031.0\n",
      "lif layer 2 self.abs_max_v: 2645.0\n",
      "lif layer 2 self.abs_max_v: 2824.5\n",
      "lif layer 2 self.abs_max_v: 2908.5\n",
      "lif layer 2 self.abs_max_v: 3589.5\n",
      "fc layer 2 self.abs_max_out: 2498.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 2 self.abs_max_out: 2768.0\n",
      "fc layer 2 self.abs_max_out: 2972.0\n",
      "lif layer 2 self.abs_max_v: 3691.0\n",
      "lif layer 2 self.abs_max_v: 4572.5\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "lif layer 1 self.abs_max_v: 5224.0\n",
      "lif layer 2 self.abs_max_v: 4620.5\n",
      "fc layer 3 self.abs_max_out: 555.0\n",
      "lif layer 2 self.abs_max_v: 4666.5\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 2 self.abs_max_out: 3146.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "fc layer 2 self.abs_max_out: 3152.0\n",
      "fc layer 1 self.abs_max_out: 5161.0\n",
      "lif layer 1 self.abs_max_v: 5387.0\n",
      "fc layer 2 self.abs_max_out: 3293.0\n",
      "fc layer 3 self.abs_max_out: 747.0\n",
      "fc layer 2 self.abs_max_out: 3361.0\n",
      "fc layer 2 self.abs_max_out: 3637.0\n",
      "fc layer 2 self.abs_max_out: 3928.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "lif layer 1 self.abs_max_v: 6106.5\n",
      "lif layer 2 self.abs_max_v: 5180.0\n",
      "lif layer 1 self.abs_max_v: 6691.5\n",
      "fc layer 1 self.abs_max_out: 6349.0\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "fc layer 1 self.abs_max_out: 6400.0\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "fc layer 2 self.abs_max_out: 4272.0\n",
      "lif layer 2 self.abs_max_v: 5310.0\n",
      "lif layer 2 self.abs_max_v: 5370.0\n",
      "lif layer 2 self.abs_max_v: 5450.0\n",
      "fc layer 1 self.abs_max_out: 7369.0\n",
      "lif layer 1 self.abs_max_v: 7369.0\n",
      "lif layer 2 self.abs_max_v: 5967.5\n",
      "fc layer 2 self.abs_max_out: 4347.0\n",
      "fc layer 2 self.abs_max_out: 4450.0\n",
      "fc layer 2 self.abs_max_out: 4460.0\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "fc layer 2 self.abs_max_out: 4491.0\n",
      "fc layer 2 self.abs_max_out: 5172.0\n",
      "fc layer 1 self.abs_max_out: 7658.0\n",
      "lif layer 1 self.abs_max_v: 7658.0\n",
      "fc layer 1 self.abs_max_out: 7953.0\n",
      "lif layer 1 self.abs_max_v: 7953.0\n",
      "fc layer 1 self.abs_max_out: 8082.0\n",
      "lif layer 1 self.abs_max_v: 8082.0\n",
      "fc layer 1 self.abs_max_out: 8423.0\n",
      "lif layer 1 self.abs_max_v: 8423.0\n",
      "fc layer 1 self.abs_max_out: 8485.0\n",
      "lif layer 1 self.abs_max_v: 8485.0\n",
      "fc layer 2 self.abs_max_out: 5235.0\n",
      "fc layer 2 self.abs_max_out: 5361.0\n",
      "fc layer 1 self.abs_max_out: 8740.0\n",
      "lif layer 1 self.abs_max_v: 8740.0\n",
      "lif layer 2 self.abs_max_v: 6039.0\n",
      "lif layer 2 self.abs_max_v: 6074.5\n",
      "lif layer 2 self.abs_max_v: 6252.0\n",
      "fc layer 3 self.abs_max_out: 951.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.074214/  2.138560, val:  35.00%, val_best:  35.00%, tr:  83.55%, tr_best:  83.55%, epoch time: 85.92 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 94.2837%\n",
      "layer   2  Sparsity: 77.5769%\n",
      "layer   3  Sparsity: 85.0440%\n",
      "total_backward_count 9790 real_backward_count 3891  39.745%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 9147.0\n",
      "lif layer 1 self.abs_max_v: 9147.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "fc layer 1 self.abs_max_out: 9444.0\n",
      "lif layer 1 self.abs_max_v: 9444.0\n",
      "fc layer 2 self.abs_max_out: 5566.0\n",
      "fc layer 2 self.abs_max_out: 5858.0\n",
      "fc layer 1 self.abs_max_out: 9449.0\n",
      "lif layer 1 self.abs_max_v: 9449.0\n",
      "lif layer 2 self.abs_max_v: 6570.0\n",
      "fc layer 3 self.abs_max_out: 1018.0\n",
      "fc layer 1 self.abs_max_out: 9575.0\n",
      "lif layer 1 self.abs_max_v: 9575.0\n",
      "fc layer 1 self.abs_max_out: 10534.0\n",
      "lif layer 1 self.abs_max_v: 10534.0\n",
      "lif layer 2 self.abs_max_v: 6657.5\n",
      "lif layer 2 self.abs_max_v: 6848.0\n",
      "fc layer 2 self.abs_max_out: 6234.0\n",
      "fc layer 3 self.abs_max_out: 1114.0\n",
      "fc layer 2 self.abs_max_out: 6541.0\n",
      "lif layer 2 self.abs_max_v: 6852.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.030427/  2.118062, val:  46.67%, val_best:  46.67%, tr:  95.30%, tr_best:  95.30%, epoch time: 88.24 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 94.2946%\n",
      "layer   2  Sparsity: 74.3144%\n",
      "layer   3  Sparsity: 80.6097%\n",
      "total_backward_count 19580 real_backward_count 6379  32.579%\n",
      "fc layer 2 self.abs_max_out: 7080.0\n",
      "lif layer 2 self.abs_max_v: 7080.0\n",
      "fc layer 1 self.abs_max_out: 10829.0\n",
      "lif layer 1 self.abs_max_v: 10829.0\n",
      "lif layer 2 self.abs_max_v: 7175.5\n",
      "lif layer 2 self.abs_max_v: 7520.0\n",
      "lif layer 2 self.abs_max_v: 7840.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.034130/  2.132420, val:  40.42%, val_best:  46.67%, tr:  97.45%, tr_best:  97.45%, epoch time: 90.43 seconds, 1.51 minutes\n",
      "layer   1  Sparsity: 94.2999%\n",
      "layer   2  Sparsity: 74.6466%\n",
      "layer   3  Sparsity: 79.3185%\n",
      "total_backward_count 29370 real_backward_count 8683  29.564%\n",
      "fc layer 2 self.abs_max_out: 7273.0\n",
      "fc layer 1 self.abs_max_out: 10830.0\n",
      "lif layer 1 self.abs_max_v: 10830.0\n",
      "lif layer 1 self.abs_max_v: 10908.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.043948/  2.121494, val:  41.25%, val_best:  46.67%, tr:  98.88%, tr_best:  98.88%, epoch time: 88.98 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 94.2875%\n",
      "layer   2  Sparsity: 74.5983%\n",
      "layer   3  Sparsity: 78.7390%\n",
      "total_backward_count 39160 real_backward_count 10728  27.395%\n",
      "fc layer 1 self.abs_max_out: 11824.0\n",
      "lif layer 1 self.abs_max_v: 11824.0\n",
      "fc layer 2 self.abs_max_out: 7891.0\n",
      "lif layer 2 self.abs_max_v: 7891.0\n",
      "fc layer 2 self.abs_max_out: 8415.0\n",
      "lif layer 2 self.abs_max_v: 8415.0\n",
      "lif layer 1 self.abs_max_v: 13148.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.032854/  2.126128, val:  39.58%, val_best:  46.67%, tr:  99.08%, tr_best:  99.08%, epoch time: 97.60 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 94.2846%\n",
      "layer   2  Sparsity: 73.8156%\n",
      "layer   3  Sparsity: 78.2099%\n",
      "total_backward_count 48950 real_backward_count 12754  26.055%\n",
      "fc layer 1 self.abs_max_out: 12186.0\n",
      "lif layer 1 self.abs_max_v: 13785.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.038272/  2.133630, val:  47.92%, val_best:  47.92%, tr:  98.26%, tr_best:  99.08%, epoch time: 98.76 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 94.2896%\n",
      "layer   2  Sparsity: 73.8391%\n",
      "layer   3  Sparsity: 77.6096%\n",
      "total_backward_count 58740 real_backward_count 14747  25.106%\n",
      "fc layer 1 self.abs_max_out: 12365.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.038472/  2.141002, val:  47.92%, val_best:  47.92%, tr:  99.08%, tr_best:  99.08%, epoch time: 99.45 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 94.2738%\n",
      "layer   2  Sparsity: 73.8403%\n",
      "layer   3  Sparsity: 77.8436%\n",
      "total_backward_count 68530 real_backward_count 16682  24.343%\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.039547/  2.130588, val:  41.67%, val_best:  47.92%, tr:  98.88%, tr_best:  99.08%, epoch time: 97.24 seconds, 1.62 minutes\n",
      "layer   1  Sparsity: 94.3001%\n",
      "layer   2  Sparsity: 73.7991%\n",
      "layer   3  Sparsity: 77.5739%\n",
      "total_backward_count 78320 real_backward_count 18511  23.635%\n",
      "fc layer 1 self.abs_max_out: 12635.0\n",
      "lif layer 1 self.abs_max_v: 14668.0\n",
      "lif layer 1 self.abs_max_v: 15459.0\n",
      "lif layer 2 self.abs_max_v: 8618.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.042575/  2.108349, val:  54.17%, val_best:  54.17%, tr:  98.67%, tr_best:  99.08%, epoch time: 98.65 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 94.3006%\n",
      "layer   2  Sparsity: 73.9214%\n",
      "layer   3  Sparsity: 77.5949%\n",
      "total_backward_count 88110 real_backward_count 20382  23.132%\n",
      "lif layer 1 self.abs_max_v: 15560.5\n",
      "lif layer 1 self.abs_max_v: 16341.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.036621/  2.142137, val:  40.00%, val_best:  54.17%, tr:  98.77%, tr_best:  99.08%, epoch time: 101.99 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2937%\n",
      "layer   2  Sparsity: 73.7153%\n",
      "layer   3  Sparsity: 77.2371%\n",
      "total_backward_count 97900 real_backward_count 22146  22.621%\n",
      "fc layer 1 self.abs_max_out: 13115.0\n",
      "lif layer 2 self.abs_max_v: 8867.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.023204/  2.142103, val:  41.67%, val_best:  54.17%, tr:  98.77%, tr_best:  99.08%, epoch time: 102.02 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2940%\n",
      "layer   2  Sparsity: 73.6939%\n",
      "layer   3  Sparsity: 76.7555%\n",
      "total_backward_count 107690 real_backward_count 23973  22.261%\n",
      "fc layer 1 self.abs_max_out: 13129.0\n",
      "lif layer 1 self.abs_max_v: 16520.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.034078/  2.124988, val:  41.67%, val_best:  54.17%, tr:  98.98%, tr_best:  99.08%, epoch time: 102.50 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2736%\n",
      "layer   2  Sparsity: 73.6784%\n",
      "layer   3  Sparsity: 77.2293%\n",
      "total_backward_count 117480 real_backward_count 25783  21.947%\n",
      "lif layer 2 self.abs_max_v: 8955.0\n",
      "lif layer 2 self.abs_max_v: 9822.5\n",
      "fc layer 1 self.abs_max_out: 14456.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.024622/  2.116485, val:  41.67%, val_best:  54.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 101.44 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2935%\n",
      "layer   2  Sparsity: 73.4627%\n",
      "layer   3  Sparsity: 77.5797%\n",
      "total_backward_count 127270 real_backward_count 27535  21.635%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.023520/  2.114647, val:  33.33%, val_best:  54.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 100.38 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2970%\n",
      "layer   2  Sparsity: 72.9695%\n",
      "layer   3  Sparsity: 76.7936%\n",
      "total_backward_count 137060 real_backward_count 29305  21.381%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.030828/  2.113914, val:  47.08%, val_best:  54.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 98.57 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 94.2758%\n",
      "layer   2  Sparsity: 73.0702%\n",
      "layer   3  Sparsity: 78.1033%\n",
      "total_backward_count 146850 real_backward_count 31032  21.132%\n",
      "lif layer 1 self.abs_max_v: 17312.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.040468/  2.129913, val:  42.08%, val_best:  54.17%, tr:  98.98%, tr_best:  99.08%, epoch time: 97.78 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 94.2802%\n",
      "layer   2  Sparsity: 73.1651%\n",
      "layer   3  Sparsity: 79.2363%\n",
      "total_backward_count 156640 real_backward_count 32872  20.986%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.043851/  2.135391, val:  50.42%, val_best:  54.17%, tr:  98.57%, tr_best:  99.08%, epoch time: 101.01 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2925%\n",
      "layer   2  Sparsity: 73.0215%\n",
      "layer   3  Sparsity: 78.9194%\n",
      "total_backward_count 166430 real_backward_count 34678  20.836%\n",
      "fc layer 1 self.abs_max_out: 15182.0\n",
      "lif layer 1 self.abs_max_v: 17561.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.048326/  2.121180, val:  55.42%, val_best:  55.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 102.06 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2647%\n",
      "layer   2  Sparsity: 73.0473%\n",
      "layer   3  Sparsity: 78.8105%\n",
      "total_backward_count 176220 real_backward_count 36461  20.691%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.048066/  2.138678, val:  45.00%, val_best:  55.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 99.44 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 94.2796%\n",
      "layer   2  Sparsity: 72.6616%\n",
      "layer   3  Sparsity: 79.5742%\n",
      "total_backward_count 186010 real_backward_count 38229  20.552%\n",
      "lif layer 1 self.abs_max_v: 17571.5\n",
      "lif layer 1 self.abs_max_v: 18448.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.043344/  2.132054, val:  40.83%, val_best:  55.42%, tr:  98.88%, tr_best:  99.18%, epoch time: 100.50 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2765%\n",
      "layer   2  Sparsity: 72.6674%\n",
      "layer   3  Sparsity: 79.0261%\n",
      "total_backward_count 195800 real_backward_count 39922  20.389%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.040805/  2.133572, val:  41.67%, val_best:  55.42%, tr:  99.08%, tr_best:  99.18%, epoch time: 100.71 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2755%\n",
      "layer   2  Sparsity: 72.4030%\n",
      "layer   3  Sparsity: 78.4817%\n",
      "total_backward_count 205590 real_backward_count 41673  20.270%\n",
      "lif layer 1 self.abs_max_v: 18455.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.039133/  2.136307, val:  47.08%, val_best:  55.42%, tr:  98.88%, tr_best:  99.18%, epoch time: 101.39 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2927%\n",
      "layer   2  Sparsity: 72.2140%\n",
      "layer   3  Sparsity: 78.5296%\n",
      "total_backward_count 215380 real_backward_count 43443  20.170%\n",
      "lif layer 1 self.abs_max_v: 18502.5\n",
      "lif layer 1 self.abs_max_v: 19444.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.042682/  2.122489, val:  55.00%, val_best:  55.42%, tr:  99.39%, tr_best:  99.39%, epoch time: 101.45 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2739%\n",
      "layer   2  Sparsity: 71.9462%\n",
      "layer   3  Sparsity: 78.4635%\n",
      "total_backward_count 225170 real_backward_count 45164  20.058%\n",
      "lif layer 1 self.abs_max_v: 19861.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.041673/  2.117188, val:  59.58%, val_best:  59.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 99.83 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 94.2624%\n",
      "layer   2  Sparsity: 72.3167%\n",
      "layer   3  Sparsity: 79.5368%\n",
      "total_backward_count 234960 real_backward_count 46894  19.958%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.044773/  2.115769, val:  52.92%, val_best:  59.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 98.37 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 94.2834%\n",
      "layer   2  Sparsity: 72.3277%\n",
      "layer   3  Sparsity: 79.9646%\n",
      "total_backward_count 244750 real_backward_count 48617  19.864%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.048353/  2.109842, val:  69.17%, val_best:  69.17%, tr:  99.08%, tr_best:  99.39%, epoch time: 99.28 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 94.2830%\n",
      "layer   2  Sparsity: 72.4524%\n",
      "layer   3  Sparsity: 79.5492%\n",
      "total_backward_count 254540 real_backward_count 50461  19.824%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.036825/  2.106584, val:  62.08%, val_best:  69.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 102.07 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2882%\n",
      "layer   2  Sparsity: 72.2534%\n",
      "layer   3  Sparsity: 78.3036%\n",
      "total_backward_count 264330 real_backward_count 52169  19.736%\n",
      "lif layer 1 self.abs_max_v: 20180.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.035831/  2.117486, val:  61.67%, val_best:  69.17%, tr:  98.57%, tr_best:  99.69%, epoch time: 101.67 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2647%\n",
      "layer   2  Sparsity: 72.2030%\n",
      "layer   3  Sparsity: 78.8798%\n",
      "total_backward_count 274120 real_backward_count 53931  19.674%\n",
      "lif layer 1 self.abs_max_v: 20359.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.035735/  2.130579, val:  37.50%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 101.96 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2751%\n",
      "layer   2  Sparsity: 72.3048%\n",
      "layer   3  Sparsity: 78.8404%\n",
      "total_backward_count 283910 real_backward_count 55588  19.579%\n",
      "lif layer 1 self.abs_max_v: 20610.5\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.042848/  2.127384, val:  46.25%, val_best:  69.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 100.17 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2852%\n",
      "layer   2  Sparsity: 72.4758%\n",
      "layer   3  Sparsity: 79.6571%\n",
      "total_backward_count 293700 real_backward_count 57284  19.504%\n",
      "lif layer 1 self.abs_max_v: 20739.0\n",
      "lif layer 1 self.abs_max_v: 21039.5\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.040173/  2.106794, val:  53.33%, val_best:  69.17%, tr:  98.98%, tr_best:  99.69%, epoch time: 100.76 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.3002%\n",
      "layer   2  Sparsity: 72.2935%\n",
      "layer   3  Sparsity: 79.3654%\n",
      "total_backward_count 303490 real_backward_count 58971  19.431%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.040382/  2.126973, val:  47.92%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 99.27 seconds, 1.65 minutes\n",
      "layer   1  Sparsity: 94.2660%\n",
      "layer   2  Sparsity: 72.1117%\n",
      "layer   3  Sparsity: 79.1162%\n",
      "total_backward_count 313280 real_backward_count 60666  19.365%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.039294/  2.133940, val:  44.58%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 102.85 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2827%\n",
      "layer   2  Sparsity: 72.1613%\n",
      "layer   3  Sparsity: 78.7808%\n",
      "total_backward_count 323070 real_backward_count 62345  19.298%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  2.034123/  2.113592, val:  60.42%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 102.64 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.3023%\n",
      "layer   2  Sparsity: 72.0539%\n",
      "layer   3  Sparsity: 78.9480%\n",
      "total_backward_count 332860 real_backward_count 64070  19.248%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  2.040568/  2.122747, val:  47.50%, val_best:  69.17%, tr:  99.59%, tr_best:  99.69%, epoch time: 100.94 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2879%\n",
      "layer   2  Sparsity: 71.9630%\n",
      "layer   3  Sparsity: 79.5130%\n",
      "total_backward_count 342650 real_backward_count 65732  19.183%\n",
      "lif layer 1 self.abs_max_v: 21248.0\n",
      "lif layer 1 self.abs_max_v: 21682.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  2.035971/  2.127858, val:  50.83%, val_best:  69.17%, tr:  99.59%, tr_best:  99.69%, epoch time: 100.20 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2760%\n",
      "layer   2  Sparsity: 72.0566%\n",
      "layer   3  Sparsity: 79.0984%\n",
      "total_backward_count 352440 real_backward_count 67384  19.119%\n",
      "lif layer 1 self.abs_max_v: 22132.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  2.028291/  2.098681, val:  57.08%, val_best:  69.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 100.58 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2968%\n",
      "layer   2  Sparsity: 71.9982%\n",
      "layer   3  Sparsity: 79.1415%\n",
      "total_backward_count 362230 real_backward_count 69023  19.055%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  2.030323/  2.126491, val:  48.75%, val_best:  69.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 100.69 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2977%\n",
      "layer   2  Sparsity: 71.7715%\n",
      "layer   3  Sparsity: 79.1157%\n",
      "total_backward_count 372020 real_backward_count 70660  18.994%\n",
      "fc layer 1 self.abs_max_out: 16432.0\n",
      "lif layer 1 self.abs_max_v: 22396.5\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  2.024263/  2.118781, val:  56.25%, val_best:  69.17%, tr:  99.18%, tr_best:  99.69%, epoch time: 101.41 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2694%\n",
      "layer   2  Sparsity: 71.6489%\n",
      "layer   3  Sparsity: 78.6452%\n",
      "total_backward_count 381810 real_backward_count 72427  18.969%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  2.025246/  2.110277, val:  60.00%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 100.15 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2835%\n",
      "layer   2  Sparsity: 71.8872%\n",
      "layer   3  Sparsity: 79.1055%\n",
      "total_backward_count 391600 real_backward_count 74072  18.915%\n",
      "lif layer 2 self.abs_max_v: 10204.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  2.025175/  2.092642, val:  69.58%, val_best:  69.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 100.49 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2784%\n",
      "layer   2  Sparsity: 71.7854%\n",
      "layer   3  Sparsity: 78.5108%\n",
      "total_backward_count 401390 real_backward_count 75705  18.861%\n",
      "lif layer 1 self.abs_max_v: 23265.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  2.012256/  2.098513, val:  53.33%, val_best:  69.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 100.73 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2846%\n",
      "layer   2  Sparsity: 71.3343%\n",
      "layer   3  Sparsity: 78.1652%\n",
      "total_backward_count 411180 real_backward_count 77332  18.807%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  2.011878/  2.110976, val:  56.25%, val_best:  69.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 98.61 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 94.2941%\n",
      "layer   2  Sparsity: 71.4953%\n",
      "layer   3  Sparsity: 79.0089%\n",
      "total_backward_count 420970 real_backward_count 78977  18.761%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  2.029046/  2.103468, val:  73.75%, val_best:  73.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 100.89 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.3018%\n",
      "layer   2  Sparsity: 71.4272%\n",
      "layer   3  Sparsity: 79.7086%\n",
      "total_backward_count 430760 real_backward_count 80613  18.714%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  2.026671/  2.103886, val:  52.50%, val_best:  73.75%, tr:  98.98%, tr_best:  99.80%, epoch time: 99.92 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.3145%\n",
      "layer   2  Sparsity: 71.2243%\n",
      "layer   3  Sparsity: 79.2116%\n",
      "total_backward_count 440550 real_backward_count 82297  18.681%\n",
      "lif layer 1 self.abs_max_v: 23896.5\n",
      "lif layer 2 self.abs_max_v: 10253.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  2.020742/  2.094563, val:  67.08%, val_best:  73.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 100.27 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2798%\n",
      "layer   2  Sparsity: 71.1217%\n",
      "layer   3  Sparsity: 78.6582%\n",
      "total_backward_count 450340 real_backward_count 83924  18.636%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  2.026363/  2.105307, val:  63.33%, val_best:  73.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 101.02 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2803%\n",
      "layer   2  Sparsity: 71.4551%\n",
      "layer   3  Sparsity: 78.8458%\n",
      "total_backward_count 460130 real_backward_count 85491  18.580%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  2.028516/  2.113519, val:  56.25%, val_best:  73.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 100.99 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2916%\n",
      "layer   2  Sparsity: 71.1537%\n",
      "layer   3  Sparsity: 79.1658%\n",
      "total_backward_count 469920 real_backward_count 87086  18.532%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  2.024355/  2.126830, val:  57.92%, val_best:  73.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 100.97 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2695%\n",
      "layer   2  Sparsity: 71.0292%\n",
      "layer   3  Sparsity: 79.7066%\n",
      "total_backward_count 479710 real_backward_count 88708  18.492%\n",
      "lif layer 1 self.abs_max_v: 23905.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  2.025297/  2.094024, val:  60.83%, val_best:  73.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 100.57 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2712%\n",
      "layer   2  Sparsity: 70.9661%\n",
      "layer   3  Sparsity: 79.4684%\n",
      "total_backward_count 489500 real_backward_count 90389  18.466%\n",
      "lif layer 1 self.abs_max_v: 25424.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  2.022347/  2.109063, val:  61.67%, val_best:  73.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 100.79 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.2879%\n",
      "layer   2  Sparsity: 71.2725%\n",
      "layer   3  Sparsity: 79.7408%\n",
      "total_backward_count 499290 real_backward_count 91979  18.422%\n",
      "fc layer 1 self.abs_max_out: 16652.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  2.030202/  2.116123, val:  61.25%, val_best:  73.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 101.68 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2636%\n",
      "layer   2  Sparsity: 70.9279%\n",
      "layer   3  Sparsity: 79.6692%\n",
      "total_backward_count 509080 real_backward_count 93575  18.381%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  2.021506/  2.109475, val:  56.67%, val_best:  73.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 100.44 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 94.2832%\n",
      "layer   2  Sparsity: 70.9240%\n",
      "layer   3  Sparsity: 79.3153%\n",
      "total_backward_count 518870 real_backward_count 95179  18.344%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  2.010634/  2.089330, val:  47.08%, val_best:  73.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 102.87 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2746%\n",
      "layer   2  Sparsity: 70.4703%\n",
      "layer   3  Sparsity: 78.9264%\n",
      "total_backward_count 528660 real_backward_count 96763  18.303%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  2.016558/  2.097611, val:  63.33%, val_best:  73.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 103.10 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2848%\n",
      "layer   2  Sparsity: 70.6278%\n",
      "layer   3  Sparsity: 78.5557%\n",
      "total_backward_count 538450 real_backward_count 98382  18.271%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  2.014572/  2.107873, val:  49.17%, val_best:  73.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 104.35 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2871%\n",
      "layer   2  Sparsity: 70.9750%\n",
      "layer   3  Sparsity: 79.1733%\n",
      "total_backward_count 548240 real_backward_count 99997  18.240%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  2.017887/  2.097901, val:  54.58%, val_best:  73.75%, tr:  99.18%, tr_best:  99.80%, epoch time: 102.38 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2684%\n",
      "layer   2  Sparsity: 70.8669%\n",
      "layer   3  Sparsity: 79.9834%\n",
      "total_backward_count 558030 real_backward_count 101651  18.216%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  2.015536/  2.085273, val:  67.08%, val_best:  73.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 103.17 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2861%\n",
      "layer   2  Sparsity: 70.7303%\n",
      "layer   3  Sparsity: 79.6922%\n",
      "total_backward_count 567820 real_backward_count 103243  18.182%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  2.006136/  2.079490, val:  61.67%, val_best:  73.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 104.18 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2851%\n",
      "layer   2  Sparsity: 70.6813%\n",
      "layer   3  Sparsity: 78.7605%\n",
      "total_backward_count 577610 real_backward_count 104768  18.138%\n",
      "lif layer 2 self.abs_max_v: 10266.5\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  2.004683/  2.100230, val:  50.83%, val_best:  73.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 106.91 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2941%\n",
      "layer   2  Sparsity: 70.3525%\n",
      "layer   3  Sparsity: 78.3726%\n",
      "total_backward_count 587400 real_backward_count 106297  18.096%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  2.013342/  2.100211, val:  47.08%, val_best:  73.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 107.33 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2874%\n",
      "layer   2  Sparsity: 70.5453%\n",
      "layer   3  Sparsity: 79.2105%\n",
      "total_backward_count 597190 real_backward_count 107880  18.065%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  2.013127/  2.086611, val:  75.00%, val_best:  75.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 105.56 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2752%\n",
      "layer   2  Sparsity: 70.4114%\n",
      "layer   3  Sparsity: 78.6673%\n",
      "total_backward_count 606980 real_backward_count 109470  18.035%\n",
      "fc layer 1 self.abs_max_out: 16815.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  2.017267/  2.107830, val:  60.42%, val_best:  75.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 103.76 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2777%\n",
      "layer   2  Sparsity: 70.3914%\n",
      "layer   3  Sparsity: 79.1396%\n",
      "total_backward_count 616770 real_backward_count 111028  18.002%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  2.009124/  2.100085, val:  60.83%, val_best:  75.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 103.82 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.3005%\n",
      "layer   2  Sparsity: 70.5473%\n",
      "layer   3  Sparsity: 79.1551%\n",
      "total_backward_count 626560 real_backward_count 112583  17.968%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  2.016512/  2.090394, val:  66.67%, val_best:  75.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 104.94 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.3061%\n",
      "layer   2  Sparsity: 70.4820%\n",
      "layer   3  Sparsity: 78.7777%\n",
      "total_backward_count 636350 real_backward_count 114215  17.948%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  2.010852/  2.099851, val:  55.83%, val_best:  75.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 105.28 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2742%\n",
      "layer   2  Sparsity: 70.3887%\n",
      "layer   3  Sparsity: 79.1011%\n",
      "total_backward_count 646140 real_backward_count 115802  17.922%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  2.021900/  2.096699, val:  57.50%, val_best:  75.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 105.75 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2715%\n",
      "layer   2  Sparsity: 70.3089%\n",
      "layer   3  Sparsity: 79.1540%\n",
      "total_backward_count 655930 real_backward_count 117395  17.897%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  2.009249/  2.099720, val:  66.25%, val_best:  75.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 104.39 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2704%\n",
      "layer   2  Sparsity: 70.6080%\n",
      "layer   3  Sparsity: 78.3491%\n",
      "total_backward_count 665720 real_backward_count 118942  17.867%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  2.009760/  2.092858, val:  62.92%, val_best:  75.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 103.91 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2753%\n",
      "layer   2  Sparsity: 70.6279%\n",
      "layer   3  Sparsity: 78.2498%\n",
      "total_backward_count 675510 real_backward_count 120512  17.840%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  2.010745/  2.108493, val:  51.25%, val_best:  75.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 104.77 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2786%\n",
      "layer   2  Sparsity: 70.5272%\n",
      "layer   3  Sparsity: 79.3524%\n",
      "total_backward_count 685300 real_backward_count 122056  17.811%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  2.007183/  2.078305, val:  74.17%, val_best:  75.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 103.82 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2844%\n",
      "layer   2  Sparsity: 70.9943%\n",
      "layer   3  Sparsity: 79.6751%\n",
      "total_backward_count 695090 real_backward_count 123655  17.790%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  2.006295/  2.103193, val:  46.67%, val_best:  75.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 102.89 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2755%\n",
      "layer   2  Sparsity: 70.8884%\n",
      "layer   3  Sparsity: 79.3957%\n",
      "total_backward_count 704880 real_backward_count 125211  17.763%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  2.016990/  2.087103, val:  72.92%, val_best:  75.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 104.75 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2837%\n",
      "layer   2  Sparsity: 70.7234%\n",
      "layer   3  Sparsity: 80.0655%\n",
      "total_backward_count 714670 real_backward_count 126738  17.734%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  2.008978/  2.095113, val:  56.67%, val_best:  75.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 106.01 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2863%\n",
      "layer   2  Sparsity: 70.2895%\n",
      "layer   3  Sparsity: 79.2543%\n",
      "total_backward_count 724460 real_backward_count 128253  17.703%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  2.007236/  2.101214, val:  57.50%, val_best:  75.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 102.64 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2909%\n",
      "layer   2  Sparsity: 70.5880%\n",
      "layer   3  Sparsity: 78.9829%\n",
      "total_backward_count 734250 real_backward_count 129868  17.687%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  2.006654/  2.076966, val:  50.83%, val_best:  75.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 103.36 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2738%\n",
      "layer   2  Sparsity: 70.4357%\n",
      "layer   3  Sparsity: 78.7455%\n",
      "total_backward_count 744040 real_backward_count 131403  17.661%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  2.002972/  2.077777, val:  64.58%, val_best:  75.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 104.78 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2953%\n",
      "layer   2  Sparsity: 70.6040%\n",
      "layer   3  Sparsity: 78.7057%\n",
      "total_backward_count 753830 real_backward_count 132978  17.640%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  2.016401/  2.114974, val:  44.17%, val_best:  75.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 102.88 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2903%\n",
      "layer   2  Sparsity: 70.6279%\n",
      "layer   3  Sparsity: 80.1152%\n",
      "total_backward_count 763620 real_backward_count 134521  17.616%\n",
      "fc layer 1 self.abs_max_out: 16862.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  2.010437/  2.093999, val:  50.42%, val_best:  75.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 103.16 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2952%\n",
      "layer   2  Sparsity: 70.4322%\n",
      "layer   3  Sparsity: 79.6934%\n",
      "total_backward_count 773410 real_backward_count 136089  17.596%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  2.012736/  2.100961, val:  65.00%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 102.18 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2821%\n",
      "layer   2  Sparsity: 70.3701%\n",
      "layer   3  Sparsity: 80.0174%\n",
      "total_backward_count 783200 real_backward_count 137637  17.574%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  2.015831/  2.092293, val:  71.25%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 102.98 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2709%\n",
      "layer   2  Sparsity: 70.1662%\n",
      "layer   3  Sparsity: 79.3665%\n",
      "total_backward_count 792990 real_backward_count 139160  17.549%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  2.012990/  2.097887, val:  49.17%, val_best:  75.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 104.40 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2922%\n",
      "layer   2  Sparsity: 70.3429%\n",
      "layer   3  Sparsity: 80.1216%\n",
      "total_backward_count 802780 real_backward_count 140646  17.520%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  2.016556/  2.078760, val:  69.17%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 104.07 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2847%\n",
      "layer   2  Sparsity: 70.3280%\n",
      "layer   3  Sparsity: 79.8472%\n",
      "total_backward_count 812570 real_backward_count 142123  17.491%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  2.011669/  2.087244, val:  61.25%, val_best:  75.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 101.96 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2732%\n",
      "layer   2  Sparsity: 70.4158%\n",
      "layer   3  Sparsity: 79.5535%\n",
      "total_backward_count 822360 real_backward_count 143684  17.472%\n",
      "lif layer 2 self.abs_max_v: 10272.5\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  2.009952/  2.083344, val:  59.17%, val_best:  75.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.74 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2907%\n",
      "layer   2  Sparsity: 70.4588%\n",
      "layer   3  Sparsity: 80.2916%\n",
      "total_backward_count 832150 real_backward_count 145238  17.453%\n",
      "lif layer 2 self.abs_max_v: 10294.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  2.016937/  2.109080, val:  68.75%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.58 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2873%\n",
      "layer   2  Sparsity: 70.3859%\n",
      "layer   3  Sparsity: 80.7894%\n",
      "total_backward_count 841940 real_backward_count 146791  17.435%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  2.011832/  2.086166, val:  55.00%, val_best:  75.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 102.90 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2702%\n",
      "layer   2  Sparsity: 70.2564%\n",
      "layer   3  Sparsity: 79.5865%\n",
      "total_backward_count 851730 real_backward_count 148261  17.407%\n",
      "lif layer 2 self.abs_max_v: 10554.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.999881/  2.073854, val:  68.33%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 102.14 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2607%\n",
      "layer   2  Sparsity: 70.1668%\n",
      "layer   3  Sparsity: 79.0486%\n",
      "total_backward_count 861520 real_backward_count 149815  17.390%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  2.000035/  2.075412, val:  64.17%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 103.00 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2693%\n",
      "layer   2  Sparsity: 70.1000%\n",
      "layer   3  Sparsity: 79.1614%\n",
      "total_backward_count 871310 real_backward_count 151315  17.366%\n",
      "fc layer 3 self.abs_max_out: 1135.0\n",
      "lif layer 2 self.abs_max_v: 10612.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  2.004006/  2.071607, val:  75.00%, val_best:  75.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 103.37 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2893%\n",
      "layer   2  Sparsity: 69.7999%\n",
      "layer   3  Sparsity: 78.9025%\n",
      "total_backward_count 881100 real_backward_count 152822  17.344%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.993427/  2.090294, val:  52.08%, val_best:  75.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 102.58 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2864%\n",
      "layer   2  Sparsity: 69.9685%\n",
      "layer   3  Sparsity: 78.8205%\n",
      "total_backward_count 890890 real_backward_count 154407  17.332%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  2.003416/  2.076604, val:  76.67%, val_best:  76.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.11 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2678%\n",
      "layer   2  Sparsity: 69.9293%\n",
      "layer   3  Sparsity: 78.9993%\n",
      "total_backward_count 900680 real_backward_count 155872  17.306%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  2.014779/  2.073722, val:  77.92%, val_best:  77.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 103.45 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2977%\n",
      "layer   2  Sparsity: 70.1271%\n",
      "layer   3  Sparsity: 79.7829%\n",
      "total_backward_count 910470 real_backward_count 157353  17.283%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  2.009069/  2.102286, val:  67.50%, val_best:  77.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 102.84 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 94.2982%\n",
      "layer   2  Sparsity: 70.0157%\n",
      "layer   3  Sparsity: 79.1824%\n",
      "total_backward_count 920260 real_backward_count 158871  17.264%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  2.005224/  2.083811, val:  53.75%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 101.59 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2963%\n",
      "layer   2  Sparsity: 69.9491%\n",
      "layer   3  Sparsity: 78.9873%\n",
      "total_backward_count 930050 real_backward_count 160360  17.242%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  2.005188/  2.071726, val:  79.58%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 103.69 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2964%\n",
      "layer   2  Sparsity: 70.2111%\n",
      "layer   3  Sparsity: 79.8276%\n",
      "total_backward_count 939840 real_backward_count 161918  17.228%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  2.009229/  2.097728, val:  65.83%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 103.67 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2643%\n",
      "layer   2  Sparsity: 70.1221%\n",
      "layer   3  Sparsity: 80.0020%\n",
      "total_backward_count 949630 real_backward_count 163431  17.210%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  2.006327/  2.077756, val:  71.25%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.00 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2956%\n",
      "layer   2  Sparsity: 69.7619%\n",
      "layer   3  Sparsity: 79.5002%\n",
      "total_backward_count 959420 real_backward_count 164943  17.192%\n",
      "fc layer 1 self.abs_max_out: 17119.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  2.004575/  2.096066, val:  52.50%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.89 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.2934%\n",
      "layer   2  Sparsity: 69.9672%\n",
      "layer   3  Sparsity: 79.3131%\n",
      "total_backward_count 969210 real_backward_count 166477  17.177%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  2.002618/  2.085476, val:  66.25%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 103.48 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2953%\n",
      "layer   2  Sparsity: 69.9995%\n",
      "layer   3  Sparsity: 78.9571%\n",
      "total_backward_count 979000 real_backward_count 167996  17.160%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.991791/  2.074269, val:  68.75%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.34 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 94.2831%\n",
      "layer   2  Sparsity: 70.1073%\n",
      "layer   3  Sparsity: 78.5124%\n",
      "total_backward_count 988790 real_backward_count 169543  17.147%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.998249/  2.085333, val:  73.33%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 104.15 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2811%\n",
      "layer   2  Sparsity: 70.0911%\n",
      "layer   3  Sparsity: 79.5116%\n",
      "total_backward_count 998580 real_backward_count 171088  17.133%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  2.005489/  2.083929, val:  71.67%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 104.02 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 94.3098%\n",
      "layer   2  Sparsity: 69.9596%\n",
      "layer   3  Sparsity: 79.4000%\n",
      "total_backward_count 1008370 real_backward_count 172616  17.118%\n",
      "fc layer 1 self.abs_max_out: 17151.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  2.009132/  2.095878, val:  53.75%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 101.66 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2718%\n",
      "layer   2  Sparsity: 69.9041%\n",
      "layer   3  Sparsity: 79.5925%\n",
      "total_backward_count 1018160 real_backward_count 174063  17.096%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  2.013162/  2.074229, val:  70.83%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 105.79 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2969%\n",
      "layer   2  Sparsity: 69.9206%\n",
      "layer   3  Sparsity: 79.7797%\n",
      "total_backward_count 1027950 real_backward_count 175588  17.081%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  2.009047/  2.081069, val:  60.83%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 106.12 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2779%\n",
      "layer   2  Sparsity: 69.7737%\n",
      "layer   3  Sparsity: 79.7279%\n",
      "total_backward_count 1037740 real_backward_count 177085  17.064%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  2.003178/  2.110646, val:  48.75%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 106.35 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2872%\n",
      "layer   2  Sparsity: 69.9008%\n",
      "layer   3  Sparsity: 79.9228%\n",
      "total_backward_count 1047530 real_backward_count 178561  17.046%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  2.003036/  2.084269, val:  63.75%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 107.12 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2803%\n",
      "layer   2  Sparsity: 69.8653%\n",
      "layer   3  Sparsity: 78.7851%\n",
      "total_backward_count 1057320 real_backward_count 180071  17.031%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.996292/  2.072971, val:  73.75%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.40 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2820%\n",
      "layer   2  Sparsity: 69.7124%\n",
      "layer   3  Sparsity: 78.6460%\n",
      "total_backward_count 1067110 real_backward_count 181563  17.014%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.999100/  2.071743, val:  72.08%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 107.14 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2683%\n",
      "layer   2  Sparsity: 69.8619%\n",
      "layer   3  Sparsity: 79.0272%\n",
      "total_backward_count 1076900 real_backward_count 183021  16.995%\n",
      "lif layer 1 self.abs_max_v: 25493.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.998918/  2.067691, val:  66.67%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 108.47 seconds, 1.81 minutes\n",
      "layer   1  Sparsity: 94.2899%\n",
      "layer   2  Sparsity: 69.7812%\n",
      "layer   3  Sparsity: 79.2794%\n",
      "total_backward_count 1086690 real_backward_count 184511  16.979%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.996853/  2.074364, val:  58.33%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.14 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2894%\n",
      "layer   2  Sparsity: 69.9390%\n",
      "layer   3  Sparsity: 79.2227%\n",
      "total_backward_count 1096480 real_backward_count 185949  16.959%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  2.000784/  2.084026, val:  62.92%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 107.16 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2972%\n",
      "layer   2  Sparsity: 69.9488%\n",
      "layer   3  Sparsity: 79.0341%\n",
      "total_backward_count 1106270 real_backward_count 187409  16.941%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.989036/  2.087380, val:  57.50%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 107.33 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2584%\n",
      "layer   2  Sparsity: 69.8852%\n",
      "layer   3  Sparsity: 79.2226%\n",
      "total_backward_count 1116060 real_backward_count 188786  16.915%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.979232/  2.082715, val:  58.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 107.81 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 94.2982%\n",
      "layer   2  Sparsity: 69.6729%\n",
      "layer   3  Sparsity: 78.1460%\n",
      "total_backward_count 1125850 real_backward_count 190302  16.903%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.992341/  2.075269, val:  65.00%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 106.51 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2864%\n",
      "layer   2  Sparsity: 69.7019%\n",
      "layer   3  Sparsity: 78.9085%\n",
      "total_backward_count 1135640 real_backward_count 191744  16.884%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.981672/  2.077629, val:  67.92%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 105.55 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2858%\n",
      "layer   2  Sparsity: 69.7917%\n",
      "layer   3  Sparsity: 78.6614%\n",
      "total_backward_count 1145430 real_backward_count 193221  16.869%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.992312/  2.073304, val:  77.50%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 105.64 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2917%\n",
      "layer   2  Sparsity: 70.0170%\n",
      "layer   3  Sparsity: 79.0799%\n",
      "total_backward_count 1155220 real_backward_count 194646  16.849%\n",
      "fc layer 1 self.abs_max_out: 17233.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.989255/  2.066797, val:  75.83%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 107.26 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2702%\n",
      "layer   2  Sparsity: 69.7136%\n",
      "layer   3  Sparsity: 78.6585%\n",
      "total_backward_count 1165010 real_backward_count 196047  16.828%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.989719/  2.055327, val:  70.42%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 107.24 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2781%\n",
      "layer   2  Sparsity: 69.9189%\n",
      "layer   3  Sparsity: 79.1503%\n",
      "total_backward_count 1174800 real_backward_count 197525  16.814%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.980745/  2.060814, val:  66.25%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.66 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2717%\n",
      "layer   2  Sparsity: 69.8136%\n",
      "layer   3  Sparsity: 78.4526%\n",
      "total_backward_count 1184590 real_backward_count 198957  16.795%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.978180/  2.077547, val:  58.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 107.36 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2821%\n",
      "layer   2  Sparsity: 69.9526%\n",
      "layer   3  Sparsity: 78.3274%\n",
      "total_backward_count 1194380 real_backward_count 200418  16.780%\n",
      "lif layer 1 self.abs_max_v: 25516.5\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.983724/  2.066533, val:  77.50%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 107.24 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2727%\n",
      "layer   2  Sparsity: 69.8317%\n",
      "layer   3  Sparsity: 78.6639%\n",
      "total_backward_count 1204170 real_backward_count 201901  16.767%\n",
      "lif layer 2 self.abs_max_v: 10745.5\n",
      "lif layer 1 self.abs_max_v: 25527.5\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.997508/  2.080176, val:  72.92%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.81 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2858%\n",
      "layer   2  Sparsity: 69.6610%\n",
      "layer   3  Sparsity: 79.0469%\n",
      "total_backward_count 1213960 real_backward_count 203333  16.750%\n",
      "lif layer 1 self.abs_max_v: 25759.5\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  2.000503/  2.072955, val:  77.50%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 105.39 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2950%\n",
      "layer   2  Sparsity: 69.8980%\n",
      "layer   3  Sparsity: 79.9200%\n",
      "total_backward_count 1223750 real_backward_count 204853  16.740%\n",
      "lif layer 1 self.abs_max_v: 25972.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  2.001908/  2.080966, val:  71.25%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 104.43 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2674%\n",
      "layer   2  Sparsity: 69.7275%\n",
      "layer   3  Sparsity: 79.9122%\n",
      "total_backward_count 1233540 real_backward_count 206348  16.728%\n",
      "lif layer 1 self.abs_max_v: 26273.5\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  2.001856/  2.087425, val:  71.25%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 106.19 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2960%\n",
      "layer   2  Sparsity: 69.8429%\n",
      "layer   3  Sparsity: 79.5191%\n",
      "total_backward_count 1243330 real_backward_count 207794  16.713%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.997185/  2.082556, val:  68.75%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 105.92 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2915%\n",
      "layer   2  Sparsity: 69.8525%\n",
      "layer   3  Sparsity: 79.5298%\n",
      "total_backward_count 1253120 real_backward_count 209248  16.698%\n",
      "lif layer 1 self.abs_max_v: 26359.5\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.990405/  2.078887, val:  72.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 106.73 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2657%\n",
      "layer   2  Sparsity: 69.5461%\n",
      "layer   3  Sparsity: 79.6881%\n",
      "total_backward_count 1262910 real_backward_count 210677  16.682%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.984009/  2.076484, val:  66.25%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.28 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2889%\n",
      "layer   2  Sparsity: 69.6483%\n",
      "layer   3  Sparsity: 79.5612%\n",
      "total_backward_count 1272700 real_backward_count 212079  16.664%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  2.000340/  2.083882, val:  58.33%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 106.80 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2661%\n",
      "layer   2  Sparsity: 69.6870%\n",
      "layer   3  Sparsity: 80.0468%\n",
      "total_backward_count 1282490 real_backward_count 213523  16.649%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  2.008484/  2.085361, val:  58.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.73 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2785%\n",
      "layer   2  Sparsity: 69.7430%\n",
      "layer   3  Sparsity: 80.6205%\n",
      "total_backward_count 1292280 real_backward_count 214991  16.637%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.998287/  2.065238, val:  61.67%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 104.67 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 94.2908%\n",
      "layer   2  Sparsity: 69.5634%\n",
      "layer   3  Sparsity: 79.2393%\n",
      "total_backward_count 1302070 real_backward_count 216377  16.618%\n",
      "fc layer 1 self.abs_max_out: 17372.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.994581/  2.096431, val:  74.17%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 105.06 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2899%\n",
      "layer   2  Sparsity: 69.4597%\n",
      "layer   3  Sparsity: 79.6069%\n",
      "total_backward_count 1311860 real_backward_count 217831  16.605%\n",
      "fc layer 1 self.abs_max_out: 17408.0\n",
      "lif layer 1 self.abs_max_v: 26543.5\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.993349/  2.077657, val:  65.00%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 104.88 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2670%\n",
      "layer   2  Sparsity: 69.4719%\n",
      "layer   3  Sparsity: 79.6155%\n",
      "total_backward_count 1321650 real_backward_count 219283  16.592%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.991758/  2.054755, val:  71.67%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 106.26 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2693%\n",
      "layer   2  Sparsity: 69.2854%\n",
      "layer   3  Sparsity: 79.7052%\n",
      "total_backward_count 1331440 real_backward_count 220745  16.579%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.980120/  2.054546, val:  78.33%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 110.17 seconds, 1.84 minutes\n",
      "layer   1  Sparsity: 94.2840%\n",
      "layer   2  Sparsity: 69.4589%\n",
      "layer   3  Sparsity: 79.5715%\n",
      "total_backward_count 1341230 real_backward_count 222157  16.564%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.981640/  2.071584, val:  66.67%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 106.70 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2958%\n",
      "layer   2  Sparsity: 69.4998%\n",
      "layer   3  Sparsity: 79.7355%\n",
      "total_backward_count 1351020 real_backward_count 223580  16.549%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.990811/  2.074896, val:  73.33%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 107.70 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2771%\n",
      "layer   2  Sparsity: 69.5187%\n",
      "layer   3  Sparsity: 80.1553%\n",
      "total_backward_count 1360810 real_backward_count 225038  16.537%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.995849/  2.064411, val:  67.08%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 107.45 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2835%\n",
      "layer   2  Sparsity: 69.3069%\n",
      "layer   3  Sparsity: 79.6476%\n",
      "total_backward_count 1370600 real_backward_count 226527  16.528%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.996744/  2.075817, val:  68.33%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 108.96 seconds, 1.82 minutes\n",
      "layer   1  Sparsity: 94.2659%\n",
      "layer   2  Sparsity: 69.1236%\n",
      "layer   3  Sparsity: 79.4195%\n",
      "total_backward_count 1380390 real_backward_count 227960  16.514%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.999511/  2.070246, val:  62.92%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 109.54 seconds, 1.83 minutes\n",
      "layer   1  Sparsity: 94.2933%\n",
      "layer   2  Sparsity: 69.4909%\n",
      "layer   3  Sparsity: 80.0042%\n",
      "total_backward_count 1390180 real_backward_count 229380  16.500%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.990248/  2.082613, val:  67.92%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 106.71 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2760%\n",
      "layer   2  Sparsity: 69.4397%\n",
      "layer   3  Sparsity: 79.9831%\n",
      "total_backward_count 1399970 real_backward_count 230836  16.489%\n",
      "lif layer 1 self.abs_max_v: 26920.5\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  2.002895/  2.073744, val:  74.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 106.69 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2662%\n",
      "layer   2  Sparsity: 69.4419%\n",
      "layer   3  Sparsity: 80.5872%\n",
      "total_backward_count 1409760 real_backward_count 232274  16.476%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.996957/  2.079878, val:  62.92%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 106.89 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2925%\n",
      "layer   2  Sparsity: 69.4906%\n",
      "layer   3  Sparsity: 80.2145%\n",
      "total_backward_count 1419550 real_backward_count 233753  16.467%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.996308/  2.085512, val:  47.08%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.85 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2898%\n",
      "layer   2  Sparsity: 69.5414%\n",
      "layer   3  Sparsity: 79.4144%\n",
      "total_backward_count 1429340 real_backward_count 235173  16.453%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.993967/  2.091542, val:  67.08%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.19 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2899%\n",
      "layer   2  Sparsity: 69.5587%\n",
      "layer   3  Sparsity: 80.1334%\n",
      "total_backward_count 1439130 real_backward_count 236576  16.439%\n",
      "fc layer 1 self.abs_max_out: 17534.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.998659/  2.076364, val:  54.17%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 107.27 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2821%\n",
      "layer   2  Sparsity: 69.4226%\n",
      "layer   3  Sparsity: 79.8794%\n",
      "total_backward_count 1448920 real_backward_count 237963  16.423%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.999526/  2.062644, val:  72.50%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 107.96 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 94.2884%\n",
      "layer   2  Sparsity: 69.5444%\n",
      "layer   3  Sparsity: 80.1720%\n",
      "total_backward_count 1458710 real_backward_count 239412  16.413%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.989033/  2.058463, val:  75.42%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 106.93 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2902%\n",
      "layer   2  Sparsity: 69.6248%\n",
      "layer   3  Sparsity: 80.0727%\n",
      "total_backward_count 1468500 real_backward_count 240829  16.400%\n",
      "lif layer 1 self.abs_max_v: 27322.5\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.991824/  2.078776, val:  68.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 105.73 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.3062%\n",
      "layer   2  Sparsity: 69.7173%\n",
      "layer   3  Sparsity: 80.2788%\n",
      "total_backward_count 1478290 real_backward_count 242194  16.383%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  2.004451/  2.087197, val:  68.33%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 106.91 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2878%\n",
      "layer   2  Sparsity: 69.7387%\n",
      "layer   3  Sparsity: 80.2630%\n",
      "total_backward_count 1488080 real_backward_count 243604  16.370%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.997395/  2.076744, val:  71.25%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 107.77 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 94.2896%\n",
      "layer   2  Sparsity: 69.6320%\n",
      "layer   3  Sparsity: 79.0123%\n",
      "total_backward_count 1497870 real_backward_count 244991  16.356%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.998197/  2.065638, val:  74.17%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.80 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2840%\n",
      "layer   2  Sparsity: 69.3944%\n",
      "layer   3  Sparsity: 79.4763%\n",
      "total_backward_count 1507660 real_backward_count 246418  16.344%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  2.004057/  2.087469, val:  62.50%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 106.72 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2746%\n",
      "layer   2  Sparsity: 69.4547%\n",
      "layer   3  Sparsity: 80.1379%\n",
      "total_backward_count 1517450 real_backward_count 247837  16.332%\n",
      "lif layer 2 self.abs_max_v: 10795.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.997720/  2.073012, val:  65.83%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.84 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2910%\n",
      "layer   2  Sparsity: 69.4672%\n",
      "layer   3  Sparsity: 80.0005%\n",
      "total_backward_count 1527240 real_backward_count 249249  16.320%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  2.010335/  2.088330, val:  53.75%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.73 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2862%\n",
      "layer   2  Sparsity: 69.4158%\n",
      "layer   3  Sparsity: 80.2315%\n",
      "total_backward_count 1537030 real_backward_count 250676  16.309%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  2.000832/  2.080528, val:  67.50%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.96 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2725%\n",
      "layer   2  Sparsity: 69.4842%\n",
      "layer   3  Sparsity: 80.1166%\n",
      "total_backward_count 1546820 real_backward_count 252064  16.296%\n",
      "fc layer 1 self.abs_max_out: 17641.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  2.008380/  2.094265, val:  62.92%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.73 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2962%\n",
      "layer   2  Sparsity: 69.4920%\n",
      "layer   3  Sparsity: 80.4527%\n",
      "total_backward_count 1556610 real_backward_count 253505  16.286%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  2.000577/  2.067379, val:  70.42%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 107.22 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2861%\n",
      "layer   2  Sparsity: 69.5544%\n",
      "layer   3  Sparsity: 79.8495%\n",
      "total_backward_count 1566400 real_backward_count 254822  16.268%\n",
      "lif layer 1 self.abs_max_v: 27417.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.993490/  2.073566, val:  66.25%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 107.47 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2996%\n",
      "layer   2  Sparsity: 69.5381%\n",
      "layer   3  Sparsity: 79.3741%\n",
      "total_backward_count 1576190 real_backward_count 256276  16.259%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.997378/  2.067971, val:  68.75%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 106.31 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2825%\n",
      "layer   2  Sparsity: 69.6439%\n",
      "layer   3  Sparsity: 79.3895%\n",
      "total_backward_count 1585980 real_backward_count 257675  16.247%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.991619/  2.073341, val:  62.92%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 104.92 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2821%\n",
      "layer   2  Sparsity: 69.3076%\n",
      "layer   3  Sparsity: 79.2961%\n",
      "total_backward_count 1595770 real_backward_count 259077  16.235%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.987752/  2.077046, val:  58.75%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 106.29 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2899%\n",
      "layer   2  Sparsity: 69.5582%\n",
      "layer   3  Sparsity: 79.2972%\n",
      "total_backward_count 1605560 real_backward_count 260444  16.221%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.989322/  2.068182, val:  67.50%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 108.69 seconds, 1.81 minutes\n",
      "layer   1  Sparsity: 94.2751%\n",
      "layer   2  Sparsity: 69.4890%\n",
      "layer   3  Sparsity: 79.3041%\n",
      "total_backward_count 1615350 real_backward_count 261924  16.215%\n",
      "fc layer 1 self.abs_max_out: 17646.0\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.988742/  2.082428, val:  58.75%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 106.67 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2886%\n",
      "layer   2  Sparsity: 69.5001%\n",
      "layer   3  Sparsity: 79.8283%\n",
      "total_backward_count 1625140 real_backward_count 263380  16.207%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.995726/  2.070758, val:  55.42%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.56 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2883%\n",
      "layer   2  Sparsity: 69.3330%\n",
      "layer   3  Sparsity: 78.9033%\n",
      "total_backward_count 1634930 real_backward_count 264744  16.193%\n",
      "fc layer 1 self.abs_max_out: 17705.0\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.989550/  2.070220, val:  66.25%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 104.88 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2739%\n",
      "layer   2  Sparsity: 69.3482%\n",
      "layer   3  Sparsity: 79.0785%\n",
      "total_backward_count 1644720 real_backward_count 266156  16.182%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.989312/  2.058762, val:  65.00%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 107.34 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.3022%\n",
      "layer   2  Sparsity: 69.4351%\n",
      "layer   3  Sparsity: 79.4632%\n",
      "total_backward_count 1654510 real_backward_count 267593  16.174%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.979580/  2.060345, val:  76.67%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 108.10 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 94.2854%\n",
      "layer   2  Sparsity: 69.4786%\n",
      "layer   3  Sparsity: 79.0272%\n",
      "total_backward_count 1664300 real_backward_count 268942  16.159%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.987739/  2.081847, val:  48.33%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 106.44 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2837%\n",
      "layer   2  Sparsity: 69.5316%\n",
      "layer   3  Sparsity: 79.7972%\n",
      "total_backward_count 1674090 real_backward_count 270404  16.152%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.990286/  2.091792, val:  47.08%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 107.15 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2637%\n",
      "layer   2  Sparsity: 69.2394%\n",
      "layer   3  Sparsity: 79.2477%\n",
      "total_backward_count 1683880 real_backward_count 271826  16.143%\n",
      "fc layer 1 self.abs_max_out: 17927.0\n",
      "lif layer 1 self.abs_max_v: 28021.5\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.995077/  2.074326, val:  66.67%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 106.37 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2847%\n",
      "layer   2  Sparsity: 69.5439%\n",
      "layer   3  Sparsity: 80.1519%\n",
      "total_backward_count 1693670 real_backward_count 273306  16.137%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.995373/  2.066559, val:  65.83%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 106.36 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2786%\n",
      "layer   2  Sparsity: 69.4255%\n",
      "layer   3  Sparsity: 79.7737%\n",
      "total_backward_count 1703460 real_backward_count 274634  16.122%\n",
      "lif layer 2 self.abs_max_v: 10818.5\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.990864/  2.064615, val:  75.00%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 106.08 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2777%\n",
      "layer   2  Sparsity: 69.5037%\n",
      "layer   3  Sparsity: 79.0454%\n",
      "total_backward_count 1713250 real_backward_count 276061  16.113%\n",
      "lif layer 2 self.abs_max_v: 10876.5\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.995382/  2.059328, val:  71.67%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 104.83 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2601%\n",
      "layer   2  Sparsity: 69.5000%\n",
      "layer   3  Sparsity: 79.4115%\n",
      "total_backward_count 1723040 real_backward_count 277481  16.104%\n",
      "lif layer 2 self.abs_max_v: 11011.0\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.977983/  2.047288, val:  75.42%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.40 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2809%\n",
      "layer   2  Sparsity: 69.4023%\n",
      "layer   3  Sparsity: 78.7996%\n",
      "total_backward_count 1732830 real_backward_count 278867  16.093%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.981909/  2.062962, val:  68.75%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.19 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2825%\n",
      "layer   2  Sparsity: 69.4049%\n",
      "layer   3  Sparsity: 79.0605%\n",
      "total_backward_count 1742620 real_backward_count 280217  16.080%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.979260/  2.078712, val:  52.92%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.95 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2798%\n",
      "layer   2  Sparsity: 69.2870%\n",
      "layer   3  Sparsity: 79.7919%\n",
      "total_backward_count 1752410 real_backward_count 281616  16.070%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.994164/  2.069293, val:  78.33%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.94 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2930%\n",
      "layer   2  Sparsity: 69.4522%\n",
      "layer   3  Sparsity: 79.7772%\n",
      "total_backward_count 1762200 real_backward_count 283042  16.062%\n",
      "fc layer 1 self.abs_max_out: 18024.0\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.989387/  2.066884, val:  72.50%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 106.17 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2700%\n",
      "layer   2  Sparsity: 69.2542%\n",
      "layer   3  Sparsity: 79.1197%\n",
      "total_backward_count 1771990 real_backward_count 284460  16.053%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.991594/  2.080541, val:  71.67%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.69 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2869%\n",
      "layer   2  Sparsity: 69.2834%\n",
      "layer   3  Sparsity: 79.7529%\n",
      "total_backward_count 1781780 real_backward_count 285827  16.042%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.992290/  2.078835, val:  66.25%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 107.16 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 94.2618%\n",
      "layer   2  Sparsity: 69.1676%\n",
      "layer   3  Sparsity: 79.5868%\n",
      "total_backward_count 1791570 real_backward_count 287163  16.029%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.991512/  2.065642, val:  56.25%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 107.84 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 94.2832%\n",
      "layer   2  Sparsity: 69.1171%\n",
      "layer   3  Sparsity: 79.0943%\n",
      "total_backward_count 1801360 real_backward_count 288544  16.018%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.981336/  2.082692, val:  55.83%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.18 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 94.2852%\n",
      "layer   2  Sparsity: 69.3461%\n",
      "layer   3  Sparsity: 79.4757%\n",
      "total_backward_count 1811150 real_backward_count 289887  16.006%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.993396/  2.076875, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.19 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2638%\n",
      "layer   2  Sparsity: 69.1401%\n",
      "layer   3  Sparsity: 79.9250%\n",
      "total_backward_count 1820940 real_backward_count 291277  15.996%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  2.001659/  2.074203, val:  65.83%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 108.45 seconds, 1.81 minutes\n",
      "layer   1  Sparsity: 94.2739%\n",
      "layer   2  Sparsity: 69.4626%\n",
      "layer   3  Sparsity: 80.1747%\n",
      "total_backward_count 1830730 real_backward_count 292649  15.985%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.992039/  2.075661, val:  58.33%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.85 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2829%\n",
      "layer   2  Sparsity: 69.5132%\n",
      "layer   3  Sparsity: 79.4983%\n",
      "total_backward_count 1840520 real_backward_count 294073  15.978%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.991681/  2.064740, val:  67.50%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.29 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2765%\n",
      "layer   2  Sparsity: 69.2088%\n",
      "layer   3  Sparsity: 79.2039%\n",
      "total_backward_count 1850310 real_backward_count 295407  15.965%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.994063/  2.062903, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.22 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2635%\n",
      "layer   2  Sparsity: 69.1417%\n",
      "layer   3  Sparsity: 79.2368%\n",
      "total_backward_count 1860100 real_backward_count 296731  15.952%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.985305/  2.063229, val:  79.58%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.98 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2724%\n",
      "layer   2  Sparsity: 69.0712%\n",
      "layer   3  Sparsity: 79.2381%\n",
      "total_backward_count 1869890 real_backward_count 298066  15.940%\n",
      "lif layer 2 self.abs_max_v: 11103.0\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.988783/  2.086505, val:  45.83%, val_best:  79.58%, tr:  99.39%, tr_best: 100.00%, epoch time: 107.09 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2843%\n",
      "layer   2  Sparsity: 69.2338%\n",
      "layer   3  Sparsity: 79.6361%\n",
      "total_backward_count 1879680 real_backward_count 299485  15.933%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.996481/  2.069259, val:  65.83%, val_best:  79.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 106.94 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 94.2722%\n",
      "layer   2  Sparsity: 69.3271%\n",
      "layer   3  Sparsity: 79.5605%\n",
      "total_backward_count 1889470 real_backward_count 300868  15.923%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.976717/  2.073613, val:  64.17%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.88 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 94.2933%\n",
      "layer   2  Sparsity: 69.3230%\n",
      "layer   3  Sparsity: 79.3009%\n",
      "total_backward_count 1899260 real_backward_count 302219  15.912%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.981636/  2.058371, val:  69.58%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 106.03 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 94.2890%\n",
      "layer   2  Sparsity: 69.3024%\n",
      "layer   3  Sparsity: 79.5642%\n",
      "total_backward_count 1909050 real_backward_count 303616  15.904%\n",
      "lif layer 2 self.abs_max_v: 11245.0\n",
      "lif layer 2 self.abs_max_v: 11604.5\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.986889/  2.070092, val:  71.67%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 101.95 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 94.2593%\n",
      "layer   2  Sparsity: 69.0879%\n",
      "layer   3  Sparsity: 79.5508%\n",
      "total_backward_count 1918840 real_backward_count 305030  15.897%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.991381/  2.087316, val:  72.08%, val_best:  79.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 101.49 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 94.2928%\n",
      "layer   2  Sparsity: 69.1097%\n",
      "layer   3  Sparsity: 80.5178%\n",
      "total_backward_count 1928630 real_backward_count 306429  15.888%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  2.001113/  2.067384, val:  62.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 100.69 seconds, 1.68 minutes\n",
      "layer   1  Sparsity: 94.3136%\n",
      "layer   2  Sparsity: 69.1832%\n",
      "layer   3  Sparsity: 79.6810%\n",
      "total_backward_count 1938420 real_backward_count 307803  15.879%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.996260/  2.079887, val:  70.42%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 98.25 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 94.2737%\n",
      "layer   2  Sparsity: 69.1382%\n",
      "layer   3  Sparsity: 80.1756%\n",
      "total_backward_count 1948210 real_backward_count 309184  15.870%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  2.001930/  2.077085, val:  66.67%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 99.78 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 94.2880%\n",
      "layer   2  Sparsity: 69.2412%\n",
      "layer   3  Sparsity: 80.3379%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e6e859ca4949f88df8da29bb01e3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÉ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÉ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñÖ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>2.00193</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.66667</td></tr><tr><td>val_loss</td><td>2.07709</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-sweep-40</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m36rifiv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m36rifiv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_144418-m36rifiv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47m1ytay with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_203325-47m1ytay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47m1ytay' target=\"_blank\">rich-sweep-49</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47m1ytay' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47m1ytay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251114_203336_333', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 3, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 204.0\n",
      "lif layer 1 self.abs_max_v: 204.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 489.0\n",
      "lif layer 2 self.abs_max_v: 489.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 1 self.abs_max_out: 223.0\n",
      "lif layer 1 self.abs_max_v: 292.0\n",
      "lif layer 2 self.abs_max_v: 722.5\n",
      "fc layer 1 self.abs_max_out: 268.0\n",
      "lif layer 1 self.abs_max_v: 414.0\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "lif layer 2 self.abs_max_v: 866.5\n",
      "fc layer 1 self.abs_max_out: 274.0\n",
      "lif layer 1 self.abs_max_v: 456.0\n",
      "fc layer 1 self.abs_max_out: 318.0\n",
      "fc layer 1 self.abs_max_out: 320.0\n",
      "fc layer 2 self.abs_max_out: 539.0\n",
      "fc layer 1 self.abs_max_out: 349.0\n",
      "lif layer 1 self.abs_max_v: 540.0\n",
      "lif layer 2 self.abs_max_v: 920.5\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 555.5\n",
      "fc layer 1 self.abs_max_out: 364.0\n",
      "lif layer 1 self.abs_max_v: 562.0\n",
      "fc layer 1 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "lif layer 2 self.abs_max_v: 959.0\n",
      "fc layer 2 self.abs_max_out: 710.0\n",
      "lif layer 1 self.abs_max_v: 588.0\n",
      "lif layer 1 self.abs_max_v: 601.0\n",
      "lif layer 2 self.abs_max_v: 1008.0\n",
      "lif layer 1 self.abs_max_v: 659.5\n",
      "fc layer 1 self.abs_max_out: 476.0\n",
      "lif layer 1 self.abs_max_v: 766.5\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 1 self.abs_max_out: 502.0\n",
      "fc layer 1 self.abs_max_out: 540.0\n",
      "lif layer 1 self.abs_max_v: 868.0\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "lif layer 2 self.abs_max_v: 1028.5\n",
      "fc layer 2 self.abs_max_out: 715.0\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "lif layer 2 self.abs_max_v: 1096.5\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 872.0\n",
      "lif layer 2 self.abs_max_v: 1120.0\n",
      "lif layer 2 self.abs_max_v: 1142.5\n",
      "lif layer 2 self.abs_max_v: 1153.5\n",
      "fc layer 2 self.abs_max_out: 734.0\n",
      "lif layer 2 self.abs_max_v: 1206.5\n",
      "fc layer 1 self.abs_max_out: 580.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 1230.5\n",
      "lif layer 1 self.abs_max_v: 885.5\n",
      "lif layer 1 self.abs_max_v: 1016.0\n",
      "lif layer 1 self.abs_max_v: 1022.0\n",
      "lif layer 1 self.abs_max_v: 1033.0\n",
      "fc layer 2 self.abs_max_out: 786.0\n",
      "fc layer 2 self.abs_max_out: 787.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "lif layer 2 self.abs_max_v: 1245.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.373167/  2.379876, val:  12.50%, val_best:  12.50%, tr:  12.56%, tr_best:  12.56%, epoch time: 87.96 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 88.8487%\n",
      "layer   2  Sparsity: 65.0081%\n",
      "layer   3  Sparsity: 66.3332%\n",
      "total_backward_count 9790 real_backward_count 8584  87.681%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.371742/  2.379876, val:  12.50%, val_best:  12.50%, tr:  11.95%, tr_best:  12.56%, epoch time: 88.86 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 88.8472%\n",
      "layer   2  Sparsity: 65.0269%\n",
      "layer   3  Sparsity: 66.3266%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41364fcaadfd4fe79b57ec808103d967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.11951</td></tr><tr><td>tr_epoch_loss</td><td>2.37174</td></tr><tr><td>val_acc_best</td><td>0.125</td></tr><tr><td>val_acc_now</td><td>0.125</td></tr><tr><td>val_loss</td><td>2.37988</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-sweep-49</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47m1ytay' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47m1ytay</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_203325-47m1ytay/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 47m1ytay errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_17145/822137395.py\", line 114, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_17145/2991991161.py\", line 973, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 47m1ytay errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_17145/822137395.py\", line 114, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_17145/2991991161.py\", line 973, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zp83zknp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_203652-zp83zknp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zp83zknp' target=\"_blank\">glowing-sweep-51</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zp83zknp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zp83zknp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251114_203703_131', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 591.0\n",
      "lif layer 1 self.abs_max_v: 591.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 855.0\n",
      "lif layer 1 self.abs_max_v: 1141.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "lif layer 2 self.abs_max_v: 160.0\n",
      "fc layer 1 self.abs_max_out: 915.0\n",
      "lif layer 1 self.abs_max_v: 1309.5\n",
      "fc layer 2 self.abs_max_out: 350.0\n",
      "lif layer 2 self.abs_max_v: 360.5\n",
      "fc layer 1 self.abs_max_out: 1392.0\n",
      "lif layer 1 self.abs_max_v: 1392.0\n",
      "fc layer 2 self.abs_max_out: 481.0\n",
      "lif layer 2 self.abs_max_v: 586.0\n",
      "lif layer 1 self.abs_max_v: 1706.0\n",
      "lif layer 2 self.abs_max_v: 626.0\n",
      "fc layer 1 self.abs_max_out: 1641.0\n",
      "fc layer 2 self.abs_max_out: 519.0\n",
      "lif layer 2 self.abs_max_v: 626.5\n",
      "lif layer 1 self.abs_max_v: 1761.0\n",
      "fc layer 2 self.abs_max_out: 617.0\n",
      "lif layer 2 self.abs_max_v: 790.0\n",
      "fc layer 1 self.abs_max_out: 1895.0\n",
      "lif layer 1 self.abs_max_v: 1895.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 2 self.abs_max_v: 1106.0\n",
      "fc layer 3 self.abs_max_out: 94.0\n",
      "fc layer 1 self.abs_max_out: 2271.0\n",
      "lif layer 1 self.abs_max_v: 2406.5\n",
      "lif layer 2 self.abs_max_v: 1168.0\n",
      "fc layer 1 self.abs_max_out: 3233.0\n",
      "lif layer 1 self.abs_max_v: 3233.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 1 self.abs_max_out: 4200.0\n",
      "lif layer 1 self.abs_max_v: 4200.0\n",
      "lif layer 2 self.abs_max_v: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1003.0\n",
      "lif layer 2 self.abs_max_v: 1420.5\n",
      "lif layer 2 self.abs_max_v: 1459.5\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "lif layer 2 self.abs_max_v: 1534.5\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "lif layer 2 self.abs_max_v: 1555.5\n",
      "lif layer 2 self.abs_max_v: 1686.5\n",
      "fc layer 2 self.abs_max_out: 1261.0\n",
      "lif layer 2 self.abs_max_v: 1937.0\n",
      "fc layer 2 self.abs_max_out: 1448.0\n",
      "fc layer 2 self.abs_max_out: 1530.0\n",
      "lif layer 2 self.abs_max_v: 2095.0\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "lif layer 2 self.abs_max_v: 2331.5\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 1 self.abs_max_out: 4446.0\n",
      "lif layer 1 self.abs_max_v: 4446.0\n",
      "fc layer 2 self.abs_max_out: 1775.0\n",
      "fc layer 2 self.abs_max_out: 1954.0\n",
      "lif layer 2 self.abs_max_v: 2473.0\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 2569.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 1 self.abs_max_out: 5100.0\n",
      "lif layer 1 self.abs_max_v: 5100.0\n",
      "fc layer 1 self.abs_max_out: 5786.0\n",
      "lif layer 1 self.abs_max_v: 5786.0\n",
      "fc layer 2 self.abs_max_out: 2164.0\n",
      "lif layer 2 self.abs_max_v: 2627.0\n",
      "lif layer 2 self.abs_max_v: 2695.5\n",
      "lif layer 2 self.abs_max_v: 2696.0\n",
      "lif layer 2 self.abs_max_v: 2924.0\n",
      "fc layer 2 self.abs_max_out: 2604.0\n",
      "lif layer 2 self.abs_max_v: 3143.0\n",
      "fc layer 2 self.abs_max_out: 2738.0\n",
      "lif layer 2 self.abs_max_v: 3386.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "lif layer 2 self.abs_max_v: 3407.0\n",
      "fc layer 2 self.abs_max_out: 2747.0\n",
      "fc layer 2 self.abs_max_out: 2922.0\n",
      "fc layer 1 self.abs_max_out: 5961.0\n",
      "lif layer 1 self.abs_max_v: 5961.0\n",
      "lif layer 2 self.abs_max_v: 3609.0\n",
      "fc layer 1 self.abs_max_out: 6441.0\n",
      "lif layer 1 self.abs_max_v: 6441.0\n",
      "lif layer 2 self.abs_max_v: 3649.0\n",
      "fc layer 2 self.abs_max_out: 3045.0\n",
      "fc layer 2 self.abs_max_out: 3049.0\n",
      "fc layer 1 self.abs_max_out: 6588.0\n",
      "lif layer 1 self.abs_max_v: 6588.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 1 self.abs_max_out: 6840.0\n",
      "lif layer 1 self.abs_max_v: 6840.0\n",
      "fc layer 1 self.abs_max_out: 6868.0\n",
      "lif layer 1 self.abs_max_v: 6868.0\n",
      "fc layer 1 self.abs_max_out: 7026.0\n",
      "lif layer 1 self.abs_max_v: 7026.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "lif layer 1 self.abs_max_v: 7498.0\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "fc layer 1 self.abs_max_out: 7042.0\n",
      "fc layer 2 self.abs_max_out: 3161.0\n",
      "fc layer 1 self.abs_max_out: 7907.0\n",
      "lif layer 1 self.abs_max_v: 7907.0\n",
      "fc layer 2 self.abs_max_out: 3204.0\n",
      "fc layer 1 self.abs_max_out: 8242.0\n",
      "lif layer 1 self.abs_max_v: 8242.0\n",
      "fc layer 2 self.abs_max_out: 3211.0\n",
      "fc layer 1 self.abs_max_out: 8278.0\n",
      "lif layer 1 self.abs_max_v: 8278.0\n",
      "fc layer 2 self.abs_max_out: 3240.0\n",
      "fc layer 2 self.abs_max_out: 3329.0\n",
      "fc layer 2 self.abs_max_out: 3503.0\n",
      "fc layer 1 self.abs_max_out: 8319.0\n",
      "lif layer 1 self.abs_max_v: 8319.0\n",
      "fc layer 1 self.abs_max_out: 8585.0\n",
      "lif layer 1 self.abs_max_v: 8585.0\n",
      "fc layer 1 self.abs_max_out: 9198.0\n",
      "lif layer 1 self.abs_max_v: 9198.0\n",
      "fc layer 1 self.abs_max_out: 9362.0\n",
      "lif layer 1 self.abs_max_v: 9362.0\n",
      "lif layer 2 self.abs_max_v: 3853.5\n",
      "fc layer 1 self.abs_max_out: 9506.0\n",
      "lif layer 1 self.abs_max_v: 9506.0\n",
      "fc layer 2 self.abs_max_out: 3656.0\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.157634/  2.190823, val:  36.25%, val_best:  36.25%, tr:  83.25%, tr_best:  83.25%, epoch time: 85.40 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.5871%\n",
      "layer   2  Sparsity: 86.4312%\n",
      "layer   3  Sparsity: 93.5872%\n",
      "total_backward_count 9790 real_backward_count 3788  38.693%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 4038.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "lif layer 2 self.abs_max_v: 4069.0\n",
      "fc layer 1 self.abs_max_out: 9703.0\n",
      "lif layer 1 self.abs_max_v: 9703.0\n",
      "fc layer 1 self.abs_max_out: 10444.0\n",
      "lif layer 1 self.abs_max_v: 10444.0\n",
      "fc layer 1 self.abs_max_out: 10632.0\n",
      "lif layer 1 self.abs_max_v: 10632.0\n",
      "fc layer 1 self.abs_max_out: 10732.0\n",
      "lif layer 1 self.abs_max_v: 10732.0\n",
      "lif layer 1 self.abs_max_v: 11039.5\n",
      "lif layer 1 self.abs_max_v: 11107.0\n",
      "lif layer 1 self.abs_max_v: 11445.5\n",
      "lif layer 1 self.abs_max_v: 12184.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.138709/  2.181875, val:  43.33%, val_best:  43.33%, tr:  95.51%, tr_best:  95.51%, epoch time: 85.77 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.5928%\n",
      "layer   2  Sparsity: 85.0208%\n",
      "layer   3  Sparsity: 92.6260%\n",
      "total_backward_count 19580 real_backward_count 6383  32.600%\n",
      "fc layer 1 self.abs_max_out: 11019.0\n",
      "fc layer 3 self.abs_max_out: 663.0\n",
      "fc layer 1 self.abs_max_out: 11148.0\n",
      "lif layer 2 self.abs_max_v: 4197.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  2.123821/  2.181415, val:  39.17%, val_best:  43.33%, tr:  97.96%, tr_best:  97.96%, epoch time: 86.13 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.6092%\n",
      "layer   2  Sparsity: 85.0063%\n",
      "layer   3  Sparsity: 91.6030%\n",
      "total_backward_count 29370 real_backward_count 8648  29.445%\n",
      "fc layer 1 self.abs_max_out: 11422.0\n",
      "lif layer 2 self.abs_max_v: 4290.0\n",
      "lif layer 2 self.abs_max_v: 4410.0\n",
      "lif layer 2 self.abs_max_v: 4513.0\n",
      "fc layer 2 self.abs_max_out: 3721.0\n",
      "lif layer 2 self.abs_max_v: 4513.5\n",
      "lif layer 2 self.abs_max_v: 4609.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 2 self.abs_max_out: 3772.0\n",
      "lif layer 2 self.abs_max_v: 4801.0\n",
      "fc layer 3 self.abs_max_out: 719.0\n",
      "fc layer 1 self.abs_max_out: 11673.0\n",
      "lif layer 1 self.abs_max_v: 12450.5\n",
      "lif layer 1 self.abs_max_v: 13098.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  2.101749/  2.189219, val:  41.25%, val_best:  43.33%, tr:  98.06%, tr_best:  98.06%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.5924%\n",
      "layer   2  Sparsity: 84.9555%\n",
      "layer   3  Sparsity: 90.6598%\n",
      "total_backward_count 39160 real_backward_count 10769  27.500%\n",
      "lif layer 2 self.abs_max_v: 4889.5\n",
      "fc layer 1 self.abs_max_out: 11688.0\n",
      "fc layer 2 self.abs_max_out: 3900.0\n",
      "lif layer 1 self.abs_max_v: 13267.0\n",
      "fc layer 1 self.abs_max_out: 12249.0\n",
      "fc layer 2 self.abs_max_out: 3921.0\n",
      "fc layer 2 self.abs_max_out: 3971.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "lif layer 1 self.abs_max_v: 14587.5\n",
      "lif layer 1 self.abs_max_v: 14808.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  2.100555/  2.155201, val:  49.58%, val_best:  49.58%, tr:  98.67%, tr_best:  98.67%, epoch time: 85.83 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.5597%\n",
      "layer   2  Sparsity: 84.5499%\n",
      "layer   3  Sparsity: 90.3704%\n",
      "total_backward_count 48950 real_backward_count 12857  26.266%\n",
      "fc layer 1 self.abs_max_out: 12359.0\n",
      "lif layer 2 self.abs_max_v: 4899.0\n",
      "fc layer 1 self.abs_max_out: 12620.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  2.084715/  2.178283, val:  47.92%, val_best:  49.58%, tr:  98.77%, tr_best:  98.77%, epoch time: 85.52 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 91.5926%\n",
      "layer   2  Sparsity: 84.1768%\n",
      "layer   3  Sparsity: 89.6171%\n",
      "total_backward_count 58740 real_backward_count 14875  25.323%\n",
      "fc layer 2 self.abs_max_out: 4395.0\n",
      "lif layer 2 self.abs_max_v: 4978.5\n",
      "lif layer 2 self.abs_max_v: 5173.5\n",
      "fc layer 1 self.abs_max_out: 12651.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  2.090096/  2.160247, val:  45.83%, val_best:  49.58%, tr:  98.16%, tr_best:  98.77%, epoch time: 84.43 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.5977%\n",
      "layer   2  Sparsity: 84.4811%\n",
      "layer   3  Sparsity: 89.4639%\n",
      "total_backward_count 68530 real_backward_count 16833  24.563%\n",
      "fc layer 1 self.abs_max_out: 13002.0\n",
      "fc layer 2 self.abs_max_out: 4424.0\n",
      "fc layer 2 self.abs_max_out: 4518.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  2.064657/  2.151361, val:  43.33%, val_best:  49.58%, tr:  98.77%, tr_best:  98.77%, epoch time: 81.81 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.5906%\n",
      "layer   2  Sparsity: 83.9347%\n",
      "layer   3  Sparsity: 88.9861%\n",
      "total_backward_count 78320 real_backward_count 18706  23.884%\n",
      "fc layer 1 self.abs_max_out: 13023.0\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "lif layer 1 self.abs_max_v: 15616.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 2 self.abs_max_out: 4578.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  2.070796/  2.147578, val:  45.42%, val_best:  49.58%, tr:  98.57%, tr_best:  98.77%, epoch time: 81.64 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.5966%\n",
      "layer   2  Sparsity: 84.2979%\n",
      "layer   3  Sparsity: 89.0760%\n",
      "total_backward_count 88110 real_backward_count 20705  23.499%\n",
      "fc layer 1 self.abs_max_out: 13314.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  2.060510/  2.150471, val:  46.25%, val_best:  49.58%, tr:  98.57%, tr_best:  98.77%, epoch time: 81.15 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5829%\n",
      "layer   2  Sparsity: 83.7697%\n",
      "layer   3  Sparsity: 88.8299%\n",
      "total_backward_count 97900 real_backward_count 22599  23.084%\n",
      "fc layer 2 self.abs_max_out: 4856.0\n",
      "fc layer 2 self.abs_max_out: 5022.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  2.065777/  2.133658, val:  53.75%, val_best:  53.75%, tr:  98.57%, tr_best:  98.77%, epoch time: 81.01 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5652%\n",
      "layer   2  Sparsity: 83.8669%\n",
      "layer   3  Sparsity: 89.1381%\n",
      "total_backward_count 107690 real_backward_count 24496  22.747%\n",
      "fc layer 2 self.abs_max_out: 5062.0\n",
      "fc layer 2 self.abs_max_out: 5097.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  2.057585/  2.145308, val:  47.08%, val_best:  53.75%, tr:  99.39%, tr_best:  99.39%, epoch time: 81.35 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.5457%\n",
      "layer   2  Sparsity: 83.2950%\n",
      "layer   3  Sparsity: 88.9884%\n",
      "total_backward_count 117480 real_backward_count 26343  22.423%\n",
      "fc layer 2 self.abs_max_out: 5113.0\n",
      "fc layer 2 self.abs_max_out: 5115.0\n",
      "lif layer 1 self.abs_max_v: 15738.5\n",
      "fc layer 2 self.abs_max_out: 5244.0\n",
      "lif layer 2 self.abs_max_v: 5244.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  2.070710/  2.162326, val:  34.58%, val_best:  53.75%, tr:  99.28%, tr_best:  99.39%, epoch time: 80.81 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5754%\n",
      "layer   2  Sparsity: 83.5495%\n",
      "layer   3  Sparsity: 89.7562%\n",
      "total_backward_count 127270 real_backward_count 28229  22.180%\n",
      "fc layer 1 self.abs_max_out: 13373.0\n",
      "lif layer 1 self.abs_max_v: 15834.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  2.073570/  2.150261, val:  46.67%, val_best:  53.75%, tr:  98.77%, tr_best:  99.39%, epoch time: 80.71 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5468%\n",
      "layer   2  Sparsity: 83.3680%\n",
      "layer   3  Sparsity: 89.6336%\n",
      "total_backward_count 137060 real_backward_count 30094  21.957%\n",
      "fc layer 1 self.abs_max_out: 13709.0\n",
      "lif layer 1 self.abs_max_v: 16160.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  2.059191/  2.145299, val:  37.50%, val_best:  53.75%, tr:  98.98%, tr_best:  99.39%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5498%\n",
      "layer   2  Sparsity: 82.7104%\n",
      "layer   3  Sparsity: 88.6511%\n",
      "total_backward_count 146850 real_backward_count 31849  21.688%\n",
      "lif layer 1 self.abs_max_v: 16816.0\n",
      "fc layer 3 self.abs_max_out: 729.0\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "lif layer 1 self.abs_max_v: 17171.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  2.055978/  2.143383, val:  47.92%, val_best:  53.75%, tr:  98.77%, tr_best:  99.39%, epoch time: 81.03 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5621%\n",
      "layer   2  Sparsity: 82.6253%\n",
      "layer   3  Sparsity: 88.8097%\n",
      "total_backward_count 156640 real_backward_count 33701  21.515%\n",
      "fc layer 1 self.abs_max_out: 13873.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  2.055587/  2.143780, val:  55.83%, val_best:  55.83%, tr:  98.88%, tr_best:  99.39%, epoch time: 81.11 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5746%\n",
      "layer   2  Sparsity: 82.6607%\n",
      "layer   3  Sparsity: 88.9987%\n",
      "total_backward_count 166430 real_backward_count 35493  21.326%\n",
      "fc layer 1 self.abs_max_out: 13985.0\n",
      "lif layer 1 self.abs_max_v: 17225.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  2.064517/  2.151659, val:  55.00%, val_best:  55.83%, tr:  99.28%, tr_best:  99.39%, epoch time: 81.39 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.6060%\n",
      "layer   2  Sparsity: 82.5297%\n",
      "layer   3  Sparsity: 89.1753%\n",
      "total_backward_count 176220 real_backward_count 37320  21.178%\n",
      "lif layer 1 self.abs_max_v: 17323.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  2.046782/  2.133185, val:  49.58%, val_best:  55.83%, tr:  98.37%, tr_best:  99.39%, epoch time: 81.19 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.5771%\n",
      "layer   2  Sparsity: 82.5216%\n",
      "layer   3  Sparsity: 88.9279%\n",
      "total_backward_count 186010 real_backward_count 39085  21.012%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  2.055197/  2.141070, val:  41.67%, val_best:  55.83%, tr:  99.08%, tr_best:  99.39%, epoch time: 82.30 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.5769%\n",
      "layer   2  Sparsity: 82.1399%\n",
      "layer   3  Sparsity: 89.4548%\n",
      "total_backward_count 195800 real_backward_count 40839  20.858%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  2.054401/  2.162573, val:  40.42%, val_best:  55.83%, tr:  98.77%, tr_best:  99.39%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.6009%\n",
      "layer   2  Sparsity: 82.0375%\n",
      "layer   3  Sparsity: 89.3795%\n",
      "total_backward_count 205590 real_backward_count 42604  20.723%\n",
      "fc layer 1 self.abs_max_out: 14215.0\n",
      "lif layer 1 self.abs_max_v: 18380.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  2.061072/  2.152057, val:  47.08%, val_best:  55.83%, tr:  98.88%, tr_best:  99.39%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5854%\n",
      "layer   2  Sparsity: 82.6164%\n",
      "layer   3  Sparsity: 89.0576%\n",
      "total_backward_count 215380 real_backward_count 44453  20.639%\n",
      "lif layer 2 self.abs_max_v: 5324.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  2.072375/  2.144595, val:  54.58%, val_best:  55.83%, tr:  98.98%, tr_best:  99.39%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.5666%\n",
      "layer   2  Sparsity: 82.3637%\n",
      "layer   3  Sparsity: 89.6767%\n",
      "total_backward_count 225170 real_backward_count 46299  20.562%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  2.049635/  2.133247, val:  51.67%, val_best:  55.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.5821%\n",
      "layer   2  Sparsity: 81.7746%\n",
      "layer   3  Sparsity: 88.6666%\n",
      "total_backward_count 234960 real_backward_count 48063  20.456%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  2.046358/  2.134745, val:  52.08%, val_best:  55.83%, tr:  98.57%, tr_best:  99.69%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.5981%\n",
      "layer   2  Sparsity: 81.7925%\n",
      "layer   3  Sparsity: 88.9446%\n",
      "total_backward_count 244750 real_backward_count 49896  20.387%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  2.053125/  2.126394, val:  54.58%, val_best:  55.83%, tr:  98.57%, tr_best:  99.69%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5654%\n",
      "layer   2  Sparsity: 82.0523%\n",
      "layer   3  Sparsity: 88.9808%\n",
      "total_backward_count 254540 real_backward_count 51838  20.365%\n",
      "fc layer 2 self.abs_max_out: 5337.0\n",
      "lif layer 2 self.abs_max_v: 5337.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  2.051662/  2.124625, val:  55.83%, val_best:  55.83%, tr:  98.88%, tr_best:  99.69%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5909%\n",
      "layer   2  Sparsity: 82.8431%\n",
      "layer   3  Sparsity: 88.9568%\n",
      "total_backward_count 264330 real_backward_count 53665  20.302%\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  2.045990/  2.129165, val:  59.58%, val_best:  59.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5711%\n",
      "layer   2  Sparsity: 82.9905%\n",
      "layer   3  Sparsity: 88.8147%\n",
      "total_backward_count 274120 real_backward_count 55440  20.225%\n",
      "fc layer 2 self.abs_max_out: 5713.0\n",
      "lif layer 2 self.abs_max_v: 5713.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  2.049081/  2.154564, val:  50.83%, val_best:  59.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5809%\n",
      "layer   2  Sparsity: 82.5075%\n",
      "layer   3  Sparsity: 88.8788%\n",
      "total_backward_count 283910 real_backward_count 57184  20.142%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  2.046670/  2.125682, val:  52.92%, val_best:  59.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5662%\n",
      "layer   2  Sparsity: 82.6176%\n",
      "layer   3  Sparsity: 88.6947%\n",
      "total_backward_count 293700 real_backward_count 58971  20.079%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  2.061755/  2.139881, val:  56.25%, val_best:  59.58%, tr:  98.67%, tr_best:  99.69%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.5611%\n",
      "layer   2  Sparsity: 82.8521%\n",
      "layer   3  Sparsity: 89.4532%\n",
      "total_backward_count 303490 real_backward_count 60722  20.008%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  2.060381/  2.150441, val:  47.92%, val_best:  59.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5624%\n",
      "layer   2  Sparsity: 82.7088%\n",
      "layer   3  Sparsity: 89.0807%\n",
      "total_backward_count 313280 real_backward_count 62544  19.964%\n",
      "fc layer 1 self.abs_max_out: 14370.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  2.052365/  2.148264, val:  48.75%, val_best:  59.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5746%\n",
      "layer   2  Sparsity: 82.6606%\n",
      "layer   3  Sparsity: 88.8952%\n",
      "total_backward_count 323070 real_backward_count 64317  19.908%\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  2.051527/  2.128809, val:  43.75%, val_best:  59.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5869%\n",
      "layer   2  Sparsity: 82.6760%\n",
      "layer   3  Sparsity: 88.8904%\n",
      "total_backward_count 332860 real_backward_count 66061  19.846%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  2.044348/  2.134546, val:  47.50%, val_best:  59.58%, tr:  98.88%, tr_best:  99.69%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5539%\n",
      "layer   2  Sparsity: 82.8404%\n",
      "layer   3  Sparsity: 89.1272%\n",
      "total_backward_count 342650 real_backward_count 67875  19.809%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  2.041571/  2.126782, val:  50.42%, val_best:  59.58%, tr:  98.57%, tr_best:  99.69%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.5761%\n",
      "layer   2  Sparsity: 82.5920%\n",
      "layer   3  Sparsity: 88.6447%\n",
      "total_backward_count 352440 real_backward_count 69685  19.772%\n",
      "fc layer 3 self.abs_max_out: 818.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  2.041275/  2.126081, val:  59.58%, val_best:  59.58%, tr:  99.08%, tr_best:  99.69%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5762%\n",
      "layer   2  Sparsity: 82.5542%\n",
      "layer   3  Sparsity: 88.6194%\n",
      "total_backward_count 362230 real_backward_count 71395  19.710%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "lif layer 1 self.abs_max_v: 18418.5\n",
      "lif layer 1 self.abs_max_v: 18825.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  2.033744/  2.116179, val:  44.17%, val_best:  59.58%, tr:  99.18%, tr_best:  99.69%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5890%\n",
      "layer   2  Sparsity: 82.4048%\n",
      "layer   3  Sparsity: 88.1641%\n",
      "total_backward_count 372020 real_backward_count 73057  19.638%\n",
      "fc layer 3 self.abs_max_out: 865.0\n",
      "fc layer 3 self.abs_max_out: 879.0\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  2.023753/  2.113147, val:  52.50%, val_best:  59.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6194%\n",
      "layer   2  Sparsity: 82.3083%\n",
      "layer   3  Sparsity: 88.0253%\n",
      "total_backward_count 381810 real_backward_count 74809  19.593%\n",
      "fc layer 1 self.abs_max_out: 14479.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  2.035202/  2.108841, val:  57.92%, val_best:  59.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5855%\n",
      "layer   2  Sparsity: 82.6230%\n",
      "layer   3  Sparsity: 88.2520%\n",
      "total_backward_count 391600 real_backward_count 76477  19.529%\n",
      "lif layer 1 self.abs_max_v: 19223.5\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  2.031829/  2.105989, val:  61.25%, val_best:  61.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5939%\n",
      "layer   2  Sparsity: 82.2637%\n",
      "layer   3  Sparsity: 88.0539%\n",
      "total_backward_count 401390 real_backward_count 78199  19.482%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  2.039712/  2.123101, val:  61.67%, val_best:  61.67%, tr:  98.98%, tr_best:  99.69%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5664%\n",
      "layer   2  Sparsity: 82.5354%\n",
      "layer   3  Sparsity: 88.8165%\n",
      "total_backward_count 411180 real_backward_count 79894  19.430%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  2.023860/  2.132831, val:  51.25%, val_best:  61.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5635%\n",
      "layer   2  Sparsity: 82.5299%\n",
      "layer   3  Sparsity: 88.9011%\n",
      "total_backward_count 420970 real_backward_count 81501  19.360%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  2.030757/  2.126225, val:  65.00%, val_best:  65.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.6098%\n",
      "layer   2  Sparsity: 82.4470%\n",
      "layer   3  Sparsity: 88.5128%\n",
      "total_backward_count 430760 real_backward_count 83199  19.314%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  2.032698/  2.114763, val:  49.58%, val_best:  65.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5952%\n",
      "layer   2  Sparsity: 81.9943%\n",
      "layer   3  Sparsity: 88.7168%\n",
      "total_backward_count 440550 real_backward_count 84919  19.276%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  2.021742/  2.104918, val:  54.58%, val_best:  65.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5951%\n",
      "layer   2  Sparsity: 81.6581%\n",
      "layer   3  Sparsity: 88.2982%\n",
      "total_backward_count 450340 real_backward_count 86633  19.237%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  2.032418/  2.104701, val:  55.42%, val_best:  65.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5953%\n",
      "layer   2  Sparsity: 81.7779%\n",
      "layer   3  Sparsity: 88.4236%\n",
      "total_backward_count 460130 real_backward_count 88349  19.201%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  2.011840/  2.123288, val:  44.58%, val_best:  65.00%, tr:  98.98%, tr_best:  99.69%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5802%\n",
      "layer   2  Sparsity: 82.0400%\n",
      "layer   3  Sparsity: 88.3148%\n",
      "total_backward_count 469920 real_backward_count 89976  19.147%\n",
      "lif layer 2 self.abs_max_v: 5842.5\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  2.013097/  2.120701, val:  57.92%, val_best:  65.00%, tr:  99.08%, tr_best:  99.69%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5925%\n",
      "layer   2  Sparsity: 82.0157%\n",
      "layer   3  Sparsity: 87.9822%\n",
      "total_backward_count 479710 real_backward_count 91621  19.099%\n",
      "fc layer 1 self.abs_max_out: 14607.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  2.032850/  2.119298, val:  59.17%, val_best:  65.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5702%\n",
      "layer   2  Sparsity: 82.3702%\n",
      "layer   3  Sparsity: 88.6303%\n",
      "total_backward_count 489500 real_backward_count 93328  19.066%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  2.041049/  2.146756, val:  42.50%, val_best:  65.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5803%\n",
      "layer   2  Sparsity: 82.2347%\n",
      "layer   3  Sparsity: 89.0788%\n",
      "total_backward_count 499290 real_backward_count 95048  19.037%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  2.035995/  2.110707, val:  43.33%, val_best:  65.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5582%\n",
      "layer   2  Sparsity: 82.3589%\n",
      "layer   3  Sparsity: 88.6481%\n",
      "total_backward_count 509080 real_backward_count 96714  18.998%\n",
      "lif layer 2 self.abs_max_v: 6114.5\n",
      "lif layer 2 self.abs_max_v: 6143.0\n",
      "fc layer 1 self.abs_max_out: 14705.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  2.018831/  2.124056, val:  54.58%, val_best:  65.00%, tr:  99.18%, tr_best:  99.69%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5835%\n",
      "layer   2  Sparsity: 82.1630%\n",
      "layer   3  Sparsity: 87.9231%\n",
      "total_backward_count 518870 real_backward_count 98400  18.964%\n",
      "lif layer 2 self.abs_max_v: 6478.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  2.017871/  2.110589, val:  49.17%, val_best:  65.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5798%\n",
      "layer   2  Sparsity: 82.1057%\n",
      "layer   3  Sparsity: 87.7522%\n",
      "total_backward_count 528660 real_backward_count 100129  18.940%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  2.009555/  2.113138, val:  66.67%, val_best:  66.67%, tr:  99.08%, tr_best:  99.69%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5899%\n",
      "layer   2  Sparsity: 81.9329%\n",
      "layer   3  Sparsity: 87.9429%\n",
      "total_backward_count 538450 real_backward_count 101788  18.904%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  2.016663/  2.092565, val:  52.92%, val_best:  66.67%, tr:  99.18%, tr_best:  99.69%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5904%\n",
      "layer   2  Sparsity: 81.8706%\n",
      "layer   3  Sparsity: 88.2610%\n",
      "total_backward_count 548240 real_backward_count 103546  18.887%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  2.000920/  2.098571, val:  53.75%, val_best:  66.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5996%\n",
      "layer   2  Sparsity: 81.6936%\n",
      "layer   3  Sparsity: 88.0312%\n",
      "total_backward_count 558030 real_backward_count 105170  18.847%\n",
      "lif layer 1 self.abs_max_v: 19396.5\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  2.007796/  2.116512, val:  59.17%, val_best:  66.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5761%\n",
      "layer   2  Sparsity: 81.6986%\n",
      "layer   3  Sparsity: 88.1511%\n",
      "total_backward_count 567820 real_backward_count 106735  18.797%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  2.017964/  2.121189, val:  57.50%, val_best:  66.67%, tr:  98.98%, tr_best:  99.69%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5681%\n",
      "layer   2  Sparsity: 82.0930%\n",
      "layer   3  Sparsity: 88.2205%\n",
      "total_backward_count 577610 real_backward_count 108315  18.752%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  2.010445/  2.104095, val:  55.83%, val_best:  66.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5597%\n",
      "layer   2  Sparsity: 82.2289%\n",
      "layer   3  Sparsity: 88.3706%\n",
      "total_backward_count 587400 real_backward_count 109886  18.707%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  2.013366/  2.105483, val:  53.75%, val_best:  66.67%, tr:  99.39%, tr_best:  99.69%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6023%\n",
      "layer   2  Sparsity: 82.3754%\n",
      "layer   3  Sparsity: 88.3046%\n",
      "total_backward_count 597190 real_backward_count 111525  18.675%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  2.014570/  2.087404, val:  65.00%, val_best:  66.67%, tr:  98.88%, tr_best:  99.69%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5701%\n",
      "layer   2  Sparsity: 82.1269%\n",
      "layer   3  Sparsity: 88.1138%\n",
      "total_backward_count 606980 real_backward_count 113238  18.656%\n",
      "fc layer 1 self.abs_max_out: 14917.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.990827/  2.100340, val:  52.08%, val_best:  66.67%, tr:  99.18%, tr_best:  99.69%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5525%\n",
      "layer   2  Sparsity: 81.8512%\n",
      "layer   3  Sparsity: 87.5432%\n",
      "total_backward_count 616770 real_backward_count 114849  18.621%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  2.001675/  2.100182, val:  60.00%, val_best:  66.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5818%\n",
      "layer   2  Sparsity: 81.8108%\n",
      "layer   3  Sparsity: 87.7940%\n",
      "total_backward_count 626560 real_backward_count 116470  18.589%\n",
      "lif layer 1 self.abs_max_v: 19426.0\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  2.001406/  2.080407, val:  60.83%, val_best:  66.67%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5598%\n",
      "layer   2  Sparsity: 81.7626%\n",
      "layer   3  Sparsity: 88.0707%\n",
      "total_backward_count 636350 real_backward_count 118137  18.565%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  2.000629/  2.101323, val:  51.25%, val_best:  66.67%, tr:  98.88%, tr_best:  99.69%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5660%\n",
      "layer   2  Sparsity: 81.7212%\n",
      "layer   3  Sparsity: 88.2477%\n",
      "total_backward_count 646140 real_backward_count 119799  18.541%\n",
      "lif layer 1 self.abs_max_v: 19430.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  2.011920/  2.101516, val:  50.00%, val_best:  66.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5773%\n",
      "layer   2  Sparsity: 81.6791%\n",
      "layer   3  Sparsity: 88.2427%\n",
      "total_backward_count 655930 real_backward_count 121418  18.511%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  2.021254/  2.104017, val:  54.17%, val_best:  66.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5772%\n",
      "layer   2  Sparsity: 81.8026%\n",
      "layer   3  Sparsity: 88.2429%\n",
      "total_backward_count 665720 real_backward_count 123078  18.488%\n",
      "lif layer 1 self.abs_max_v: 19730.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  2.000092/  2.075969, val:  69.17%, val_best:  69.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5561%\n",
      "layer   2  Sparsity: 81.7221%\n",
      "layer   3  Sparsity: 88.0155%\n",
      "total_backward_count 675510 real_backward_count 124712  18.462%\n",
      "fc layer 1 self.abs_max_out: 15059.0\n",
      "fc layer 3 self.abs_max_out: 965.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.998453/  2.092286, val:  59.58%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5990%\n",
      "layer   2  Sparsity: 82.4010%\n",
      "layer   3  Sparsity: 87.3119%\n",
      "total_backward_count 685300 real_backward_count 126344  18.436%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.999559/  2.099832, val:  57.92%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5876%\n",
      "layer   2  Sparsity: 82.2299%\n",
      "layer   3  Sparsity: 87.9998%\n",
      "total_backward_count 695090 real_backward_count 128004  18.415%\n",
      "lif layer 1 self.abs_max_v: 19809.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.991575/  2.119498, val:  45.83%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5724%\n",
      "layer   2  Sparsity: 82.2132%\n",
      "layer   3  Sparsity: 87.4647%\n",
      "total_backward_count 704880 real_backward_count 129661  18.395%\n",
      "fc layer 3 self.abs_max_out: 1008.0\n",
      "lif layer 2 self.abs_max_v: 6888.0\n",
      "lif layer 2 self.abs_max_v: 7068.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.995526/  2.102082, val:  55.83%, val_best:  69.17%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5742%\n",
      "layer   2  Sparsity: 82.1067%\n",
      "layer   3  Sparsity: 87.5907%\n",
      "total_backward_count 714670 real_backward_count 131329  18.376%\n",
      "fc layer 3 self.abs_max_out: 1012.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.994838/  2.094101, val:  53.33%, val_best:  69.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5890%\n",
      "layer   2  Sparsity: 82.4114%\n",
      "layer   3  Sparsity: 87.6135%\n",
      "total_backward_count 724460 real_backward_count 132930  18.349%\n",
      "fc layer 3 self.abs_max_out: 1026.0\n",
      "fc layer 3 self.abs_max_out: 1029.0\n",
      "fc layer 1 self.abs_max_out: 15172.0\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.981058/  2.082807, val:  56.67%, val_best:  69.17%, tr:  99.08%, tr_best:  99.69%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5514%\n",
      "layer   2  Sparsity: 82.4738%\n",
      "layer   3  Sparsity: 87.0005%\n",
      "total_backward_count 734250 real_backward_count 134681  18.343%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.985080/  2.063766, val:  68.33%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5705%\n",
      "layer   2  Sparsity: 82.4278%\n",
      "layer   3  Sparsity: 87.1749%\n",
      "total_backward_count 744040 real_backward_count 136364  18.328%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.979670/  2.088284, val:  57.92%, val_best:  69.17%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5785%\n",
      "layer   2  Sparsity: 82.4402%\n",
      "layer   3  Sparsity: 86.9458%\n",
      "total_backward_count 753830 real_backward_count 138088  18.318%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.987875/  2.106011, val:  51.25%, val_best:  69.17%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6074%\n",
      "layer   2  Sparsity: 82.4943%\n",
      "layer   3  Sparsity: 87.1382%\n",
      "total_backward_count 763620 real_backward_count 139701  18.295%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.977898/  2.075922, val:  60.42%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5601%\n",
      "layer   2  Sparsity: 82.4084%\n",
      "layer   3  Sparsity: 86.9251%\n",
      "total_backward_count 773410 real_backward_count 141302  18.270%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.983109/  2.086689, val:  57.50%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5866%\n",
      "layer   2  Sparsity: 82.2794%\n",
      "layer   3  Sparsity: 86.9476%\n",
      "total_backward_count 783200 real_backward_count 142914  18.247%\n",
      "fc layer 3 self.abs_max_out: 1037.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.982490/  2.070220, val:  52.08%, val_best:  69.17%, tr:  98.88%, tr_best:  99.69%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5806%\n",
      "layer   2  Sparsity: 82.2856%\n",
      "layer   3  Sparsity: 87.0543%\n",
      "total_backward_count 792990 real_backward_count 144577  18.232%\n",
      "fc layer 1 self.abs_max_out: 15191.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.991285/  2.118911, val:  37.08%, val_best:  69.17%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5936%\n",
      "layer   2  Sparsity: 82.1275%\n",
      "layer   3  Sparsity: 87.5372%\n",
      "total_backward_count 802780 real_backward_count 146230  18.215%\n",
      "fc layer 3 self.abs_max_out: 1050.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  2.001379/  2.097232, val:  58.75%, val_best:  69.17%, tr:  98.98%, tr_best:  99.69%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5773%\n",
      "layer   2  Sparsity: 82.3374%\n",
      "layer   3  Sparsity: 87.9279%\n",
      "total_backward_count 812570 real_backward_count 147943  18.207%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.986282/  2.106340, val:  48.33%, val_best:  69.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5928%\n",
      "layer   2  Sparsity: 82.1726%\n",
      "layer   3  Sparsity: 86.7532%\n",
      "total_backward_count 822360 real_backward_count 149611  18.193%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.983666/  2.058942, val:  64.58%, val_best:  69.17%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5772%\n",
      "layer   2  Sparsity: 82.0228%\n",
      "layer   3  Sparsity: 86.5897%\n",
      "total_backward_count 832150 real_backward_count 151240  18.175%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.985646/  2.104527, val:  53.33%, val_best:  69.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6016%\n",
      "layer   2  Sparsity: 82.1641%\n",
      "layer   3  Sparsity: 87.5099%\n",
      "total_backward_count 841940 real_backward_count 152846  18.154%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.977717/  2.077230, val:  58.33%, val_best:  69.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.6089%\n",
      "layer   2  Sparsity: 82.2494%\n",
      "layer   3  Sparsity: 87.3587%\n",
      "total_backward_count 851730 real_backward_count 154445  18.133%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  2.004131/  2.094148, val:  58.33%, val_best:  69.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5793%\n",
      "layer   2  Sparsity: 82.1396%\n",
      "layer   3  Sparsity: 88.2181%\n",
      "total_backward_count 861520 real_backward_count 156034  18.111%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  2.001992/  2.061116, val:  60.00%, val_best:  69.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5835%\n",
      "layer   2  Sparsity: 82.2238%\n",
      "layer   3  Sparsity: 87.9677%\n",
      "total_backward_count 871310 real_backward_count 157635  18.092%\n",
      "fc layer 3 self.abs_max_out: 1101.0\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.989681/  2.065987, val:  58.33%, val_best:  69.17%, tr:  98.88%, tr_best:  99.80%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5834%\n",
      "layer   2  Sparsity: 82.1737%\n",
      "layer   3  Sparsity: 87.1246%\n",
      "total_backward_count 881100 real_backward_count 159328  18.083%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.998833/  2.116816, val:  60.00%, val_best:  69.17%, tr:  98.67%, tr_best:  99.80%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.6096%\n",
      "layer   2  Sparsity: 82.3614%\n",
      "layer   3  Sparsity: 88.0917%\n",
      "total_backward_count 890890 real_backward_count 160947  18.066%\n",
      "lif layer 1 self.abs_max_v: 19815.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.995016/  2.066893, val:  63.33%, val_best:  69.17%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5789%\n",
      "layer   2  Sparsity: 82.5387%\n",
      "layer   3  Sparsity: 87.3808%\n",
      "total_backward_count 900680 real_backward_count 162527  18.045%\n",
      "lif layer 1 self.abs_max_v: 20260.5\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.998799/  2.079483, val:  54.58%, val_best:  69.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5859%\n",
      "layer   2  Sparsity: 82.3821%\n",
      "layer   3  Sparsity: 87.3248%\n",
      "total_backward_count 910470 real_backward_count 164190  18.034%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.992192/  2.079843, val:  60.42%, val_best:  69.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5736%\n",
      "layer   2  Sparsity: 82.2555%\n",
      "layer   3  Sparsity: 86.9841%\n",
      "total_backward_count 920260 real_backward_count 165842  18.021%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.986680/  2.090625, val:  59.17%, val_best:  69.17%, tr:  99.08%, tr_best:  99.80%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5638%\n",
      "layer   2  Sparsity: 82.0226%\n",
      "layer   3  Sparsity: 86.8838%\n",
      "total_backward_count 930050 real_backward_count 167468  18.006%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.984810/  2.076528, val:  62.92%, val_best:  69.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5777%\n",
      "layer   2  Sparsity: 82.0646%\n",
      "layer   3  Sparsity: 86.2580%\n",
      "total_backward_count 939840 real_backward_count 169107  17.993%\n",
      "lif layer 1 self.abs_max_v: 20410.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.980532/  2.090573, val:  55.42%, val_best:  69.17%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5965%\n",
      "layer   2  Sparsity: 82.1003%\n",
      "layer   3  Sparsity: 87.2806%\n",
      "total_backward_count 949630 real_backward_count 170675  17.973%\n",
      "fc layer 3 self.abs_max_out: 1109.0\n",
      "fc layer 3 self.abs_max_out: 1207.0\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.976445/  2.058739, val:  56.25%, val_best:  69.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5575%\n",
      "layer   2  Sparsity: 82.5826%\n",
      "layer   3  Sparsity: 87.0933%\n",
      "total_backward_count 959420 real_backward_count 172303  17.959%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.975810/  2.075230, val:  55.00%, val_best:  69.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5541%\n",
      "layer   2  Sparsity: 82.5394%\n",
      "layer   3  Sparsity: 86.8990%\n",
      "total_backward_count 969210 real_backward_count 173935  17.946%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.979679/  2.072025, val:  67.92%, val_best:  69.17%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5671%\n",
      "layer   2  Sparsity: 82.1879%\n",
      "layer   3  Sparsity: 86.9535%\n",
      "total_backward_count 979000 real_backward_count 175542  17.931%\n",
      "lif layer 1 self.abs_max_v: 20412.5\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.976250/  2.096390, val:  60.83%, val_best:  69.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5835%\n",
      "layer   2  Sparsity: 82.0580%\n",
      "layer   3  Sparsity: 86.6110%\n",
      "total_backward_count 988790 real_backward_count 177150  17.916%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.971319/  2.086594, val:  58.75%, val_best:  69.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.6039%\n",
      "layer   2  Sparsity: 82.1335%\n",
      "layer   3  Sparsity: 86.7993%\n",
      "total_backward_count 998580 real_backward_count 178818  17.907%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.993994/  2.095381, val:  53.75%, val_best:  69.17%, tr:  98.98%, tr_best:  99.80%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5935%\n",
      "layer   2  Sparsity: 82.1891%\n",
      "layer   3  Sparsity: 87.4160%\n",
      "total_backward_count 1008370 real_backward_count 180425  17.893%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.975576/  2.086160, val:  53.33%, val_best:  69.17%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5503%\n",
      "layer   2  Sparsity: 81.9618%\n",
      "layer   3  Sparsity: 86.9241%\n",
      "total_backward_count 1018160 real_backward_count 181995  17.875%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.973667/  2.062799, val:  70.42%, val_best:  70.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5680%\n",
      "layer   2  Sparsity: 82.0977%\n",
      "layer   3  Sparsity: 86.9678%\n",
      "total_backward_count 1027950 real_backward_count 183581  17.859%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.978973/  2.070688, val:  60.00%, val_best:  70.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5577%\n",
      "layer   2  Sparsity: 81.9788%\n",
      "layer   3  Sparsity: 87.2571%\n",
      "total_backward_count 1037740 real_backward_count 185240  17.850%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.978724/  2.085924, val:  45.42%, val_best:  70.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.6132%\n",
      "layer   2  Sparsity: 82.1252%\n",
      "layer   3  Sparsity: 87.7794%\n",
      "total_backward_count 1047530 real_backward_count 186822  17.835%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.985308/  2.089183, val:  52.08%, val_best:  70.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5570%\n",
      "layer   2  Sparsity: 82.1754%\n",
      "layer   3  Sparsity: 87.2614%\n",
      "total_backward_count 1057320 real_backward_count 188419  17.820%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.994271/  2.071813, val:  67.92%, val_best:  70.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5888%\n",
      "layer   2  Sparsity: 82.4822%\n",
      "layer   3  Sparsity: 87.8646%\n",
      "total_backward_count 1067110 real_backward_count 190011  17.806%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.990331/  2.098348, val:  55.83%, val_best:  70.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5893%\n",
      "layer   2  Sparsity: 81.9085%\n",
      "layer   3  Sparsity: 87.7243%\n",
      "total_backward_count 1076900 real_backward_count 191610  17.793%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.992483/  2.069924, val:  62.92%, val_best:  70.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5696%\n",
      "layer   2  Sparsity: 82.2683%\n",
      "layer   3  Sparsity: 87.8019%\n",
      "total_backward_count 1086690 real_backward_count 193217  17.780%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.991973/  2.092723, val:  56.25%, val_best:  70.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5711%\n",
      "layer   2  Sparsity: 82.4259%\n",
      "layer   3  Sparsity: 87.6976%\n",
      "total_backward_count 1096480 real_backward_count 194741  17.761%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.971146/  2.050972, val:  75.42%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5614%\n",
      "layer   2  Sparsity: 82.4398%\n",
      "layer   3  Sparsity: 86.9416%\n",
      "total_backward_count 1106270 real_backward_count 196391  17.753%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.975625/  2.100477, val:  38.75%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5733%\n",
      "layer   2  Sparsity: 82.3335%\n",
      "layer   3  Sparsity: 87.1112%\n",
      "total_backward_count 1116060 real_backward_count 197969  17.738%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.981035/  2.083495, val:  42.08%, val_best:  75.42%, tr:  98.98%, tr_best:  99.80%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5744%\n",
      "layer   2  Sparsity: 82.1994%\n",
      "layer   3  Sparsity: 87.4939%\n",
      "total_backward_count 1125850 real_backward_count 199535  17.723%\n",
      "fc layer 1 self.abs_max_out: 15202.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.984142/  2.076356, val:  50.83%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5858%\n",
      "layer   2  Sparsity: 82.1973%\n",
      "layer   3  Sparsity: 87.1583%\n",
      "total_backward_count 1135640 real_backward_count 201157  17.713%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.982645/  2.067269, val:  54.17%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5727%\n",
      "layer   2  Sparsity: 82.1084%\n",
      "layer   3  Sparsity: 87.0001%\n",
      "total_backward_count 1145430 real_backward_count 202645  17.692%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.975591/  2.053934, val:  65.00%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5906%\n",
      "layer   2  Sparsity: 81.8432%\n",
      "layer   3  Sparsity: 86.8968%\n",
      "total_backward_count 1155220 real_backward_count 204204  17.677%\n",
      "fc layer 1 self.abs_max_out: 15272.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.964784/  2.073018, val:  64.17%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5845%\n",
      "layer   2  Sparsity: 82.1729%\n",
      "layer   3  Sparsity: 86.9366%\n",
      "total_backward_count 1165010 real_backward_count 205794  17.665%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.967415/  2.068424, val:  66.25%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5719%\n",
      "layer   2  Sparsity: 82.3399%\n",
      "layer   3  Sparsity: 86.8669%\n",
      "total_backward_count 1174800 real_backward_count 207380  17.652%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.971077/  2.070255, val:  60.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5974%\n",
      "layer   2  Sparsity: 82.1929%\n",
      "layer   3  Sparsity: 86.7887%\n",
      "total_backward_count 1184590 real_backward_count 208910  17.636%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.986151/  2.082264, val:  57.92%, val_best:  75.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5858%\n",
      "layer   2  Sparsity: 82.0619%\n",
      "layer   3  Sparsity: 88.0295%\n",
      "total_backward_count 1194380 real_backward_count 210486  17.623%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.985270/  2.075098, val:  67.92%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5985%\n",
      "layer   2  Sparsity: 82.5394%\n",
      "layer   3  Sparsity: 87.7082%\n",
      "total_backward_count 1204170 real_backward_count 212112  17.615%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.987101/  2.069601, val:  60.83%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6024%\n",
      "layer   2  Sparsity: 82.1385%\n",
      "layer   3  Sparsity: 87.1727%\n",
      "total_backward_count 1213960 real_backward_count 213695  17.603%\n",
      "fc layer 3 self.abs_max_out: 1227.0\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.972803/  2.076805, val:  67.92%, val_best:  75.42%, tr:  98.98%, tr_best:  99.80%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.6063%\n",
      "layer   2  Sparsity: 82.3672%\n",
      "layer   3  Sparsity: 87.1269%\n",
      "total_backward_count 1223750 real_backward_count 215344  17.597%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.964592/  2.059037, val:  59.58%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5984%\n",
      "layer   2  Sparsity: 82.1536%\n",
      "layer   3  Sparsity: 87.0057%\n",
      "total_backward_count 1233540 real_backward_count 216899  17.583%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.971793/  2.091257, val:  52.92%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5957%\n",
      "layer   2  Sparsity: 82.1248%\n",
      "layer   3  Sparsity: 87.1822%\n",
      "total_backward_count 1243330 real_backward_count 218442  17.569%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.976810/  2.081901, val:  61.67%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5506%\n",
      "layer   2  Sparsity: 82.4607%\n",
      "layer   3  Sparsity: 87.3201%\n",
      "total_backward_count 1253120 real_backward_count 219986  17.555%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.979509/  2.082391, val:  65.00%, val_best:  75.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5913%\n",
      "layer   2  Sparsity: 82.1961%\n",
      "layer   3  Sparsity: 87.2354%\n",
      "total_backward_count 1262910 real_backward_count 221557  17.543%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.988835/  2.079139, val:  55.00%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5884%\n",
      "layer   2  Sparsity: 82.3977%\n",
      "layer   3  Sparsity: 87.4684%\n",
      "total_backward_count 1272700 real_backward_count 223089  17.529%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.993108/  2.098636, val:  39.58%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5967%\n",
      "layer   2  Sparsity: 82.5547%\n",
      "layer   3  Sparsity: 87.8413%\n",
      "total_backward_count 1282490 real_backward_count 224583  17.511%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.993532/  2.100632, val:  60.00%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5710%\n",
      "layer   2  Sparsity: 82.2623%\n",
      "layer   3  Sparsity: 87.6343%\n",
      "total_backward_count 1292280 real_backward_count 226181  17.502%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.977599/  2.070758, val:  59.58%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5875%\n",
      "layer   2  Sparsity: 82.1687%\n",
      "layer   3  Sparsity: 87.0082%\n",
      "total_backward_count 1302070 real_backward_count 227729  17.490%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.965823/  2.074784, val:  53.33%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5932%\n",
      "layer   2  Sparsity: 81.9384%\n",
      "layer   3  Sparsity: 87.1389%\n",
      "total_backward_count 1311860 real_backward_count 229286  17.478%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.984239/  2.096542, val:  50.83%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5900%\n",
      "layer   2  Sparsity: 82.5271%\n",
      "layer   3  Sparsity: 87.4790%\n",
      "total_backward_count 1321650 real_backward_count 230817  17.464%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.986047/  2.069523, val:  70.00%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5739%\n",
      "layer   2  Sparsity: 82.2518%\n",
      "layer   3  Sparsity: 87.2851%\n",
      "total_backward_count 1331440 real_backward_count 232425  17.457%\n",
      "fc layer 1 self.abs_max_out: 15284.0\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.995627/  2.084580, val:  51.67%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.5477%\n",
      "layer   2  Sparsity: 82.1605%\n",
      "layer   3  Sparsity: 87.5727%\n",
      "total_backward_count 1341230 real_backward_count 233916  17.440%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.977104/  2.078954, val:  50.83%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5530%\n",
      "layer   2  Sparsity: 82.0566%\n",
      "layer   3  Sparsity: 87.4289%\n",
      "total_backward_count 1351020 real_backward_count 235510  17.432%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.983968/  2.078632, val:  56.25%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5835%\n",
      "layer   2  Sparsity: 82.2979%\n",
      "layer   3  Sparsity: 87.1904%\n",
      "total_backward_count 1360810 real_backward_count 237144  17.427%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.983977/  2.089938, val:  59.58%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5602%\n",
      "layer   2  Sparsity: 82.6838%\n",
      "layer   3  Sparsity: 87.2620%\n",
      "total_backward_count 1370600 real_backward_count 238801  17.423%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.980719/  2.091447, val:  60.42%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5742%\n",
      "layer   2  Sparsity: 82.9758%\n",
      "layer   3  Sparsity: 87.2557%\n",
      "total_backward_count 1380390 real_backward_count 240360  17.412%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.987045/  2.094400, val:  60.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5826%\n",
      "layer   2  Sparsity: 82.8341%\n",
      "layer   3  Sparsity: 87.0770%\n",
      "total_backward_count 1390180 real_backward_count 241879  17.399%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.968842/  2.061204, val:  67.50%, val_best:  75.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5914%\n",
      "layer   2  Sparsity: 82.6133%\n",
      "layer   3  Sparsity: 86.9217%\n",
      "total_backward_count 1399970 real_backward_count 243469  17.391%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.969201/  2.071036, val:  67.50%, val_best:  75.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5814%\n",
      "layer   2  Sparsity: 82.6879%\n",
      "layer   3  Sparsity: 87.0529%\n",
      "total_backward_count 1409760 real_backward_count 245013  17.380%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.967405/  2.049465, val:  60.42%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5437%\n",
      "layer   2  Sparsity: 82.1536%\n",
      "layer   3  Sparsity: 87.0464%\n",
      "total_backward_count 1419550 real_backward_count 246574  17.370%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.984029/  2.075989, val:  50.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5871%\n",
      "layer   2  Sparsity: 82.3284%\n",
      "layer   3  Sparsity: 87.5574%\n",
      "total_backward_count 1429340 real_backward_count 248155  17.362%\n",
      "lif layer 1 self.abs_max_v: 20763.5\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.985007/  2.100476, val:  54.58%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5573%\n",
      "layer   2  Sparsity: 82.2870%\n",
      "layer   3  Sparsity: 87.4506%\n",
      "total_backward_count 1439130 real_backward_count 249759  17.355%\n",
      "lif layer 1 self.abs_max_v: 20947.0\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.978968/  2.094656, val:  52.50%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5508%\n",
      "layer   2  Sparsity: 82.2984%\n",
      "layer   3  Sparsity: 87.0096%\n",
      "total_backward_count 1448920 real_backward_count 251268  17.342%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.969119/  2.080143, val:  66.25%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6080%\n",
      "layer   2  Sparsity: 82.4295%\n",
      "layer   3  Sparsity: 86.7725%\n",
      "total_backward_count 1458710 real_backward_count 252816  17.331%\n",
      "lif layer 1 self.abs_max_v: 20964.5\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.962151/  2.091831, val:  50.83%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5859%\n",
      "layer   2  Sparsity: 82.3155%\n",
      "layer   3  Sparsity: 86.6906%\n",
      "total_backward_count 1468500 real_backward_count 254387  17.323%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.962541/  2.066462, val:  64.17%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5884%\n",
      "layer   2  Sparsity: 82.2788%\n",
      "layer   3  Sparsity: 86.8261%\n",
      "total_backward_count 1478290 real_backward_count 255921  17.312%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.967432/  2.089023, val:  54.58%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5797%\n",
      "layer   2  Sparsity: 82.2731%\n",
      "layer   3  Sparsity: 87.2186%\n",
      "total_backward_count 1488080 real_backward_count 257475  17.302%\n",
      "lif layer 1 self.abs_max_v: 21406.0\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.971130/  2.107769, val:  58.33%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5735%\n",
      "layer   2  Sparsity: 82.2430%\n",
      "layer   3  Sparsity: 87.7979%\n",
      "total_backward_count 1497870 real_backward_count 259021  17.293%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.965770/  2.027170, val:  66.25%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5603%\n",
      "layer   2  Sparsity: 82.2153%\n",
      "layer   3  Sparsity: 87.0626%\n",
      "total_backward_count 1507660 real_backward_count 260622  17.287%\n",
      "fc layer 3 self.abs_max_out: 1283.0\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.959231/  2.071023, val:  60.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5697%\n",
      "layer   2  Sparsity: 82.2480%\n",
      "layer   3  Sparsity: 87.0044%\n",
      "total_backward_count 1517450 real_backward_count 262176  17.277%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.984803/  2.086516, val:  64.17%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6008%\n",
      "layer   2  Sparsity: 82.5557%\n",
      "layer   3  Sparsity: 87.6030%\n",
      "total_backward_count 1527240 real_backward_count 263790  17.272%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.972696/  2.078104, val:  55.42%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5315%\n",
      "layer   2  Sparsity: 82.4135%\n",
      "layer   3  Sparsity: 87.3503%\n",
      "total_backward_count 1537030 real_backward_count 265357  17.264%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.977413/  2.093917, val:  65.00%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.6235%\n",
      "layer   2  Sparsity: 82.5792%\n",
      "layer   3  Sparsity: 87.3407%\n",
      "total_backward_count 1546820 real_backward_count 266895  17.254%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.975345/  2.119578, val:  54.17%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5854%\n",
      "layer   2  Sparsity: 82.5175%\n",
      "layer   3  Sparsity: 87.0115%\n",
      "total_backward_count 1556610 real_backward_count 268411  17.243%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.972687/  2.092609, val:  59.58%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5982%\n",
      "layer   2  Sparsity: 82.7039%\n",
      "layer   3  Sparsity: 87.1537%\n",
      "total_backward_count 1566400 real_backward_count 270006  17.237%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.963708/  2.065215, val:  62.92%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5899%\n",
      "layer   2  Sparsity: 82.7380%\n",
      "layer   3  Sparsity: 86.7153%\n",
      "total_backward_count 1576190 real_backward_count 271522  17.226%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.969187/  2.065463, val:  61.67%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5711%\n",
      "layer   2  Sparsity: 82.5102%\n",
      "layer   3  Sparsity: 86.9174%\n",
      "total_backward_count 1585980 real_backward_count 273082  17.219%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.976573/  2.077610, val:  62.50%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5635%\n",
      "layer   2  Sparsity: 82.4148%\n",
      "layer   3  Sparsity: 87.6145%\n",
      "total_backward_count 1595770 real_backward_count 274626  17.210%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.978556/  2.101509, val:  61.67%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5708%\n",
      "layer   2  Sparsity: 82.3950%\n",
      "layer   3  Sparsity: 87.3255%\n",
      "total_backward_count 1605560 real_backward_count 276137  17.199%\n",
      "lif layer 1 self.abs_max_v: 21882.0\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.975842/  2.078876, val:  51.67%, val_best:  75.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6180%\n",
      "layer   2  Sparsity: 82.2284%\n",
      "layer   3  Sparsity: 86.7343%\n",
      "total_backward_count 1615350 real_backward_count 277668  17.189%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.974791/  2.063722, val:  59.58%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5849%\n",
      "layer   2  Sparsity: 82.4991%\n",
      "layer   3  Sparsity: 87.2588%\n",
      "total_backward_count 1625140 real_backward_count 279268  17.184%\n",
      "lif layer 1 self.abs_max_v: 22363.0\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.966873/  2.078116, val:  54.58%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5689%\n",
      "layer   2  Sparsity: 82.1795%\n",
      "layer   3  Sparsity: 86.9875%\n",
      "total_backward_count 1634930 real_backward_count 280814  17.176%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.977191/  2.058901, val:  51.25%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5696%\n",
      "layer   2  Sparsity: 81.9079%\n",
      "layer   3  Sparsity: 86.5871%\n",
      "total_backward_count 1644720 real_backward_count 282343  17.167%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.959673/  2.067587, val:  58.33%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5715%\n",
      "layer   2  Sparsity: 82.2060%\n",
      "layer   3  Sparsity: 86.7564%\n",
      "total_backward_count 1654510 real_backward_count 283892  17.159%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.972215/  2.066825, val:  69.58%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.6064%\n",
      "layer   2  Sparsity: 82.1340%\n",
      "layer   3  Sparsity: 87.1308%\n",
      "total_backward_count 1664300 real_backward_count 285426  17.150%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.969952/  2.095235, val:  57.92%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.5679%\n",
      "layer   2  Sparsity: 82.4142%\n",
      "layer   3  Sparsity: 86.8336%\n",
      "total_backward_count 1674090 real_backward_count 286944  17.140%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.974075/  2.095135, val:  53.75%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5599%\n",
      "layer   2  Sparsity: 82.2684%\n",
      "layer   3  Sparsity: 86.7263%\n",
      "total_backward_count 1683880 real_backward_count 288511  17.134%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.974450/  2.088091, val:  68.33%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5705%\n",
      "layer   2  Sparsity: 81.9191%\n",
      "layer   3  Sparsity: 87.0409%\n",
      "total_backward_count 1693670 real_backward_count 290060  17.126%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.985367/  2.068459, val:  65.83%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5802%\n",
      "layer   2  Sparsity: 82.3278%\n",
      "layer   3  Sparsity: 88.0045%\n",
      "total_backward_count 1703460 real_backward_count 291538  17.114%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.979032/  2.070434, val:  65.83%, val_best:  75.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5618%\n",
      "layer   2  Sparsity: 81.9346%\n",
      "layer   3  Sparsity: 86.5787%\n",
      "total_backward_count 1713250 real_backward_count 293029  17.104%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.973236/  2.074294, val:  73.33%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5919%\n",
      "layer   2  Sparsity: 82.1589%\n",
      "layer   3  Sparsity: 86.7661%\n",
      "total_backward_count 1723040 real_backward_count 294632  17.100%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.974738/  2.055131, val:  67.50%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5838%\n",
      "layer   2  Sparsity: 81.9308%\n",
      "layer   3  Sparsity: 87.0023%\n",
      "total_backward_count 1732830 real_backward_count 296115  17.089%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.970279/  2.081044, val:  65.42%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5738%\n",
      "layer   2  Sparsity: 82.1527%\n",
      "layer   3  Sparsity: 87.1733%\n",
      "total_backward_count 1742620 real_backward_count 297631  17.080%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.980129/  2.094360, val:  50.00%, val_best:  75.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5746%\n",
      "layer   2  Sparsity: 82.0935%\n",
      "layer   3  Sparsity: 87.5230%\n",
      "total_backward_count 1752410 real_backward_count 299138  17.070%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.959569/  2.085025, val:  56.67%, val_best:  75.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5583%\n",
      "layer   2  Sparsity: 81.8991%\n",
      "layer   3  Sparsity: 86.7845%\n",
      "total_backward_count 1762200 real_backward_count 300676  17.063%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.964170/  2.069516, val:  61.25%, val_best:  75.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5775%\n",
      "layer   2  Sparsity: 82.2631%\n",
      "layer   3  Sparsity: 86.8372%\n",
      "total_backward_count 1771990 real_backward_count 302224  17.056%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.964304/  2.096661, val:  60.83%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5567%\n",
      "layer   2  Sparsity: 82.1319%\n",
      "layer   3  Sparsity: 87.6368%\n",
      "total_backward_count 1781780 real_backward_count 303710  17.045%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.969790/  2.080702, val:  67.92%, val_best:  75.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5813%\n",
      "layer   2  Sparsity: 82.3381%\n",
      "layer   3  Sparsity: 87.3197%\n",
      "total_backward_count 1791570 real_backward_count 305238  17.037%\n",
      "lif layer 1 self.abs_max_v: 22563.5\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.981388/  2.120169, val:  42.92%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5507%\n",
      "layer   2  Sparsity: 82.3124%\n",
      "layer   3  Sparsity: 87.7781%\n",
      "total_backward_count 1801360 real_backward_count 306792  17.031%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.999486/  2.102354, val:  65.42%, val_best:  75.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5833%\n",
      "layer   2  Sparsity: 82.5048%\n",
      "layer   3  Sparsity: 87.9828%\n",
      "total_backward_count 1811150 real_backward_count 308359  17.026%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.987888/  2.115390, val:  52.92%, val_best:  75.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5820%\n",
      "layer   2  Sparsity: 82.2908%\n",
      "layer   3  Sparsity: 87.7510%\n",
      "total_backward_count 1820940 real_backward_count 309929  17.020%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.979313/  2.068995, val:  55.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5800%\n",
      "layer   2  Sparsity: 82.2135%\n",
      "layer   3  Sparsity: 87.3950%\n",
      "total_backward_count 1830730 real_backward_count 311477  17.014%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.973072/  2.090573, val:  57.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5831%\n",
      "layer   2  Sparsity: 82.5744%\n",
      "layer   3  Sparsity: 87.8693%\n",
      "total_backward_count 1840520 real_backward_count 313045  17.009%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.996559/  2.089281, val:  74.17%, val_best:  75.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5841%\n",
      "layer   2  Sparsity: 82.7247%\n",
      "layer   3  Sparsity: 88.0938%\n",
      "total_backward_count 1850310 real_backward_count 314631  17.004%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  2.003844/  2.066236, val:  65.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5662%\n",
      "layer   2  Sparsity: 82.4241%\n",
      "layer   3  Sparsity: 87.8807%\n",
      "total_backward_count 1860100 real_backward_count 316131  16.995%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.990881/  2.077631, val:  68.33%, val_best:  75.42%, tr:  99.08%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5921%\n",
      "layer   2  Sparsity: 82.4348%\n",
      "layer   3  Sparsity: 87.8715%\n",
      "total_backward_count 1869890 real_backward_count 317596  16.985%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.989800/  2.083347, val:  55.00%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5843%\n",
      "layer   2  Sparsity: 82.4782%\n",
      "layer   3  Sparsity: 87.9158%\n",
      "total_backward_count 1879680 real_backward_count 319159  16.979%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.991529/  2.090328, val:  53.75%, val_best:  75.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.6186%\n",
      "layer   2  Sparsity: 82.8195%\n",
      "layer   3  Sparsity: 87.6382%\n",
      "total_backward_count 1889470 real_backward_count 320698  16.973%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.974462/  2.103279, val:  52.50%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5799%\n",
      "layer   2  Sparsity: 82.5548%\n",
      "layer   3  Sparsity: 87.0099%\n",
      "total_backward_count 1899260 real_backward_count 322135  16.961%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.983089/  2.093783, val:  42.92%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5680%\n",
      "layer   2  Sparsity: 82.6244%\n",
      "layer   3  Sparsity: 87.7948%\n",
      "total_backward_count 1909050 real_backward_count 323652  16.954%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.979523/  2.081925, val:  61.67%, val_best:  75.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.5997%\n",
      "layer   2  Sparsity: 82.5432%\n",
      "layer   3  Sparsity: 87.5260%\n",
      "total_backward_count 1918840 real_backward_count 325194  16.947%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.976328/  2.079158, val:  55.42%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5909%\n",
      "layer   2  Sparsity: 82.5960%\n",
      "layer   3  Sparsity: 87.3435%\n",
      "total_backward_count 1928630 real_backward_count 326688  16.939%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.967748/  2.072850, val:  65.00%, val_best:  75.42%, tr:  99.18%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.5640%\n",
      "layer   2  Sparsity: 82.3466%\n",
      "layer   3  Sparsity: 87.4858%\n",
      "total_backward_count 1938420 real_backward_count 328228  16.933%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.972121/  2.084633, val:  53.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5747%\n",
      "layer   2  Sparsity: 82.3572%\n",
      "layer   3  Sparsity: 86.9338%\n",
      "total_backward_count 1948210 real_backward_count 329732  16.925%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.969817/  2.081754, val:  61.67%, val_best:  75.42%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.5586%\n",
      "layer   2  Sparsity: 82.4494%\n",
      "layer   3  Sparsity: 87.2270%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede9ecfeec454d1091e8e7f1cdf82caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñÑ‚ñà‚ñà‚ñÇ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñÑ‚ñà‚ñà‚ñÇ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99285</td></tr><tr><td>tr_epoch_loss</td><td>1.96982</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.61667</td></tr><tr><td>val_loss</td><td>2.08175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-sweep-51</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zp83zknp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zp83zknp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_203652-zp83zknp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1wpz2hx8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_010008-1wpz2hx8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1wpz2hx8' target=\"_blank\">stellar-sweep-58</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1wpz2hx8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1wpz2hx8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251115_010016_309', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 553.0\n",
      "lif layer 1 self.abs_max_v: 553.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "lif layer 1 self.abs_max_v: 791.5\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 1090.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 833.0\n",
      "lif layer 1 self.abs_max_v: 1155.0\n",
      "fc layer 2 self.abs_max_out: 404.0\n",
      "lif layer 2 self.abs_max_v: 408.0\n",
      "fc layer 1 self.abs_max_out: 1395.0\n",
      "lif layer 1 self.abs_max_v: 1831.0\n",
      "lif layer 2 self.abs_max_v: 416.0\n",
      "fc layer 1 self.abs_max_out: 1807.0\n",
      "fc layer 2 self.abs_max_out: 700.0\n",
      "lif layer 2 self.abs_max_v: 887.0\n",
      "fc layer 1 self.abs_max_out: 2210.0\n",
      "lif layer 1 self.abs_max_v: 2210.0\n",
      "lif layer 2 self.abs_max_v: 906.5\n",
      "lif layer 1 self.abs_max_v: 2276.0\n",
      "lif layer 2 self.abs_max_v: 1146.5\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "lif layer 2 self.abs_max_v: 1387.5\n",
      "fc layer 3 self.abs_max_out: 156.0\n",
      "fc layer 2 self.abs_max_out: 1084.0\n",
      "fc layer 1 self.abs_max_out: 2666.0\n",
      "lif layer 1 self.abs_max_v: 2666.0\n",
      "fc layer 2 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 1681.5\n",
      "fc layer 1 self.abs_max_out: 3063.0\n",
      "lif layer 1 self.abs_max_v: 3063.0\n",
      "lif layer 2 self.abs_max_v: 1788.5\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 1 self.abs_max_out: 4569.0\n",
      "lif layer 1 self.abs_max_v: 4569.0\n",
      "lif layer 2 self.abs_max_v: 1872.5\n",
      "fc layer 1 self.abs_max_out: 5452.0\n",
      "lif layer 1 self.abs_max_v: 5452.0\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "lif layer 2 self.abs_max_v: 1970.5\n",
      "fc layer 2 self.abs_max_out: 1295.0\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "lif layer 2 self.abs_max_v: 1978.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 2 self.abs_max_out: 1677.0\n",
      "lif layer 2 self.abs_max_v: 2098.0\n",
      "lif layer 2 self.abs_max_v: 2141.5\n",
      "lif layer 2 self.abs_max_v: 2211.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 2 self.abs_max_out: 1872.0\n",
      "fc layer 2 self.abs_max_out: 2030.0\n",
      "lif layer 2 self.abs_max_v: 2310.0\n",
      "fc layer 2 self.abs_max_out: 2126.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "lif layer 2 self.abs_max_v: 2671.0\n",
      "fc layer 2 self.abs_max_out: 3026.0\n",
      "lif layer 2 self.abs_max_v: 3026.0\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 3074.5\n",
      "fc layer 1 self.abs_max_out: 7038.0\n",
      "lif layer 1 self.abs_max_v: 7038.0\n",
      "lif layer 2 self.abs_max_v: 3075.0\n",
      "lif layer 2 self.abs_max_v: 3958.5\n",
      "fc layer 2 self.abs_max_out: 3027.0\n",
      "fc layer 1 self.abs_max_out: 7576.0\n",
      "lif layer 1 self.abs_max_v: 7576.0\n",
      "fc layer 2 self.abs_max_out: 3213.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 594.0\n",
      "fc layer 2 self.abs_max_out: 3344.0\n",
      "fc layer 2 self.abs_max_out: 3377.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 2 self.abs_max_out: 3380.0\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "lif layer 2 self.abs_max_v: 4190.0\n",
      "lif layer 2 self.abs_max_v: 4428.5\n",
      "fc layer 2 self.abs_max_out: 3678.0\n",
      "fc layer 2 self.abs_max_out: 3706.0\n",
      "fc layer 2 self.abs_max_out: 3748.0\n",
      "fc layer 2 self.abs_max_out: 3849.0\n",
      "lif layer 1 self.abs_max_v: 8179.5\n",
      "fc layer 3 self.abs_max_out: 693.0\n",
      "lif layer 2 self.abs_max_v: 4504.0\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "lif layer 2 self.abs_max_v: 4627.5\n",
      "lif layer 2 self.abs_max_v: 4648.0\n",
      "lif layer 2 self.abs_max_v: 4812.0\n",
      "lif layer 2 self.abs_max_v: 4956.5\n",
      "fc layer 2 self.abs_max_out: 3898.0\n",
      "fc layer 2 self.abs_max_out: 3907.0\n",
      "fc layer 3 self.abs_max_out: 731.0\n",
      "fc layer 1 self.abs_max_out: 8079.0\n",
      "lif layer 1 self.abs_max_v: 8501.5\n",
      "fc layer 2 self.abs_max_out: 3985.0\n",
      "fc layer 2 self.abs_max_out: 4092.0\n",
      "lif layer 1 self.abs_max_v: 9167.0\n",
      "fc layer 1 self.abs_max_out: 8121.0\n",
      "fc layer 1 self.abs_max_out: 8144.0\n",
      "lif layer 2 self.abs_max_v: 5033.5\n",
      "fc layer 1 self.abs_max_out: 8172.0\n",
      "fc layer 1 self.abs_max_out: 8265.0\n",
      "fc layer 1 self.abs_max_out: 8385.0\n",
      "lif layer 1 self.abs_max_v: 9320.5\n",
      "fc layer 1 self.abs_max_out: 8424.0\n",
      "fc layer 2 self.abs_max_out: 4544.0\n",
      "lif layer 2 self.abs_max_v: 5073.0\n",
      "lif layer 2 self.abs_max_v: 5186.5\n",
      "lif layer 2 self.abs_max_v: 5273.5\n",
      "lif layer 2 self.abs_max_v: 5430.0\n",
      "lif layer 1 self.abs_max_v: 9460.0\n",
      "fc layer 1 self.abs_max_out: 8491.0\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "fc layer 1 self.abs_max_out: 8771.0\n",
      "lif layer 1 self.abs_max_v: 11071.0\n",
      "lif layer 1 self.abs_max_v: 12591.5\n",
      "fc layer 1 self.abs_max_out: 8815.0\n",
      "lif layer 1 self.abs_max_v: 13094.5\n",
      "lif layer 1 self.abs_max_v: 14291.5\n",
      "fc layer 2 self.abs_max_out: 4697.0\n",
      "fc layer 1 self.abs_max_out: 9829.0\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "fc layer 3 self.abs_max_out: 926.0\n",
      "fc layer 3 self.abs_max_out: 948.0\n",
      "fc layer 1 self.abs_max_out: 9921.0\n",
      "lif layer 1 self.abs_max_v: 14572.0\n",
      "fc layer 1 self.abs_max_out: 10110.0\n",
      "lif layer 2 self.abs_max_v: 5460.5\n",
      "lif layer 1 self.abs_max_v: 14967.5\n",
      "lif layer 1 self.abs_max_v: 16430.0\n",
      "lif layer 1 self.abs_max_v: 16956.0\n",
      "lif layer 1 self.abs_max_v: 18080.0\n",
      "lif layer 1 self.abs_max_v: 18769.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  2.023908/  2.142735, val:  31.25%, val_best:  31.25%, tr:  94.18%, tr_best:  94.18%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7141%\n",
      "layer   2  Sparsity: 82.5777%\n",
      "layer   3  Sparsity: 91.2689%\n",
      "total_backward_count 9790 real_backward_count 2547  26.016%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 4773.0\n",
      "fc layer 2 self.abs_max_out: 4883.0\n",
      "fc layer 2 self.abs_max_out: 5076.0\n",
      "fc layer 1 self.abs_max_out: 11608.0\n",
      "fc layer 3 self.abs_max_out: 1064.0\n",
      "lif layer 2 self.abs_max_v: 5881.0\n",
      "lif layer 2 self.abs_max_v: 5983.5\n",
      "lif layer 2 self.abs_max_v: 5993.5\n",
      "lif layer 2 self.abs_max_v: 6741.0\n",
      "lif layer 1 self.abs_max_v: 19340.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.997176/  2.142069, val:  35.83%, val_best:  35.83%, tr:  98.77%, tr_best:  98.77%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7183%\n",
      "layer   2  Sparsity: 81.5119%\n",
      "layer   3  Sparsity: 91.2746%\n",
      "total_backward_count 19580 real_backward_count 4642  23.708%\n",
      "fc layer 1 self.abs_max_out: 12237.0\n",
      "lif layer 2 self.abs_max_v: 6789.0\n",
      "lif layer 2 self.abs_max_v: 6855.0\n",
      "lif layer 2 self.abs_max_v: 6950.5\n",
      "fc layer 2 self.abs_max_out: 5152.0\n",
      "fc layer 2 self.abs_max_out: 5282.0\n",
      "lif layer 1 self.abs_max_v: 19904.5\n",
      "lif layer 1 self.abs_max_v: 20078.0\n",
      "fc layer 2 self.abs_max_out: 5436.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.977059/  2.114673, val:  40.00%, val_best:  40.00%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7199%\n",
      "layer   2  Sparsity: 80.9023%\n",
      "layer   3  Sparsity: 90.3640%\n",
      "total_backward_count 29370 real_backward_count 6599  22.469%\n",
      "lif layer 2 self.abs_max_v: 7475.0\n",
      "fc layer 1 self.abs_max_out: 12353.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.991042/  2.156623, val:  42.92%, val_best:  42.92%, tr:  98.67%, tr_best:  98.88%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7165%\n",
      "layer   2  Sparsity: 80.6658%\n",
      "layer   3  Sparsity: 91.5866%\n",
      "total_backward_count 39160 real_backward_count 8604  21.971%\n",
      "lif layer 2 self.abs_max_v: 7720.5\n",
      "lif layer 2 self.abs_max_v: 7957.0\n",
      "lif layer 2 self.abs_max_v: 8025.5\n",
      "lif layer 2 self.abs_max_v: 8261.5\n",
      "lif layer 2 self.abs_max_v: 8800.5\n",
      "fc layer 2 self.abs_max_out: 5683.0\n",
      "lif layer 1 self.abs_max_v: 20600.0\n",
      "lif layer 1 self.abs_max_v: 20793.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.995444/  2.124808, val:  40.83%, val_best:  42.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7248%\n",
      "layer   2  Sparsity: 80.1850%\n",
      "layer   3  Sparsity: 91.1121%\n",
      "total_backward_count 48950 real_backward_count 10543  21.538%\n",
      "fc layer 2 self.abs_max_out: 5804.0\n",
      "fc layer 1 self.abs_max_out: 13425.0\n",
      "fc layer 2 self.abs_max_out: 5814.0\n",
      "lif layer 1 self.abs_max_v: 21275.0\n",
      "lif layer 1 self.abs_max_v: 21995.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.976288/  2.119562, val:  48.33%, val_best:  48.33%, tr:  98.06%, tr_best:  99.49%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7055%\n",
      "layer   2  Sparsity: 80.7223%\n",
      "layer   3  Sparsity: 90.5040%\n",
      "total_backward_count 58740 real_backward_count 12499  21.279%\n",
      "fc layer 3 self.abs_max_out: 1085.0\n",
      "fc layer 3 self.abs_max_out: 1113.0\n",
      "fc layer 1 self.abs_max_out: 13728.0\n",
      "fc layer 3 self.abs_max_out: 1128.0\n",
      "fc layer 3 self.abs_max_out: 1162.0\n",
      "fc layer 3 self.abs_max_out: 1198.0\n",
      "lif layer 1 self.abs_max_v: 22223.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.965564/  2.105972, val:  46.67%, val_best:  48.33%, tr:  98.37%, tr_best:  99.49%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7398%\n",
      "layer   2  Sparsity: 80.8154%\n",
      "layer   3  Sparsity: 90.2538%\n",
      "total_backward_count 68530 real_backward_count 14424  21.048%\n",
      "fc layer 1 self.abs_max_out: 14504.0\n",
      "lif layer 1 self.abs_max_v: 23100.0\n",
      "lif layer 1 self.abs_max_v: 24223.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.957197/  2.092060, val:  44.17%, val_best:  48.33%, tr:  98.26%, tr_best:  99.49%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7148%\n",
      "layer   2  Sparsity: 80.3875%\n",
      "layer   3  Sparsity: 90.2674%\n",
      "total_backward_count 78320 real_backward_count 16359  20.887%\n",
      "fc layer 3 self.abs_max_out: 1237.0\n",
      "fc layer 3 self.abs_max_out: 1269.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.943616/  2.105721, val:  40.00%, val_best:  48.33%, tr:  98.88%, tr_best:  99.49%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7263%\n",
      "layer   2  Sparsity: 80.8526%\n",
      "layer   3  Sparsity: 90.1033%\n",
      "total_backward_count 88110 real_backward_count 18315  20.787%\n",
      "fc layer 3 self.abs_max_out: 1300.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.923817/  2.117571, val:  40.83%, val_best:  48.33%, tr:  98.16%, tr_best:  99.49%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7052%\n",
      "layer   2  Sparsity: 80.9638%\n",
      "layer   3  Sparsity: 90.1950%\n",
      "total_backward_count 97900 real_backward_count 20209  20.642%\n",
      "fc layer 1 self.abs_max_out: 14737.0\n",
      "lif layer 1 self.abs_max_v: 24805.5\n",
      "lif layer 1 self.abs_max_v: 24820.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.961906/  2.158565, val:  37.50%, val_best:  48.33%, tr:  98.37%, tr_best:  99.49%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7046%\n",
      "layer   2  Sparsity: 80.3852%\n",
      "layer   3  Sparsity: 90.6411%\n",
      "total_backward_count 107690 real_backward_count 22164  20.581%\n",
      "lif layer 2 self.abs_max_v: 9267.0\n",
      "fc layer 1 self.abs_max_out: 14757.0\n",
      "lif layer 1 self.abs_max_v: 24954.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.949320/  2.018647, val:  44.58%, val_best:  48.33%, tr:  99.39%, tr_best:  99.49%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7229%\n",
      "layer   2  Sparsity: 80.0185%\n",
      "layer   3  Sparsity: 89.3793%\n",
      "total_backward_count 117480 real_backward_count 24075  20.493%\n",
      "fc layer 3 self.abs_max_out: 1312.0\n",
      "lif layer 1 self.abs_max_v: 25002.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.912791/  2.082799, val:  32.50%, val_best:  48.33%, tr:  98.88%, tr_best:  99.49%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7317%\n",
      "layer   2  Sparsity: 80.2316%\n",
      "layer   3  Sparsity: 89.1237%\n",
      "total_backward_count 127270 real_backward_count 25950  20.390%\n",
      "lif layer 1 self.abs_max_v: 25016.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.892648/  2.095245, val:  40.42%, val_best:  48.33%, tr:  98.98%, tr_best:  99.49%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7391%\n",
      "layer   2  Sparsity: 79.9652%\n",
      "layer   3  Sparsity: 88.3571%\n",
      "total_backward_count 137060 real_backward_count 27851  20.320%\n",
      "fc layer 1 self.abs_max_out: 15192.0\n",
      "lif layer 1 self.abs_max_v: 25067.0\n",
      "fc layer 3 self.abs_max_out: 1369.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.905522/  2.041524, val:  39.17%, val_best:  48.33%, tr:  98.88%, tr_best:  99.49%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7433%\n",
      "layer   2  Sparsity: 80.1286%\n",
      "layer   3  Sparsity: 88.8678%\n",
      "total_backward_count 146850 real_backward_count 29738  20.251%\n",
      "lif layer 1 self.abs_max_v: 25320.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.920731/  2.090024, val:  43.33%, val_best:  48.33%, tr:  98.67%, tr_best:  99.49%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7051%\n",
      "layer   2  Sparsity: 80.8178%\n",
      "layer   3  Sparsity: 89.8072%\n",
      "total_backward_count 156640 real_backward_count 31685  20.228%\n",
      "lif layer 1 self.abs_max_v: 25368.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.931670/  2.086970, val:  50.83%, val_best:  50.83%, tr:  98.67%, tr_best:  99.49%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7175%\n",
      "layer   2  Sparsity: 80.0461%\n",
      "layer   3  Sparsity: 89.4221%\n",
      "total_backward_count 166430 real_backward_count 33517  20.139%\n",
      "lif layer 1 self.abs_max_v: 25413.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.900275/  2.055653, val:  45.42%, val_best:  50.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7241%\n",
      "layer   2  Sparsity: 79.8059%\n",
      "layer   3  Sparsity: 89.2028%\n",
      "total_backward_count 176220 real_backward_count 35407  20.092%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.926478/  2.088498, val:  41.67%, val_best:  50.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7100%\n",
      "layer   2  Sparsity: 80.3895%\n",
      "layer   3  Sparsity: 89.5188%\n",
      "total_backward_count 186010 real_backward_count 37433  20.124%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.918428/  2.076104, val:  41.25%, val_best:  50.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7260%\n",
      "layer   2  Sparsity: 79.9326%\n",
      "layer   3  Sparsity: 89.4375%\n",
      "total_backward_count 195800 real_backward_count 39249  20.045%\n",
      "lif layer 1 self.abs_max_v: 26291.5\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.897806/  2.089541, val:  24.58%, val_best:  50.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7126%\n",
      "layer   2  Sparsity: 80.1334%\n",
      "layer   3  Sparsity: 89.0209%\n",
      "total_backward_count 205590 real_backward_count 41056  19.970%\n",
      "lif layer 1 self.abs_max_v: 26650.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.917017/  2.126318, val:  30.00%, val_best:  50.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6909%\n",
      "layer   2  Sparsity: 80.1446%\n",
      "layer   3  Sparsity: 89.1073%\n",
      "total_backward_count 215380 real_backward_count 43005  19.967%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.919997/  2.069023, val:  45.42%, val_best:  50.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7030%\n",
      "layer   2  Sparsity: 79.4915%\n",
      "layer   3  Sparsity: 88.7209%\n",
      "total_backward_count 225170 real_backward_count 44914  19.947%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.912257/  2.065601, val:  48.75%, val_best:  50.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7295%\n",
      "layer   2  Sparsity: 79.9342%\n",
      "layer   3  Sparsity: 89.2781%\n",
      "total_backward_count 234960 real_backward_count 46922  19.970%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.904960/  2.064939, val:  40.42%, val_best:  50.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7060%\n",
      "layer   2  Sparsity: 80.1329%\n",
      "layer   3  Sparsity: 89.2473%\n",
      "total_backward_count 244750 real_backward_count 48786  19.933%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.918547/  2.049059, val:  53.33%, val_best:  53.33%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7115%\n",
      "layer   2  Sparsity: 80.8470%\n",
      "layer   3  Sparsity: 89.7208%\n",
      "total_backward_count 254540 real_backward_count 50856  19.980%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.927161/  2.051252, val:  52.08%, val_best:  53.33%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.6997%\n",
      "layer   2  Sparsity: 80.6382%\n",
      "layer   3  Sparsity: 89.1528%\n",
      "total_backward_count 264330 real_backward_count 52806  19.977%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.931266/  2.066868, val:  54.17%, val_best:  54.17%, tr:  98.26%, tr_best:  99.59%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7273%\n",
      "layer   2  Sparsity: 80.6026%\n",
      "layer   3  Sparsity: 89.7846%\n",
      "total_backward_count 274120 real_backward_count 54831  20.003%\n",
      "lif layer 1 self.abs_max_v: 26799.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.936153/  2.112482, val:  27.50%, val_best:  54.17%, tr:  97.34%, tr_best:  99.59%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7317%\n",
      "layer   2  Sparsity: 79.6791%\n",
      "layer   3  Sparsity: 89.6788%\n",
      "total_backward_count 283910 real_backward_count 56753  19.990%\n",
      "fc layer 3 self.abs_max_out: 1390.0\n",
      "fc layer 1 self.abs_max_out: 15296.0\n",
      "lif layer 1 self.abs_max_v: 27185.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.904187/  2.085313, val:  39.58%, val_best:  54.17%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7305%\n",
      "layer   2  Sparsity: 79.7234%\n",
      "layer   3  Sparsity: 89.0695%\n",
      "total_backward_count 293700 real_backward_count 58649  19.969%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.895644/  2.063330, val:  47.08%, val_best:  54.17%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7307%\n",
      "layer   2  Sparsity: 79.7539%\n",
      "layer   3  Sparsity: 88.3982%\n",
      "total_backward_count 303490 real_backward_count 60527  19.944%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.883097/  2.058428, val:  38.75%, val_best:  54.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7364%\n",
      "layer   2  Sparsity: 80.4031%\n",
      "layer   3  Sparsity: 88.4221%\n",
      "total_backward_count 313280 real_backward_count 62500  19.950%\n",
      "fc layer 2 self.abs_max_out: 6039.0\n",
      "fc layer 1 self.abs_max_out: 16815.0\n",
      "lif layer 1 self.abs_max_v: 29432.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.903452/  2.078196, val:  48.75%, val_best:  54.17%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7047%\n",
      "layer   2  Sparsity: 80.3482%\n",
      "layer   3  Sparsity: 88.6811%\n",
      "total_backward_count 323070 real_backward_count 64396  19.933%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.904628/  2.041576, val:  33.33%, val_best:  54.17%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7311%\n",
      "layer   2  Sparsity: 80.3542%\n",
      "layer   3  Sparsity: 88.0813%\n",
      "total_backward_count 332860 real_backward_count 66322  19.925%\n",
      "fc layer 3 self.abs_max_out: 1468.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.845285/  2.020952, val:  46.25%, val_best:  54.17%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7362%\n",
      "layer   2  Sparsity: 80.5536%\n",
      "layer   3  Sparsity: 87.1816%\n",
      "total_backward_count 342650 real_backward_count 68176  19.897%\n",
      "fc layer 3 self.abs_max_out: 1796.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.828278/  2.007074, val:  46.25%, val_best:  54.17%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7374%\n",
      "layer   2  Sparsity: 79.8568%\n",
      "layer   3  Sparsity: 86.4765%\n",
      "total_backward_count 352440 real_backward_count 70075  19.883%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.801321/  1.973650, val:  45.42%, val_best:  54.17%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7172%\n",
      "layer   2  Sparsity: 79.5926%\n",
      "layer   3  Sparsity: 86.0377%\n",
      "total_backward_count 362230 real_backward_count 71938  19.860%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.830169/  2.097878, val:  30.83%, val_best:  54.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7221%\n",
      "layer   2  Sparsity: 79.8675%\n",
      "layer   3  Sparsity: 87.2449%\n",
      "total_backward_count 372020 real_backward_count 73715  19.815%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.847910/  2.043617, val:  37.92%, val_best:  54.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7365%\n",
      "layer   2  Sparsity: 79.8498%\n",
      "layer   3  Sparsity: 87.4921%\n",
      "total_backward_count 381810 real_backward_count 75587  19.797%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.822383/  2.058021, val:  33.75%, val_best:  54.17%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7399%\n",
      "layer   2  Sparsity: 79.4126%\n",
      "layer   3  Sparsity: 86.9961%\n",
      "total_backward_count 391600 real_backward_count 77408  19.767%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.859482/  2.015192, val:  43.33%, val_best:  54.17%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7055%\n",
      "layer   2  Sparsity: 79.7160%\n",
      "layer   3  Sparsity: 87.5333%\n",
      "total_backward_count 401390 real_backward_count 79393  19.780%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.832430/  2.022863, val:  56.67%, val_best:  56.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7276%\n",
      "layer   2  Sparsity: 80.1634%\n",
      "layer   3  Sparsity: 87.1820%\n",
      "total_backward_count 411180 real_backward_count 81269  19.765%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.898030/  2.099661, val:  43.75%, val_best:  56.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7245%\n",
      "layer   2  Sparsity: 79.7540%\n",
      "layer   3  Sparsity: 89.1157%\n",
      "total_backward_count 420970 real_backward_count 83202  19.764%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.900510/  2.071616, val:  50.00%, val_best:  56.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7312%\n",
      "layer   2  Sparsity: 80.0974%\n",
      "layer   3  Sparsity: 88.8671%\n",
      "total_backward_count 430760 real_backward_count 85053  19.745%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.861021/  2.100522, val:  43.75%, val_best:  56.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7426%\n",
      "layer   2  Sparsity: 80.3829%\n",
      "layer   3  Sparsity: 88.0721%\n",
      "total_backward_count 440550 real_backward_count 86952  19.737%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.845847/  2.027255, val:  50.83%, val_best:  56.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7309%\n",
      "layer   2  Sparsity: 80.2963%\n",
      "layer   3  Sparsity: 86.9408%\n",
      "total_backward_count 450340 real_backward_count 88828  19.725%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.823281/  2.035751, val:  44.17%, val_best:  56.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6940%\n",
      "layer   2  Sparsity: 79.6913%\n",
      "layer   3  Sparsity: 87.2260%\n",
      "total_backward_count 460130 real_backward_count 90795  19.732%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.824509/  2.072770, val:  35.42%, val_best:  56.67%, tr:  98.16%, tr_best:  99.59%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7180%\n",
      "layer   2  Sparsity: 80.1528%\n",
      "layer   3  Sparsity: 86.5962%\n",
      "total_backward_count 469920 real_backward_count 92787  19.745%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.844282/  2.083316, val:  35.83%, val_best:  56.67%, tr:  98.06%, tr_best:  99.59%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6970%\n",
      "layer   2  Sparsity: 79.8317%\n",
      "layer   3  Sparsity: 86.8878%\n",
      "total_backward_count 479710 real_backward_count 94731  19.748%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.814455/  1.954966, val:  46.25%, val_best:  56.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7151%\n",
      "layer   2  Sparsity: 79.7965%\n",
      "layer   3  Sparsity: 85.9285%\n",
      "total_backward_count 489500 real_backward_count 96655  19.746%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.771963/  2.022577, val:  45.83%, val_best:  56.67%, tr:  99.28%, tr_best:  99.59%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7104%\n",
      "layer   2  Sparsity: 79.9182%\n",
      "layer   3  Sparsity: 85.7050%\n",
      "total_backward_count 499290 real_backward_count 98435  19.715%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.822833/  2.035713, val:  35.83%, val_best:  56.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7330%\n",
      "layer   2  Sparsity: 79.9284%\n",
      "layer   3  Sparsity: 86.8579%\n",
      "total_backward_count 509080 real_backward_count 100366  19.715%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.832988/  2.024402, val:  49.17%, val_best:  56.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7042%\n",
      "layer   2  Sparsity: 80.3334%\n",
      "layer   3  Sparsity: 87.3788%\n",
      "total_backward_count 518870 real_backward_count 102429  19.741%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.829385/  1.988351, val:  33.33%, val_best:  56.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7174%\n",
      "layer   2  Sparsity: 79.9940%\n",
      "layer   3  Sparsity: 87.0881%\n",
      "total_backward_count 528660 real_backward_count 104382  19.745%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.805884/  2.022283, val:  43.75%, val_best:  56.67%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7130%\n",
      "layer   2  Sparsity: 80.2822%\n",
      "layer   3  Sparsity: 86.3782%\n",
      "total_backward_count 538450 real_backward_count 106265  19.735%\n",
      "fc layer 1 self.abs_max_out: 16880.0\n",
      "lif layer 1 self.abs_max_v: 30569.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.858574/  2.038721, val:  37.92%, val_best:  56.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7152%\n",
      "layer   2  Sparsity: 80.7628%\n",
      "layer   3  Sparsity: 88.1826%\n",
      "total_backward_count 548240 real_backward_count 108322  19.758%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.836397/  2.053665, val:  41.67%, val_best:  56.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7152%\n",
      "layer   2  Sparsity: 80.4003%\n",
      "layer   3  Sparsity: 86.8080%\n",
      "total_backward_count 558030 real_backward_count 110193  19.747%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.803087/  1.975241, val:  53.33%, val_best:  56.67%, tr:  97.65%, tr_best:  99.59%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7304%\n",
      "layer   2  Sparsity: 80.3940%\n",
      "layer   3  Sparsity: 86.0792%\n",
      "total_backward_count 567820 real_backward_count 112072  19.737%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.794910/  2.024891, val:  43.75%, val_best:  56.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7090%\n",
      "layer   2  Sparsity: 80.2196%\n",
      "layer   3  Sparsity: 85.6425%\n",
      "total_backward_count 577610 real_backward_count 113859  19.712%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.779602/  2.019678, val:  47.08%, val_best:  56.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7306%\n",
      "layer   2  Sparsity: 79.8610%\n",
      "layer   3  Sparsity: 85.6861%\n",
      "total_backward_count 587400 real_backward_count 115659  19.690%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.806651/  2.041060, val:  44.17%, val_best:  56.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7013%\n",
      "layer   2  Sparsity: 80.5136%\n",
      "layer   3  Sparsity: 86.7877%\n",
      "total_backward_count 597190 real_backward_count 117467  19.670%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.848456/  1.991522, val:  40.83%, val_best:  56.67%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7072%\n",
      "layer   2  Sparsity: 80.7800%\n",
      "layer   3  Sparsity: 86.8815%\n",
      "total_backward_count 606980 real_backward_count 119366  19.666%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.792008/  2.013983, val:  37.92%, val_best:  56.67%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7139%\n",
      "layer   2  Sparsity: 79.8690%\n",
      "layer   3  Sparsity: 85.5005%\n",
      "total_backward_count 616770 real_backward_count 121236  19.657%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.800979/  2.073989, val:  30.83%, val_best:  56.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7073%\n",
      "layer   2  Sparsity: 80.4308%\n",
      "layer   3  Sparsity: 86.4639%\n",
      "total_backward_count 626560 real_backward_count 123061  19.641%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.801153/  2.013757, val:  45.42%, val_best:  56.67%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7308%\n",
      "layer   2  Sparsity: 80.4154%\n",
      "layer   3  Sparsity: 85.8831%\n",
      "total_backward_count 636350 real_backward_count 124937  19.633%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.777461/  2.047839, val:  37.92%, val_best:  56.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7233%\n",
      "layer   2  Sparsity: 80.3663%\n",
      "layer   3  Sparsity: 84.9955%\n",
      "total_backward_count 646140 real_backward_count 126826  19.628%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.793408/  1.987208, val:  36.25%, val_best:  56.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7183%\n",
      "layer   2  Sparsity: 79.5701%\n",
      "layer   3  Sparsity: 84.8429%\n",
      "total_backward_count 655930 real_backward_count 128642  19.612%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.785354/  2.018234, val:  52.92%, val_best:  56.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7102%\n",
      "layer   2  Sparsity: 80.3521%\n",
      "layer   3  Sparsity: 85.6783%\n",
      "total_backward_count 665720 real_backward_count 130539  19.609%\n",
      "fc layer 1 self.abs_max_out: 16890.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.787950/  1.989686, val:  55.00%, val_best:  56.67%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7047%\n",
      "layer   2  Sparsity: 79.8996%\n",
      "layer   3  Sparsity: 85.8354%\n",
      "total_backward_count 675510 real_backward_count 132472  19.611%\n",
      "fc layer 1 self.abs_max_out: 17064.0\n",
      "lif layer 1 self.abs_max_v: 30760.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.753177/  2.040586, val:  33.75%, val_best:  56.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7329%\n",
      "layer   2  Sparsity: 79.4161%\n",
      "layer   3  Sparsity: 84.1915%\n",
      "total_backward_count 685300 real_backward_count 134301  19.597%\n",
      "fc layer 1 self.abs_max_out: 17428.0\n",
      "lif layer 1 self.abs_max_v: 31110.0\n",
      "lif layer 1 self.abs_max_v: 31570.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.735895/  1.999249, val:  45.83%, val_best:  56.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7459%\n",
      "layer   2  Sparsity: 79.4774%\n",
      "layer   3  Sparsity: 84.6963%\n",
      "total_backward_count 695090 real_backward_count 136121  19.583%\n",
      "fc layer 1 self.abs_max_out: 18074.0\n",
      "lif layer 1 self.abs_max_v: 32403.0\n",
      "lif layer 1 self.abs_max_v: 32946.5\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.738672/  1.984180, val:  43.33%, val_best:  56.67%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7170%\n",
      "layer   2  Sparsity: 79.2292%\n",
      "layer   3  Sparsity: 83.5523%\n",
      "total_backward_count 704880 real_backward_count 137995  19.577%\n",
      "fc layer 3 self.abs_max_out: 1925.0\n",
      "fc layer 1 self.abs_max_out: 18120.0\n",
      "lif layer 1 self.abs_max_v: 33062.5\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.719405/  1.970069, val:  42.08%, val_best:  56.67%, tr:  99.18%, tr_best:  99.59%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7218%\n",
      "layer   2  Sparsity: 79.1797%\n",
      "layer   3  Sparsity: 83.2913%\n",
      "total_backward_count 714670 real_backward_count 139799  19.561%\n",
      "fc layer 3 self.abs_max_out: 2026.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.705743/  2.048563, val:  35.00%, val_best:  56.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7246%\n",
      "layer   2  Sparsity: 79.4280%\n",
      "layer   3  Sparsity: 84.0105%\n",
      "total_backward_count 724460 real_backward_count 141535  19.537%\n",
      "fc layer 3 self.abs_max_out: 2069.0\n",
      "fc layer 3 self.abs_max_out: 2105.0\n",
      "fc layer 1 self.abs_max_out: 18274.0\n",
      "lif layer 1 self.abs_max_v: 33384.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.713569/  2.003184, val:  40.42%, val_best:  56.67%, tr:  97.96%, tr_best:  99.59%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7206%\n",
      "layer   2  Sparsity: 78.8332%\n",
      "layer   3  Sparsity: 83.4281%\n",
      "total_backward_count 734250 real_backward_count 143413  19.532%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.713991/  1.901545, val:  53.33%, val_best:  56.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6970%\n",
      "layer   2  Sparsity: 79.3707%\n",
      "layer   3  Sparsity: 83.3164%\n",
      "total_backward_count 744040 real_backward_count 145162  19.510%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.774417/  1.982213, val:  59.17%, val_best:  59.17%, tr:  98.16%, tr_best:  99.59%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7397%\n",
      "layer   2  Sparsity: 80.1283%\n",
      "layer   3  Sparsity: 86.3867%\n",
      "total_backward_count 753830 real_backward_count 147094  19.513%\n",
      "fc layer 1 self.abs_max_out: 18325.0\n",
      "lif layer 1 self.abs_max_v: 33468.5\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.807762/  2.024411, val:  37.08%, val_best:  59.17%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7034%\n",
      "layer   2  Sparsity: 79.9544%\n",
      "layer   3  Sparsity: 86.3227%\n",
      "total_backward_count 763620 real_backward_count 149020  19.515%\n",
      "fc layer 1 self.abs_max_out: 18492.0\n",
      "lif layer 1 self.abs_max_v: 33541.0\n",
      "lif layer 1 self.abs_max_v: 33795.5\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.727634/  2.018737, val:  37.08%, val_best:  59.17%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.6958%\n",
      "layer   2  Sparsity: 78.9606%\n",
      "layer   3  Sparsity: 83.9070%\n",
      "total_backward_count 773410 real_backward_count 150911  19.512%\n",
      "fc layer 2 self.abs_max_out: 6075.0\n",
      "fc layer 1 self.abs_max_out: 18498.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.754657/  2.005552, val:  45.42%, val_best:  59.17%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7200%\n",
      "layer   2  Sparsity: 79.2043%\n",
      "layer   3  Sparsity: 85.2311%\n",
      "total_backward_count 783200 real_backward_count 152768  19.506%\n",
      "fc layer 1 self.abs_max_out: 18653.0\n",
      "lif layer 1 self.abs_max_v: 33892.5\n",
      "lif layer 1 self.abs_max_v: 34104.5\n",
      "lif layer 2 self.abs_max_v: 9366.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.796385/  1.973401, val:  44.17%, val_best:  59.17%, tr:  98.77%, tr_best:  99.59%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7214%\n",
      "layer   2  Sparsity: 79.5757%\n",
      "layer   3  Sparsity: 86.2433%\n",
      "total_backward_count 792990 real_backward_count 154665  19.504%\n",
      "fc layer 1 self.abs_max_out: 18847.0\n",
      "lif layer 1 self.abs_max_v: 34351.0\n",
      "lif layer 1 self.abs_max_v: 34607.5\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.776018/  2.055868, val:  28.33%, val_best:  59.17%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7112%\n",
      "layer   2  Sparsity: 79.2082%\n",
      "layer   3  Sparsity: 85.2420%\n",
      "total_backward_count 802780 real_backward_count 156443  19.488%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.780742/  1.995139, val:  48.33%, val_best:  59.17%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7309%\n",
      "layer   2  Sparsity: 79.0123%\n",
      "layer   3  Sparsity: 85.4714%\n",
      "total_backward_count 812570 real_backward_count 158414  19.495%\n",
      "fc layer 1 self.abs_max_out: 18904.0\n",
      "lif layer 1 self.abs_max_v: 34688.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.776033/  2.045344, val:  40.83%, val_best:  59.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7120%\n",
      "layer   2  Sparsity: 79.2499%\n",
      "layer   3  Sparsity: 85.7167%\n",
      "total_backward_count 822360 real_backward_count 160405  19.505%\n",
      "fc layer 1 self.abs_max_out: 19214.0\n",
      "lif layer 1 self.abs_max_v: 35148.5\n",
      "lif layer 1 self.abs_max_v: 35341.5\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.786823/  2.023434, val:  43.33%, val_best:  59.17%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7317%\n",
      "layer   2  Sparsity: 79.6826%\n",
      "layer   3  Sparsity: 86.3464%\n",
      "total_backward_count 832150 real_backward_count 162334  19.508%\n",
      "fc layer 1 self.abs_max_out: 19232.0\n",
      "lif layer 1 self.abs_max_v: 35409.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.828257/  2.086753, val:  36.25%, val_best:  59.17%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7367%\n",
      "layer   2  Sparsity: 81.3126%\n",
      "layer   3  Sparsity: 87.8916%\n",
      "total_backward_count 841940 real_backward_count 164281  19.512%\n",
      "fc layer 1 self.abs_max_out: 19258.0\n",
      "lif layer 1 self.abs_max_v: 35486.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.809431/  1.991785, val:  39.17%, val_best:  59.17%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7265%\n",
      "layer   2  Sparsity: 80.8082%\n",
      "layer   3  Sparsity: 86.6438%\n",
      "total_backward_count 851730 real_backward_count 166140  19.506%\n",
      "fc layer 1 self.abs_max_out: 19260.0\n",
      "lif layer 1 self.abs_max_v: 35494.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.819493/  2.043157, val:  45.00%, val_best:  59.17%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7133%\n",
      "layer   2  Sparsity: 80.4477%\n",
      "layer   3  Sparsity: 87.0237%\n",
      "total_backward_count 861520 real_backward_count 168116  19.514%\n",
      "fc layer 1 self.abs_max_out: 19329.0\n",
      "lif layer 1 self.abs_max_v: 35503.5\n",
      "lif layer 1 self.abs_max_v: 35672.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.819062/  2.040067, val:  52.08%, val_best:  59.17%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7324%\n",
      "layer   2  Sparsity: 80.3081%\n",
      "layer   3  Sparsity: 87.0325%\n",
      "total_backward_count 871310 real_backward_count 170008  19.512%\n",
      "fc layer 1 self.abs_max_out: 19345.0\n",
      "lif layer 1 self.abs_max_v: 35718.5\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.871175/  2.058977, val:  34.17%, val_best:  59.17%, tr:  97.85%, tr_best:  99.59%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7009%\n",
      "layer   2  Sparsity: 79.9911%\n",
      "layer   3  Sparsity: 87.4612%\n",
      "total_backward_count 881100 real_backward_count 172080  19.530%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.866123/  2.065300, val:  44.58%, val_best:  59.17%, tr:  97.45%, tr_best:  99.59%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6973%\n",
      "layer   2  Sparsity: 79.4824%\n",
      "layer   3  Sparsity: 87.6927%\n",
      "total_backward_count 890890 real_backward_count 174018  19.533%\n",
      "fc layer 1 self.abs_max_out: 19421.0\n",
      "lif layer 1 self.abs_max_v: 35773.0\n",
      "lif layer 1 self.abs_max_v: 35908.5\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.833651/  1.997305, val:  52.08%, val_best:  59.17%, tr:  97.65%, tr_best:  99.59%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7233%\n",
      "layer   2  Sparsity: 79.1916%\n",
      "layer   3  Sparsity: 85.8985%\n",
      "total_backward_count 900680 real_backward_count 175908  19.531%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.851010/  2.001209, val:  47.08%, val_best:  59.17%, tr:  97.55%, tr_best:  99.59%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6982%\n",
      "layer   2  Sparsity: 79.6247%\n",
      "layer   3  Sparsity: 87.2617%\n",
      "total_backward_count 910470 real_backward_count 177873  19.536%\n",
      "fc layer 1 self.abs_max_out: 19451.0\n",
      "lif layer 1 self.abs_max_v: 35922.0\n",
      "lif layer 1 self.abs_max_v: 36005.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.861749/  2.049109, val:  39.58%, val_best:  59.17%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7393%\n",
      "layer   2  Sparsity: 80.6988%\n",
      "layer   3  Sparsity: 87.9603%\n",
      "total_backward_count 920260 real_backward_count 179874  19.546%\n",
      "fc layer 1 self.abs_max_out: 19461.0\n",
      "lif layer 1 self.abs_max_v: 36010.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.852814/  2.057548, val:  39.17%, val_best:  59.17%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7248%\n",
      "layer   2  Sparsity: 80.2735%\n",
      "layer   3  Sparsity: 87.9959%\n",
      "total_backward_count 930050 real_backward_count 181845  19.552%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.895382/  2.041512, val:  60.83%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7080%\n",
      "layer   2  Sparsity: 79.8267%\n",
      "layer   3  Sparsity: 88.3096%\n",
      "total_backward_count 939840 real_backward_count 183861  19.563%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.873931/  2.045229, val:  48.33%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7258%\n",
      "layer   2  Sparsity: 79.6119%\n",
      "layer   3  Sparsity: 87.2749%\n",
      "total_backward_count 949630 real_backward_count 185687  19.554%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.851849/  2.030720, val:  43.75%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7218%\n",
      "layer   2  Sparsity: 79.7584%\n",
      "layer   3  Sparsity: 87.0954%\n",
      "total_backward_count 959420 real_backward_count 187591  19.553%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.863915/  2.087363, val:  31.67%, val_best:  60.83%, tr:  97.96%, tr_best:  99.59%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7097%\n",
      "layer   2  Sparsity: 79.3653%\n",
      "layer   3  Sparsity: 87.4210%\n",
      "total_backward_count 969210 real_backward_count 189495  19.551%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.860332/  2.048244, val:  48.33%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7219%\n",
      "layer   2  Sparsity: 80.1454%\n",
      "layer   3  Sparsity: 87.6349%\n",
      "total_backward_count 979000 real_backward_count 191395  19.550%\n",
      "lif layer 2 self.abs_max_v: 9409.0\n",
      "lif layer 2 self.abs_max_v: 9447.0\n",
      "lif layer 2 self.abs_max_v: 9592.5\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.869463/  2.029868, val:  48.33%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7416%\n",
      "layer   2  Sparsity: 79.3207%\n",
      "layer   3  Sparsity: 88.2502%\n",
      "total_backward_count 988790 real_backward_count 193290  19.548%\n",
      "lif layer 2 self.abs_max_v: 9750.0\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.884879/  2.022820, val:  57.08%, val_best:  60.83%, tr:  98.06%, tr_best:  99.59%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7198%\n",
      "layer   2  Sparsity: 79.6071%\n",
      "layer   3  Sparsity: 88.4696%\n",
      "total_backward_count 998580 real_backward_count 195238  19.552%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.861054/  2.057127, val:  37.50%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7188%\n",
      "layer   2  Sparsity: 80.2557%\n",
      "layer   3  Sparsity: 87.2977%\n",
      "total_backward_count 1008370 real_backward_count 197183  19.555%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.799458/  2.019603, val:  45.00%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7052%\n",
      "layer   2  Sparsity: 79.7059%\n",
      "layer   3  Sparsity: 86.1986%\n",
      "total_backward_count 1018160 real_backward_count 199052  19.550%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.791172/  2.041578, val:  36.25%, val_best:  60.83%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7200%\n",
      "layer   2  Sparsity: 79.5231%\n",
      "layer   3  Sparsity: 85.5722%\n",
      "total_backward_count 1027950 real_backward_count 200916  19.545%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.816131/  2.018904, val:  55.00%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7174%\n",
      "layer   2  Sparsity: 79.8495%\n",
      "layer   3  Sparsity: 86.0427%\n",
      "total_backward_count 1037740 real_backward_count 202813  19.544%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.818606/  2.050418, val:  31.25%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7112%\n",
      "layer   2  Sparsity: 79.9686%\n",
      "layer   3  Sparsity: 86.4330%\n",
      "total_backward_count 1047530 real_backward_count 204659  19.537%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.860032/  2.082854, val:  35.00%, val_best:  60.83%, tr:  97.65%, tr_best:  99.59%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7068%\n",
      "layer   2  Sparsity: 80.6130%\n",
      "layer   3  Sparsity: 87.5111%\n",
      "total_backward_count 1057320 real_backward_count 206680  19.548%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.831724/  1.990599, val:  42.92%, val_best:  60.83%, tr:  97.75%, tr_best:  99.59%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6956%\n",
      "layer   2  Sparsity: 80.5671%\n",
      "layer   3  Sparsity: 87.0308%\n",
      "total_backward_count 1067110 real_backward_count 208658  19.554%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.831289/  2.026937, val:  39.58%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7349%\n",
      "layer   2  Sparsity: 80.1247%\n",
      "layer   3  Sparsity: 87.3835%\n",
      "total_backward_count 1076900 real_backward_count 210652  19.561%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.815836/  1.947442, val:  48.33%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7209%\n",
      "layer   2  Sparsity: 79.9563%\n",
      "layer   3  Sparsity: 85.1419%\n",
      "total_backward_count 1086690 real_backward_count 212527  19.557%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.764778/  2.032810, val:  37.50%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6994%\n",
      "layer   2  Sparsity: 79.2569%\n",
      "layer   3  Sparsity: 84.9420%\n",
      "total_backward_count 1096480 real_backward_count 214295  19.544%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.845430/  2.051854, val:  52.92%, val_best:  60.83%, tr:  97.96%, tr_best:  99.59%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7104%\n",
      "layer   2  Sparsity: 79.9148%\n",
      "layer   3  Sparsity: 87.4703%\n",
      "total_backward_count 1106270 real_backward_count 216305  19.553%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.851483/  1.999866, val:  39.58%, val_best:  60.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7131%\n",
      "layer   2  Sparsity: 79.4315%\n",
      "layer   3  Sparsity: 87.2825%\n",
      "total_backward_count 1116060 real_backward_count 218223  19.553%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.887116/  2.105133, val:  29.58%, val_best:  60.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7368%\n",
      "layer   2  Sparsity: 79.6037%\n",
      "layer   3  Sparsity: 88.6056%\n",
      "total_backward_count 1125850 real_backward_count 220256  19.564%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.881257/  2.052498, val:  40.83%, val_best:  60.83%, tr:  97.55%, tr_best:  99.59%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7297%\n",
      "layer   2  Sparsity: 79.8196%\n",
      "layer   3  Sparsity: 87.5876%\n",
      "total_backward_count 1135640 real_backward_count 222222  19.568%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.824538/  2.017456, val:  45.83%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7396%\n",
      "layer   2  Sparsity: 79.5021%\n",
      "layer   3  Sparsity: 86.0729%\n",
      "total_backward_count 1145430 real_backward_count 224096  19.564%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.780856/  1.983457, val:  38.75%, val_best:  60.83%, tr:  97.55%, tr_best:  99.59%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7164%\n",
      "layer   2  Sparsity: 79.8351%\n",
      "layer   3  Sparsity: 85.6745%\n",
      "total_backward_count 1155220 real_backward_count 226002  19.564%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.828386/  1.996663, val:  47.08%, val_best:  60.83%, tr:  97.85%, tr_best:  99.59%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7160%\n",
      "layer   2  Sparsity: 79.6829%\n",
      "layer   3  Sparsity: 86.8847%\n",
      "total_backward_count 1165010 real_backward_count 227976  19.569%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.781409/  2.014739, val:  60.00%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7333%\n",
      "layer   2  Sparsity: 80.3841%\n",
      "layer   3  Sparsity: 85.5094%\n",
      "total_backward_count 1174800 real_backward_count 229781  19.559%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.805284/  2.024019, val:  45.83%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7236%\n",
      "layer   2  Sparsity: 80.9524%\n",
      "layer   3  Sparsity: 86.9115%\n",
      "total_backward_count 1184590 real_backward_count 231676  19.557%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.796722/  2.031783, val:  38.75%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7193%\n",
      "layer   2  Sparsity: 79.7817%\n",
      "layer   3  Sparsity: 86.8654%\n",
      "total_backward_count 1194380 real_backward_count 233572  19.556%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.781592/  2.056058, val:  25.00%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7359%\n",
      "layer   2  Sparsity: 79.7600%\n",
      "layer   3  Sparsity: 86.0621%\n",
      "total_backward_count 1204170 real_backward_count 235473  19.555%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.795712/  1.971740, val:  52.08%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7478%\n",
      "layer   2  Sparsity: 79.5484%\n",
      "layer   3  Sparsity: 85.9390%\n",
      "total_backward_count 1213960 real_backward_count 237363  19.553%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.726245/  1.949835, val:  56.67%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7154%\n",
      "layer   2  Sparsity: 80.0043%\n",
      "layer   3  Sparsity: 84.9680%\n",
      "total_backward_count 1223750 real_backward_count 239171  19.544%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.780370/  1.937711, val:  42.92%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 79.7762%\n",
      "layer   3  Sparsity: 85.4590%\n",
      "total_backward_count 1233540 real_backward_count 241003  19.538%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.773674/  2.081905, val:  30.83%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7194%\n",
      "layer   2  Sparsity: 80.4712%\n",
      "layer   3  Sparsity: 85.9083%\n",
      "total_backward_count 1243330 real_backward_count 242833  19.531%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.774781/  2.009930, val:  49.17%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7020%\n",
      "layer   2  Sparsity: 80.5089%\n",
      "layer   3  Sparsity: 85.8729%\n",
      "total_backward_count 1253120 real_backward_count 244661  19.524%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.821447/  2.022918, val:  41.67%, val_best:  60.83%, tr:  99.18%, tr_best:  99.59%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7155%\n",
      "layer   2  Sparsity: 80.3142%\n",
      "layer   3  Sparsity: 86.1681%\n",
      "total_backward_count 1262910 real_backward_count 246520  19.520%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.775783/  2.030176, val:  40.83%, val_best:  60.83%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7375%\n",
      "layer   2  Sparsity: 80.8857%\n",
      "layer   3  Sparsity: 85.3993%\n",
      "total_backward_count 1272700 real_backward_count 248420  19.519%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.760301/  2.014602, val:  35.42%, val_best:  60.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7286%\n",
      "layer   2  Sparsity: 80.5939%\n",
      "layer   3  Sparsity: 85.1951%\n",
      "total_backward_count 1282490 real_backward_count 250244  19.512%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.804425/  2.039544, val:  43.33%, val_best:  60.83%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7383%\n",
      "layer   2  Sparsity: 80.3034%\n",
      "layer   3  Sparsity: 87.0512%\n",
      "total_backward_count 1292280 real_backward_count 252161  19.513%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.852582/  2.023067, val:  44.17%, val_best:  60.83%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7374%\n",
      "layer   2  Sparsity: 80.7706%\n",
      "layer   3  Sparsity: 88.2297%\n",
      "total_backward_count 1302070 real_backward_count 254081  19.514%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.841758/  2.046156, val:  35.00%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7108%\n",
      "layer   2  Sparsity: 80.7741%\n",
      "layer   3  Sparsity: 86.8736%\n",
      "total_backward_count 1311860 real_backward_count 256004  19.515%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.855292/  2.025698, val:  45.83%, val_best:  60.83%, tr:  97.96%, tr_best:  99.59%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7323%\n",
      "layer   2  Sparsity: 81.4756%\n",
      "layer   3  Sparsity: 87.2151%\n",
      "total_backward_count 1321650 real_backward_count 257950  19.517%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.888222/  2.056817, val:  46.67%, val_best:  60.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7037%\n",
      "layer   2  Sparsity: 81.7008%\n",
      "layer   3  Sparsity: 88.1601%\n",
      "total_backward_count 1331440 real_backward_count 259884  19.519%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.849480/  2.028325, val:  48.33%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7277%\n",
      "layer   2  Sparsity: 80.7542%\n",
      "layer   3  Sparsity: 87.1021%\n",
      "total_backward_count 1341230 real_backward_count 261705  19.512%\n",
      "lif layer 1 self.abs_max_v: 36298.0\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.853441/  2.056479, val:  42.08%, val_best:  60.83%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6969%\n",
      "layer   2  Sparsity: 81.0795%\n",
      "layer   3  Sparsity: 87.8842%\n",
      "total_backward_count 1351020 real_backward_count 263600  19.511%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.891713/  2.058913, val:  47.92%, val_best:  60.83%, tr:  98.26%, tr_best:  99.59%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7043%\n",
      "layer   2  Sparsity: 80.2309%\n",
      "layer   3  Sparsity: 88.1141%\n",
      "total_backward_count 1360810 real_backward_count 265643  19.521%\n",
      "lif layer 1 self.abs_max_v: 36710.0\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.859078/  2.064987, val:  52.92%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7337%\n",
      "layer   2  Sparsity: 80.2820%\n",
      "layer   3  Sparsity: 87.0058%\n",
      "total_backward_count 1370600 real_backward_count 267577  19.523%\n",
      "fc layer 1 self.abs_max_out: 19512.0\n",
      "lif layer 1 self.abs_max_v: 36995.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.820616/  2.030084, val:  40.42%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7386%\n",
      "layer   2  Sparsity: 78.9720%\n",
      "layer   3  Sparsity: 85.6956%\n",
      "total_backward_count 1380390 real_backward_count 269363  19.514%\n",
      "lif layer 2 self.abs_max_v: 9811.0\n",
      "lif layer 2 self.abs_max_v: 10073.0\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.790387/  2.068768, val:  38.33%, val_best:  60.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6986%\n",
      "layer   2  Sparsity: 79.2339%\n",
      "layer   3  Sparsity: 85.4404%\n",
      "total_backward_count 1390180 real_backward_count 271200  19.508%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.802476/  2.037939, val:  45.42%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7351%\n",
      "layer   2  Sparsity: 79.7968%\n",
      "layer   3  Sparsity: 86.3023%\n",
      "total_backward_count 1399970 real_backward_count 273077  19.506%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.790308/  1.976017, val:  46.67%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7565%\n",
      "layer   2  Sparsity: 79.9011%\n",
      "layer   3  Sparsity: 85.5160%\n",
      "total_backward_count 1409760 real_backward_count 274906  19.500%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.830283/  2.010269, val:  48.75%, val_best:  60.83%, tr:  97.75%, tr_best:  99.59%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7321%\n",
      "layer   2  Sparsity: 79.3435%\n",
      "layer   3  Sparsity: 86.8082%\n",
      "total_backward_count 1419550 real_backward_count 276770  19.497%\n",
      "fc layer 1 self.abs_max_out: 19759.0\n",
      "lif layer 1 self.abs_max_v: 37493.5\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.839087/  1.994223, val:  43.33%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7036%\n",
      "layer   2  Sparsity: 79.9388%\n",
      "layer   3  Sparsity: 86.4393%\n",
      "total_backward_count 1429340 real_backward_count 278687  19.498%\n",
      "fc layer 1 self.abs_max_out: 19833.0\n",
      "lif layer 1 self.abs_max_v: 37634.0\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.817942/  2.041051, val:  48.75%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7285%\n",
      "layer   2  Sparsity: 79.6473%\n",
      "layer   3  Sparsity: 86.4715%\n",
      "total_backward_count 1439130 real_backward_count 280582  19.497%\n",
      "fc layer 1 self.abs_max_out: 19883.0\n",
      "lif layer 1 self.abs_max_v: 37745.5\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.849555/  2.075982, val:  39.17%, val_best:  60.83%, tr:  97.45%, tr_best:  99.59%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7325%\n",
      "layer   2  Sparsity: 80.0486%\n",
      "layer   3  Sparsity: 87.2585%\n",
      "total_backward_count 1448920 real_backward_count 282481  19.496%\n",
      "fc layer 1 self.abs_max_out: 20023.0\n",
      "lif layer 1 self.abs_max_v: 38035.5\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.826728/  2.011258, val:  43.33%, val_best:  60.83%, tr:  97.96%, tr_best:  99.59%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7453%\n",
      "layer   2  Sparsity: 79.8210%\n",
      "layer   3  Sparsity: 86.6391%\n",
      "total_backward_count 1458710 real_backward_count 284422  19.498%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.825606/  1.982619, val:  48.33%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7209%\n",
      "layer   2  Sparsity: 79.4095%\n",
      "layer   3  Sparsity: 86.2737%\n",
      "total_backward_count 1468500 real_backward_count 286377  19.501%\n",
      "fc layer 1 self.abs_max_out: 20065.0\n",
      "lif layer 1 self.abs_max_v: 38124.5\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.803903/  2.018448, val:  48.75%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7214%\n",
      "layer   2  Sparsity: 80.1799%\n",
      "layer   3  Sparsity: 86.5893%\n",
      "total_backward_count 1478290 real_backward_count 288169  19.493%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.788132/  2.042955, val:  34.17%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6934%\n",
      "layer   2  Sparsity: 79.3127%\n",
      "layer   3  Sparsity: 86.0648%\n",
      "total_backward_count 1488080 real_backward_count 290019  19.489%\n",
      "lif layer 2 self.abs_max_v: 10290.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.817736/  2.024822, val:  48.33%, val_best:  60.83%, tr:  97.85%, tr_best:  99.59%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7199%\n",
      "layer   2  Sparsity: 79.2042%\n",
      "layer   3  Sparsity: 86.3557%\n",
      "total_backward_count 1497870 real_backward_count 291834  19.483%\n",
      "lif layer 2 self.abs_max_v: 10559.0\n",
      "fc layer 2 self.abs_max_out: 6124.0\n",
      "lif layer 2 self.abs_max_v: 10765.0\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.744999/  1.935737, val:  59.17%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6654%\n",
      "layer   2  Sparsity: 79.1445%\n",
      "layer   3  Sparsity: 83.5479%\n",
      "total_backward_count 1507660 real_backward_count 293632  19.476%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.766674/  2.010112, val:  42.50%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7030%\n",
      "layer   2  Sparsity: 79.4671%\n",
      "layer   3  Sparsity: 84.9459%\n",
      "total_backward_count 1517450 real_backward_count 295424  19.468%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.797740/  1.972737, val:  50.42%, val_best:  60.83%, tr:  97.45%, tr_best:  99.59%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7150%\n",
      "layer   2  Sparsity: 80.1792%\n",
      "layer   3  Sparsity: 85.7261%\n",
      "total_backward_count 1527240 real_backward_count 297311  19.467%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.798669/  2.005217, val:  42.08%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7007%\n",
      "layer   2  Sparsity: 80.2391%\n",
      "layer   3  Sparsity: 86.6506%\n",
      "total_backward_count 1537030 real_backward_count 299106  19.460%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.813689/  2.011647, val:  49.17%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7293%\n",
      "layer   2  Sparsity: 80.8053%\n",
      "layer   3  Sparsity: 86.9446%\n",
      "total_backward_count 1546820 real_backward_count 300978  19.458%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.825141/  2.049816, val:  42.08%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7130%\n",
      "layer   2  Sparsity: 80.8913%\n",
      "layer   3  Sparsity: 86.9646%\n",
      "total_backward_count 1556610 real_backward_count 302867  19.457%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.760024/  1.981473, val:  46.25%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7197%\n",
      "layer   2  Sparsity: 80.2684%\n",
      "layer   3  Sparsity: 83.9325%\n",
      "total_backward_count 1566400 real_backward_count 304699  19.452%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.736878/  1.975147, val:  47.50%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7155%\n",
      "layer   2  Sparsity: 79.7389%\n",
      "layer   3  Sparsity: 84.4794%\n",
      "total_backward_count 1576190 real_backward_count 306507  19.446%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.748930/  1.939689, val:  50.42%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7085%\n",
      "layer   2  Sparsity: 79.3290%\n",
      "layer   3  Sparsity: 84.1123%\n",
      "total_backward_count 1585980 real_backward_count 308414  19.446%\n",
      "fc layer 3 self.abs_max_out: 2193.0\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.689020/  1.948826, val:  38.33%, val_best:  60.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6969%\n",
      "layer   2  Sparsity: 79.5595%\n",
      "layer   3  Sparsity: 82.9073%\n",
      "total_backward_count 1595770 real_backward_count 310090  19.432%\n",
      "fc layer 3 self.abs_max_out: 2279.0\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.693883/  2.025609, val:  39.58%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6937%\n",
      "layer   2  Sparsity: 80.4835%\n",
      "layer   3  Sparsity: 84.0822%\n",
      "total_backward_count 1605560 real_backward_count 311902  19.426%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.745695/  2.027948, val:  35.00%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7072%\n",
      "layer   2  Sparsity: 80.1728%\n",
      "layer   3  Sparsity: 84.8771%\n",
      "total_backward_count 1615350 real_backward_count 313815  19.427%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.680567/  1.946367, val:  46.25%, val_best:  60.83%, tr:  99.08%, tr_best:  99.59%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7102%\n",
      "layer   2  Sparsity: 78.8695%\n",
      "layer   3  Sparsity: 81.9254%\n",
      "total_backward_count 1625140 real_backward_count 315589  19.419%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.683218/  1.993369, val:  42.08%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7175%\n",
      "layer   2  Sparsity: 78.9598%\n",
      "layer   3  Sparsity: 82.8024%\n",
      "total_backward_count 1634930 real_backward_count 317381  19.413%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.790993/  2.056540, val:  38.75%, val_best:  60.83%, tr:  98.98%, tr_best:  99.59%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.7361%\n",
      "layer   2  Sparsity: 80.2995%\n",
      "layer   3  Sparsity: 87.3064%\n",
      "total_backward_count 1644720 real_backward_count 319304  19.414%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.854435/  2.083310, val:  29.58%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7104%\n",
      "layer   2  Sparsity: 80.0653%\n",
      "layer   3  Sparsity: 87.7947%\n",
      "total_backward_count 1654510 real_backward_count 321243  19.416%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.803265/  2.063794, val:  50.83%, val_best:  60.83%, tr:  98.06%, tr_best:  99.59%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7301%\n",
      "layer   2  Sparsity: 80.0777%\n",
      "layer   3  Sparsity: 87.0052%\n",
      "total_backward_count 1664300 real_backward_count 323224  19.421%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.826322/  2.011560, val:  39.17%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6948%\n",
      "layer   2  Sparsity: 79.7208%\n",
      "layer   3  Sparsity: 86.6973%\n",
      "total_backward_count 1674090 real_backward_count 325113  19.420%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.821284/  2.027206, val:  37.92%, val_best:  60.83%, tr:  98.06%, tr_best:  99.59%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6972%\n",
      "layer   2  Sparsity: 80.2442%\n",
      "layer   3  Sparsity: 85.7772%\n",
      "total_backward_count 1683880 real_backward_count 327006  19.420%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.793089/  2.000201, val:  57.08%, val_best:  60.83%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7335%\n",
      "layer   2  Sparsity: 79.6630%\n",
      "layer   3  Sparsity: 85.7845%\n",
      "total_backward_count 1693670 real_backward_count 328891  19.419%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.817639/  2.007111, val:  43.75%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7326%\n",
      "layer   2  Sparsity: 79.7816%\n",
      "layer   3  Sparsity: 86.1580%\n",
      "total_backward_count 1703460 real_backward_count 330724  19.415%\n",
      "lif layer 2 self.abs_max_v: 11023.5\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.801600/  1.985673, val:  47.92%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7334%\n",
      "layer   2  Sparsity: 79.5145%\n",
      "layer   3  Sparsity: 86.1936%\n",
      "total_backward_count 1713250 real_backward_count 332509  19.408%\n",
      "lif layer 2 self.abs_max_v: 11126.5\n",
      "fc layer 2 self.abs_max_out: 6400.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.870533/  2.037758, val:  51.67%, val_best:  60.83%, tr:  98.06%, tr_best:  99.59%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.6814%\n",
      "layer   2  Sparsity: 80.3577%\n",
      "layer   3  Sparsity: 87.6355%\n",
      "total_backward_count 1723040 real_backward_count 334476  19.412%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.826886/  1.969820, val:  42.08%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7270%\n",
      "layer   2  Sparsity: 79.8906%\n",
      "layer   3  Sparsity: 85.9660%\n",
      "total_backward_count 1732830 real_backward_count 336299  19.408%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.799097/  1.964025, val:  46.67%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7396%\n",
      "layer   2  Sparsity: 80.2085%\n",
      "layer   3  Sparsity: 85.5289%\n",
      "total_backward_count 1742620 real_backward_count 338140  19.404%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.776085/  2.031065, val:  43.33%, val_best:  60.83%, tr:  98.26%, tr_best:  99.59%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7252%\n",
      "layer   2  Sparsity: 80.2362%\n",
      "layer   3  Sparsity: 85.7541%\n",
      "total_backward_count 1752410 real_backward_count 339957  19.399%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.785072/  2.049639, val:  41.25%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7143%\n",
      "layer   2  Sparsity: 80.3055%\n",
      "layer   3  Sparsity: 86.4654%\n",
      "total_backward_count 1762200 real_backward_count 341827  19.398%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.818204/  2.029054, val:  53.33%, val_best:  60.83%, tr:  98.06%, tr_best:  99.59%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7361%\n",
      "layer   2  Sparsity: 80.2986%\n",
      "layer   3  Sparsity: 87.5683%\n",
      "total_backward_count 1771990 real_backward_count 343784  19.401%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.821365/  2.026155, val:  42.92%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7017%\n",
      "layer   2  Sparsity: 80.3425%\n",
      "layer   3  Sparsity: 87.1416%\n",
      "total_backward_count 1781780 real_backward_count 345643  19.399%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.807349/  2.025519, val:  31.67%, val_best:  60.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7178%\n",
      "layer   2  Sparsity: 80.3932%\n",
      "layer   3  Sparsity: 86.4714%\n",
      "total_backward_count 1791570 real_backward_count 347511  19.397%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.788934/  2.031163, val:  41.67%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.6997%\n",
      "layer   2  Sparsity: 80.1281%\n",
      "layer   3  Sparsity: 86.1598%\n",
      "total_backward_count 1801360 real_backward_count 349392  19.396%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.814358/  2.038586, val:  45.00%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7251%\n",
      "layer   2  Sparsity: 80.0783%\n",
      "layer   3  Sparsity: 86.9829%\n",
      "total_backward_count 1811150 real_backward_count 351305  19.397%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.789262/  2.063324, val:  33.33%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7043%\n",
      "layer   2  Sparsity: 80.1535%\n",
      "layer   3  Sparsity: 86.4559%\n",
      "total_backward_count 1820940 real_backward_count 353214  19.397%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.856904/  2.003648, val:  50.42%, val_best:  60.83%, tr:  97.85%, tr_best:  99.59%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7206%\n",
      "layer   2  Sparsity: 80.0994%\n",
      "layer   3  Sparsity: 87.7387%\n",
      "total_backward_count 1830730 real_backward_count 355165  19.400%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.855604/  2.043924, val:  38.75%, val_best:  60.83%, tr:  97.34%, tr_best:  99.59%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7022%\n",
      "layer   2  Sparsity: 80.4783%\n",
      "layer   3  Sparsity: 87.3387%\n",
      "total_backward_count 1840520 real_backward_count 357147  19.405%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.875479/  2.008911, val:  52.08%, val_best:  60.83%, tr:  96.83%, tr_best:  99.59%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7107%\n",
      "layer   2  Sparsity: 80.8357%\n",
      "layer   3  Sparsity: 87.5582%\n",
      "total_backward_count 1850310 real_backward_count 359120  19.409%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.821597/  1.992468, val:  49.17%, val_best:  60.83%, tr:  97.75%, tr_best:  99.59%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7271%\n",
      "layer   2  Sparsity: 80.4736%\n",
      "layer   3  Sparsity: 86.3059%\n",
      "total_backward_count 1860100 real_backward_count 361083  19.412%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.768061/  1.952012, val:  52.92%, val_best:  60.83%, tr:  98.16%, tr_best:  99.59%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7126%\n",
      "layer   2  Sparsity: 80.4279%\n",
      "layer   3  Sparsity: 84.3297%\n",
      "total_backward_count 1869890 real_backward_count 362950  19.410%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.736885/  1.986767, val:  37.08%, val_best:  60.83%, tr:  98.57%, tr_best:  99.59%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7011%\n",
      "layer   2  Sparsity: 81.1784%\n",
      "layer   3  Sparsity: 84.8290%\n",
      "total_backward_count 1879680 real_backward_count 364894  19.413%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.818032/  2.049096, val:  33.33%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7052%\n",
      "layer   2  Sparsity: 81.0141%\n",
      "layer   3  Sparsity: 86.2289%\n",
      "total_backward_count 1889470 real_backward_count 366779  19.412%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.838100/  2.063434, val:  39.17%, val_best:  60.83%, tr:  97.75%, tr_best:  99.59%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7161%\n",
      "layer   2  Sparsity: 80.8084%\n",
      "layer   3  Sparsity: 87.1142%\n",
      "total_backward_count 1899260 real_backward_count 368720  19.414%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.843904/  2.020952, val:  37.08%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7081%\n",
      "layer   2  Sparsity: 80.8556%\n",
      "layer   3  Sparsity: 86.8483%\n",
      "total_backward_count 1909050 real_backward_count 370584  19.412%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.815161/  2.031495, val:  50.42%, val_best:  60.83%, tr:  98.37%, tr_best:  99.59%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7338%\n",
      "layer   2  Sparsity: 80.5176%\n",
      "layer   3  Sparsity: 85.7339%\n",
      "total_backward_count 1918840 real_backward_count 372429  19.409%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.820958/  2.075268, val:  40.00%, val_best:  60.83%, tr:  98.47%, tr_best:  99.59%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7226%\n",
      "layer   2  Sparsity: 81.1085%\n",
      "layer   3  Sparsity: 86.5555%\n",
      "total_backward_count 1928630 real_backward_count 374288  19.407%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.776582/  1.982261, val:  45.83%, val_best:  60.83%, tr:  98.77%, tr_best:  99.59%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.6952%\n",
      "layer   2  Sparsity: 79.9716%\n",
      "layer   3  Sparsity: 84.6597%\n",
      "total_backward_count 1938420 real_backward_count 376111  19.403%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.779265/  2.048586, val:  42.50%, val_best:  60.83%, tr:  98.67%, tr_best:  99.59%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7340%\n",
      "layer   2  Sparsity: 79.7196%\n",
      "layer   3  Sparsity: 85.7687%\n",
      "total_backward_count 1948210 real_backward_count 377981  19.401%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.806755/  2.032311, val:  47.92%, val_best:  60.83%, tr:  98.88%, tr_best:  99.59%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7234%\n",
      "layer   2  Sparsity: 79.8353%\n",
      "layer   3  Sparsity: 85.9501%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcf29b9789e4c8fa52810a74e90e476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÜ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98876</td></tr><tr><td>tr_epoch_loss</td><td>1.80676</td></tr><tr><td>val_acc_best</td><td>0.60833</td></tr><tr><td>val_acc_now</td><td>0.47917</td></tr><tr><td>val_loss</td><td>2.03231</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-58</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1wpz2hx8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1wpz2hx8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_010008-1wpz2hx8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4pb4jhyj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_052146-4pb4jhyj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4pb4jhyj' target=\"_blank\">brisk-sweep-63</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4pb4jhyj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4pb4jhyj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251115_052154_103', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 552.0\n",
      "lif layer 2 self.abs_max_v: 552.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "lif layer 2 self.abs_max_v: 643.5\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "lif layer 1 self.abs_max_v: 339.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 1 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 413.0\n",
      "fc layer 1 self.abs_max_out: 447.0\n",
      "lif layer 1 self.abs_max_v: 447.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 1 self.abs_max_out: 461.0\n",
      "lif layer 1 self.abs_max_v: 509.5\n",
      "lif layer 2 self.abs_max_v: 823.0\n",
      "fc layer 2 self.abs_max_out: 554.0\n",
      "lif layer 2 self.abs_max_v: 825.0\n",
      "lif layer 2 self.abs_max_v: 843.0\n",
      "fc layer 1 self.abs_max_out: 485.0\n",
      "fc layer 2 self.abs_max_out: 595.0\n",
      "fc layer 2 self.abs_max_out: 610.0\n",
      "lif layer 1 self.abs_max_v: 620.0\n",
      "lif layer 2 self.abs_max_v: 878.5\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 1 self.abs_max_v: 677.5\n",
      "lif layer 1 self.abs_max_v: 685.0\n",
      "lif layer 1 self.abs_max_v: 733.5\n",
      "fc layer 2 self.abs_max_out: 666.0\n",
      "fc layer 1 self.abs_max_out: 587.0\n",
      "lif layer 1 self.abs_max_v: 759.0\n",
      "fc layer 1 self.abs_max_out: 654.0\n",
      "lif layer 1 self.abs_max_v: 786.0\n",
      "lif layer 2 self.abs_max_v: 924.5\n",
      "lif layer 2 self.abs_max_v: 949.5\n",
      "fc layer 1 self.abs_max_out: 671.0\n",
      "fc layer 1 self.abs_max_out: 825.0\n",
      "lif layer 1 self.abs_max_v: 825.0\n",
      "fc layer 1 self.abs_max_out: 890.0\n",
      "lif layer 1 self.abs_max_v: 993.5\n",
      "fc layer 1 self.abs_max_out: 1019.0\n",
      "lif layer 1 self.abs_max_v: 1121.0\n",
      "lif layer 2 self.abs_max_v: 1055.5\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 1146.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "fc layer 2 self.abs_max_out: 731.0\n",
      "lif layer 2 self.abs_max_v: 1172.5\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "fc layer 1 self.abs_max_out: 1068.0\n",
      "lif layer 1 self.abs_max_v: 1206.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "lif layer 1 self.abs_max_v: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1190.0\n",
      "lif layer 2 self.abs_max_v: 1360.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "lif layer 2 self.abs_max_v: 1369.0\n",
      "lif layer 1 self.abs_max_v: 1330.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "lif layer 1 self.abs_max_v: 1402.5\n",
      "fc layer 1 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 1534.5\n",
      "lif layer 1 self.abs_max_v: 1830.5\n",
      "lif layer 2 self.abs_max_v: 1404.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 1 self.abs_max_out: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1412.0\n",
      "lif layer 2 self.abs_max_v: 1441.0\n",
      "lif layer 2 self.abs_max_v: 1472.0\n",
      "fc layer 2 self.abs_max_out: 845.0\n",
      "fc layer 2 self.abs_max_out: 846.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 1 self.abs_max_out: 1223.0\n",
      "lif layer 2 self.abs_max_v: 1487.5\n",
      "lif layer 1 self.abs_max_v: 1864.5\n",
      "fc layer 2 self.abs_max_out: 876.0\n",
      "fc layer 2 self.abs_max_out: 925.0\n",
      "lif layer 2 self.abs_max_v: 1509.0\n",
      "lif layer 2 self.abs_max_v: 1633.5\n",
      "lif layer 2 self.abs_max_v: 1644.0\n",
      "lif layer 2 self.abs_max_v: 1654.0\n",
      "fc layer 2 self.abs_max_out: 952.0\n",
      "fc layer 1 self.abs_max_out: 1265.0\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 1002.0\n",
      "fc layer 2 self.abs_max_out: 1022.0\n",
      "fc layer 2 self.abs_max_out: 1043.0\n",
      "fc layer 1 self.abs_max_out: 1302.0\n",
      "fc layer 2 self.abs_max_out: 1064.0\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1655.5\n",
      "lif layer 2 self.abs_max_v: 1675.0\n",
      "lif layer 2 self.abs_max_v: 1708.5\n",
      "lif layer 2 self.abs_max_v: 1739.0\n",
      "lif layer 2 self.abs_max_v: 1753.5\n",
      "lif layer 1 self.abs_max_v: 2051.5\n",
      "lif layer 1 self.abs_max_v: 2195.5\n",
      "fc layer 1 self.abs_max_out: 1346.0\n",
      "lif layer 1 self.abs_max_v: 2396.0\n",
      "lif layer 1 self.abs_max_v: 2526.0\n",
      "fc layer 1 self.abs_max_out: 1407.0\n",
      "fc layer 1 self.abs_max_out: 1408.0\n",
      "lif layer 1 self.abs_max_v: 2559.5\n",
      "fc layer 1 self.abs_max_out: 1494.0\n",
      "lif layer 1 self.abs_max_v: 2774.0\n",
      "fc layer 1 self.abs_max_out: 1606.0\n",
      "lif layer 2 self.abs_max_v: 1759.5\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 1812.5\n",
      "lif layer 2 self.abs_max_v: 1831.5\n",
      "lif layer 2 self.abs_max_v: 1873.0\n",
      "fc layer 2 self.abs_max_out: 1136.0\n",
      "lif layer 1 self.abs_max_v: 2865.5\n",
      "fc layer 1 self.abs_max_out: 1639.0\n",
      "fc layer 1 self.abs_max_out: 1730.0\n",
      "lif layer 1 self.abs_max_v: 3072.5\n",
      "lif layer 2 self.abs_max_v: 1903.0\n",
      "lif layer 2 self.abs_max_v: 2003.5\n",
      "lif layer 2 self.abs_max_v: 2025.0\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "fc layer 1 self.abs_max_out: 1841.0\n",
      "lif layer 1 self.abs_max_v: 3177.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 1843.0\n",
      "lif layer 1 self.abs_max_v: 3369.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 1 self.abs_max_out: 1858.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "lif layer 2 self.abs_max_v: 2056.0\n",
      "lif layer 2 self.abs_max_v: 2087.0\n",
      "fc layer 2 self.abs_max_out: 1140.0\n",
      "fc layer 2 self.abs_max_out: 1158.0\n",
      "fc layer 1 self.abs_max_out: 2025.0\n",
      "lif layer 1 self.abs_max_v: 3587.5\n",
      "lif layer 1 self.abs_max_v: 3599.5\n",
      "fc layer 1 self.abs_max_out: 2260.0\n",
      "lif layer 1 self.abs_max_v: 4008.0\n",
      "lif layer 1 self.abs_max_v: 4232.0\n",
      "fc layer 2 self.abs_max_out: 1171.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "fc layer 2 self.abs_max_out: 1254.0\n",
      "lif layer 2 self.abs_max_v: 2092.0\n",
      "lif layer 2 self.abs_max_v: 2133.0\n",
      "lif layer 2 self.abs_max_v: 2180.5\n",
      "lif layer 2 self.abs_max_v: 2220.0\n",
      "fc layer 1 self.abs_max_out: 2375.0\n",
      "lif layer 2 self.abs_max_v: 2227.5\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "lif layer 1 self.abs_max_v: 4244.0\n",
      "lif layer 1 self.abs_max_v: 4279.5\n",
      "lif layer 1 self.abs_max_v: 4450.0\n",
      "fc layer 1 self.abs_max_out: 2394.0\n",
      "lif layer 1 self.abs_max_v: 4526.0\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "lif layer 2 self.abs_max_v: 2353.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.673764/  2.036208, val:  27.50%, val_best:  27.50%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 66.4071%\n",
      "layer   3  Sparsity: 58.8991%\n",
      "total_backward_count 9790 real_backward_count 1505  15.373%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "lif layer 2 self.abs_max_v: 2438.5\n",
      "lif layer 2 self.abs_max_v: 2463.5\n",
      "lif layer 2 self.abs_max_v: 2483.0\n",
      "lif layer 2 self.abs_max_v: 2526.0\n",
      "fc layer 2 self.abs_max_out: 1332.0\n",
      "lif layer 2 self.abs_max_v: 2560.0\n",
      "fc layer 2 self.abs_max_out: 1357.0\n",
      "fc layer 1 self.abs_max_out: 2444.0\n",
      "lif layer 1 self.abs_max_v: 4579.0\n",
      "lif layer 1 self.abs_max_v: 4710.5\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "fc layer 2 self.abs_max_out: 1475.0\n",
      "lif layer 2 self.abs_max_v: 2640.5\n",
      "lif layer 2 self.abs_max_v: 2699.5\n",
      "lif layer 2 self.abs_max_v: 2709.5\n",
      "fc layer 1 self.abs_max_out: 2628.0\n",
      "fc layer 2 self.abs_max_out: 1500.0\n",
      "fc layer 2 self.abs_max_out: 1504.0\n",
      "fc layer 1 self.abs_max_out: 2640.0\n",
      "lif layer 1 self.abs_max_v: 4761.5\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "lif layer 2 self.abs_max_v: 2712.5\n",
      "fc layer 1 self.abs_max_out: 2648.0\n",
      "lif layer 2 self.abs_max_v: 2715.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "lif layer 2 self.abs_max_v: 2756.5\n",
      "fc layer 3 self.abs_max_out: 521.0\n",
      "lif layer 2 self.abs_max_v: 2771.0\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "fc layer 1 self.abs_max_out: 2772.0\n",
      "fc layer 1 self.abs_max_out: 2975.0\n",
      "lif layer 1 self.abs_max_v: 4848.0\n",
      "lif layer 1 self.abs_max_v: 4903.0\n",
      "lif layer 1 self.abs_max_v: 5006.5\n",
      "lif layer 1 self.abs_max_v: 5097.5\n",
      "lif layer 1 self.abs_max_v: 5294.0\n",
      "lif layer 1 self.abs_max_v: 5315.0\n",
      "lif layer 1 self.abs_max_v: 5627.5\n",
      "fc layer 2 self.abs_max_out: 1612.0\n",
      "fc layer 2 self.abs_max_out: 1626.0\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "fc layer 2 self.abs_max_out: 1671.0\n",
      "lif layer 2 self.abs_max_v: 2832.5\n",
      "lif layer 2 self.abs_max_v: 2887.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.568700/  1.932911, val:  35.00%, val_best:  35.00%, tr:  98.88%, tr_best:  99.39%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4660%\n",
      "layer   2  Sparsity: 70.5045%\n",
      "layer   3  Sparsity: 60.0125%\n",
      "total_backward_count 19580 real_backward_count 2815  14.377%\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "fc layer 1 self.abs_max_out: 2989.0\n",
      "fc layer 1 self.abs_max_out: 3147.0\n",
      "lif layer 1 self.abs_max_v: 5752.0\n",
      "lif layer 1 self.abs_max_v: 5817.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.498459/  1.854591, val:  38.33%, val_best:  38.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 71.4432%\n",
      "layer   3  Sparsity: 58.4516%\n",
      "total_backward_count 29370 real_backward_count 4028  13.715%\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "fc layer 2 self.abs_max_out: 1752.0\n",
      "lif layer 2 self.abs_max_v: 2926.5\n",
      "lif layer 2 self.abs_max_v: 2964.5\n",
      "fc layer 1 self.abs_max_out: 3281.0\n",
      "fc layer 1 self.abs_max_out: 3574.0\n",
      "lif layer 1 self.abs_max_v: 5871.0\n",
      "lif layer 1 self.abs_max_v: 5996.5\n",
      "lif layer 1 self.abs_max_v: 6100.0\n",
      "lif layer 2 self.abs_max_v: 2965.0\n",
      "lif layer 2 self.abs_max_v: 3157.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.474028/  1.778981, val:  46.25%, val_best:  46.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4286%\n",
      "layer   2  Sparsity: 72.3722%\n",
      "layer   3  Sparsity: 60.5877%\n",
      "total_backward_count 39160 real_backward_count 5268  13.453%\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 1 self.abs_max_out: 3599.0\n",
      "lif layer 1 self.abs_max_v: 6130.0\n",
      "lif layer 1 self.abs_max_v: 6273.5\n",
      "lif layer 1 self.abs_max_v: 6416.0\n",
      "lif layer 1 self.abs_max_v: 6595.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.458820/  1.806853, val:  47.08%, val_best:  47.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 73.5269%\n",
      "layer   3  Sparsity: 62.1917%\n",
      "total_backward_count 48950 real_backward_count 6435  13.146%\n",
      "fc layer 1 self.abs_max_out: 3773.0\n",
      "fc layer 1 self.abs_max_out: 4106.0\n",
      "lif layer 1 self.abs_max_v: 6743.5\n",
      "lif layer 1 self.abs_max_v: 6865.0\n",
      "lif layer 1 self.abs_max_v: 6902.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.433281/  1.758754, val:  53.33%, val_best:  53.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 73.2842%\n",
      "layer   3  Sparsity: 61.8300%\n",
      "total_backward_count 58740 real_backward_count 7538  12.833%\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 2 self.abs_max_out: 1847.0\n",
      "lif layer 2 self.abs_max_v: 3228.0\n",
      "lif layer 2 self.abs_max_v: 3360.0\n",
      "lif layer 2 self.abs_max_v: 3391.0\n",
      "lif layer 2 self.abs_max_v: 3533.5\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "lif layer 1 self.abs_max_v: 7135.5\n",
      "fc layer 1 self.abs_max_out: 4156.0\n",
      "lif layer 1 self.abs_max_v: 7404.5\n",
      "lif layer 1 self.abs_max_v: 7646.5\n",
      "lif layer 1 self.abs_max_v: 7747.5\n",
      "fc layer 1 self.abs_max_out: 4192.0\n",
      "lif layer 1 self.abs_max_v: 8066.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.419936/  1.692670, val:  50.42%, val_best:  53.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 72.6899%\n",
      "layer   3  Sparsity: 60.5963%\n",
      "total_backward_count 68530 real_backward_count 8612  12.567%\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 1 self.abs_max_out: 4308.0\n",
      "fc layer 1 self.abs_max_out: 4325.0\n",
      "lif layer 1 self.abs_max_v: 8350.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.398030/  1.704258, val:  51.25%, val_best:  53.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 73.0483%\n",
      "layer   3  Sparsity: 60.7612%\n",
      "total_backward_count 78320 real_backward_count 9669  12.346%\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1900.0\n",
      "fc layer 2 self.abs_max_out: 1938.0\n",
      "fc layer 1 self.abs_max_out: 4440.0\n",
      "lif layer 1 self.abs_max_v: 8378.0\n",
      "fc layer 1 self.abs_max_out: 4464.0\n",
      "lif layer 1 self.abs_max_v: 8653.0\n",
      "fc layer 2 self.abs_max_out: 1959.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.398592/  1.660727, val:  54.17%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4599%\n",
      "layer   2  Sparsity: 73.4050%\n",
      "layer   3  Sparsity: 60.8696%\n",
      "total_backward_count 88110 real_backward_count 10758  12.210%\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "fc layer 2 self.abs_max_out: 1966.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 2 self.abs_max_out: 2014.0\n",
      "fc layer 2 self.abs_max_out: 2036.0\n",
      "lif layer 2 self.abs_max_v: 3589.5\n",
      "lif layer 2 self.abs_max_v: 3755.0\n",
      "lif layer 2 self.abs_max_v: 3831.5\n",
      "lif layer 2 self.abs_max_v: 3854.0\n",
      "lif layer 2 self.abs_max_v: 3904.0\n",
      "fc layer 1 self.abs_max_out: 4586.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.326260/  1.649979, val:  56.25%, val_best:  56.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4528%\n",
      "layer   2  Sparsity: 73.2768%\n",
      "layer   3  Sparsity: 61.2790%\n",
      "total_backward_count 97900 real_backward_count 11743  11.995%\n",
      "fc layer 2 self.abs_max_out: 2068.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.320196/  1.682651, val:  38.33%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4366%\n",
      "layer   2  Sparsity: 73.3570%\n",
      "layer   3  Sparsity: 60.9901%\n",
      "total_backward_count 107690 real_backward_count 12728  11.819%\n",
      "fc layer 2 self.abs_max_out: 2095.0\n",
      "fc layer 2 self.abs_max_out: 2129.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.349073/  1.637066, val:  56.25%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4093%\n",
      "layer   2  Sparsity: 73.2995%\n",
      "layer   3  Sparsity: 61.8388%\n",
      "total_backward_count 117480 real_backward_count 13705  11.666%\n",
      "fc layer 1 self.abs_max_out: 4630.0\n",
      "fc layer 1 self.abs_max_out: 4671.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.319948/  1.635311, val:  50.42%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4209%\n",
      "layer   2  Sparsity: 73.6310%\n",
      "layer   3  Sparsity: 62.2648%\n",
      "total_backward_count 127270 real_backward_count 14640  11.503%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.323965/  1.667109, val:  49.17%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 73.8237%\n",
      "layer   3  Sparsity: 61.8330%\n",
      "total_backward_count 137060 real_backward_count 15619  11.396%\n",
      "fc layer 1 self.abs_max_out: 4727.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.342750/  1.636796, val:  58.33%, val_best:  58.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 73.4359%\n",
      "layer   3  Sparsity: 62.5655%\n",
      "total_backward_count 146850 real_backward_count 16571  11.284%\n",
      "lif layer 2 self.abs_max_v: 3928.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.318302/  1.595331, val:  54.17%, val_best:  58.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 73.2035%\n",
      "layer   3  Sparsity: 62.2766%\n",
      "total_backward_count 156640 real_backward_count 17554  11.207%\n",
      "fc layer 2 self.abs_max_out: 2242.0\n",
      "fc layer 3 self.abs_max_out: 752.0\n",
      "fc layer 3 self.abs_max_out: 758.0\n",
      "lif layer 2 self.abs_max_v: 3981.5\n",
      "lif layer 2 self.abs_max_v: 4002.0\n",
      "lif layer 2 self.abs_max_v: 4121.0\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.285144/  1.599413, val:  52.92%, val_best:  58.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 73.2776%\n",
      "layer   3  Sparsity: 62.3475%\n",
      "total_backward_count 166430 real_backward_count 18443  11.082%\n",
      "fc layer 1 self.abs_max_out: 4793.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.296999/  1.561802, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 73.5371%\n",
      "layer   3  Sparsity: 62.2629%\n",
      "total_backward_count 176220 real_backward_count 19416  11.018%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.282530/  1.561790, val:  55.00%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4203%\n",
      "layer   2  Sparsity: 73.0158%\n",
      "layer   3  Sparsity: 62.4056%\n",
      "total_backward_count 186010 real_backward_count 20357  10.944%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.283550/  1.597373, val:  52.92%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4281%\n",
      "layer   2  Sparsity: 71.6694%\n",
      "layer   3  Sparsity: 62.7231%\n",
      "total_backward_count 195800 real_backward_count 21280  10.868%\n",
      "fc layer 1 self.abs_max_out: 5000.0\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "lif layer 1 self.abs_max_v: 8676.0\n",
      "lif layer 1 self.abs_max_v: 9188.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.266135/  1.556044, val:  58.33%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 72.1377%\n",
      "layer   3  Sparsity: 63.9925%\n",
      "total_backward_count 205590 real_backward_count 22171  10.784%\n",
      "fc layer 1 self.abs_max_out: 5246.0\n",
      "lif layer 1 self.abs_max_v: 9214.0\n",
      "lif layer 1 self.abs_max_v: 9280.0\n",
      "lif layer 1 self.abs_max_v: 9652.0\n",
      "fc layer 1 self.abs_max_out: 5398.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.245194/  1.540519, val:  65.42%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4473%\n",
      "layer   2  Sparsity: 71.7706%\n",
      "layer   3  Sparsity: 62.8941%\n",
      "total_backward_count 215380 real_backward_count 23089  10.720%\n",
      "fc layer 2 self.abs_max_out: 2351.0\n",
      "fc layer 2 self.abs_max_out: 2390.0\n",
      "lif layer 2 self.abs_max_v: 4170.5\n",
      "lif layer 2 self.abs_max_v: 4171.0\n",
      "lif layer 2 self.abs_max_v: 4223.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.233268/  1.503448, val:  65.00%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4432%\n",
      "layer   2  Sparsity: 71.5269%\n",
      "layer   3  Sparsity: 62.7788%\n",
      "total_backward_count 225170 real_backward_count 24000  10.659%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.220268/  1.534588, val:  59.17%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3690%\n",
      "layer   2  Sparsity: 71.8464%\n",
      "layer   3  Sparsity: 62.8644%\n",
      "total_backward_count 234960 real_backward_count 24908  10.601%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.229972/  1.538267, val:  59.58%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4389%\n",
      "layer   2  Sparsity: 70.7710%\n",
      "layer   3  Sparsity: 63.7828%\n",
      "total_backward_count 244750 real_backward_count 25791  10.538%\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "lif layer 1 self.abs_max_v: 9943.5\n",
      "fc layer 1 self.abs_max_out: 5416.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.215548/  1.500508, val:  65.00%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 88.4408%\n",
      "layer   2  Sparsity: 70.4628%\n",
      "layer   3  Sparsity: 61.9931%\n",
      "total_backward_count 254540 real_backward_count 26756  10.512%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.186823/  1.510727, val:  58.75%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 71.4626%\n",
      "layer   3  Sparsity: 62.4980%\n",
      "total_backward_count 264330 real_backward_count 27628  10.452%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.188085/  1.501058, val:  59.58%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.3871%\n",
      "layer   2  Sparsity: 70.6961%\n",
      "layer   3  Sparsity: 63.2201%\n",
      "total_backward_count 274120 real_backward_count 28539  10.411%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.175657/  1.493079, val:  57.92%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4363%\n",
      "layer   2  Sparsity: 70.9481%\n",
      "layer   3  Sparsity: 63.2049%\n",
      "total_backward_count 283910 real_backward_count 29378  10.348%\n",
      "lif layer 2 self.abs_max_v: 4277.5\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.185411/  1.492048, val:  62.92%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 70.5020%\n",
      "layer   3  Sparsity: 62.6529%\n",
      "total_backward_count 293700 real_backward_count 30245  10.298%\n",
      "fc layer 1 self.abs_max_out: 5506.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.182057/  1.460899, val:  69.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4707%\n",
      "layer   2  Sparsity: 70.0146%\n",
      "layer   3  Sparsity: 61.8482%\n",
      "total_backward_count 303490 real_backward_count 31082  10.242%\n",
      "fc layer 1 self.abs_max_out: 5529.0\n",
      "fc layer 1 self.abs_max_out: 5875.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.176468/  1.540229, val:  51.25%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 70.2393%\n",
      "layer   3  Sparsity: 62.3137%\n",
      "total_backward_count 313280 real_backward_count 31916  10.188%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.187704/  1.486755, val:  63.33%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4215%\n",
      "layer   2  Sparsity: 69.9993%\n",
      "layer   3  Sparsity: 62.4810%\n",
      "total_backward_count 323070 real_backward_count 32678  10.115%\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.168464/  1.475350, val:  63.33%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4620%\n",
      "layer   2  Sparsity: 70.0061%\n",
      "layer   3  Sparsity: 62.4973%\n",
      "total_backward_count 332860 real_backward_count 33502  10.065%\n",
      "fc layer 1 self.abs_max_out: 6134.0\n",
      "lif layer 1 self.abs_max_v: 10015.5\n",
      "lif layer 1 self.abs_max_v: 10403.0\n",
      "lif layer 1 self.abs_max_v: 10507.0\n",
      "lif layer 1 self.abs_max_v: 10772.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.150372/  1.451450, val:  61.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4190%\n",
      "layer   2  Sparsity: 70.2626%\n",
      "layer   3  Sparsity: 63.5989%\n",
      "total_backward_count 342650 real_backward_count 34282  10.005%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.141809/  1.454851, val:  65.00%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4137%\n",
      "layer   2  Sparsity: 70.2295%\n",
      "layer   3  Sparsity: 63.1946%\n",
      "total_backward_count 352440 real_backward_count 35100   9.959%\n",
      "lif layer 2 self.abs_max_v: 4451.0\n",
      "lif layer 2 self.abs_max_v: 4458.5\n",
      "lif layer 2 self.abs_max_v: 4480.5\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.109100/  1.395862, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4756%\n",
      "layer   2  Sparsity: 70.2851%\n",
      "layer   3  Sparsity: 62.8838%\n",
      "total_backward_count 362230 real_backward_count 35837   9.893%\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "lif layer 2 self.abs_max_v: 4578.0\n",
      "lif layer 2 self.abs_max_v: 4624.5\n",
      "lif layer 2 self.abs_max_v: 4775.5\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.098556/  1.404100, val:  69.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4652%\n",
      "layer   2  Sparsity: 70.3461%\n",
      "layer   3  Sparsity: 61.9703%\n",
      "total_backward_count 372020 real_backward_count 36580   9.833%\n",
      "fc layer 1 self.abs_max_out: 6168.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.086156/  1.415977, val:  67.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3838%\n",
      "layer   2  Sparsity: 70.1820%\n",
      "layer   3  Sparsity: 62.2699%\n",
      "total_backward_count 381810 real_backward_count 37303   9.770%\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.075828/  1.395276, val:  70.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 70.3607%\n",
      "layer   3  Sparsity: 61.7660%\n",
      "total_backward_count 391600 real_backward_count 38041   9.714%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.090549/  1.427869, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4060%\n",
      "layer   2  Sparsity: 70.6638%\n",
      "layer   3  Sparsity: 62.7786%\n",
      "total_backward_count 401390 real_backward_count 38772   9.659%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.102684/  1.442479, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4285%\n",
      "layer   2  Sparsity: 70.5857%\n",
      "layer   3  Sparsity: 62.4650%\n",
      "total_backward_count 411180 real_backward_count 39538   9.616%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.084785/  1.363738, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4793%\n",
      "layer   2  Sparsity: 70.8928%\n",
      "layer   3  Sparsity: 63.2180%\n",
      "total_backward_count 420970 real_backward_count 40328   9.580%\n",
      "lif layer 1 self.abs_max_v: 11000.5\n",
      "fc layer 1 self.abs_max_out: 6463.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.082231/  1.409364, val:  68.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4836%\n",
      "layer   2  Sparsity: 70.4859%\n",
      "layer   3  Sparsity: 63.5301%\n",
      "total_backward_count 430760 real_backward_count 41012   9.521%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.081824/  1.390513, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4907%\n",
      "layer   2  Sparsity: 70.3867%\n",
      "layer   3  Sparsity: 63.1236%\n",
      "total_backward_count 440550 real_backward_count 41722   9.470%\n",
      "lif layer 1 self.abs_max_v: 11079.0\n",
      "fc layer 1 self.abs_max_out: 6679.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.058461/  1.331375, val:  72.50%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 70.2764%\n",
      "layer   3  Sparsity: 63.2175%\n",
      "total_backward_count 450340 real_backward_count 42376   9.410%\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "lif layer 2 self.abs_max_v: 4829.0\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.056252/  1.424452, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4026%\n",
      "layer   2  Sparsity: 70.4437%\n",
      "layer   3  Sparsity: 63.4012%\n",
      "total_backward_count 460130 real_backward_count 43045   9.355%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.053372/  1.399387, val:  60.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4800%\n",
      "layer   2  Sparsity: 69.5239%\n",
      "layer   3  Sparsity: 64.2544%\n",
      "total_backward_count 469920 real_backward_count 43686   9.296%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.042926/  1.384872, val:  69.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3908%\n",
      "layer   2  Sparsity: 69.9657%\n",
      "layer   3  Sparsity: 63.4684%\n",
      "total_backward_count 479710 real_backward_count 44349   9.245%\n",
      "fc layer 1 self.abs_max_out: 6757.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.048069/  1.350940, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 70.0102%\n",
      "layer   3  Sparsity: 63.0650%\n",
      "total_backward_count 489500 real_backward_count 44996   9.192%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.049109/  1.358044, val:  73.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4516%\n",
      "layer   2  Sparsity: 69.8295%\n",
      "layer   3  Sparsity: 63.6196%\n",
      "total_backward_count 499290 real_backward_count 45637   9.140%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.029077/  1.337546, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4048%\n",
      "layer   2  Sparsity: 69.4585%\n",
      "layer   3  Sparsity: 63.3463%\n",
      "total_backward_count 509080 real_backward_count 46217   9.079%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.001552/  1.340153, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4455%\n",
      "layer   2  Sparsity: 69.6950%\n",
      "layer   3  Sparsity: 62.9430%\n",
      "total_backward_count 518870 real_backward_count 46908   9.040%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.019636/  1.337019, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 70.1810%\n",
      "layer   3  Sparsity: 63.2513%\n",
      "total_backward_count 528660 real_backward_count 47546   8.994%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.014781/  1.365995, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 69.9622%\n",
      "layer   3  Sparsity: 63.3345%\n",
      "total_backward_count 538450 real_backward_count 48100   8.933%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.003967/  1.317896, val:  72.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4457%\n",
      "layer   2  Sparsity: 69.7071%\n",
      "layer   3  Sparsity: 63.3895%\n",
      "total_backward_count 548240 real_backward_count 48725   8.888%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.011787/  1.335760, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4068%\n",
      "layer   2  Sparsity: 69.9850%\n",
      "layer   3  Sparsity: 63.0640%\n",
      "total_backward_count 558030 real_backward_count 49342   8.842%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.009309/  1.343084, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4583%\n",
      "layer   2  Sparsity: 69.8669%\n",
      "layer   3  Sparsity: 63.3387%\n",
      "total_backward_count 567820 real_backward_count 49932   8.794%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.994086/  1.286990, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 70.1117%\n",
      "layer   3  Sparsity: 63.9731%\n",
      "total_backward_count 577610 real_backward_count 50524   8.747%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.987773/  1.370890, val:  62.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4405%\n",
      "layer   2  Sparsity: 70.1050%\n",
      "layer   3  Sparsity: 63.5355%\n",
      "total_backward_count 587400 real_backward_count 51109   8.701%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.995754/  1.316439, val:  74.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4515%\n",
      "layer   2  Sparsity: 70.0020%\n",
      "layer   3  Sparsity: 63.9070%\n",
      "total_backward_count 597190 real_backward_count 51741   8.664%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.987482/  1.269222, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 70.3347%\n",
      "layer   3  Sparsity: 63.9177%\n",
      "total_backward_count 606980 real_backward_count 52359   8.626%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.986822/  1.360027, val:  67.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4370%\n",
      "layer   2  Sparsity: 70.6358%\n",
      "layer   3  Sparsity: 64.5556%\n",
      "total_backward_count 616770 real_backward_count 52980   8.590%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.990820/  1.333758, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4600%\n",
      "layer   2  Sparsity: 70.2036%\n",
      "layer   3  Sparsity: 64.2921%\n",
      "total_backward_count 626560 real_backward_count 53493   8.538%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.007418/  1.307160, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4439%\n",
      "layer   2  Sparsity: 70.2407%\n",
      "layer   3  Sparsity: 64.7160%\n",
      "total_backward_count 636350 real_backward_count 54053   8.494%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.984665/  1.327416, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 69.9275%\n",
      "layer   3  Sparsity: 64.6597%\n",
      "total_backward_count 646140 real_backward_count 54606   8.451%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.978512/  1.310116, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4253%\n",
      "layer   2  Sparsity: 70.0448%\n",
      "layer   3  Sparsity: 64.2442%\n",
      "total_backward_count 655930 real_backward_count 55190   8.414%\n",
      "fc layer 2 self.abs_max_out: 2662.0\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.971501/  1.289212, val:  74.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4125%\n",
      "layer   2  Sparsity: 69.9595%\n",
      "layer   3  Sparsity: 63.9414%\n",
      "total_backward_count 665720 real_backward_count 55770   8.377%\n",
      "lif layer 1 self.abs_max_v: 11090.5\n",
      "lif layer 1 self.abs_max_v: 11348.5\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.952972/  1.261978, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 69.8794%\n",
      "layer   3  Sparsity: 63.8400%\n",
      "total_backward_count 675510 real_backward_count 56317   8.337%\n",
      "fc layer 1 self.abs_max_out: 6772.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.959155/  1.280597, val:  72.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 69.7966%\n",
      "layer   3  Sparsity: 63.2972%\n",
      "total_backward_count 685300 real_backward_count 56881   8.300%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.948098/  1.351658, val:  71.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4428%\n",
      "layer   2  Sparsity: 69.9832%\n",
      "layer   3  Sparsity: 63.6228%\n",
      "total_backward_count 695090 real_backward_count 57451   8.265%\n",
      "fc layer 1 self.abs_max_out: 7083.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.956008/  1.294340, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4284%\n",
      "layer   2  Sparsity: 69.9274%\n",
      "layer   3  Sparsity: 63.8742%\n",
      "total_backward_count 704880 real_backward_count 58003   8.229%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.966961/  1.269990, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4367%\n",
      "layer   2  Sparsity: 69.9643%\n",
      "layer   3  Sparsity: 63.8007%\n",
      "total_backward_count 714670 real_backward_count 58548   8.192%\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.942217/  1.290774, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   2  Sparsity: 69.8661%\n",
      "layer   3  Sparsity: 64.8126%\n",
      "total_backward_count 724460 real_backward_count 59067   8.153%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.949908/  1.318629, val:  72.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 69.8066%\n",
      "layer   3  Sparsity: 64.5841%\n",
      "total_backward_count 734250 real_backward_count 59575   8.114%\n",
      "lif layer 2 self.abs_max_v: 4831.0\n",
      "lif layer 1 self.abs_max_v: 11379.5\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.934215/  1.276006, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4329%\n",
      "layer   2  Sparsity: 69.6269%\n",
      "layer   3  Sparsity: 64.2249%\n",
      "total_backward_count 744040 real_backward_count 60105   8.078%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.934731/  1.268048, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 69.5591%\n",
      "layer   3  Sparsity: 63.9075%\n",
      "total_backward_count 753830 real_backward_count 60632   8.043%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.915083/  1.284115, val:  72.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4431%\n",
      "layer   2  Sparsity: 69.4998%\n",
      "layer   3  Sparsity: 64.0157%\n",
      "total_backward_count 763620 real_backward_count 61089   8.000%\n",
      "fc layer 3 self.abs_max_out: 838.0\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.917452/  1.260487, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4742%\n",
      "layer   2  Sparsity: 69.1728%\n",
      "layer   3  Sparsity: 63.6605%\n",
      "total_backward_count 773410 real_backward_count 61573   7.961%\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.892924/  1.245008, val:  77.08%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4255%\n",
      "layer   2  Sparsity: 69.5434%\n",
      "layer   3  Sparsity: 63.5493%\n",
      "total_backward_count 783200 real_backward_count 62077   7.926%\n",
      "fc layer 2 self.abs_max_out: 2719.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.918627/  1.254392, val:  73.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4084%\n",
      "layer   2  Sparsity: 69.2939%\n",
      "layer   3  Sparsity: 64.0658%\n",
      "total_backward_count 792990 real_backward_count 62554   7.888%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.921571/  1.261821, val:  68.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 69.1603%\n",
      "layer   3  Sparsity: 63.8297%\n",
      "total_backward_count 802780 real_backward_count 63022   7.850%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.917529/  1.226000, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4212%\n",
      "layer   2  Sparsity: 69.0253%\n",
      "layer   3  Sparsity: 63.2569%\n",
      "total_backward_count 812570 real_backward_count 63485   7.813%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.910628/  1.222837, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3966%\n",
      "layer   2  Sparsity: 69.1335%\n",
      "layer   3  Sparsity: 63.8745%\n",
      "total_backward_count 822360 real_backward_count 63895   7.770%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.888752/  1.183048, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4206%\n",
      "layer   2  Sparsity: 69.5741%\n",
      "layer   3  Sparsity: 63.4891%\n",
      "total_backward_count 832150 real_backward_count 64337   7.731%\n",
      "lif layer 2 self.abs_max_v: 4881.0\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.873760/  1.230956, val:  75.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4530%\n",
      "layer   2  Sparsity: 69.4676%\n",
      "layer   3  Sparsity: 63.2574%\n",
      "total_backward_count 841940 real_backward_count 64788   7.695%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.885365/  1.230420, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4248%\n",
      "layer   2  Sparsity: 69.3380%\n",
      "layer   3  Sparsity: 62.8449%\n",
      "total_backward_count 851730 real_backward_count 65233   7.659%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.876841/  1.187118, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3822%\n",
      "layer   2  Sparsity: 69.7734%\n",
      "layer   3  Sparsity: 63.3799%\n",
      "total_backward_count 861520 real_backward_count 65648   7.620%\n",
      "fc layer 3 self.abs_max_out: 855.0\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.868652/  1.203479, val:  75.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3983%\n",
      "layer   2  Sparsity: 69.4878%\n",
      "layer   3  Sparsity: 63.3061%\n",
      "total_backward_count 871310 real_backward_count 66123   7.589%\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.868859/  1.213739, val:  76.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4472%\n",
      "layer   2  Sparsity: 69.3557%\n",
      "layer   3  Sparsity: 63.9224%\n",
      "total_backward_count 881100 real_backward_count 66582   7.557%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.870026/  1.228338, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 69.3527%\n",
      "layer   3  Sparsity: 64.3709%\n",
      "total_backward_count 890890 real_backward_count 66954   7.515%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.861025/  1.182067, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 69.2877%\n",
      "layer   3  Sparsity: 64.0351%\n",
      "total_backward_count 900680 real_backward_count 67355   7.478%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.869088/  1.235905, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4502%\n",
      "layer   2  Sparsity: 69.2057%\n",
      "layer   3  Sparsity: 64.1053%\n",
      "total_backward_count 910470 real_backward_count 67758   7.442%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.878734/  1.206724, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4451%\n",
      "layer   2  Sparsity: 68.9509%\n",
      "layer   3  Sparsity: 64.4765%\n",
      "total_backward_count 920260 real_backward_count 68206   7.412%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.874221/  1.220904, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4670%\n",
      "layer   2  Sparsity: 68.9673%\n",
      "layer   3  Sparsity: 64.6738%\n",
      "total_backward_count 930050 real_backward_count 68639   7.380%\n",
      "lif layer 2 self.abs_max_v: 4894.0\n",
      "fc layer 2 self.abs_max_out: 2738.0\n",
      "lif layer 2 self.abs_max_v: 4930.0\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.871174/  1.179774, val:  82.50%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4638%\n",
      "layer   2  Sparsity: 69.2628%\n",
      "layer   3  Sparsity: 64.4989%\n",
      "total_backward_count 939840 real_backward_count 69046   7.347%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.889494/  1.213808, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4033%\n",
      "layer   2  Sparsity: 69.3199%\n",
      "layer   3  Sparsity: 64.4280%\n",
      "total_backward_count 949630 real_backward_count 69477   7.316%\n",
      "fc layer 2 self.abs_max_out: 2739.0\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.871378/  1.199565, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4446%\n",
      "layer   2  Sparsity: 69.2169%\n",
      "layer   3  Sparsity: 64.3796%\n",
      "total_backward_count 959420 real_backward_count 69826   7.278%\n",
      "lif layer 2 self.abs_max_v: 5005.0\n",
      "fc layer 2 self.abs_max_out: 2742.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.854183/  1.213352, val:  80.83%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 69.2497%\n",
      "layer   3  Sparsity: 65.1313%\n",
      "total_backward_count 969210 real_backward_count 70223   7.245%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.858562/  1.169050, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4208%\n",
      "layer   2  Sparsity: 69.0831%\n",
      "layer   3  Sparsity: 65.0156%\n",
      "total_backward_count 979000 real_backward_count 70623   7.214%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.856179/  1.213563, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4256%\n",
      "layer   2  Sparsity: 69.3463%\n",
      "layer   3  Sparsity: 64.4389%\n",
      "total_backward_count 988790 real_backward_count 71003   7.181%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.870807/  1.179581, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4261%\n",
      "layer   2  Sparsity: 69.1648%\n",
      "layer   3  Sparsity: 63.9049%\n",
      "total_backward_count 998580 real_backward_count 71421   7.152%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.860594/  1.177633, val:  85.83%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4798%\n",
      "layer   2  Sparsity: 69.5849%\n",
      "layer   3  Sparsity: 63.9140%\n",
      "total_backward_count 1008370 real_backward_count 71783   7.119%\n",
      "fc layer 2 self.abs_max_out: 2751.0\n",
      "lif layer 2 self.abs_max_v: 5140.5\n",
      "fc layer 2 self.abs_max_out: 2811.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.852315/  1.190184, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4214%\n",
      "layer   2  Sparsity: 69.4227%\n",
      "layer   3  Sparsity: 64.3215%\n",
      "total_backward_count 1018160 real_backward_count 72174   7.089%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.848713/  1.153394, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4603%\n",
      "layer   2  Sparsity: 69.7388%\n",
      "layer   3  Sparsity: 64.4857%\n",
      "total_backward_count 1027950 real_backward_count 72534   7.056%\n",
      "fc layer 2 self.abs_max_out: 2837.0\n",
      "fc layer 2 self.abs_max_out: 2846.0\n",
      "lif layer 2 self.abs_max_v: 5188.5\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.840653/  1.165045, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4458%\n",
      "layer   2  Sparsity: 69.6737%\n",
      "layer   3  Sparsity: 64.3078%\n",
      "total_backward_count 1037740 real_backward_count 72899   7.025%\n",
      "fc layer 2 self.abs_max_out: 2847.0\n",
      "fc layer 2 self.abs_max_out: 2875.0\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.834511/  1.154782, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4293%\n",
      "layer   2  Sparsity: 69.6247%\n",
      "layer   3  Sparsity: 64.7579%\n",
      "total_backward_count 1047530 real_backward_count 73250   6.993%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.809602/  1.195100, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 69.5776%\n",
      "layer   3  Sparsity: 64.9312%\n",
      "total_backward_count 1057320 real_backward_count 73595   6.961%\n",
      "fc layer 2 self.abs_max_out: 2939.0\n",
      "lif layer 2 self.abs_max_v: 5288.0\n",
      "lif layer 2 self.abs_max_v: 5302.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.823690/  1.133208, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4353%\n",
      "layer   2  Sparsity: 69.5035%\n",
      "layer   3  Sparsity: 64.9060%\n",
      "total_backward_count 1067110 real_backward_count 73972   6.932%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.796873/  1.163637, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3850%\n",
      "layer   2  Sparsity: 69.7672%\n",
      "layer   3  Sparsity: 65.0637%\n",
      "total_backward_count 1076900 real_backward_count 74297   6.899%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.817412/  1.129967, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4796%\n",
      "layer   2  Sparsity: 69.5500%\n",
      "layer   3  Sparsity: 64.9744%\n",
      "total_backward_count 1086690 real_backward_count 74658   6.870%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.809780/  1.162096, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 69.7000%\n",
      "layer   3  Sparsity: 64.8160%\n",
      "total_backward_count 1096480 real_backward_count 74975   6.838%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.823309/  1.148648, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4570%\n",
      "layer   2  Sparsity: 69.6204%\n",
      "layer   3  Sparsity: 64.4961%\n",
      "total_backward_count 1106270 real_backward_count 75320   6.808%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.807976/  1.163409, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3622%\n",
      "layer   2  Sparsity: 69.9565%\n",
      "layer   3  Sparsity: 64.5532%\n",
      "total_backward_count 1116060 real_backward_count 75626   6.776%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.808369/  1.153473, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4309%\n",
      "layer   2  Sparsity: 69.9545%\n",
      "layer   3  Sparsity: 64.2688%\n",
      "total_backward_count 1125850 real_backward_count 75992   6.750%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.804283/  1.154485, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 70.3090%\n",
      "layer   3  Sparsity: 64.6452%\n",
      "total_backward_count 1135640 real_backward_count 76315   6.720%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.804793/  1.150404, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 70.1346%\n",
      "layer   3  Sparsity: 64.6839%\n",
      "total_backward_count 1145430 real_backward_count 76625   6.690%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.789911/  1.117559, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4663%\n",
      "layer   2  Sparsity: 69.6877%\n",
      "layer   3  Sparsity: 65.1065%\n",
      "total_backward_count 1155220 real_backward_count 76937   6.660%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.785760/  1.131766, val:  73.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4193%\n",
      "layer   2  Sparsity: 69.7534%\n",
      "layer   3  Sparsity: 64.6938%\n",
      "total_backward_count 1165010 real_backward_count 77295   6.635%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.805041/  1.169464, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4288%\n",
      "layer   2  Sparsity: 69.6024%\n",
      "layer   3  Sparsity: 64.5910%\n",
      "total_backward_count 1174800 real_backward_count 77648   6.609%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.804794/  1.193959, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4224%\n",
      "layer   2  Sparsity: 69.6766%\n",
      "layer   3  Sparsity: 64.3727%\n",
      "total_backward_count 1184590 real_backward_count 77965   6.582%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.795638/  1.118337, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4552%\n",
      "layer   2  Sparsity: 69.5434%\n",
      "layer   3  Sparsity: 64.0488%\n",
      "total_backward_count 1194380 real_backward_count 78277   6.554%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.787272/  1.137883, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4233%\n",
      "layer   2  Sparsity: 69.6021%\n",
      "layer   3  Sparsity: 64.6738%\n",
      "total_backward_count 1204170 real_backward_count 78589   6.526%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.779327/  1.130431, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 69.5107%\n",
      "layer   3  Sparsity: 64.4000%\n",
      "total_backward_count 1213960 real_backward_count 78891   6.499%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.789539/  1.139848, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4392%\n",
      "layer   2  Sparsity: 69.5520%\n",
      "layer   3  Sparsity: 64.9164%\n",
      "total_backward_count 1223750 real_backward_count 79191   6.471%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.782711/  1.143272, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3975%\n",
      "layer   2  Sparsity: 69.5941%\n",
      "layer   3  Sparsity: 65.1693%\n",
      "total_backward_count 1233540 real_backward_count 79482   6.443%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.771940/  1.131633, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4549%\n",
      "layer   2  Sparsity: 69.5648%\n",
      "layer   3  Sparsity: 64.7519%\n",
      "total_backward_count 1243330 real_backward_count 79767   6.416%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.766436/  1.122498, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 69.4267%\n",
      "layer   3  Sparsity: 64.4933%\n",
      "total_backward_count 1253120 real_backward_count 80061   6.389%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.775094/  1.117587, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4191%\n",
      "layer   2  Sparsity: 69.5247%\n",
      "layer   3  Sparsity: 64.6324%\n",
      "total_backward_count 1262910 real_backward_count 80366   6.364%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.774643/  1.124763, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4315%\n",
      "layer   2  Sparsity: 69.4986%\n",
      "layer   3  Sparsity: 64.2545%\n",
      "total_backward_count 1272700 real_backward_count 80653   6.337%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.764921/  1.112081, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4136%\n",
      "layer   2  Sparsity: 69.4638%\n",
      "layer   3  Sparsity: 64.9799%\n",
      "total_backward_count 1282490 real_backward_count 80934   6.311%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.772444/  1.105760, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4198%\n",
      "layer   2  Sparsity: 69.4774%\n",
      "layer   3  Sparsity: 65.3650%\n",
      "total_backward_count 1292280 real_backward_count 81215   6.285%\n",
      "fc layer 2 self.abs_max_out: 2943.0\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.764299/  1.097106, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 69.4561%\n",
      "layer   3  Sparsity: 65.0000%\n",
      "total_backward_count 1302070 real_backward_count 81522   6.261%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.766809/  1.118514, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 69.7299%\n",
      "layer   3  Sparsity: 64.9512%\n",
      "total_backward_count 1311860 real_backward_count 81821   6.237%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.759716/  1.095623, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4386%\n",
      "layer   2  Sparsity: 69.8805%\n",
      "layer   3  Sparsity: 65.1287%\n",
      "total_backward_count 1321650 real_backward_count 82107   6.212%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.748896/  1.095023, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3999%\n",
      "layer   2  Sparsity: 69.8554%\n",
      "layer   3  Sparsity: 65.2089%\n",
      "total_backward_count 1331440 real_backward_count 82369   6.186%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.748185/  1.100036, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4437%\n",
      "layer   2  Sparsity: 69.6660%\n",
      "layer   3  Sparsity: 65.3025%\n",
      "total_backward_count 1341230 real_backward_count 82672   6.164%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.750892/  1.107167, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4640%\n",
      "layer   2  Sparsity: 69.2978%\n",
      "layer   3  Sparsity: 65.6543%\n",
      "total_backward_count 1351020 real_backward_count 82915   6.137%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.759588/  1.117751, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 69.6833%\n",
      "layer   3  Sparsity: 65.6100%\n",
      "total_backward_count 1360810 real_backward_count 83165   6.111%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.754344/  1.117020, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4500%\n",
      "layer   2  Sparsity: 69.1656%\n",
      "layer   3  Sparsity: 65.2487%\n",
      "total_backward_count 1370600 real_backward_count 83437   6.088%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.748420/  1.091657, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3876%\n",
      "layer   2  Sparsity: 69.3655%\n",
      "layer   3  Sparsity: 64.9294%\n",
      "total_backward_count 1380390 real_backward_count 83686   6.062%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.734145/  1.095564, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 69.4159%\n",
      "layer   3  Sparsity: 65.1143%\n",
      "total_backward_count 1390180 real_backward_count 83939   6.038%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.739796/  1.079230, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 69.6258%\n",
      "layer   3  Sparsity: 64.6667%\n",
      "total_backward_count 1399970 real_backward_count 84191   6.014%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.722795/  1.106609, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4159%\n",
      "layer   2  Sparsity: 69.4117%\n",
      "layer   3  Sparsity: 64.7449%\n",
      "total_backward_count 1409760 real_backward_count 84452   5.991%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.732186/  1.061057, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4419%\n",
      "layer   2  Sparsity: 69.5740%\n",
      "layer   3  Sparsity: 65.1512%\n",
      "total_backward_count 1419550 real_backward_count 84678   5.965%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.737563/  1.090759, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4641%\n",
      "layer   2  Sparsity: 69.3853%\n",
      "layer   3  Sparsity: 64.9736%\n",
      "total_backward_count 1429340 real_backward_count 84920   5.941%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.745549/  1.084750, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4555%\n",
      "layer   2  Sparsity: 69.2096%\n",
      "layer   3  Sparsity: 65.4455%\n",
      "total_backward_count 1439130 real_backward_count 85176   5.919%\n",
      "lif layer 2 self.abs_max_v: 5344.5\n",
      "lif layer 2 self.abs_max_v: 5360.5\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.736513/  1.063903, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4359%\n",
      "layer   2  Sparsity: 69.1187%\n",
      "layer   3  Sparsity: 65.4844%\n",
      "total_backward_count 1448920 real_backward_count 85454   5.898%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.728608/  1.076735, val:  80.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4338%\n",
      "layer   2  Sparsity: 68.9364%\n",
      "layer   3  Sparsity: 65.4903%\n",
      "total_backward_count 1458710 real_backward_count 85714   5.876%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.715119/  1.090007, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4263%\n",
      "layer   2  Sparsity: 69.2501%\n",
      "layer   3  Sparsity: 65.8130%\n",
      "total_backward_count 1468500 real_backward_count 85957   5.853%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.723336/  1.067418, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4890%\n",
      "layer   2  Sparsity: 69.3865%\n",
      "layer   3  Sparsity: 65.1039%\n",
      "total_backward_count 1478290 real_backward_count 86226   5.833%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.728474/  1.099636, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4407%\n",
      "layer   2  Sparsity: 69.4869%\n",
      "layer   3  Sparsity: 64.9068%\n",
      "total_backward_count 1488080 real_backward_count 86459   5.810%\n",
      "lif layer 2 self.abs_max_v: 5381.5\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.727081/  1.087391, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4509%\n",
      "layer   2  Sparsity: 69.2860%\n",
      "layer   3  Sparsity: 65.1219%\n",
      "total_backward_count 1497870 real_backward_count 86696   5.788%\n",
      "lif layer 2 self.abs_max_v: 5404.5\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.724638/  1.050863, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4051%\n",
      "layer   2  Sparsity: 69.4888%\n",
      "layer   3  Sparsity: 65.0156%\n",
      "total_backward_count 1507660 real_backward_count 86944   5.767%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.724028/  1.081912, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 69.8560%\n",
      "layer   3  Sparsity: 64.9017%\n",
      "total_backward_count 1517450 real_backward_count 87203   5.747%\n",
      "fc layer 2 self.abs_max_out: 3035.0\n",
      "lif layer 2 self.abs_max_v: 5470.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.723052/  1.091299, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4632%\n",
      "layer   2  Sparsity: 69.6590%\n",
      "layer   3  Sparsity: 65.0011%\n",
      "total_backward_count 1527240 real_backward_count 87417   5.724%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.727109/  1.133821, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4156%\n",
      "layer   2  Sparsity: 69.4464%\n",
      "layer   3  Sparsity: 64.9731%\n",
      "total_backward_count 1537030 real_backward_count 87659   5.703%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.725689/  1.092246, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4274%\n",
      "layer   2  Sparsity: 69.3605%\n",
      "layer   3  Sparsity: 65.2197%\n",
      "total_backward_count 1546820 real_backward_count 87887   5.682%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.721165/  1.109210, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4578%\n",
      "layer   2  Sparsity: 69.3679%\n",
      "layer   3  Sparsity: 65.4221%\n",
      "total_backward_count 1556610 real_backward_count 88119   5.661%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.715326/  1.095336, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4591%\n",
      "layer   2  Sparsity: 69.2472%\n",
      "layer   3  Sparsity: 65.5698%\n",
      "total_backward_count 1566400 real_backward_count 88330   5.639%\n",
      "lif layer 2 self.abs_max_v: 5576.0\n",
      "fc layer 2 self.abs_max_out: 3044.0\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.710983/  1.102293, val:  80.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4628%\n",
      "layer   2  Sparsity: 69.3914%\n",
      "layer   3  Sparsity: 66.3533%\n",
      "total_backward_count 1576190 real_backward_count 88531   5.617%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.705037/  1.093461, val:  78.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4337%\n",
      "layer   2  Sparsity: 69.1941%\n",
      "layer   3  Sparsity: 66.6903%\n",
      "total_backward_count 1585980 real_backward_count 88743   5.595%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.709758/  1.092863, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 69.4420%\n",
      "layer   3  Sparsity: 66.5311%\n",
      "total_backward_count 1595770 real_backward_count 88929   5.573%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.719124/  1.076343, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4257%\n",
      "layer   2  Sparsity: 69.3238%\n",
      "layer   3  Sparsity: 65.6539%\n",
      "total_backward_count 1605560 real_backward_count 89160   5.553%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.717629/  1.070387, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4210%\n",
      "layer   2  Sparsity: 69.0715%\n",
      "layer   3  Sparsity: 65.8598%\n",
      "total_backward_count 1615350 real_backward_count 89385   5.533%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.701004/  1.069665, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4491%\n",
      "layer   2  Sparsity: 69.1310%\n",
      "layer   3  Sparsity: 66.2864%\n",
      "total_backward_count 1625140 real_backward_count 89600   5.513%\n",
      "fc layer 3 self.abs_max_out: 961.0\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.704260/  1.074285, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4731%\n",
      "layer   2  Sparsity: 69.3062%\n",
      "layer   3  Sparsity: 65.7922%\n",
      "total_backward_count 1634930 real_backward_count 89810   5.493%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.702441/  1.099619, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3998%\n",
      "layer   2  Sparsity: 69.3785%\n",
      "layer   3  Sparsity: 65.9582%\n",
      "total_backward_count 1644720 real_backward_count 90000   5.472%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.695531/  1.043835, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4671%\n",
      "layer   2  Sparsity: 69.1360%\n",
      "layer   3  Sparsity: 66.1112%\n",
      "total_backward_count 1654510 real_backward_count 90190   5.451%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.694560/  1.070918, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4466%\n",
      "layer   2  Sparsity: 69.5406%\n",
      "layer   3  Sparsity: 65.6947%\n",
      "total_backward_count 1664300 real_backward_count 90384   5.431%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.686414/  1.073757, val:  79.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4414%\n",
      "layer   2  Sparsity: 69.5113%\n",
      "layer   3  Sparsity: 66.0738%\n",
      "total_backward_count 1674090 real_backward_count 90584   5.411%\n",
      "fc layer 3 self.abs_max_out: 962.0\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.683614/  1.062023, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3944%\n",
      "layer   2  Sparsity: 69.6781%\n",
      "layer   3  Sparsity: 65.8231%\n",
      "total_backward_count 1683880 real_backward_count 90765   5.390%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.683334/  1.074646, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 69.7775%\n",
      "layer   3  Sparsity: 66.1749%\n",
      "total_backward_count 1693670 real_backward_count 90989   5.372%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.686306/  1.056486, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 70.1454%\n",
      "layer   3  Sparsity: 65.9388%\n",
      "total_backward_count 1703460 real_backward_count 91204   5.354%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.682787/  1.069237, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4142%\n",
      "layer   2  Sparsity: 69.5361%\n",
      "layer   3  Sparsity: 66.2323%\n",
      "total_backward_count 1713250 real_backward_count 91422   5.336%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.689047/  1.062255, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3973%\n",
      "layer   2  Sparsity: 69.1209%\n",
      "layer   3  Sparsity: 65.8933%\n",
      "total_backward_count 1723040 real_backward_count 91597   5.316%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.679957/  1.051837, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4291%\n",
      "layer   2  Sparsity: 69.4612%\n",
      "layer   3  Sparsity: 65.7612%\n",
      "total_backward_count 1732830 real_backward_count 91785   5.297%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.678627/  1.063504, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 69.5103%\n",
      "layer   3  Sparsity: 65.2984%\n",
      "total_backward_count 1742620 real_backward_count 91976   5.278%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.688958/  1.081601, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 69.1689%\n",
      "layer   3  Sparsity: 65.7801%\n",
      "total_backward_count 1752410 real_backward_count 92159   5.259%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.687299/  1.049295, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4441%\n",
      "layer   2  Sparsity: 69.4241%\n",
      "layer   3  Sparsity: 65.7272%\n",
      "total_backward_count 1762200 real_backward_count 92366   5.242%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.676436/  1.090559, val:  78.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4087%\n",
      "layer   2  Sparsity: 69.5326%\n",
      "layer   3  Sparsity: 66.1261%\n",
      "total_backward_count 1771990 real_backward_count 92548   5.223%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.684905/  1.056911, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 69.7555%\n",
      "layer   3  Sparsity: 65.5749%\n",
      "total_backward_count 1781780 real_backward_count 92734   5.205%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.682219/  1.038225, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 69.7274%\n",
      "layer   3  Sparsity: 65.6600%\n",
      "total_backward_count 1791570 real_backward_count 92938   5.188%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.672260/  1.102764, val:  78.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4260%\n",
      "layer   2  Sparsity: 69.6997%\n",
      "layer   3  Sparsity: 65.5103%\n",
      "total_backward_count 1801360 real_backward_count 93090   5.168%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.686300/  1.077820, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4424%\n",
      "layer   2  Sparsity: 69.8458%\n",
      "layer   3  Sparsity: 65.9062%\n",
      "total_backward_count 1811150 real_backward_count 93263   5.149%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.692329/  1.089219, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 69.5804%\n",
      "layer   3  Sparsity: 65.7920%\n",
      "total_backward_count 1820940 real_backward_count 93456   5.132%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.678998/  1.071636, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4092%\n",
      "layer   2  Sparsity: 69.5553%\n",
      "layer   3  Sparsity: 65.9394%\n",
      "total_backward_count 1830730 real_backward_count 93620   5.114%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.674121/  1.071599, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4323%\n",
      "layer   2  Sparsity: 69.5319%\n",
      "layer   3  Sparsity: 65.8397%\n",
      "total_backward_count 1840520 real_backward_count 93795   5.096%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.669025/  1.020096, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4132%\n",
      "layer   2  Sparsity: 69.5176%\n",
      "layer   3  Sparsity: 65.8672%\n",
      "total_backward_count 1850310 real_backward_count 93945   5.077%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.665762/  1.106118, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 69.6324%\n",
      "layer   3  Sparsity: 65.6110%\n",
      "total_backward_count 1860100 real_backward_count 94118   5.060%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.663598/  1.061393, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3947%\n",
      "layer   2  Sparsity: 69.3039%\n",
      "layer   3  Sparsity: 65.2814%\n",
      "total_backward_count 1869890 real_backward_count 94281   5.042%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.668495/  1.043608, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4354%\n",
      "layer   2  Sparsity: 69.4123%\n",
      "layer   3  Sparsity: 65.1824%\n",
      "total_backward_count 1879680 real_backward_count 94443   5.024%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.658819/  1.036041, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 69.4854%\n",
      "layer   3  Sparsity: 65.3908%\n",
      "total_backward_count 1889470 real_backward_count 94621   5.008%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.657491/  1.049034, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4487%\n",
      "layer   2  Sparsity: 69.4428%\n",
      "layer   3  Sparsity: 66.2241%\n",
      "total_backward_count 1899260 real_backward_count 94787   4.991%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.668242/  1.075520, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4382%\n",
      "layer   2  Sparsity: 69.3666%\n",
      "layer   3  Sparsity: 66.3944%\n",
      "total_backward_count 1909050 real_backward_count 94973   4.975%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.669571/  1.083785, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3872%\n",
      "layer   2  Sparsity: 69.6206%\n",
      "layer   3  Sparsity: 66.4536%\n",
      "total_backward_count 1918840 real_backward_count 95148   4.959%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.675315/  1.088723, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4464%\n",
      "layer   2  Sparsity: 69.5940%\n",
      "layer   3  Sparsity: 66.5998%\n",
      "total_backward_count 1928630 real_backward_count 95333   4.943%\n",
      "fc layer 3 self.abs_max_out: 966.0\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.679097/  1.046024, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.5142%\n",
      "layer   2  Sparsity: 69.6470%\n",
      "layer   3  Sparsity: 66.7713%\n",
      "total_backward_count 1938420 real_backward_count 95488   4.926%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.669595/  1.043052, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 69.3653%\n",
      "layer   3  Sparsity: 66.6048%\n",
      "total_backward_count 1948210 real_backward_count 95627   4.908%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.664449/  1.051994, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4394%\n",
      "layer   2  Sparsity: 69.6651%\n",
      "layer   3  Sparsity: 66.5248%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99155e9e22e640a6aeaffff74f4b3448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.66445</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>1.05199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-sweep-63</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4pb4jhyj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4pb4jhyj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_052146-4pb4jhyj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b1rpnoqf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_094338-b1rpnoqf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b1rpnoqf' target=\"_blank\">frosty-sweep-68</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b1rpnoqf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b1rpnoqf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251115_094347_593', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1141.0\n",
      "lif layer 1 self.abs_max_v: 1141.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1964.0\n",
      "lif layer 2 self.abs_max_v: 1964.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 877.0\n",
      "lif layer 1 self.abs_max_v: 1322.0\n",
      "lif layer 2 self.abs_max_v: 2546.0\n",
      "fc layer 3 self.abs_max_out: 890.0\n",
      "fc layer 1 self.abs_max_out: 1295.0\n",
      "lif layer 1 self.abs_max_v: 1749.0\n",
      "fc layer 2 self.abs_max_out: 1982.0\n",
      "lif layer 2 self.abs_max_v: 2823.0\n",
      "fc layer 3 self.abs_max_out: 1054.0\n",
      "fc layer 1 self.abs_max_out: 1703.0\n",
      "lif layer 1 self.abs_max_v: 2080.5\n",
      "lif layer 2 self.abs_max_v: 3214.5\n",
      "fc layer 1 self.abs_max_out: 2887.0\n",
      "lif layer 1 self.abs_max_v: 2887.0\n",
      "fc layer 2 self.abs_max_out: 2388.0\n",
      "lif layer 2 self.abs_max_v: 3995.5\n",
      "lif layer 1 self.abs_max_v: 3000.5\n",
      "fc layer 3 self.abs_max_out: 1066.0\n",
      "lif layer 1 self.abs_max_v: 3054.5\n",
      "fc layer 3 self.abs_max_out: 1102.0\n",
      "lif layer 1 self.abs_max_v: 3060.5\n",
      "lif layer 2 self.abs_max_v: 4131.5\n",
      "fc layer 2 self.abs_max_out: 2565.0\n",
      "lif layer 1 self.abs_max_v: 3241.5\n",
      "lif layer 1 self.abs_max_v: 3591.0\n",
      "lif layer 1 self.abs_max_v: 3641.0\n",
      "fc layer 1 self.abs_max_out: 2903.0\n",
      "fc layer 3 self.abs_max_out: 1152.0\n",
      "fc layer 1 self.abs_max_out: 3091.0\n",
      "fc layer 3 self.abs_max_out: 1311.0\n",
      "fc layer 1 self.abs_max_out: 4481.0\n",
      "lif layer 1 self.abs_max_v: 4481.0\n",
      "fc layer 1 self.abs_max_out: 4624.0\n",
      "lif layer 1 self.abs_max_v: 4624.0\n",
      "lif layer 1 self.abs_max_v: 4713.5\n",
      "fc layer 1 self.abs_max_out: 5666.0\n",
      "lif layer 1 self.abs_max_v: 5666.0\n",
      "fc layer 1 self.abs_max_out: 6676.0\n",
      "lif layer 1 self.abs_max_v: 7866.0\n",
      "fc layer 2 self.abs_max_out: 2573.0\n",
      "fc layer 2 self.abs_max_out: 2711.0\n",
      "fc layer 2 self.abs_max_out: 2913.0\n",
      "lif layer 2 self.abs_max_v: 4288.5\n",
      "lif layer 2 self.abs_max_v: 4561.5\n",
      "lif layer 2 self.abs_max_v: 4965.0\n",
      "lif layer 2 self.abs_max_v: 5022.5\n",
      "lif layer 2 self.abs_max_v: 5179.5\n",
      "lif layer 1 self.abs_max_v: 8059.0\n",
      "lif layer 1 self.abs_max_v: 8060.5\n",
      "fc layer 1 self.abs_max_out: 6967.0\n",
      "fc layer 1 self.abs_max_out: 7571.0\n",
      "lif layer 1 self.abs_max_v: 8634.0\n",
      "lif layer 1 self.abs_max_v: 8874.0\n",
      "fc layer 2 self.abs_max_out: 3052.0\n",
      "fc layer 1 self.abs_max_out: 8975.0\n",
      "lif layer 1 self.abs_max_v: 8975.0\n",
      "lif layer 1 self.abs_max_v: 9293.0\n",
      "lif layer 2 self.abs_max_v: 5462.0\n",
      "lif layer 1 self.abs_max_v: 9547.0\n",
      "fc layer 1 self.abs_max_out: 9254.0\n",
      "lif layer 1 self.abs_max_v: 9947.5\n",
      "lif layer 1 self.abs_max_v: 9997.0\n",
      "fc layer 2 self.abs_max_out: 3456.0\n",
      "fc layer 2 self.abs_max_out: 3524.0\n",
      "fc layer 2 self.abs_max_out: 3601.0\n",
      "fc layer 2 self.abs_max_out: 3998.0\n",
      "lif layer 2 self.abs_max_v: 5819.5\n",
      "lif layer 2 self.abs_max_v: 5853.0\n",
      "lif layer 2 self.abs_max_v: 6035.5\n",
      "lif layer 1 self.abs_max_v: 10242.0\n",
      "lif layer 1 self.abs_max_v: 10327.0\n",
      "lif layer 2 self.abs_max_v: 6062.5\n",
      "lif layer 2 self.abs_max_v: 6103.5\n",
      "lif layer 2 self.abs_max_v: 6317.0\n",
      "lif layer 1 self.abs_max_v: 10329.5\n",
      "lif layer 2 self.abs_max_v: 6513.0\n",
      "lif layer 2 self.abs_max_v: 6868.5\n",
      "lif layer 1 self.abs_max_v: 10684.5\n",
      "lif layer 1 self.abs_max_v: 11587.5\n",
      "lif layer 1 self.abs_max_v: 11935.0\n",
      "lif layer 1 self.abs_max_v: 12091.0\n",
      "lif layer 1 self.abs_max_v: 12114.5\n",
      "fc layer 3 self.abs_max_out: 1392.0\n",
      "fc layer 3 self.abs_max_out: 1416.0\n",
      "fc layer 3 self.abs_max_out: 1628.0\n",
      "fc layer 3 self.abs_max_out: 1658.0\n",
      "fc layer 3 self.abs_max_out: 1876.0\n",
      "fc layer 1 self.abs_max_out: 9416.0\n",
      "lif layer 1 self.abs_max_v: 12461.5\n",
      "fc layer 1 self.abs_max_out: 9531.0\n",
      "lif layer 1 self.abs_max_v: 13297.0\n",
      "lif layer 2 self.abs_max_v: 6904.0\n",
      "fc layer 2 self.abs_max_out: 4010.0\n",
      "lif layer 2 self.abs_max_v: 7095.0\n",
      "lif layer 2 self.abs_max_v: 7143.5\n",
      "lif layer 2 self.abs_max_v: 7285.0\n",
      "fc layer 2 self.abs_max_out: 4277.0\n",
      "lif layer 2 self.abs_max_v: 7508.0\n",
      "lif layer 2 self.abs_max_v: 7607.0\n",
      "lif layer 2 self.abs_max_v: 7782.5\n",
      "lif layer 2 self.abs_max_v: 8088.5\n",
      "lif layer 2 self.abs_max_v: 8206.5\n",
      "fc layer 3 self.abs_max_out: 1877.0\n",
      "lif layer 1 self.abs_max_v: 14081.0\n",
      "lif layer 1 self.abs_max_v: 14523.5\n",
      "fc layer 3 self.abs_max_out: 2008.0\n",
      "fc layer 2 self.abs_max_out: 4329.0\n",
      "fc layer 2 self.abs_max_out: 4481.0\n",
      "lif layer 1 self.abs_max_v: 14910.5\n",
      "lif layer 1 self.abs_max_v: 15432.0\n",
      "lif layer 1 self.abs_max_v: 15801.0\n",
      "lif layer 1 self.abs_max_v: 16384.5\n",
      "fc layer 2 self.abs_max_out: 4553.0\n",
      "fc layer 3 self.abs_max_out: 2121.0\n",
      "fc layer 3 self.abs_max_out: 2629.0\n",
      "lif layer 1 self.abs_max_v: 16485.0\n",
      "lif layer 1 self.abs_max_v: 17241.0\n",
      "fc layer 1 self.abs_max_out: 9631.0\n",
      "lif layer 1 self.abs_max_v: 18251.5\n",
      "fc layer 1 self.abs_max_out: 10365.0\n",
      "lif layer 1 self.abs_max_v: 19491.0\n",
      "fc layer 1 self.abs_max_out: 10771.0\n",
      "lif layer 1 self.abs_max_v: 20516.5\n",
      "fc layer 2 self.abs_max_out: 4866.0\n",
      "fc layer 2 self.abs_max_out: 4913.0\n",
      "fc layer 1 self.abs_max_out: 10818.0\n",
      "lif layer 1 self.abs_max_v: 20596.5\n",
      "lif layer 2 self.abs_max_v: 8312.5\n",
      "lif layer 2 self.abs_max_v: 8340.0\n",
      "fc layer 1 self.abs_max_out: 10989.0\n",
      "fc layer 1 self.abs_max_out: 11121.0\n",
      "lif layer 1 self.abs_max_v: 21211.5\n",
      "fc layer 1 self.abs_max_out: 11309.0\n",
      "fc layer 1 self.abs_max_out: 11387.0\n",
      "fc layer 1 self.abs_max_out: 12106.0\n",
      "lif layer 1 self.abs_max_v: 21728.5\n",
      "fc layer 1 self.abs_max_out: 12255.0\n",
      "lif layer 1 self.abs_max_v: 23119.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.509675/  1.932527, val:  33.75%, val_best:  33.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 62.1172%\n",
      "layer   3  Sparsity: 57.5997%\n",
      "total_backward_count 9790 real_backward_count 1139  11.634%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 2643.0\n",
      "fc layer 1 self.abs_max_out: 12354.0\n",
      "fc layer 3 self.abs_max_out: 2740.0\n",
      "fc layer 3 self.abs_max_out: 2745.0\n",
      "fc layer 1 self.abs_max_out: 12539.0\n",
      "fc layer 1 self.abs_max_out: 12701.0\n",
      "lif layer 1 self.abs_max_v: 23705.5\n",
      "lif layer 1 self.abs_max_v: 24301.0\n",
      "fc layer 1 self.abs_max_out: 13603.0\n",
      "lif layer 1 self.abs_max_v: 25327.5\n",
      "fc layer 1 self.abs_max_out: 13676.0\n",
      "lif layer 2 self.abs_max_v: 8606.0\n",
      "lif layer 2 self.abs_max_v: 8677.0\n",
      "fc layer 1 self.abs_max_out: 13743.0\n",
      "fc layer 1 self.abs_max_out: 14649.0\n",
      "lif layer 1 self.abs_max_v: 25747.5\n",
      "lif layer 1 self.abs_max_v: 25903.0\n",
      "fc layer 1 self.abs_max_out: 14761.0\n",
      "fc layer 1 self.abs_max_out: 15395.0\n",
      "fc layer 1 self.abs_max_out: 15790.0\n",
      "lif layer 1 self.abs_max_v: 28620.0\n",
      "lif layer 1 self.abs_max_v: 29310.0\n",
      "fc layer 1 self.abs_max_out: 16092.0\n",
      "lif layer 1 self.abs_max_v: 30698.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.341696/  1.858519, val:  29.58%, val_best:  33.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 71.8086%\n",
      "layer   3  Sparsity: 63.8952%\n",
      "total_backward_count 19580 real_backward_count 2265  11.568%\n",
      "fc layer 3 self.abs_max_out: 2856.0\n",
      "fc layer 3 self.abs_max_out: 2903.0\n",
      "fc layer 3 self.abs_max_out: 2937.0\n",
      "fc layer 3 self.abs_max_out: 2953.0\n",
      "fc layer 3 self.abs_max_out: 2978.0\n",
      "lif layer 2 self.abs_max_v: 8706.0\n",
      "lif layer 2 self.abs_max_v: 8870.0\n",
      "fc layer 3 self.abs_max_out: 2990.0\n",
      "fc layer 1 self.abs_max_out: 16289.0\n",
      "fc layer 1 self.abs_max_out: 16291.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.352080/  1.828545, val:  34.58%, val_best:  34.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.5033%\n",
      "layer   3  Sparsity: 66.5107%\n",
      "total_backward_count 29370 real_backward_count 3480  11.849%\n",
      "fc layer 1 self.abs_max_out: 16903.0\n",
      "fc layer 1 self.abs_max_out: 17325.0\n",
      "lif layer 2 self.abs_max_v: 9170.5\n",
      "lif layer 2 self.abs_max_v: 9174.5\n",
      "fc layer 2 self.abs_max_out: 5019.0\n",
      "lif layer 2 self.abs_max_v: 9245.0\n",
      "fc layer 2 self.abs_max_out: 5050.0\n",
      "lif layer 2 self.abs_max_v: 9566.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.356889/  1.854645, val:  31.25%, val_best:  34.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 73.6621%\n",
      "layer   3  Sparsity: 67.9948%\n",
      "total_backward_count 39160 real_backward_count 4668  11.920%\n",
      "lif layer 2 self.abs_max_v: 9568.0\n",
      "lif layer 2 self.abs_max_v: 9761.0\n",
      "fc layer 2 self.abs_max_out: 5061.0\n",
      "lif layer 2 self.abs_max_v: 9941.5\n",
      "fc layer 3 self.abs_max_out: 3037.0\n",
      "fc layer 2 self.abs_max_out: 5425.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.322427/  1.741107, val:  50.00%, val_best:  50.00%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.7317%\n",
      "layer   3  Sparsity: 68.9657%\n",
      "total_backward_count 48950 real_backward_count 5862  11.975%\n",
      "lif layer 1 self.abs_max_v: 30728.5\n",
      "lif layer 1 self.abs_max_v: 31091.0\n",
      "fc layer 1 self.abs_max_out: 17395.0\n",
      "fc layer 1 self.abs_max_out: 17536.0\n",
      "lif layer 1 self.abs_max_v: 31599.0\n",
      "lif layer 1 self.abs_max_v: 32515.5\n",
      "fc layer 1 self.abs_max_out: 19638.0\n",
      "lif layer 1 self.abs_max_v: 34489.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.331475/  1.777447, val:  39.58%, val_best:  50.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.3975%\n",
      "layer   3  Sparsity: 70.7865%\n",
      "total_backward_count 58740 real_backward_count 7055  12.011%\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.282768/  1.752739, val:  44.17%, val_best:  50.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.2620%\n",
      "layer   3  Sparsity: 68.6431%\n",
      "total_backward_count 68530 real_backward_count 8276  12.076%\n",
      "lif layer 2 self.abs_max_v: 10064.5\n",
      "lif layer 2 self.abs_max_v: 10122.5\n",
      "lif layer 1 self.abs_max_v: 34512.5\n",
      "lif layer 1 self.abs_max_v: 34720.5\n",
      "lif layer 1 self.abs_max_v: 35114.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.318330/  1.737136, val:  44.58%, val_best:  50.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.4107%\n",
      "layer   3  Sparsity: 71.5151%\n",
      "total_backward_count 78320 real_backward_count 9493  12.121%\n",
      "lif layer 1 self.abs_max_v: 35451.0\n",
      "lif layer 1 self.abs_max_v: 36629.5\n",
      "fc layer 1 self.abs_max_out: 20597.0\n",
      "lif layer 2 self.abs_max_v: 10261.0\n",
      "lif layer 2 self.abs_max_v: 10292.5\n",
      "lif layer 2 self.abs_max_v: 10423.5\n",
      "lif layer 2 self.abs_max_v: 10479.0\n",
      "fc layer 2 self.abs_max_out: 5468.0\n",
      "lif layer 2 self.abs_max_v: 10627.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.367472/  1.797493, val:  41.67%, val_best:  50.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.1235%\n",
      "layer   3  Sparsity: 74.0165%\n",
      "total_backward_count 88110 real_backward_count 10771  12.224%\n",
      "fc layer 2 self.abs_max_out: 5521.0\n",
      "fc layer 2 self.abs_max_out: 5541.0\n",
      "fc layer 2 self.abs_max_out: 5728.0\n",
      "lif layer 2 self.abs_max_v: 10747.0\n",
      "lif layer 2 self.abs_max_v: 10986.5\n",
      "fc layer 2 self.abs_max_out: 5850.0\n",
      "lif layer 2 self.abs_max_v: 11343.5\n",
      "fc layer 2 self.abs_max_out: 5965.0\n",
      "lif layer 2 self.abs_max_v: 11576.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.383474/  1.770951, val:  43.33%, val_best:  50.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.1262%\n",
      "layer   3  Sparsity: 73.8203%\n",
      "total_backward_count 97900 real_backward_count 12012  12.270%\n",
      "lif layer 1 self.abs_max_v: 36943.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.334650/  1.767239, val:  39.17%, val_best:  50.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.8299%\n",
      "layer   3  Sparsity: 72.7100%\n",
      "total_backward_count 107690 real_backward_count 13238  12.293%\n",
      "lif layer 1 self.abs_max_v: 37241.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.349862/  1.770946, val:  39.17%, val_best:  50.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.7724%\n",
      "layer   3  Sparsity: 73.6817%\n",
      "total_backward_count 117480 real_backward_count 14485  12.330%\n",
      "lif layer 1 self.abs_max_v: 37273.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.340841/  1.859870, val:  29.17%, val_best:  50.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.9785%\n",
      "layer   3  Sparsity: 72.6559%\n",
      "total_backward_count 127270 real_backward_count 15766  12.388%\n",
      "fc layer 1 self.abs_max_out: 20691.0\n",
      "lif layer 1 self.abs_max_v: 38085.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.357956/  1.816586, val:  35.83%, val_best:  50.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.2155%\n",
      "layer   3  Sparsity: 74.2879%\n",
      "total_backward_count 137060 real_backward_count 17091  12.470%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.320342/  1.853584, val:  34.17%, val_best:  50.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.2445%\n",
      "layer   3  Sparsity: 71.7347%\n",
      "total_backward_count 146850 real_backward_count 18325  12.479%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.250398/  1.690412, val:  45.83%, val_best:  50.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.1539%\n",
      "layer   3  Sparsity: 69.0104%\n",
      "total_backward_count 156640 real_backward_count 19514  12.458%\n",
      "fc layer 1 self.abs_max_out: 20931.0\n",
      "lif layer 1 self.abs_max_v: 38209.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.275064/  1.778111, val:  38.75%, val_best:  50.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.5742%\n",
      "layer   3  Sparsity: 71.8414%\n",
      "total_backward_count 166430 real_backward_count 20728  12.454%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.305455/  1.809895, val:  36.67%, val_best:  50.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6031%\n",
      "layer   3  Sparsity: 73.5615%\n",
      "total_backward_count 176220 real_backward_count 22063  12.520%\n",
      "lif layer 1 self.abs_max_v: 38440.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.273958/  1.708913, val:  41.25%, val_best:  50.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.4530%\n",
      "layer   3  Sparsity: 69.6336%\n",
      "total_backward_count 186010 real_backward_count 23347  12.551%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.257958/  1.928287, val:  30.00%, val_best:  50.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.7749%\n",
      "layer   3  Sparsity: 71.3521%\n",
      "total_backward_count 195800 real_backward_count 24594  12.561%\n",
      "fc layer 1 self.abs_max_out: 20953.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.276734/  1.840418, val:  30.42%, val_best:  50.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.2692%\n",
      "layer   3  Sparsity: 71.7528%\n",
      "total_backward_count 205590 real_backward_count 25868  12.582%\n",
      "fc layer 3 self.abs_max_out: 3040.0\n",
      "lif layer 1 self.abs_max_v: 38724.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.244114/  1.762688, val:  42.08%, val_best:  50.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.0689%\n",
      "layer   3  Sparsity: 71.8280%\n",
      "total_backward_count 215380 real_backward_count 27167  12.614%\n",
      "fc layer 2 self.abs_max_out: 6126.0\n",
      "fc layer 2 self.abs_max_out: 6347.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.308389/  1.699141, val:  51.67%, val_best:  51.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0011%\n",
      "layer   3  Sparsity: 74.1330%\n",
      "total_backward_count 225170 real_backward_count 28490  12.653%\n",
      "fc layer 3 self.abs_max_out: 3081.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.269555/  1.783432, val:  47.50%, val_best:  51.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.2660%\n",
      "layer   3  Sparsity: 74.3345%\n",
      "total_backward_count 234960 real_backward_count 29819  12.691%\n",
      "fc layer 1 self.abs_max_out: 21109.0\n",
      "lif layer 2 self.abs_max_v: 11663.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.346161/  1.678161, val:  53.75%, val_best:  53.75%, tr:  98.98%, tr_best:  99.80%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.5680%\n",
      "layer   3  Sparsity: 76.8840%\n",
      "total_backward_count 244750 real_backward_count 31156  12.730%\n",
      "lif layer 1 self.abs_max_v: 39143.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.341897/  1.692779, val:  45.42%, val_best:  53.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.8537%\n",
      "layer   3  Sparsity: 75.1529%\n",
      "total_backward_count 254540 real_backward_count 32610  12.811%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.252860/  1.646238, val:  46.25%, val_best:  53.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.6962%\n",
      "layer   3  Sparsity: 71.4209%\n",
      "total_backward_count 264330 real_backward_count 33938  12.839%\n",
      "lif layer 1 self.abs_max_v: 39233.5\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.274529/  1.742145, val:  40.83%, val_best:  53.75%, tr:  98.88%, tr_best:  99.80%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.6439%\n",
      "layer   3  Sparsity: 72.1176%\n",
      "total_backward_count 274120 real_backward_count 35290  12.874%\n",
      "fc layer 1 self.abs_max_out: 21197.0\n",
      "fc layer 1 self.abs_max_out: 21345.0\n",
      "lif layer 1 self.abs_max_v: 39689.5\n",
      "lif layer 1 self.abs_max_v: 39950.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.267095/  1.701739, val:  42.92%, val_best:  53.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.4720%\n",
      "layer   3  Sparsity: 72.0338%\n",
      "total_backward_count 283910 real_backward_count 36560  12.877%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.266897/  1.695985, val:  40.00%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.2991%\n",
      "layer   3  Sparsity: 71.8142%\n",
      "total_backward_count 293700 real_backward_count 37874  12.895%\n",
      "fc layer 3 self.abs_max_out: 3109.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.273947/  1.773211, val:  35.83%, val_best:  53.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.5116%\n",
      "layer   3  Sparsity: 71.9243%\n",
      "total_backward_count 303490 real_backward_count 39219  12.923%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.318417/  1.703128, val:  37.08%, val_best:  53.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.7995%\n",
      "layer   3  Sparsity: 73.2328%\n",
      "total_backward_count 313280 real_backward_count 40584  12.955%\n",
      "fc layer 1 self.abs_max_out: 21522.0\n",
      "fc layer 3 self.abs_max_out: 3288.0\n",
      "fc layer 3 self.abs_max_out: 3568.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.181325/  1.647361, val:  49.17%, val_best:  53.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0246%\n",
      "layer   3  Sparsity: 69.7497%\n",
      "total_backward_count 323070 real_backward_count 41852  12.954%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.250690/  1.783149, val:  30.42%, val_best:  53.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.8465%\n",
      "layer   3  Sparsity: 71.6753%\n",
      "total_backward_count 332860 real_backward_count 43212  12.982%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.264279/  1.749272, val:  42.50%, val_best:  53.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.9883%\n",
      "layer   3  Sparsity: 73.4979%\n",
      "total_backward_count 342650 real_backward_count 44471  12.979%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.306811/  1.796229, val:  31.67%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.3328%\n",
      "layer   3  Sparsity: 74.2225%\n",
      "total_backward_count 352440 real_backward_count 45809  12.998%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.276400/  1.696543, val:  52.50%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.2532%\n",
      "layer   3  Sparsity: 71.5402%\n",
      "total_backward_count 362230 real_backward_count 47098  13.002%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.156061/  1.634906, val:  51.25%, val_best:  53.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6318%\n",
      "layer   3  Sparsity: 68.7527%\n",
      "total_backward_count 372020 real_backward_count 48325  12.990%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.225488/  1.650436, val:  41.25%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.6704%\n",
      "layer   3  Sparsity: 71.2498%\n",
      "total_backward_count 381810 real_backward_count 49613  12.994%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.287150/  1.822135, val:  34.17%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.8642%\n",
      "layer   3  Sparsity: 73.3366%\n",
      "total_backward_count 391600 real_backward_count 50903  12.999%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.307526/  1.702595, val:  51.25%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.3480%\n",
      "layer   3  Sparsity: 74.4371%\n",
      "total_backward_count 401390 real_backward_count 52199  13.005%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.339508/  1.687535, val:  41.67%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.1804%\n",
      "layer   3  Sparsity: 73.7590%\n",
      "total_backward_count 411180 real_backward_count 53513  13.014%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.157263/  1.639731, val:  47.08%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.8614%\n",
      "layer   3  Sparsity: 69.4755%\n",
      "total_backward_count 420970 real_backward_count 54736  13.002%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.253908/  1.714831, val:  47.92%, val_best:  53.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0167%\n",
      "layer   3  Sparsity: 71.3709%\n",
      "total_backward_count 430760 real_backward_count 56030  13.007%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.285537/  1.730852, val:  33.33%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.9440%\n",
      "layer   3  Sparsity: 74.5732%\n",
      "total_backward_count 440550 real_backward_count 57354  13.019%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.244630/  1.705727, val:  49.58%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.8663%\n",
      "layer   3  Sparsity: 72.0354%\n",
      "total_backward_count 450340 real_backward_count 58644  13.022%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.238141/  1.820934, val:  34.58%, val_best:  53.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.3034%\n",
      "layer   3  Sparsity: 72.3251%\n",
      "total_backward_count 460130 real_backward_count 59925  13.023%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.230129/  1.721350, val:  39.17%, val_best:  53.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.2673%\n",
      "layer   3  Sparsity: 71.2181%\n",
      "total_backward_count 469920 real_backward_count 61134  13.009%\n",
      "fc layer 1 self.abs_max_out: 23524.0\n",
      "lif layer 1 self.abs_max_v: 41840.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.197538/  1.673334, val:  46.25%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.3695%\n",
      "layer   3  Sparsity: 71.1409%\n",
      "total_backward_count 479710 real_backward_count 62362  13.000%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.189619/  1.714373, val:  40.42%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.0754%\n",
      "layer   3  Sparsity: 69.6139%\n",
      "total_backward_count 489500 real_backward_count 63682  13.010%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.220143/  1.640276, val:  47.50%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6747%\n",
      "layer   3  Sparsity: 69.2333%\n",
      "total_backward_count 499290 real_backward_count 64988  13.016%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.264889/  1.745534, val:  39.17%, val_best:  53.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6153%\n",
      "layer   3  Sparsity: 74.8541%\n",
      "total_backward_count 509080 real_backward_count 66282  13.020%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.340031/  1.750695, val:  39.58%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.6998%\n",
      "layer   3  Sparsity: 74.9570%\n",
      "total_backward_count 518870 real_backward_count 67665  13.041%\n",
      "lif layer 2 self.abs_max_v: 11746.5\n",
      "lif layer 2 self.abs_max_v: 12111.5\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.288902/  1.662284, val:  50.42%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0222%\n",
      "layer   3  Sparsity: 73.4254%\n",
      "total_backward_count 528660 real_backward_count 68993  13.051%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.336781/  1.650226, val:  52.08%, val_best:  53.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.2957%\n",
      "layer   3  Sparsity: 75.7158%\n",
      "total_backward_count 538450 real_backward_count 70375  13.070%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.273542/  1.621793, val:  50.83%, val_best:  53.75%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.4635%\n",
      "layer   3  Sparsity: 72.8552%\n",
      "total_backward_count 548240 real_backward_count 71696  13.077%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.239254/  1.756143, val:  30.42%, val_best:  53.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.4062%\n",
      "layer   3  Sparsity: 71.7088%\n",
      "total_backward_count 558030 real_backward_count 73030  13.087%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.245616/  1.769355, val:  37.50%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.5882%\n",
      "layer   3  Sparsity: 72.5816%\n",
      "total_backward_count 567820 real_backward_count 74331  13.091%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.226905/  1.637051, val:  53.33%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.1360%\n",
      "layer   3  Sparsity: 72.6905%\n",
      "total_backward_count 577610 real_backward_count 75655  13.098%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.310282/  1.730988, val:  39.17%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.2413%\n",
      "layer   3  Sparsity: 74.0207%\n",
      "total_backward_count 587400 real_backward_count 77025  13.113%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.253275/  1.707201, val:  47.92%, val_best:  53.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.6475%\n",
      "layer   3  Sparsity: 73.3107%\n",
      "total_backward_count 597190 real_backward_count 78363  13.122%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.220247/  1.651961, val:  47.08%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.2031%\n",
      "layer   3  Sparsity: 71.7662%\n",
      "total_backward_count 606980 real_backward_count 79672  13.126%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.223471/  1.680931, val:  47.08%, val_best:  53.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.6911%\n",
      "layer   3  Sparsity: 71.1029%\n",
      "total_backward_count 616770 real_backward_count 80991  13.131%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.295746/  1.754455, val:  46.67%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.4227%\n",
      "layer   3  Sparsity: 73.8301%\n",
      "total_backward_count 626560 real_backward_count 82286  13.133%\n",
      "fc layer 3 self.abs_max_out: 3635.0\n",
      "fc layer 3 self.abs_max_out: 3675.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.294453/  1.738438, val:  46.67%, val_best:  53.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.1626%\n",
      "layer   3  Sparsity: 73.4838%\n",
      "total_backward_count 636350 real_backward_count 83643  13.144%\n",
      "fc layer 3 self.abs_max_out: 3735.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.272968/  1.793304, val:  40.00%, val_best:  53.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.7150%\n",
      "layer   3  Sparsity: 74.6873%\n",
      "total_backward_count 646140 real_backward_count 84974  13.151%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.306639/  1.750112, val:  39.58%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.8482%\n",
      "layer   3  Sparsity: 73.7555%\n",
      "total_backward_count 655930 real_backward_count 86332  13.162%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.178780/  1.607044, val:  45.83%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.3776%\n",
      "layer   3  Sparsity: 69.8149%\n",
      "total_backward_count 665720 real_backward_count 87608  13.160%\n",
      "fc layer 3 self.abs_max_out: 3854.0\n",
      "fc layer 3 self.abs_max_out: 3885.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.233021/  1.620658, val:  54.17%, val_best:  54.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6471%\n",
      "layer   3  Sparsity: 72.1391%\n",
      "total_backward_count 675510 real_backward_count 88919  13.163%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.148243/  1.707196, val:  49.58%, val_best:  54.17%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.0110%\n",
      "layer   3  Sparsity: 68.2894%\n",
      "total_backward_count 685300 real_backward_count 90138  13.153%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.160345/  1.602499, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.5953%\n",
      "layer   3  Sparsity: 66.8604%\n",
      "total_backward_count 695090 real_backward_count 91419  13.152%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.195091/  1.620636, val:  54.17%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.8830%\n",
      "layer   3  Sparsity: 69.4709%\n",
      "total_backward_count 704880 real_backward_count 92770  13.161%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.296549/  1.700060, val:  54.58%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.4361%\n",
      "layer   3  Sparsity: 75.8219%\n",
      "total_backward_count 714670 real_backward_count 94180  13.178%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.372946/  1.756945, val:  42.50%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.8603%\n",
      "layer   3  Sparsity: 76.6395%\n",
      "total_backward_count 724460 real_backward_count 95499  13.182%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.363176/  1.787915, val:  45.42%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.0768%\n",
      "layer   3  Sparsity: 77.3385%\n",
      "total_backward_count 734250 real_backward_count 96932  13.201%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.302435/  1.709788, val:  51.67%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.1190%\n",
      "layer   3  Sparsity: 74.4027%\n",
      "total_backward_count 744040 real_backward_count 98309  13.213%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.298400/  1.720689, val:  47.50%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6025%\n",
      "layer   3  Sparsity: 71.9014%\n",
      "total_backward_count 753830 real_backward_count 99707  13.227%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.258059/  1.810552, val:  28.33%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.9699%\n",
      "layer   3  Sparsity: 70.8249%\n",
      "total_backward_count 763620 real_backward_count 101034  13.231%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.172293/  1.757569, val:  42.50%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.3756%\n",
      "layer   3  Sparsity: 69.7771%\n",
      "total_backward_count 773410 real_backward_count 102309  13.228%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.211078/  1.713759, val:  45.42%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.8137%\n",
      "layer   3  Sparsity: 71.6258%\n",
      "total_backward_count 783200 real_backward_count 103625  13.231%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.245118/  1.688179, val:  47.92%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.0525%\n",
      "layer   3  Sparsity: 72.4719%\n",
      "total_backward_count 792990 real_backward_count 104935  13.233%\n",
      "fc layer 3 self.abs_max_out: 3899.0\n",
      "fc layer 3 self.abs_max_out: 4340.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.192279/  1.886529, val:  24.17%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.2777%\n",
      "layer   3  Sparsity: 69.6091%\n",
      "total_backward_count 802780 real_backward_count 106195  13.228%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.213940/  1.713894, val:  45.00%, val_best:  55.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.5851%\n",
      "layer   3  Sparsity: 69.7984%\n",
      "total_backward_count 812570 real_backward_count 107454  13.224%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.276105/  1.709714, val:  46.25%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.8505%\n",
      "layer   3  Sparsity: 72.6791%\n",
      "total_backward_count 822360 real_backward_count 108806  13.231%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.293650/  1.708601, val:  40.83%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.3125%\n",
      "layer   3  Sparsity: 72.7970%\n",
      "total_backward_count 832150 real_backward_count 110194  13.242%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.278785/  1.716663, val:  38.75%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.5873%\n",
      "layer   3  Sparsity: 72.3884%\n",
      "total_backward_count 841940 real_backward_count 111581  13.253%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.280190/  1.669787, val:  53.75%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.7948%\n",
      "layer   3  Sparsity: 73.1222%\n",
      "total_backward_count 851730 real_backward_count 112925  13.258%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.338403/  1.792583, val:  40.00%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0644%\n",
      "layer   3  Sparsity: 75.0847%\n",
      "total_backward_count 861520 real_backward_count 114335  13.271%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.302936/  1.732531, val:  46.67%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.2382%\n",
      "layer   3  Sparsity: 73.1956%\n",
      "total_backward_count 871310 real_backward_count 115652  13.273%\n",
      "lif layer 1 self.abs_max_v: 42008.5\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.342152/  1.726240, val:  38.33%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.4803%\n",
      "layer   3  Sparsity: 73.5251%\n",
      "total_backward_count 881100 real_backward_count 117070  13.287%\n",
      "lif layer 1 self.abs_max_v: 42029.0\n",
      "fc layer 1 self.abs_max_out: 24082.0\n",
      "lif layer 1 self.abs_max_v: 42415.0\n",
      "lif layer 1 self.abs_max_v: 42655.5\n",
      "lif layer 1 self.abs_max_v: 43430.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.191930/  1.853360, val:  37.50%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.9068%\n",
      "layer   3  Sparsity: 69.1782%\n",
      "total_backward_count 890890 real_backward_count 118323  13.281%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.223664/  1.666119, val:  52.50%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.8347%\n",
      "layer   3  Sparsity: 70.3078%\n",
      "total_backward_count 900680 real_backward_count 119612  13.280%\n",
      "fc layer 1 self.abs_max_out: 24154.0\n",
      "lif layer 1 self.abs_max_v: 43578.5\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.191966/  1.771433, val:  31.25%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.7975%\n",
      "layer   3  Sparsity: 69.8358%\n",
      "total_backward_count 910470 real_backward_count 120886  13.277%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.219351/  1.685536, val:  41.25%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.4652%\n",
      "layer   3  Sparsity: 69.8712%\n",
      "total_backward_count 920260 real_backward_count 122124  13.271%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.268261/  1.700712, val:  46.67%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.1903%\n",
      "layer   3  Sparsity: 72.9973%\n",
      "total_backward_count 930050 real_backward_count 123449  13.273%\n",
      "fc layer 1 self.abs_max_out: 24190.0\n",
      "lif layer 1 self.abs_max_v: 43667.0\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.251058/  1.670921, val:  46.67%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0984%\n",
      "layer   3  Sparsity: 71.9941%\n",
      "total_backward_count 939840 real_backward_count 124730  13.271%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.333399/  1.759823, val:  47.92%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.5670%\n",
      "layer   3  Sparsity: 75.6379%\n",
      "total_backward_count 949630 real_backward_count 126041  13.273%\n",
      "fc layer 1 self.abs_max_out: 24210.0\n",
      "lif layer 1 self.abs_max_v: 43712.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.377606/  1.718130, val:  42.92%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6636%\n",
      "layer   3  Sparsity: 75.5524%\n",
      "total_backward_count 959420 real_backward_count 127412  13.280%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.257886/  1.813802, val:  34.17%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.6188%\n",
      "layer   3  Sparsity: 71.5952%\n",
      "total_backward_count 969210 real_backward_count 128744  13.283%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.202790/  1.653397, val:  51.67%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.4820%\n",
      "layer   3  Sparsity: 70.1600%\n",
      "total_backward_count 979000 real_backward_count 130031  13.282%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.283734/  1.775328, val:  52.50%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.1344%\n",
      "layer   3  Sparsity: 75.3607%\n",
      "total_backward_count 988790 real_backward_count 131339  13.283%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.287553/  1.641051, val:  45.00%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.1141%\n",
      "layer   3  Sparsity: 73.0807%\n",
      "total_backward_count 998580 real_backward_count 132723  13.291%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.229986/  1.622642, val:  49.58%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.3792%\n",
      "layer   3  Sparsity: 72.4143%\n",
      "total_backward_count 1008370 real_backward_count 134015  13.290%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.237783/  1.707004, val:  44.58%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.2833%\n",
      "layer   3  Sparsity: 73.4724%\n",
      "total_backward_count 1018160 real_backward_count 135304  13.289%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.188357/  1.697319, val:  43.75%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.2562%\n",
      "layer   3  Sparsity: 68.7964%\n",
      "total_backward_count 1027950 real_backward_count 136544  13.283%\n",
      "fc layer 2 self.abs_max_out: 6478.0\n",
      "lif layer 2 self.abs_max_v: 12117.0\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.166735/  1.794289, val:  41.67%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.5888%\n",
      "layer   3  Sparsity: 70.7438%\n",
      "total_backward_count 1037740 real_backward_count 137837  13.282%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.301753/  1.829904, val:  34.58%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.4884%\n",
      "layer   3  Sparsity: 74.4932%\n",
      "total_backward_count 1047530 real_backward_count 139179  13.286%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.241601/  1.644446, val:  44.17%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.1213%\n",
      "layer   3  Sparsity: 72.1441%\n",
      "total_backward_count 1057320 real_backward_count 140520  13.290%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.262405/  1.795352, val:  39.58%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.8790%\n",
      "layer   3  Sparsity: 72.5509%\n",
      "total_backward_count 1067110 real_backward_count 141916  13.299%\n",
      "fc layer 1 self.abs_max_out: 24244.0\n",
      "lif layer 1 self.abs_max_v: 43810.5\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.172015/  1.661919, val:  42.08%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.2347%\n",
      "layer   3  Sparsity: 69.0093%\n",
      "total_backward_count 1076900 real_backward_count 143205  13.298%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.197532/  1.654810, val:  43.75%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.4312%\n",
      "layer   3  Sparsity: 70.0422%\n",
      "total_backward_count 1086690 real_backward_count 144571  13.304%\n",
      "lif layer 2 self.abs_max_v: 12210.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.222609/  1.655216, val:  44.58%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 74.9336%\n",
      "layer   3  Sparsity: 71.2348%\n",
      "total_backward_count 1096480 real_backward_count 145948  13.311%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.193426/  1.651274, val:  52.50%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.3035%\n",
      "layer   3  Sparsity: 70.7627%\n",
      "total_backward_count 1106270 real_backward_count 147268  13.312%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.128346/  1.648826, val:  39.17%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 75.7568%\n",
      "layer   3  Sparsity: 67.0813%\n",
      "total_backward_count 1116060 real_backward_count 148530  13.308%\n",
      "fc layer 1 self.abs_max_out: 24344.0\n",
      "lif layer 1 self.abs_max_v: 44003.0\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.105491/  1.673027, val:  39.17%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.0859%\n",
      "layer   3  Sparsity: 67.0339%\n",
      "total_backward_count 1125850 real_backward_count 149843  13.309%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.219691/  1.669996, val:  45.00%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.9167%\n",
      "layer   3  Sparsity: 71.5457%\n",
      "total_backward_count 1135640 real_backward_count 151157  13.310%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.269979/  1.697234, val:  49.58%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.6880%\n",
      "layer   3  Sparsity: 74.0891%\n",
      "total_backward_count 1145430 real_backward_count 152509  13.315%\n",
      "fc layer 1 self.abs_max_out: 26534.0\n",
      "lif layer 1 self.abs_max_v: 47303.5\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.211306/  1.703654, val:  50.00%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.4583%\n",
      "layer   3  Sparsity: 69.4601%\n",
      "total_backward_count 1155220 real_backward_count 153735  13.308%\n",
      "fc layer 1 self.abs_max_out: 26688.0\n",
      "lif layer 1 self.abs_max_v: 47596.5\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.279318/  1.734620, val:  39.58%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.6571%\n",
      "layer   3  Sparsity: 75.4373%\n",
      "total_backward_count 1165010 real_backward_count 155098  13.313%\n",
      "fc layer 2 self.abs_max_out: 6664.0\n",
      "lif layer 2 self.abs_max_v: 12586.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.323120/  1.688081, val:  49.58%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.9699%\n",
      "layer   3  Sparsity: 76.7187%\n",
      "total_backward_count 1174800 real_backward_count 156480  13.320%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.299381/  1.718306, val:  45.83%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.7727%\n",
      "layer   3  Sparsity: 74.7913%\n",
      "total_backward_count 1184590 real_backward_count 157805  13.321%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.215147/  1.780195, val:  37.08%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.5913%\n",
      "layer   3  Sparsity: 68.6603%\n",
      "total_backward_count 1194380 real_backward_count 159062  13.318%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.208426/  1.645352, val:  39.58%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.2329%\n",
      "layer   3  Sparsity: 70.1437%\n",
      "total_backward_count 1204170 real_backward_count 160317  13.313%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.264807/  1.624617, val:  53.75%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.7710%\n",
      "layer   3  Sparsity: 71.2558%\n",
      "total_backward_count 1213960 real_backward_count 161710  13.321%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.263876/  1.723529, val:  54.17%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.0313%\n",
      "layer   3  Sparsity: 72.7431%\n",
      "total_backward_count 1223750 real_backward_count 163139  13.331%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.231171/  1.755858, val:  29.17%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.4927%\n",
      "layer   3  Sparsity: 69.5925%\n",
      "total_backward_count 1233540 real_backward_count 164471  13.333%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.255908/  1.764610, val:  32.92%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 76.4814%\n",
      "layer   3  Sparsity: 73.4432%\n",
      "total_backward_count 1243330 real_backward_count 165801  13.335%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.258611/  1.699478, val:  47.08%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.8634%\n",
      "layer   3  Sparsity: 73.5214%\n",
      "total_backward_count 1253120 real_backward_count 167165  13.340%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.269091/  1.800587, val:  41.67%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.1123%\n",
      "layer   3  Sparsity: 73.5576%\n",
      "total_backward_count 1262910 real_backward_count 168590  13.349%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.294502/  1.699693, val:  42.08%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.5666%\n",
      "layer   3  Sparsity: 73.3564%\n",
      "total_backward_count 1272700 real_backward_count 169907  13.350%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.280726/  1.831900, val:  36.67%, val_best:  55.00%, tr:  98.67%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.7836%\n",
      "layer   3  Sparsity: 72.8897%\n",
      "total_backward_count 1282490 real_backward_count 171299  13.357%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.318605/  1.729717, val:  45.83%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.1445%\n",
      "layer   3  Sparsity: 73.1644%\n",
      "total_backward_count 1292280 real_backward_count 172661  13.361%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.296760/  1.707527, val:  49.58%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.0977%\n",
      "layer   3  Sparsity: 72.3878%\n",
      "total_backward_count 1302070 real_backward_count 174001  13.363%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.282058/  1.811737, val:  42.08%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.8521%\n",
      "layer   3  Sparsity: 72.4336%\n",
      "total_backward_count 1311860 real_backward_count 175306  13.363%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.383491/  1.800595, val:  42.08%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.6447%\n",
      "layer   3  Sparsity: 76.0320%\n",
      "total_backward_count 1321650 real_backward_count 176686  13.369%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.379544/  1.761106, val:  54.58%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.2162%\n",
      "layer   3  Sparsity: 76.0031%\n",
      "total_backward_count 1331440 real_backward_count 178088  13.376%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.448621/  1.853420, val:  47.08%, val_best:  55.00%, tr:  98.57%, tr_best:  99.90%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.0159%\n",
      "layer   3  Sparsity: 79.0902%\n",
      "total_backward_count 1341230 real_backward_count 179478  13.382%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.429892/  1.745410, val:  44.17%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.9797%\n",
      "layer   3  Sparsity: 77.0045%\n",
      "total_backward_count 1351020 real_backward_count 180891  13.389%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.426048/  1.733282, val:  38.75%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.6403%\n",
      "layer   3  Sparsity: 76.6761%\n",
      "total_backward_count 1360810 real_backward_count 182370  13.402%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.267952/  1.667599, val:  49.58%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.9630%\n",
      "layer   3  Sparsity: 72.7736%\n",
      "total_backward_count 1370600 real_backward_count 183649  13.399%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.298139/  1.781226, val:  37.08%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.6583%\n",
      "layer   3  Sparsity: 74.4354%\n",
      "total_backward_count 1380390 real_backward_count 185014  13.403%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.473457/  1.819062, val:  46.67%, val_best:  55.00%, tr:  98.57%, tr_best:  99.90%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.3259%\n",
      "layer   3  Sparsity: 79.3639%\n",
      "total_backward_count 1390180 real_backward_count 186553  13.419%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.385518/  1.774082, val:  42.92%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.0512%\n",
      "layer   3  Sparsity: 76.3093%\n",
      "total_backward_count 1399970 real_backward_count 187955  13.426%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.331355/  1.764862, val:  39.58%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.7293%\n",
      "layer   3  Sparsity: 73.7768%\n",
      "total_backward_count 1409760 real_backward_count 189378  13.433%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.377837/  1.782731, val:  46.67%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.1030%\n",
      "layer   3  Sparsity: 75.9872%\n",
      "total_backward_count 1419550 real_backward_count 190858  13.445%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.318544/  1.724728, val:  40.83%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.5432%\n",
      "layer   3  Sparsity: 73.0088%\n",
      "total_backward_count 1429340 real_backward_count 192251  13.450%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.386657/  1.760821, val:  43.33%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.2497%\n",
      "layer   3  Sparsity: 77.4217%\n",
      "total_backward_count 1439130 real_backward_count 193745  13.463%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.403639/  1.856238, val:  33.33%, val_best:  55.00%, tr:  98.77%, tr_best:  99.90%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.7305%\n",
      "layer   3  Sparsity: 76.9258%\n",
      "total_backward_count 1448920 real_backward_count 195223  13.474%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.390332/  1.704146, val:  45.83%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.0950%\n",
      "layer   3  Sparsity: 75.4212%\n",
      "total_backward_count 1458710 real_backward_count 196646  13.481%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.379612/  1.688346, val:  47.92%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.0056%\n",
      "layer   3  Sparsity: 75.1616%\n",
      "total_backward_count 1468500 real_backward_count 198112  13.491%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.350794/  1.850262, val:  37.50%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.3403%\n",
      "layer   3  Sparsity: 76.7573%\n",
      "total_backward_count 1478290 real_backward_count 199515  13.496%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.409291/  1.722695, val:  47.50%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.1143%\n",
      "layer   3  Sparsity: 76.4968%\n",
      "total_backward_count 1488080 real_backward_count 200975  13.506%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.442425/  1.752674, val:  53.75%, val_best:  55.00%, tr:  98.67%, tr_best:  99.90%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.6431%\n",
      "layer   3  Sparsity: 77.8801%\n",
      "total_backward_count 1497870 real_backward_count 202446  13.516%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.374154/  1.806678, val:  42.92%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.4825%\n",
      "layer   3  Sparsity: 75.7814%\n",
      "total_backward_count 1507660 real_backward_count 203878  13.523%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.449939/  1.780076, val:  50.42%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.7340%\n",
      "layer   3  Sparsity: 78.8811%\n",
      "total_backward_count 1517450 real_backward_count 205324  13.531%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.391981/  1.801967, val:  47.50%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.4885%\n",
      "layer   3  Sparsity: 76.2142%\n",
      "total_backward_count 1527240 real_backward_count 206732  13.536%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.380842/  1.881716, val:  34.17%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.8013%\n",
      "layer   3  Sparsity: 75.8880%\n",
      "total_backward_count 1537030 real_backward_count 208079  13.538%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.273289/  1.673471, val:  51.67%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.4573%\n",
      "layer   3  Sparsity: 73.4747%\n",
      "total_backward_count 1546820 real_backward_count 209396  13.537%\n",
      "fc layer 3 self.abs_max_out: 4388.0\n",
      "fc layer 3 self.abs_max_out: 4411.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.278541/  1.763894, val:  37.50%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.3920%\n",
      "layer   3  Sparsity: 73.2747%\n",
      "total_backward_count 1556610 real_backward_count 210809  13.543%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.325313/  1.678046, val:  45.83%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.7797%\n",
      "layer   3  Sparsity: 73.6672%\n",
      "total_backward_count 1566400 real_backward_count 212196  13.547%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.293611/  1.693985, val:  55.00%, val_best:  55.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.1913%\n",
      "layer   3  Sparsity: 72.9977%\n",
      "total_backward_count 1576190 real_backward_count 213577  13.550%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.217672/  1.724087, val:  51.67%, val_best:  55.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.8041%\n",
      "layer   3  Sparsity: 71.3989%\n",
      "total_backward_count 1585980 real_backward_count 214923  13.551%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.232233/  1.766816, val:  37.08%, val_best:  55.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.5506%\n",
      "layer   3  Sparsity: 71.6775%\n",
      "total_backward_count 1595770 real_backward_count 216237  13.551%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.317747/  1.736186, val:  44.17%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.6575%\n",
      "layer   3  Sparsity: 76.6867%\n",
      "total_backward_count 1605560 real_backward_count 217596  13.553%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.381676/  1.856201, val:  35.83%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.5444%\n",
      "layer   3  Sparsity: 76.3215%\n",
      "total_backward_count 1615350 real_backward_count 219081  13.562%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.373893/  1.695647, val:  48.33%, val_best:  55.00%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.4089%\n",
      "layer   3  Sparsity: 75.0677%\n",
      "total_backward_count 1625140 real_backward_count 220592  13.574%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.330071/  1.781114, val:  38.75%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.5780%\n",
      "layer   3  Sparsity: 74.8329%\n",
      "total_backward_count 1634930 real_backward_count 221989  13.578%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.346251/  1.748433, val:  40.42%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.6305%\n",
      "layer   3  Sparsity: 76.6761%\n",
      "total_backward_count 1644720 real_backward_count 223417  13.584%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.413926/  1.861023, val:  39.58%, val_best:  55.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.1498%\n",
      "layer   3  Sparsity: 78.8126%\n",
      "total_backward_count 1654510 real_backward_count 224897  13.593%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.468542/  1.822116, val:  46.25%, val_best:  55.00%, tr:  98.57%, tr_best:  99.90%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.5458%\n",
      "layer   3  Sparsity: 78.5225%\n",
      "total_backward_count 1664300 real_backward_count 226354  13.601%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.445717/  1.804399, val:  45.83%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.7903%\n",
      "layer   3  Sparsity: 78.1517%\n",
      "total_backward_count 1674090 real_backward_count 227814  13.608%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.469459/  1.891641, val:  33.75%, val_best:  55.00%, tr:  98.77%, tr_best:  99.90%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 77.8166%\n",
      "layer   3  Sparsity: 79.9055%\n",
      "total_backward_count 1683880 real_backward_count 229319  13.618%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.471538/  1.830224, val:  53.33%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.0322%\n",
      "layer   3  Sparsity: 79.8183%\n",
      "total_backward_count 1693670 real_backward_count 230789  13.627%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.511380/  1.911692, val:  39.58%, val_best:  55.00%, tr:  98.67%, tr_best:  99.90%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.9372%\n",
      "layer   3  Sparsity: 81.1102%\n",
      "total_backward_count 1703460 real_backward_count 232327  13.639%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.601729/  1.870318, val:  47.92%, val_best:  55.00%, tr:  98.37%, tr_best:  99.90%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.8542%\n",
      "layer   3  Sparsity: 82.5129%\n",
      "total_backward_count 1713250 real_backward_count 233914  13.653%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.630283/  1.869359, val:  48.33%, val_best:  55.00%, tr:  97.85%, tr_best:  99.90%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.2295%\n",
      "layer   3  Sparsity: 83.3309%\n",
      "total_backward_count 1723040 real_backward_count 235501  13.668%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.701797/  1.954221, val:  44.17%, val_best:  55.00%, tr:  97.96%, tr_best:  99.90%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 81.7759%\n",
      "layer   3  Sparsity: 85.9729%\n",
      "total_backward_count 1732830 real_backward_count 237140  13.685%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.677977/  1.965967, val:  44.58%, val_best:  55.00%, tr:  97.85%, tr_best:  99.90%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 81.4521%\n",
      "layer   3  Sparsity: 84.1180%\n",
      "total_backward_count 1742620 real_backward_count 238726  13.699%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.597195/  1.890936, val:  42.50%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.8655%\n",
      "layer   3  Sparsity: 81.9960%\n",
      "total_backward_count 1752410 real_backward_count 240183  13.706%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.487615/  1.830322, val:  42.50%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.3280%\n",
      "layer   3  Sparsity: 78.7485%\n",
      "total_backward_count 1762200 real_backward_count 241654  13.713%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.493652/  1.844690, val:  49.17%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.5441%\n",
      "layer   3  Sparsity: 79.1612%\n",
      "total_backward_count 1771990 real_backward_count 243108  13.719%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.461645/  1.800402, val:  47.50%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.3665%\n",
      "layer   3  Sparsity: 78.0970%\n",
      "total_backward_count 1781780 real_backward_count 244468  13.720%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.419187/  1.852346, val:  45.83%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.4944%\n",
      "layer   3  Sparsity: 77.2931%\n",
      "total_backward_count 1791570 real_backward_count 245842  13.722%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.504990/  1.853505, val:  41.67%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.3515%\n",
      "layer   3  Sparsity: 81.1071%\n",
      "total_backward_count 1801360 real_backward_count 247263  13.726%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.594231/  1.910624, val:  40.00%, val_best:  55.00%, tr:  98.47%, tr_best:  99.90%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.6776%\n",
      "layer   3  Sparsity: 83.3715%\n",
      "total_backward_count 1811150 real_backward_count 248766  13.735%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.649587/  1.949953, val:  40.42%, val_best:  55.00%, tr:  98.47%, tr_best:  99.90%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 82.2494%\n",
      "layer   3  Sparsity: 84.4078%\n",
      "total_backward_count 1820940 real_backward_count 250442  13.753%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.604683/  1.906295, val:  51.25%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 81.2658%\n",
      "layer   3  Sparsity: 82.3826%\n",
      "total_backward_count 1830730 real_backward_count 251946  13.762%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.655496/  1.954231, val:  35.83%, val_best:  55.00%, tr:  98.26%, tr_best:  99.90%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.2689%\n",
      "layer   3  Sparsity: 83.1130%\n",
      "total_backward_count 1840520 real_backward_count 253546  13.776%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.557485/  1.834031, val:  54.17%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.0779%\n",
      "layer   3  Sparsity: 81.7412%\n",
      "total_backward_count 1850310 real_backward_count 255043  13.784%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.599890/  1.932949, val:  44.58%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.9268%\n",
      "layer   3  Sparsity: 83.1788%\n",
      "total_backward_count 1860100 real_backward_count 256515  13.790%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.569564/  1.786682, val:  54.58%, val_best:  55.00%, tr:  98.57%, tr_best:  99.90%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.7045%\n",
      "layer   3  Sparsity: 81.9370%\n",
      "total_backward_count 1869890 real_backward_count 258049  13.800%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.586107/  1.824663, val:  40.00%, val_best:  55.00%, tr:  98.57%, tr_best:  99.90%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.8732%\n",
      "layer   3  Sparsity: 81.2696%\n",
      "total_backward_count 1879680 real_backward_count 259638  13.813%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.518371/  1.886803, val:  45.00%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 78.6242%\n",
      "layer   3  Sparsity: 81.4792%\n",
      "total_backward_count 1889470 real_backward_count 261158  13.822%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.526838/  1.870561, val:  38.33%, val_best:  55.00%, tr:  98.77%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.2162%\n",
      "layer   3  Sparsity: 80.3637%\n",
      "total_backward_count 1899260 real_backward_count 262608  13.827%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.458027/  1.820445, val:  40.83%, val_best:  55.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.7783%\n",
      "layer   3  Sparsity: 78.6829%\n",
      "total_backward_count 1909050 real_backward_count 264006  13.829%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.486716/  1.835003, val:  44.17%, val_best:  55.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.5584%\n",
      "layer   3  Sparsity: 78.4684%\n",
      "total_backward_count 1918840 real_backward_count 265443  13.834%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.478048/  1.762617, val:  48.75%, val_best:  55.00%, tr:  98.98%, tr_best:  99.90%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 79.9553%\n",
      "layer   3  Sparsity: 78.3937%\n",
      "total_backward_count 1928630 real_backward_count 266920  13.840%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.474292/  1.814083, val:  45.42%, val_best:  55.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.3285%\n",
      "layer   3  Sparsity: 78.4822%\n",
      "total_backward_count 1938420 real_backward_count 268356  13.844%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.497761/  1.905149, val:  33.75%, val_best:  55.00%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.0672%\n",
      "layer   3  Sparsity: 79.6459%\n",
      "total_backward_count 1948210 real_backward_count 269783  13.848%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.510305/  1.888520, val:  52.50%, val_best:  55.00%, tr:  98.77%, tr_best:  99.90%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.4491%\n",
      "layer   2  Sparsity: 80.9543%\n",
      "layer   3  Sparsity: 80.5232%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7a1aa7052e42dfab3e092c248c6b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÜ‚ñÇ‚ñÑ‚ñÑ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98774</td></tr><tr><td>tr_epoch_loss</td><td>1.51031</td></tr><tr><td>val_acc_best</td><td>0.55</td></tr><tr><td>val_acc_now</td><td>0.525</td></tr><tr><td>val_loss</td><td>1.88852</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-68</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b1rpnoqf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b1rpnoqf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_094338-b1rpnoqf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 64t53hx0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_140524-64t53hx0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/64t53hx0' target=\"_blank\">floral-sweep-74</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/64t53hx0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/64t53hx0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251115_140532_727', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 146.0\n",
      "lif layer 1 self.abs_max_v: 146.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 235.0\n",
      "lif layer 2 self.abs_max_v: 235.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 1 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 207.5\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 508.5\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 201.0\n",
      "lif layer 1 self.abs_max_v: 263.5\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "lif layer 2 self.abs_max_v: 595.5\n",
      "fc layer 1 self.abs_max_out: 221.0\n",
      "lif layer 1 self.abs_max_v: 296.0\n",
      "fc layer 2 self.abs_max_out: 446.0\n",
      "fc layer 1 self.abs_max_out: 293.0\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "fc layer 1 self.abs_max_out: 322.0\n",
      "lif layer 1 self.abs_max_v: 322.0\n",
      "fc layer 1 self.abs_max_out: 398.0\n",
      "lif layer 1 self.abs_max_v: 398.0\n",
      "lif layer 2 self.abs_max_v: 655.0\n",
      "fc layer 3 self.abs_max_out: 232.0\n",
      "fc layer 1 self.abs_max_out: 399.0\n",
      "lif layer 1 self.abs_max_v: 399.0\n",
      "lif layer 2 self.abs_max_v: 741.5\n",
      "fc layer 2 self.abs_max_out: 484.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "fc layer 1 self.abs_max_out: 567.0\n",
      "lif layer 1 self.abs_max_v: 567.0\n",
      "fc layer 1 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "fc layer 1 self.abs_max_out: 585.0\n",
      "lif layer 1 self.abs_max_v: 585.0\n",
      "lif layer 2 self.abs_max_v: 800.5\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "lif layer 1 self.abs_max_v: 671.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 2 self.abs_max_out: 573.0\n",
      "fc layer 2 self.abs_max_out: 581.0\n",
      "lif layer 2 self.abs_max_v: 808.5\n",
      "fc layer 2 self.abs_max_out: 625.0\n",
      "lif layer 2 self.abs_max_v: 864.0\n",
      "lif layer 2 self.abs_max_v: 885.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "fc layer 1 self.abs_max_out: 740.0\n",
      "lif layer 1 self.abs_max_v: 740.0\n",
      "lif layer 2 self.abs_max_v: 913.5\n",
      "fc layer 1 self.abs_max_out: 782.0\n",
      "lif layer 1 self.abs_max_v: 782.0\n",
      "lif layer 2 self.abs_max_v: 1005.0\n",
      "lif layer 2 self.abs_max_v: 1094.0\n",
      "lif layer 1 self.abs_max_v: 785.0\n",
      "lif layer 1 self.abs_max_v: 1007.5\n",
      "fc layer 1 self.abs_max_out: 872.0\n",
      "lif layer 1 self.abs_max_v: 1318.0\n",
      "fc layer 2 self.abs_max_out: 653.0\n",
      "lif layer 2 self.abs_max_v: 1144.0\n",
      "lif layer 1 self.abs_max_v: 1445.0\n",
      "lif layer 1 self.abs_max_v: 1478.5\n",
      "fc layer 2 self.abs_max_out: 655.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "fc layer 2 self.abs_max_out: 792.0\n",
      "fc layer 1 self.abs_max_out: 1009.0\n",
      "fc layer 1 self.abs_max_out: 1114.0\n",
      "lif layer 2 self.abs_max_v: 1161.5\n",
      "lif layer 1 self.abs_max_v: 1605.0\n",
      "fc layer 1 self.abs_max_out: 1125.0\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "lif layer 1 self.abs_max_v: 1747.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "lif layer 2 self.abs_max_v: 1192.5\n",
      "lif layer 2 self.abs_max_v: 1213.0\n",
      "lif layer 2 self.abs_max_v: 1235.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "lif layer 2 self.abs_max_v: 1245.0\n",
      "lif layer 2 self.abs_max_v: 1273.5\n",
      "lif layer 2 self.abs_max_v: 1366.5\n",
      "lif layer 2 self.abs_max_v: 1387.5\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "lif layer 2 self.abs_max_v: 1423.0\n",
      "lif layer 2 self.abs_max_v: 1491.5\n",
      "fc layer 2 self.abs_max_out: 892.0\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "fc layer 2 self.abs_max_out: 935.0\n",
      "lif layer 2 self.abs_max_v: 1552.0\n",
      "lif layer 2 self.abs_max_v: 1574.5\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 2 self.abs_max_out: 994.0\n",
      "fc layer 2 self.abs_max_out: 1002.0\n",
      "fc layer 1 self.abs_max_out: 1200.0\n",
      "fc layer 2 self.abs_max_out: 1109.0\n",
      "fc layer 1 self.abs_max_out: 1242.0\n",
      "lif layer 1 self.abs_max_v: 1906.5\n",
      "fc layer 1 self.abs_max_out: 1249.0\n",
      "lif layer 2 self.abs_max_v: 1598.5\n",
      "lif layer 2 self.abs_max_v: 1600.5\n",
      "lif layer 2 self.abs_max_v: 1681.5\n",
      "lif layer 2 self.abs_max_v: 1707.0\n",
      "lif layer 2 self.abs_max_v: 1715.5\n",
      "lif layer 2 self.abs_max_v: 1767.0\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 1 self.abs_max_out: 1404.0\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 1 self.abs_max_out: 1422.0\n",
      "fc layer 1 self.abs_max_out: 1473.0\n",
      "fc layer 1 self.abs_max_out: 1493.0\n",
      "lif layer 1 self.abs_max_v: 1989.5\n",
      "fc layer 1 self.abs_max_out: 1654.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "lif layer 1 self.abs_max_v: 2125.0\n",
      "lif layer 1 self.abs_max_v: 2314.0\n",
      "lif layer 1 self.abs_max_v: 2404.5\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "lif layer 2 self.abs_max_v: 1830.5\n",
      "lif layer 2 self.abs_max_v: 1912.0\n",
      "lif layer 2 self.abs_max_v: 1933.5\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "lif layer 1 self.abs_max_v: 2442.5\n",
      "lif layer 2 self.abs_max_v: 1953.5\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "fc layer 2 self.abs_max_out: 1303.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "lif layer 1 self.abs_max_v: 2467.0\n",
      "lif layer 1 self.abs_max_v: 2533.5\n",
      "lif layer 1 self.abs_max_v: 2534.5\n",
      "fc layer 1 self.abs_max_out: 1684.0\n",
      "fc layer 1 self.abs_max_out: 1708.0\n",
      "lif layer 1 self.abs_max_v: 2693.5\n",
      "lif layer 2 self.abs_max_v: 1979.5\n",
      "lif layer 2 self.abs_max_v: 1985.5\n",
      "lif layer 2 self.abs_max_v: 2071.5\n",
      "lif layer 2 self.abs_max_v: 2088.0\n",
      "lif layer 2 self.abs_max_v: 2096.0\n",
      "lif layer 2 self.abs_max_v: 2138.0\n",
      "fc layer 1 self.abs_max_out: 1916.0\n",
      "lif layer 1 self.abs_max_v: 2800.0\n",
      "fc layer 1 self.abs_max_out: 1952.0\n",
      "lif layer 1 self.abs_max_v: 3306.0\n",
      "lif layer 1 self.abs_max_v: 3518.0\n",
      "lif layer 1 self.abs_max_v: 3574.0\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "lif layer 2 self.abs_max_v: 2151.5\n",
      "lif layer 2 self.abs_max_v: 2158.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.411437/  1.879582, val:  32.50%, val_best:  32.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9324%\n",
      "layer   2  Sparsity: 71.9842%\n",
      "layer   3  Sparsity: 65.5858%\n",
      "total_backward_count 9790 real_backward_count 1441  14.719%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2204.5\n",
      "lif layer 2 self.abs_max_v: 2220.0\n",
      "fc layer 1 self.abs_max_out: 1991.0\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "fc layer 1 self.abs_max_out: 2007.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "lif layer 2 self.abs_max_v: 2222.5\n",
      "fc layer 1 self.abs_max_out: 2030.0\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "fc layer 1 self.abs_max_out: 2073.0\n",
      "lif layer 1 self.abs_max_v: 3587.0\n",
      "fc layer 1 self.abs_max_out: 2091.0\n",
      "fc layer 1 self.abs_max_out: 2104.0\n",
      "lif layer 2 self.abs_max_v: 2323.0\n",
      "fc layer 1 self.abs_max_out: 2117.0\n",
      "fc layer 1 self.abs_max_out: 2403.0\n",
      "fc layer 1 self.abs_max_out: 2418.0\n",
      "lif layer 2 self.abs_max_v: 2346.5\n",
      "lif layer 1 self.abs_max_v: 3764.0\n",
      "lif layer 2 self.abs_max_v: 2372.5\n",
      "fc layer 1 self.abs_max_out: 2765.0\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "lif layer 1 self.abs_max_v: 4488.0\n",
      "lif layer 1 self.abs_max_v: 4736.0\n",
      "lif layer 1 self.abs_max_v: 4848.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.282942/  1.772093, val:  35.42%, val_best:  35.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9304%\n",
      "layer   2  Sparsity: 74.8594%\n",
      "layer   3  Sparsity: 66.4642%\n",
      "total_backward_count 19580 real_backward_count 2744  14.014%\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "lif layer 2 self.abs_max_v: 2465.0\n",
      "fc layer 2 self.abs_max_out: 1479.0\n",
      "lif layer 2 self.abs_max_v: 2639.0\n",
      "fc layer 2 self.abs_max_out: 1485.0\n",
      "lif layer 2 self.abs_max_v: 2710.5\n",
      "lif layer 2 self.abs_max_v: 2738.0\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "lif layer 2 self.abs_max_v: 2764.0\n",
      "fc layer 3 self.abs_max_out: 855.0\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "lif layer 2 self.abs_max_v: 2765.5\n",
      "lif layer 2 self.abs_max_v: 2791.0\n",
      "lif layer 2 self.abs_max_v: 2843.0\n",
      "lif layer 2 self.abs_max_v: 2843.5\n",
      "lif layer 2 self.abs_max_v: 2876.5\n",
      "lif layer 2 self.abs_max_v: 2978.5\n",
      "fc layer 2 self.abs_max_out: 1590.0\n",
      "lif layer 2 self.abs_max_v: 3079.5\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "lif layer 1 self.abs_max_v: 5037.5\n",
      "fc layer 1 self.abs_max_out: 2889.0\n",
      "lif layer 1 self.abs_max_v: 5398.5\n",
      "fc layer 1 self.abs_max_out: 2929.0\n",
      "lif layer 1 self.abs_max_v: 5628.5\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "lif layer 1 self.abs_max_v: 5849.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.237294/  1.701757, val:  47.92%, val_best:  47.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9299%\n",
      "layer   2  Sparsity: 76.2749%\n",
      "layer   3  Sparsity: 68.3000%\n",
      "total_backward_count 29370 real_backward_count 4092  13.933%\n",
      "fc layer 1 self.abs_max_out: 3147.0\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "lif layer 1 self.abs_max_v: 6115.5\n",
      "lif layer 1 self.abs_max_v: 6257.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.233707/  1.721659, val:  42.92%, val_best:  47.92%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9375%\n",
      "layer   2  Sparsity: 75.9516%\n",
      "layer   3  Sparsity: 69.3654%\n",
      "total_backward_count 39160 real_backward_count 5433  13.874%\n",
      "fc layer 2 self.abs_max_out: 1670.0\n",
      "fc layer 3 self.abs_max_out: 857.0\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "fc layer 2 self.abs_max_out: 1721.0\n",
      "fc layer 2 self.abs_max_out: 1738.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.237351/  1.698680, val:  44.58%, val_best:  47.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9350%\n",
      "layer   2  Sparsity: 76.3622%\n",
      "layer   3  Sparsity: 70.8940%\n",
      "total_backward_count 48950 real_backward_count 6707  13.702%\n",
      "fc layer 1 self.abs_max_out: 3370.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.189252/  1.688617, val:  48.75%, val_best:  48.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9247%\n",
      "layer   2  Sparsity: 77.1411%\n",
      "layer   3  Sparsity: 70.6235%\n",
      "total_backward_count 58740 real_backward_count 7952  13.538%\n",
      "fc layer 3 self.abs_max_out: 910.0\n",
      "fc layer 1 self.abs_max_out: 3420.0\n",
      "fc layer 1 self.abs_max_out: 3497.0\n",
      "lif layer 1 self.abs_max_v: 6287.0\n",
      "lif layer 1 self.abs_max_v: 6331.5\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.197497/  1.640740, val:  57.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9252%\n",
      "layer   2  Sparsity: 76.8835%\n",
      "layer   3  Sparsity: 71.7351%\n",
      "total_backward_count 68530 real_backward_count 9164  13.372%\n",
      "lif layer 2 self.abs_max_v: 3087.5\n",
      "lif layer 2 self.abs_max_v: 3116.0\n",
      "fc layer 1 self.abs_max_out: 3663.0\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.187140/  1.628197, val:  47.92%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9299%\n",
      "layer   2  Sparsity: 76.9392%\n",
      "layer   3  Sparsity: 72.7807%\n",
      "total_backward_count 78320 real_backward_count 10333  13.193%\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "lif layer 2 self.abs_max_v: 3223.5\n",
      "fc layer 3 self.abs_max_out: 915.0\n",
      "fc layer 2 self.abs_max_out: 1887.0\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 1 self.abs_max_out: 3770.0\n",
      "lif layer 1 self.abs_max_v: 6849.0\n",
      "fc layer 1 self.abs_max_out: 4032.0\n",
      "lif layer 1 self.abs_max_v: 7456.5\n",
      "lif layer 1 self.abs_max_v: 7702.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.147824/  1.539318, val:  57.08%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9330%\n",
      "layer   2  Sparsity: 77.4770%\n",
      "layer   3  Sparsity: 72.6671%\n",
      "total_backward_count 88110 real_backward_count 11598  13.163%\n",
      "lif layer 2 self.abs_max_v: 3237.0\n",
      "lif layer 2 self.abs_max_v: 3259.5\n",
      "lif layer 2 self.abs_max_v: 3468.0\n",
      "fc layer 2 self.abs_max_out: 1920.0\n",
      "fc layer 1 self.abs_max_out: 4292.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.143200/  1.645938, val:  44.58%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9371%\n",
      "layer   2  Sparsity: 77.4201%\n",
      "layer   3  Sparsity: 72.4933%\n",
      "total_backward_count 97900 real_backward_count 12711  12.984%\n",
      "lif layer 1 self.abs_max_v: 7898.5\n",
      "fc layer 1 self.abs_max_out: 4311.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.100705/  1.640165, val:  42.92%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9412%\n",
      "layer   2  Sparsity: 77.6472%\n",
      "layer   3  Sparsity: 72.4938%\n",
      "total_backward_count 107690 real_backward_count 13844  12.855%\n",
      "fc layer 2 self.abs_max_out: 1965.0\n",
      "fc layer 2 self.abs_max_out: 2193.0\n",
      "lif layer 1 self.abs_max_v: 7982.0\n",
      "lif layer 1 self.abs_max_v: 8209.0\n",
      "fc layer 1 self.abs_max_out: 4312.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.107849/  1.564515, val:  49.58%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9417%\n",
      "layer   2  Sparsity: 77.6589%\n",
      "layer   3  Sparsity: 73.1280%\n",
      "total_backward_count 117480 real_backward_count 15038  12.800%\n",
      "fc layer 3 self.abs_max_out: 957.0\n",
      "fc layer 1 self.abs_max_out: 4457.0\n",
      "lif layer 1 self.abs_max_v: 8446.0\n",
      "lif layer 1 self.abs_max_v: 8607.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.064792/  1.471812, val:  56.67%, val_best:  57.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9372%\n",
      "layer   2  Sparsity: 77.4775%\n",
      "layer   3  Sparsity: 73.2913%\n",
      "total_backward_count 127270 real_backward_count 16125  12.670%\n",
      "fc layer 3 self.abs_max_out: 973.0\n",
      "lif layer 2 self.abs_max_v: 3472.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.075550/  1.596804, val:  42.92%, val_best:  57.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9344%\n",
      "layer   2  Sparsity: 77.1153%\n",
      "layer   3  Sparsity: 73.8854%\n",
      "total_backward_count 137060 real_backward_count 17222  12.565%\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "lif layer 2 self.abs_max_v: 3543.0\n",
      "fc layer 3 self.abs_max_out: 1011.0\n",
      "fc layer 1 self.abs_max_out: 4500.0\n",
      "lif layer 1 self.abs_max_v: 8668.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.065318/  1.507802, val:  52.08%, val_best:  57.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9301%\n",
      "layer   2  Sparsity: 76.7500%\n",
      "layer   3  Sparsity: 74.1357%\n",
      "total_backward_count 146850 real_backward_count 18279  12.447%\n",
      "fc layer 3 self.abs_max_out: 1029.0\n",
      "fc layer 1 self.abs_max_out: 4574.0\n",
      "lif layer 1 self.abs_max_v: 8773.5\n",
      "fc layer 1 self.abs_max_out: 4589.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.080357/  1.487083, val:  55.00%, val_best:  57.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9335%\n",
      "layer   2  Sparsity: 76.6294%\n",
      "layer   3  Sparsity: 75.3670%\n",
      "total_backward_count 156640 real_backward_count 19392  12.380%\n",
      "fc layer 1 self.abs_max_out: 4645.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.082973/  1.455387, val:  59.17%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9148%\n",
      "layer   2  Sparsity: 76.4513%\n",
      "layer   3  Sparsity: 75.9495%\n",
      "total_backward_count 166430 real_backward_count 20469  12.299%\n",
      "fc layer 3 self.abs_max_out: 1049.0\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "fc layer 2 self.abs_max_out: 2300.0\n",
      "fc layer 1 self.abs_max_out: 4844.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.061767/  1.448320, val:  60.83%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9297%\n",
      "layer   2  Sparsity: 76.6459%\n",
      "layer   3  Sparsity: 75.8651%\n",
      "total_backward_count 176220 real_backward_count 21520  12.212%\n",
      "fc layer 1 self.abs_max_out: 4976.0\n",
      "lif layer 1 self.abs_max_v: 9049.5\n",
      "fc layer 1 self.abs_max_out: 5081.0\n",
      "lif layer 1 self.abs_max_v: 9606.0\n",
      "lif layer 1 self.abs_max_v: 9773.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.044795/  1.381210, val:  63.75%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9313%\n",
      "layer   2  Sparsity: 75.9050%\n",
      "layer   3  Sparsity: 75.2184%\n",
      "total_backward_count 186010 real_backward_count 22600  12.150%\n",
      "fc layer 1 self.abs_max_out: 5304.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.003999/  1.518274, val:  45.00%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9375%\n",
      "layer   2  Sparsity: 76.3309%\n",
      "layer   3  Sparsity: 74.1790%\n",
      "total_backward_count 195800 real_backward_count 23657  12.082%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.991963/  1.494812, val:  54.17%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9213%\n",
      "layer   2  Sparsity: 75.9245%\n",
      "layer   3  Sparsity: 74.6681%\n",
      "total_backward_count 205590 real_backward_count 24628  11.979%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.982760/  1.373159, val:  66.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9518%\n",
      "layer   2  Sparsity: 76.0020%\n",
      "layer   3  Sparsity: 74.0497%\n",
      "total_backward_count 215380 real_backward_count 25687  11.926%\n",
      "fc layer 3 self.abs_max_out: 1064.0\n",
      "fc layer 3 self.abs_max_out: 1092.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.989043/  1.335644, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9485%\n",
      "layer   2  Sparsity: 76.3401%\n",
      "layer   3  Sparsity: 74.7705%\n",
      "total_backward_count 225170 real_backward_count 26641  11.832%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.947736/  1.340118, val:  65.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9234%\n",
      "layer   2  Sparsity: 76.7142%\n",
      "layer   3  Sparsity: 74.7162%\n",
      "total_backward_count 234960 real_backward_count 27579  11.738%\n",
      "fc layer 3 self.abs_max_out: 1093.0\n",
      "fc layer 3 self.abs_max_out: 1130.0\n",
      "lif layer 2 self.abs_max_v: 3659.0\n",
      "lif layer 2 self.abs_max_v: 3706.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.936872/  1.312529, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9432%\n",
      "layer   2  Sparsity: 76.4175%\n",
      "layer   3  Sparsity: 74.5451%\n",
      "total_backward_count 244750 real_backward_count 28519  11.652%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.943203/  1.281037, val:  72.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9385%\n",
      "layer   2  Sparsity: 76.5731%\n",
      "layer   3  Sparsity: 75.2568%\n",
      "total_backward_count 254540 real_backward_count 29478  11.581%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.948321/  1.310628, val:  72.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9254%\n",
      "layer   2  Sparsity: 76.0879%\n",
      "layer   3  Sparsity: 75.9838%\n",
      "total_backward_count 264330 real_backward_count 30394  11.499%\n",
      "lif layer 2 self.abs_max_v: 3897.5\n",
      "lif layer 2 self.abs_max_v: 3933.5\n",
      "lif layer 2 self.abs_max_v: 3950.5\n",
      "lif layer 2 self.abs_max_v: 4183.5\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.947822/  1.274473, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9303%\n",
      "layer   2  Sparsity: 76.2103%\n",
      "layer   3  Sparsity: 75.8908%\n",
      "total_backward_count 274120 real_backward_count 31315  11.424%\n",
      "fc layer 1 self.abs_max_out: 5483.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.913938/  1.338240, val:  62.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9298%\n",
      "layer   2  Sparsity: 76.2721%\n",
      "layer   3  Sparsity: 76.2990%\n",
      "total_backward_count 283910 real_backward_count 32211  11.345%\n",
      "fc layer 1 self.abs_max_out: 5755.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.910671/  1.265798, val:  70.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9319%\n",
      "layer   2  Sparsity: 76.3432%\n",
      "layer   3  Sparsity: 76.5908%\n",
      "total_backward_count 293700 real_backward_count 33059  11.256%\n",
      "fc layer 1 self.abs_max_out: 5897.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.892142/  1.287360, val:  67.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9307%\n",
      "layer   2  Sparsity: 76.1648%\n",
      "layer   3  Sparsity: 76.3265%\n",
      "total_backward_count 303490 real_backward_count 33980  11.196%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.914908/  1.320490, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9566%\n",
      "layer   2  Sparsity: 75.9909%\n",
      "layer   3  Sparsity: 76.0828%\n",
      "total_backward_count 313280 real_backward_count 34843  11.122%\n",
      "fc layer 1 self.abs_max_out: 5956.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.902641/  1.320051, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9329%\n",
      "layer   2  Sparsity: 75.8789%\n",
      "layer   3  Sparsity: 76.1784%\n",
      "total_backward_count 323070 real_backward_count 35626  11.027%\n",
      "fc layer 1 self.abs_max_out: 6044.0\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.906197/  1.279303, val:  67.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9317%\n",
      "layer   2  Sparsity: 76.2652%\n",
      "layer   3  Sparsity: 75.8231%\n",
      "total_backward_count 332860 real_backward_count 36431  10.945%\n",
      "fc layer 3 self.abs_max_out: 1174.0\n",
      "fc layer 3 self.abs_max_out: 1199.0\n",
      "fc layer 2 self.abs_max_out: 2504.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.854487/  1.231771, val:  70.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9286%\n",
      "layer   2  Sparsity: 76.6416%\n",
      "layer   3  Sparsity: 75.5464%\n",
      "total_backward_count 342650 real_backward_count 37214  10.861%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.846807/  1.215321, val:  75.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9397%\n",
      "layer   2  Sparsity: 76.4767%\n",
      "layer   3  Sparsity: 75.5343%\n",
      "total_backward_count 352440 real_backward_count 37985  10.778%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.815114/  1.191036, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9310%\n",
      "layer   2  Sparsity: 76.3843%\n",
      "layer   3  Sparsity: 75.1505%\n",
      "total_backward_count 362230 real_backward_count 38711  10.687%\n",
      "lif layer 1 self.abs_max_v: 10027.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.813750/  1.256404, val:  68.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9381%\n",
      "layer   2  Sparsity: 76.5153%\n",
      "layer   3  Sparsity: 75.9366%\n",
      "total_backward_count 372020 real_backward_count 39416  10.595%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.843400/  1.256754, val:  73.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9310%\n",
      "layer   2  Sparsity: 76.2516%\n",
      "layer   3  Sparsity: 75.7148%\n",
      "total_backward_count 381810 real_backward_count 40140  10.513%\n",
      "lif layer 2 self.abs_max_v: 4222.5\n",
      "lif layer 2 self.abs_max_v: 4330.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.816522/  1.227494, val:  75.00%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9357%\n",
      "layer   2  Sparsity: 76.4044%\n",
      "layer   3  Sparsity: 75.0096%\n",
      "total_backward_count 391600 real_backward_count 40869  10.436%\n",
      "lif layer 1 self.abs_max_v: 10038.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.813243/  1.198647, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9531%\n",
      "layer   2  Sparsity: 76.2210%\n",
      "layer   3  Sparsity: 74.9116%\n",
      "total_backward_count 401390 real_backward_count 41628  10.371%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.804661/  1.160775, val:  78.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9251%\n",
      "layer   2  Sparsity: 76.2153%\n",
      "layer   3  Sparsity: 75.5244%\n",
      "total_backward_count 411180 real_backward_count 42334  10.296%\n",
      "lif layer 1 self.abs_max_v: 10293.5\n",
      "lif layer 1 self.abs_max_v: 10648.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.789774/  1.183268, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9388%\n",
      "layer   2  Sparsity: 76.2327%\n",
      "layer   3  Sparsity: 74.8747%\n",
      "total_backward_count 420970 real_backward_count 42987  10.211%\n",
      "fc layer 3 self.abs_max_out: 1297.0\n",
      "lif layer 1 self.abs_max_v: 10758.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.782440/  1.129725, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9399%\n",
      "layer   2  Sparsity: 75.8840%\n",
      "layer   3  Sparsity: 73.4575%\n",
      "total_backward_count 430760 real_backward_count 43674  10.139%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.778221/  1.125680, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9349%\n",
      "layer   2  Sparsity: 75.4599%\n",
      "layer   3  Sparsity: 72.5143%\n",
      "total_backward_count 440550 real_backward_count 44311  10.058%\n",
      "fc layer 3 self.abs_max_out: 1305.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.735673/  1.162472, val:  70.42%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9369%\n",
      "layer   2  Sparsity: 75.5304%\n",
      "layer   3  Sparsity: 73.2000%\n",
      "total_backward_count 450340 real_backward_count 44954   9.982%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.739976/  1.155177, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9300%\n",
      "layer   2  Sparsity: 75.7979%\n",
      "layer   3  Sparsity: 73.4898%\n",
      "total_backward_count 460130 real_backward_count 45620   9.915%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.737819/  1.192288, val:  65.00%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9285%\n",
      "layer   2  Sparsity: 76.0027%\n",
      "layer   3  Sparsity: 73.4148%\n",
      "total_backward_count 469920 real_backward_count 46269   9.846%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.707370/  1.120676, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9478%\n",
      "layer   2  Sparsity: 76.0216%\n",
      "layer   3  Sparsity: 74.1236%\n",
      "total_backward_count 479710 real_backward_count 46876   9.772%\n",
      "lif layer 1 self.abs_max_v: 10908.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.720347/  1.110443, val:  81.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9480%\n",
      "layer   2  Sparsity: 76.2575%\n",
      "layer   3  Sparsity: 74.4145%\n",
      "total_backward_count 489500 real_backward_count 47523   9.708%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.718995/  1.086825, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9448%\n",
      "layer   2  Sparsity: 76.3527%\n",
      "layer   3  Sparsity: 74.3931%\n",
      "total_backward_count 499290 real_backward_count 48127   9.639%\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "fc layer 3 self.abs_max_out: 1335.0\n",
      "lif layer 1 self.abs_max_v: 11194.5\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.706000/  1.080139, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9368%\n",
      "layer   2  Sparsity: 76.2827%\n",
      "layer   3  Sparsity: 74.0151%\n",
      "total_backward_count 509080 real_backward_count 48686   9.564%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.689697/  1.074190, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9444%\n",
      "layer   2  Sparsity: 75.9183%\n",
      "layer   3  Sparsity: 73.4406%\n",
      "total_backward_count 518870 real_backward_count 49242   9.490%\n",
      "fc layer 3 self.abs_max_out: 1383.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.701432/  1.104053, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9231%\n",
      "layer   2  Sparsity: 76.0182%\n",
      "layer   3  Sparsity: 73.6894%\n",
      "total_backward_count 528660 real_backward_count 49831   9.426%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.680893/  1.056102, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9211%\n",
      "layer   2  Sparsity: 76.0405%\n",
      "layer   3  Sparsity: 73.6101%\n",
      "total_backward_count 538450 real_backward_count 50366   9.354%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.685171/  1.066663, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9311%\n",
      "layer   2  Sparsity: 76.1217%\n",
      "layer   3  Sparsity: 73.9222%\n",
      "total_backward_count 548240 real_backward_count 50946   9.293%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.676256/  1.085714, val:  79.58%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9385%\n",
      "layer   2  Sparsity: 76.0931%\n",
      "layer   3  Sparsity: 73.9183%\n",
      "total_backward_count 558030 real_backward_count 51470   9.224%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.670883/  1.078030, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9477%\n",
      "layer   2  Sparsity: 75.8776%\n",
      "layer   3  Sparsity: 73.8940%\n",
      "total_backward_count 567820 real_backward_count 52008   9.159%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.664898/  1.094588, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9465%\n",
      "layer   2  Sparsity: 75.8474%\n",
      "layer   3  Sparsity: 74.0737%\n",
      "total_backward_count 577610 real_backward_count 52544   9.097%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.664366/  1.083886, val:  72.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9447%\n",
      "layer   2  Sparsity: 76.0247%\n",
      "layer   3  Sparsity: 73.5829%\n",
      "total_backward_count 587400 real_backward_count 53084   9.037%\n",
      "fc layer 3 self.abs_max_out: 1387.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.659999/  1.101986, val:  74.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9287%\n",
      "layer   2  Sparsity: 75.9292%\n",
      "layer   3  Sparsity: 74.3575%\n",
      "total_backward_count 597190 real_backward_count 53612   8.977%\n",
      "fc layer 3 self.abs_max_out: 1393.0\n",
      "fc layer 3 self.abs_max_out: 1410.0\n",
      "fc layer 3 self.abs_max_out: 1462.0\n",
      "fc layer 3 self.abs_max_out: 1482.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.661959/  1.042566, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9388%\n",
      "layer   2  Sparsity: 76.0655%\n",
      "layer   3  Sparsity: 74.4926%\n",
      "total_backward_count 606980 real_backward_count 54152   8.922%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.652365/  1.112524, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9260%\n",
      "layer   2  Sparsity: 76.4591%\n",
      "layer   3  Sparsity: 74.2231%\n",
      "total_backward_count 616770 real_backward_count 54675   8.865%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.664087/  1.072498, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9256%\n",
      "layer   2  Sparsity: 76.5062%\n",
      "layer   3  Sparsity: 74.0964%\n",
      "total_backward_count 626560 real_backward_count 55155   8.803%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.640552/  1.038334, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9447%\n",
      "layer   2  Sparsity: 76.3763%\n",
      "layer   3  Sparsity: 73.9565%\n",
      "total_backward_count 636350 real_backward_count 55647   8.745%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.633586/  1.130778, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9267%\n",
      "layer   2  Sparsity: 76.4937%\n",
      "layer   3  Sparsity: 74.2737%\n",
      "total_backward_count 646140 real_backward_count 56154   8.691%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.627601/  1.046122, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9287%\n",
      "layer   2  Sparsity: 76.5641%\n",
      "layer   3  Sparsity: 74.2878%\n",
      "total_backward_count 655930 real_backward_count 56598   8.629%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.632773/  1.050036, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9307%\n",
      "layer   2  Sparsity: 76.7151%\n",
      "layer   3  Sparsity: 73.8238%\n",
      "total_backward_count 665720 real_backward_count 57065   8.572%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.615836/  1.012248, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9375%\n",
      "layer   2  Sparsity: 76.4890%\n",
      "layer   3  Sparsity: 74.0127%\n",
      "total_backward_count 675510 real_backward_count 57511   8.514%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.612543/  1.056595, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9284%\n",
      "layer   2  Sparsity: 76.3103%\n",
      "layer   3  Sparsity: 74.3462%\n",
      "total_backward_count 685300 real_backward_count 57954   8.457%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.614047/  1.020704, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9321%\n",
      "layer   2  Sparsity: 76.3806%\n",
      "layer   3  Sparsity: 74.3714%\n",
      "total_backward_count 695090 real_backward_count 58378   8.399%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.594549/  1.041704, val:  73.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9286%\n",
      "layer   2  Sparsity: 76.2433%\n",
      "layer   3  Sparsity: 74.0108%\n",
      "total_backward_count 704880 real_backward_count 58841   8.348%\n",
      "fc layer 3 self.abs_max_out: 1490.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.607275/  0.998650, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9378%\n",
      "layer   2  Sparsity: 76.3057%\n",
      "layer   3  Sparsity: 74.1910%\n",
      "total_backward_count 714670 real_backward_count 59279   8.295%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.610284/  1.004908, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9134%\n",
      "layer   2  Sparsity: 76.3146%\n",
      "layer   3  Sparsity: 74.5728%\n",
      "total_backward_count 724460 real_backward_count 59722   8.244%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.619137/  1.055274, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9225%\n",
      "layer   2  Sparsity: 76.2447%\n",
      "layer   3  Sparsity: 74.8796%\n",
      "total_backward_count 734250 real_backward_count 60161   8.194%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.618818/  1.014323, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9249%\n",
      "layer   2  Sparsity: 76.0126%\n",
      "layer   3  Sparsity: 74.8302%\n",
      "total_backward_count 744040 real_backward_count 60603   8.145%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.609699/  1.039566, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9393%\n",
      "layer   2  Sparsity: 75.8760%\n",
      "layer   3  Sparsity: 74.9060%\n",
      "total_backward_count 753830 real_backward_count 60995   8.091%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.613657/  0.991469, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9416%\n",
      "layer   2  Sparsity: 76.1178%\n",
      "layer   3  Sparsity: 75.6716%\n",
      "total_backward_count 763620 real_backward_count 61369   8.037%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.613850/  1.028287, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9441%\n",
      "layer   2  Sparsity: 76.0006%\n",
      "layer   3  Sparsity: 75.9573%\n",
      "total_backward_count 773410 real_backward_count 61766   7.986%\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.613615/  1.044678, val:  80.83%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9298%\n",
      "layer   2  Sparsity: 76.0951%\n",
      "layer   3  Sparsity: 76.4422%\n",
      "total_backward_count 783200 real_backward_count 62137   7.934%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.634769/  1.021128, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9362%\n",
      "layer   2  Sparsity: 76.0950%\n",
      "layer   3  Sparsity: 76.4401%\n",
      "total_backward_count 792990 real_backward_count 62578   7.891%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.601631/  1.079912, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9419%\n",
      "layer   2  Sparsity: 76.4048%\n",
      "layer   3  Sparsity: 76.6629%\n",
      "total_backward_count 802780 real_backward_count 62949   7.841%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.589390/  1.032463, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9323%\n",
      "layer   2  Sparsity: 76.2153%\n",
      "layer   3  Sparsity: 76.1760%\n",
      "total_backward_count 812570 real_backward_count 63311   7.791%\n",
      "lif layer 1 self.abs_max_v: 11368.5\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.578542/  1.044731, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9336%\n",
      "layer   2  Sparsity: 76.1009%\n",
      "layer   3  Sparsity: 76.1566%\n",
      "total_backward_count 822360 real_backward_count 63696   7.746%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.583473/  1.010464, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9308%\n",
      "layer   2  Sparsity: 76.1409%\n",
      "layer   3  Sparsity: 75.6966%\n",
      "total_backward_count 832150 real_backward_count 64116   7.705%\n",
      "lif layer 2 self.abs_max_v: 4363.5\n",
      "lif layer 2 self.abs_max_v: 4380.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.579450/  1.023286, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9551%\n",
      "layer   2  Sparsity: 76.2330%\n",
      "layer   3  Sparsity: 75.1728%\n",
      "total_backward_count 841940 real_backward_count 64472   7.658%\n",
      "fc layer 2 self.abs_max_out: 2584.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.589684/  0.996267, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9133%\n",
      "layer   2  Sparsity: 76.1541%\n",
      "layer   3  Sparsity: 75.2565%\n",
      "total_backward_count 851730 real_backward_count 64825   7.611%\n",
      "fc layer 1 self.abs_max_out: 6049.0\n",
      "lif layer 1 self.abs_max_v: 11386.5\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.581729/  0.981730, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9430%\n",
      "layer   2  Sparsity: 76.1746%\n",
      "layer   3  Sparsity: 75.1457%\n",
      "total_backward_count 861520 real_backward_count 65210   7.569%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.551342/  1.009455, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9535%\n",
      "layer   2  Sparsity: 76.4102%\n",
      "layer   3  Sparsity: 75.4265%\n",
      "total_backward_count 871310 real_backward_count 65552   7.523%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.572285/  0.952751, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9310%\n",
      "layer   2  Sparsity: 76.0277%\n",
      "layer   3  Sparsity: 75.5283%\n",
      "total_backward_count 881100 real_backward_count 65916   7.481%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.564352/  1.028896, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9297%\n",
      "layer   2  Sparsity: 75.8883%\n",
      "layer   3  Sparsity: 76.3991%\n",
      "total_backward_count 890890 real_backward_count 66274   7.439%\n",
      "fc layer 1 self.abs_max_out: 6105.0\n",
      "lif layer 1 self.abs_max_v: 11529.5\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.572052/  0.989317, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9212%\n",
      "layer   2  Sparsity: 75.9291%\n",
      "layer   3  Sparsity: 75.8688%\n",
      "total_backward_count 900680 real_backward_count 66619   7.397%\n",
      "fc layer 1 self.abs_max_out: 6112.0\n",
      "lif layer 1 self.abs_max_v: 11542.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.567410/  1.001709, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9270%\n",
      "layer   2  Sparsity: 76.3537%\n",
      "layer   3  Sparsity: 75.5260%\n",
      "total_backward_count 910470 real_backward_count 66987   7.357%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.582785/  0.999878, val:  83.75%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9291%\n",
      "layer   2  Sparsity: 76.4800%\n",
      "layer   3  Sparsity: 75.2348%\n",
      "total_backward_count 920260 real_backward_count 67367   7.320%\n",
      "fc layer 2 self.abs_max_out: 2690.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.578039/  1.039500, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9349%\n",
      "layer   2  Sparsity: 76.2889%\n",
      "layer   3  Sparsity: 75.3669%\n",
      "total_backward_count 930050 real_backward_count 67716   7.281%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.570424/  0.967717, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9165%\n",
      "layer   2  Sparsity: 76.2564%\n",
      "layer   3  Sparsity: 75.3603%\n",
      "total_backward_count 939840 real_backward_count 68073   7.243%\n",
      "fc layer 1 self.abs_max_out: 6128.0\n",
      "lif layer 1 self.abs_max_v: 11600.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.567690/  1.010876, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9215%\n",
      "layer   2  Sparsity: 76.3745%\n",
      "layer   3  Sparsity: 75.2741%\n",
      "total_backward_count 949630 real_backward_count 68389   7.202%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.578190/  0.988143, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9251%\n",
      "layer   2  Sparsity: 76.0099%\n",
      "layer   3  Sparsity: 75.1936%\n",
      "total_backward_count 959420 real_backward_count 68740   7.165%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.566529/  1.023237, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9147%\n",
      "layer   2  Sparsity: 76.0609%\n",
      "layer   3  Sparsity: 74.9432%\n",
      "total_backward_count 969210 real_backward_count 69071   7.127%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.563397/  0.985135, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9352%\n",
      "layer   2  Sparsity: 76.4295%\n",
      "layer   3  Sparsity: 75.0429%\n",
      "total_backward_count 979000 real_backward_count 69432   7.092%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.551367/  0.959055, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9448%\n",
      "layer   2  Sparsity: 76.3842%\n",
      "layer   3  Sparsity: 75.5430%\n",
      "total_backward_count 988790 real_backward_count 69744   7.053%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.531140/  0.937493, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9338%\n",
      "layer   2  Sparsity: 76.2485%\n",
      "layer   3  Sparsity: 75.6291%\n",
      "total_backward_count 998580 real_backward_count 70070   7.017%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.543499/  0.939877, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9324%\n",
      "layer   2  Sparsity: 76.4068%\n",
      "layer   3  Sparsity: 75.5669%\n",
      "total_backward_count 1008370 real_backward_count 70387   6.980%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.549879/  0.992931, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9301%\n",
      "layer   2  Sparsity: 76.4912%\n",
      "layer   3  Sparsity: 75.4721%\n",
      "total_backward_count 1018160 real_backward_count 70713   6.945%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.539019/  0.965903, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9514%\n",
      "layer   2  Sparsity: 76.4418%\n",
      "layer   3  Sparsity: 75.8319%\n",
      "total_backward_count 1027950 real_backward_count 71031   6.910%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.530423/  0.949114, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9427%\n",
      "layer   2  Sparsity: 76.3336%\n",
      "layer   3  Sparsity: 76.8239%\n",
      "total_backward_count 1037740 real_backward_count 71322   6.873%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.525941/  0.990579, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9254%\n",
      "layer   2  Sparsity: 76.5811%\n",
      "layer   3  Sparsity: 76.9844%\n",
      "total_backward_count 1047530 real_backward_count 71594   6.835%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.539891/  0.942891, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9316%\n",
      "layer   2  Sparsity: 76.4214%\n",
      "layer   3  Sparsity: 76.6127%\n",
      "total_backward_count 1057320 real_backward_count 71891   6.799%\n",
      "fc layer 2 self.abs_max_out: 2739.0\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.525328/  0.942800, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9108%\n",
      "layer   2  Sparsity: 76.4934%\n",
      "layer   3  Sparsity: 76.7459%\n",
      "total_backward_count 1067110 real_backward_count 72181   6.764%\n",
      "fc layer 2 self.abs_max_out: 2773.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.516284/  0.927370, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9247%\n",
      "layer   2  Sparsity: 76.6014%\n",
      "layer   3  Sparsity: 76.5789%\n",
      "total_backward_count 1076900 real_backward_count 72482   6.731%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.519980/  0.939800, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9489%\n",
      "layer   2  Sparsity: 76.2642%\n",
      "layer   3  Sparsity: 76.5756%\n",
      "total_backward_count 1086690 real_backward_count 72770   6.696%\n",
      "lif layer 2 self.abs_max_v: 4433.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.514135/  0.942294, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9243%\n",
      "layer   2  Sparsity: 76.1823%\n",
      "layer   3  Sparsity: 76.6590%\n",
      "total_backward_count 1096480 real_backward_count 73070   6.664%\n",
      "fc layer 2 self.abs_max_out: 2927.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.518131/  0.944694, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9507%\n",
      "layer   2  Sparsity: 76.4994%\n",
      "layer   3  Sparsity: 76.3337%\n",
      "total_backward_count 1106270 real_backward_count 73368   6.632%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.511052/  0.945783, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9275%\n",
      "layer   2  Sparsity: 76.5994%\n",
      "layer   3  Sparsity: 76.6188%\n",
      "total_backward_count 1116060 real_backward_count 73614   6.596%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.510872/  0.939517, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9298%\n",
      "layer   2  Sparsity: 76.7105%\n",
      "layer   3  Sparsity: 76.8164%\n",
      "total_backward_count 1125850 real_backward_count 73860   6.560%\n",
      "fc layer 1 self.abs_max_out: 6251.0\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.510875/  0.925061, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9263%\n",
      "layer   2  Sparsity: 76.8232%\n",
      "layer   3  Sparsity: 77.2595%\n",
      "total_backward_count 1135640 real_backward_count 74081   6.523%\n",
      "lif layer 2 self.abs_max_v: 4505.5\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.512975/  0.933499, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9190%\n",
      "layer   2  Sparsity: 76.9666%\n",
      "layer   3  Sparsity: 77.0562%\n",
      "total_backward_count 1145430 real_backward_count 74342   6.490%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.502098/  0.929921, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9402%\n",
      "layer   2  Sparsity: 76.9979%\n",
      "layer   3  Sparsity: 76.9553%\n",
      "total_backward_count 1155220 real_backward_count 74616   6.459%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.505210/  0.922909, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9338%\n",
      "layer   2  Sparsity: 76.7480%\n",
      "layer   3  Sparsity: 76.6871%\n",
      "total_backward_count 1165010 real_backward_count 74906   6.430%\n",
      "fc layer 1 self.abs_max_out: 6269.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.494640/  0.892215, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9544%\n",
      "layer   2  Sparsity: 76.2670%\n",
      "layer   3  Sparsity: 76.3012%\n",
      "total_backward_count 1174800 real_backward_count 75152   6.397%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.504997/  0.978737, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9258%\n",
      "layer   2  Sparsity: 76.3938%\n",
      "layer   3  Sparsity: 76.4313%\n",
      "total_backward_count 1184590 real_backward_count 75415   6.366%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.509102/  0.927091, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9384%\n",
      "layer   2  Sparsity: 76.7159%\n",
      "layer   3  Sparsity: 76.3564%\n",
      "total_backward_count 1194380 real_backward_count 75689   6.337%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.493395/  0.948252, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9411%\n",
      "layer   2  Sparsity: 76.8441%\n",
      "layer   3  Sparsity: 76.3904%\n",
      "total_backward_count 1204170 real_backward_count 75949   6.307%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.505181/  0.910540, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9294%\n",
      "layer   2  Sparsity: 76.9944%\n",
      "layer   3  Sparsity: 76.2214%\n",
      "total_backward_count 1213960 real_backward_count 76194   6.276%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.492234/  0.879973, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9391%\n",
      "layer   2  Sparsity: 76.7289%\n",
      "layer   3  Sparsity: 76.1562%\n",
      "total_backward_count 1223750 real_backward_count 76468   6.249%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.505539/  0.919249, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9419%\n",
      "layer   2  Sparsity: 76.5582%\n",
      "layer   3  Sparsity: 76.0055%\n",
      "total_backward_count 1233540 real_backward_count 76714   6.219%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.507172/  0.944408, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9531%\n",
      "layer   2  Sparsity: 76.5664%\n",
      "layer   3  Sparsity: 75.8724%\n",
      "total_backward_count 1243330 real_backward_count 76960   6.190%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.498220/  0.912658, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9315%\n",
      "layer   2  Sparsity: 76.6520%\n",
      "layer   3  Sparsity: 75.6968%\n",
      "total_backward_count 1253120 real_backward_count 77184   6.159%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.487772/  0.937076, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9409%\n",
      "layer   2  Sparsity: 76.7427%\n",
      "layer   3  Sparsity: 75.8603%\n",
      "total_backward_count 1262910 real_backward_count 77421   6.130%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.492763/  0.906023, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9223%\n",
      "layer   2  Sparsity: 76.7445%\n",
      "layer   3  Sparsity: 76.2025%\n",
      "total_backward_count 1272700 real_backward_count 77644   6.101%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.481612/  0.907539, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9397%\n",
      "layer   2  Sparsity: 76.5491%\n",
      "layer   3  Sparsity: 76.4392%\n",
      "total_backward_count 1282490 real_backward_count 77871   6.072%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.489275/  0.890961, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9462%\n",
      "layer   2  Sparsity: 76.5425%\n",
      "layer   3  Sparsity: 76.2898%\n",
      "total_backward_count 1292280 real_backward_count 78095   6.043%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.470671/  0.904139, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9296%\n",
      "layer   2  Sparsity: 76.8194%\n",
      "layer   3  Sparsity: 76.6637%\n",
      "total_backward_count 1302070 real_backward_count 78305   6.014%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.476251/  0.953606, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9319%\n",
      "layer   2  Sparsity: 76.6966%\n",
      "layer   3  Sparsity: 76.6371%\n",
      "total_backward_count 1311860 real_backward_count 78529   5.986%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.479611/  0.924484, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9433%\n",
      "layer   2  Sparsity: 76.6303%\n",
      "layer   3  Sparsity: 76.4808%\n",
      "total_backward_count 1321650 real_backward_count 78735   5.957%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.476792/  0.868802, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9368%\n",
      "layer   2  Sparsity: 76.4678%\n",
      "layer   3  Sparsity: 76.2076%\n",
      "total_backward_count 1331440 real_backward_count 78951   5.930%\n",
      "lif layer 2 self.abs_max_v: 4506.0\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.446136/  0.874246, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9452%\n",
      "layer   2  Sparsity: 76.6546%\n",
      "layer   3  Sparsity: 76.3450%\n",
      "total_backward_count 1341230 real_backward_count 79152   5.901%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.449555/  0.882326, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9359%\n",
      "layer   2  Sparsity: 76.5580%\n",
      "layer   3  Sparsity: 76.7941%\n",
      "total_backward_count 1351020 real_backward_count 79328   5.872%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.464740/  0.904432, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9421%\n",
      "layer   2  Sparsity: 76.6388%\n",
      "layer   3  Sparsity: 76.6468%\n",
      "total_backward_count 1360810 real_backward_count 79540   5.845%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.454544/  0.886079, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9441%\n",
      "layer   2  Sparsity: 76.7822%\n",
      "layer   3  Sparsity: 76.2828%\n",
      "total_backward_count 1370600 real_backward_count 79746   5.818%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.448410/  0.907066, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9242%\n",
      "layer   2  Sparsity: 76.7480%\n",
      "layer   3  Sparsity: 76.1664%\n",
      "total_backward_count 1380390 real_backward_count 79927   5.790%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.460363/  0.903450, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9407%\n",
      "layer   2  Sparsity: 76.3843%\n",
      "layer   3  Sparsity: 76.2954%\n",
      "total_backward_count 1390180 real_backward_count 80119   5.763%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.464540/  0.910901, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9408%\n",
      "layer   2  Sparsity: 76.2966%\n",
      "layer   3  Sparsity: 75.9330%\n",
      "total_backward_count 1399970 real_backward_count 80339   5.739%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.457339/  0.914763, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9163%\n",
      "layer   2  Sparsity: 76.5597%\n",
      "layer   3  Sparsity: 76.4919%\n",
      "total_backward_count 1409760 real_backward_count 80508   5.711%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.454897/  0.917892, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9394%\n",
      "layer   2  Sparsity: 76.7630%\n",
      "layer   3  Sparsity: 76.4138%\n",
      "total_backward_count 1419550 real_backward_count 80677   5.683%\n",
      "fc layer 2 self.abs_max_out: 2961.0\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.449496/  0.887806, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9395%\n",
      "layer   2  Sparsity: 76.7625%\n",
      "layer   3  Sparsity: 76.1948%\n",
      "total_backward_count 1429340 real_backward_count 80851   5.657%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.450186/  0.902468, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9314%\n",
      "layer   2  Sparsity: 76.6722%\n",
      "layer   3  Sparsity: 76.3434%\n",
      "total_backward_count 1439130 real_backward_count 81033   5.631%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.444061/  0.896833, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9322%\n",
      "layer   2  Sparsity: 76.6646%\n",
      "layer   3  Sparsity: 76.2258%\n",
      "total_backward_count 1448920 real_backward_count 81202   5.604%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.442434/  0.938363, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9343%\n",
      "layer   2  Sparsity: 76.7527%\n",
      "layer   3  Sparsity: 75.9253%\n",
      "total_backward_count 1458710 real_backward_count 81363   5.578%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.422684/  0.890164, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9401%\n",
      "layer   2  Sparsity: 76.7231%\n",
      "layer   3  Sparsity: 76.2434%\n",
      "total_backward_count 1468500 real_backward_count 81534   5.552%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.439088/  0.899394, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9331%\n",
      "layer   2  Sparsity: 76.6009%\n",
      "layer   3  Sparsity: 76.4479%\n",
      "total_backward_count 1478290 real_backward_count 81716   5.528%\n",
      "fc layer 2 self.abs_max_out: 2970.0\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.441577/  0.911398, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9342%\n",
      "layer   2  Sparsity: 76.7773%\n",
      "layer   3  Sparsity: 76.4322%\n",
      "total_backward_count 1488080 real_backward_count 81885   5.503%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.436338/  0.881176, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9277%\n",
      "layer   2  Sparsity: 76.8562%\n",
      "layer   3  Sparsity: 76.7739%\n",
      "total_backward_count 1497870 real_backward_count 82061   5.479%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.451276/  0.896311, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9434%\n",
      "layer   2  Sparsity: 76.8956%\n",
      "layer   3  Sparsity: 76.6378%\n",
      "total_backward_count 1507660 real_backward_count 82258   5.456%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.446686/  0.905931, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9443%\n",
      "layer   2  Sparsity: 76.7182%\n",
      "layer   3  Sparsity: 76.6614%\n",
      "total_backward_count 1517450 real_backward_count 82416   5.431%\n",
      "fc layer 3 self.abs_max_out: 1492.0\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.427449/  0.891976, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9192%\n",
      "layer   2  Sparsity: 76.6421%\n",
      "layer   3  Sparsity: 77.2040%\n",
      "total_backward_count 1527240 real_backward_count 82561   5.406%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.420859/  0.887405, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9479%\n",
      "layer   2  Sparsity: 76.7189%\n",
      "layer   3  Sparsity: 76.9155%\n",
      "total_backward_count 1537030 real_backward_count 82700   5.381%\n",
      "fc layer 3 self.abs_max_out: 1521.0\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.430101/  0.911159, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9227%\n",
      "layer   2  Sparsity: 76.7034%\n",
      "layer   3  Sparsity: 76.5347%\n",
      "total_backward_count 1546820 real_backward_count 82827   5.355%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.425509/  0.913201, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9294%\n",
      "layer   2  Sparsity: 76.4734%\n",
      "layer   3  Sparsity: 76.7145%\n",
      "total_backward_count 1556610 real_backward_count 83005   5.332%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.427410/  0.877535, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9369%\n",
      "layer   2  Sparsity: 76.5461%\n",
      "layer   3  Sparsity: 77.2421%\n",
      "total_backward_count 1566400 real_backward_count 83149   5.308%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.430766/  0.901138, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9340%\n",
      "layer   2  Sparsity: 76.6256%\n",
      "layer   3  Sparsity: 77.2317%\n",
      "total_backward_count 1576190 real_backward_count 83310   5.286%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.441882/  0.877544, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9254%\n",
      "layer   2  Sparsity: 76.8419%\n",
      "layer   3  Sparsity: 77.3707%\n",
      "total_backward_count 1585980 real_backward_count 83480   5.264%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.452432/  0.929772, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9348%\n",
      "layer   2  Sparsity: 76.8276%\n",
      "layer   3  Sparsity: 77.3554%\n",
      "total_backward_count 1595770 real_backward_count 83664   5.243%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.441739/  0.901811, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9352%\n",
      "layer   2  Sparsity: 76.6947%\n",
      "layer   3  Sparsity: 77.6648%\n",
      "total_backward_count 1605560 real_backward_count 83828   5.221%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.443143/  0.874451, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9152%\n",
      "layer   2  Sparsity: 76.5949%\n",
      "layer   3  Sparsity: 77.5049%\n",
      "total_backward_count 1615350 real_backward_count 84003   5.200%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.433263/  0.887312, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9244%\n",
      "layer   2  Sparsity: 76.3619%\n",
      "layer   3  Sparsity: 77.1249%\n",
      "total_backward_count 1625140 real_backward_count 84181   5.180%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.427115/  0.875468, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9195%\n",
      "layer   2  Sparsity: 76.2630%\n",
      "layer   3  Sparsity: 76.8982%\n",
      "total_backward_count 1634930 real_backward_count 84324   5.158%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.419936/  0.880440, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9366%\n",
      "layer   2  Sparsity: 76.4270%\n",
      "layer   3  Sparsity: 77.0630%\n",
      "total_backward_count 1644720 real_backward_count 84489   5.137%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.416675/  0.861109, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9573%\n",
      "layer   2  Sparsity: 76.6431%\n",
      "layer   3  Sparsity: 77.0919%\n",
      "total_backward_count 1654510 real_backward_count 84636   5.115%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.418832/  0.873204, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9464%\n",
      "layer   2  Sparsity: 76.5696%\n",
      "layer   3  Sparsity: 76.8791%\n",
      "total_backward_count 1664300 real_backward_count 84774   5.094%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.401959/  0.876584, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9458%\n",
      "layer   2  Sparsity: 76.6216%\n",
      "layer   3  Sparsity: 77.2443%\n",
      "total_backward_count 1674090 real_backward_count 84931   5.073%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.406609/  0.892534, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9556%\n",
      "layer   2  Sparsity: 76.7986%\n",
      "layer   3  Sparsity: 76.8911%\n",
      "total_backward_count 1683880 real_backward_count 85084   5.053%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.406770/  0.891044, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9521%\n",
      "layer   2  Sparsity: 76.8062%\n",
      "layer   3  Sparsity: 77.0795%\n",
      "total_backward_count 1693670 real_backward_count 85229   5.032%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.397014/  0.891405, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9421%\n",
      "layer   2  Sparsity: 76.7018%\n",
      "layer   3  Sparsity: 77.2221%\n",
      "total_backward_count 1703460 real_backward_count 85348   5.010%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.403083/  0.869477, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9321%\n",
      "layer   2  Sparsity: 76.5377%\n",
      "layer   3  Sparsity: 76.9411%\n",
      "total_backward_count 1713250 real_backward_count 85466   4.989%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.412135/  0.855526, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9184%\n",
      "layer   2  Sparsity: 76.7290%\n",
      "layer   3  Sparsity: 77.1430%\n",
      "total_backward_count 1723040 real_backward_count 85572   4.966%\n",
      "fc layer 3 self.abs_max_out: 1562.0\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.411488/  0.872643, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9422%\n",
      "layer   2  Sparsity: 76.7030%\n",
      "layer   3  Sparsity: 76.9860%\n",
      "total_backward_count 1732830 real_backward_count 85687   4.945%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.411093/  0.870401, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9352%\n",
      "layer   2  Sparsity: 76.6534%\n",
      "layer   3  Sparsity: 76.8358%\n",
      "total_backward_count 1742620 real_backward_count 85825   4.925%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.406215/  0.873699, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9462%\n",
      "layer   2  Sparsity: 76.8463%\n",
      "layer   3  Sparsity: 76.4158%\n",
      "total_backward_count 1752410 real_backward_count 85948   4.905%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.409943/  0.867038, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9376%\n",
      "layer   2  Sparsity: 76.7971%\n",
      "layer   3  Sparsity: 76.1558%\n",
      "total_backward_count 1762200 real_backward_count 86107   4.886%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.408064/  0.859793, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9409%\n",
      "layer   2  Sparsity: 76.6201%\n",
      "layer   3  Sparsity: 76.4276%\n",
      "total_backward_count 1771990 real_backward_count 86247   4.867%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.394808/  0.860070, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9296%\n",
      "layer   2  Sparsity: 76.6081%\n",
      "layer   3  Sparsity: 76.3726%\n",
      "total_backward_count 1781780 real_backward_count 86384   4.848%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.398609/  0.821382, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9242%\n",
      "layer   2  Sparsity: 76.6443%\n",
      "layer   3  Sparsity: 76.4304%\n",
      "total_backward_count 1791570 real_backward_count 86518   4.829%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.394096/  0.809602, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9409%\n",
      "layer   2  Sparsity: 76.5436%\n",
      "layer   3  Sparsity: 76.8130%\n",
      "total_backward_count 1801360 real_backward_count 86701   4.813%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.388598/  0.892730, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.9426%\n",
      "layer   2  Sparsity: 76.6284%\n",
      "layer   3  Sparsity: 77.1297%\n",
      "total_backward_count 1811150 real_backward_count 86855   4.796%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.400090/  0.877991, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9299%\n",
      "layer   2  Sparsity: 76.7191%\n",
      "layer   3  Sparsity: 77.1141%\n",
      "total_backward_count 1820940 real_backward_count 87029   4.779%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.398785/  0.879003, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9387%\n",
      "layer   2  Sparsity: 76.4813%\n",
      "layer   3  Sparsity: 77.1526%\n",
      "total_backward_count 1830730 real_backward_count 87195   4.763%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.396613/  0.876307, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9399%\n",
      "layer   2  Sparsity: 76.4815%\n",
      "layer   3  Sparsity: 76.8065%\n",
      "total_backward_count 1840520 real_backward_count 87352   4.746%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.404760/  0.886763, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9240%\n",
      "layer   2  Sparsity: 76.5913%\n",
      "layer   3  Sparsity: 77.1312%\n",
      "total_backward_count 1850310 real_backward_count 87492   4.729%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.403673/  0.865591, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9410%\n",
      "layer   2  Sparsity: 76.3445%\n",
      "layer   3  Sparsity: 77.4611%\n",
      "total_backward_count 1860100 real_backward_count 87626   4.711%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.404939/  0.875699, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9445%\n",
      "layer   2  Sparsity: 76.3928%\n",
      "layer   3  Sparsity: 77.4237%\n",
      "total_backward_count 1869890 real_backward_count 87735   4.692%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.401562/  0.883628, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9323%\n",
      "layer   2  Sparsity: 76.5351%\n",
      "layer   3  Sparsity: 77.0098%\n",
      "total_backward_count 1879680 real_backward_count 87856   4.674%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.405295/  0.913446, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9487%\n",
      "layer   2  Sparsity: 76.7464%\n",
      "layer   3  Sparsity: 77.1126%\n",
      "total_backward_count 1889470 real_backward_count 87965   4.656%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.415149/  0.902408, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9291%\n",
      "layer   2  Sparsity: 76.8141%\n",
      "layer   3  Sparsity: 76.7632%\n",
      "total_backward_count 1899260 real_backward_count 88123   4.640%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.407965/  0.886787, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9255%\n",
      "layer   2  Sparsity: 76.6851%\n",
      "layer   3  Sparsity: 76.7910%\n",
      "total_backward_count 1909050 real_backward_count 88248   4.623%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.399048/  0.883011, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9237%\n",
      "layer   2  Sparsity: 76.6662%\n",
      "layer   3  Sparsity: 77.1557%\n",
      "total_backward_count 1918840 real_backward_count 88350   4.604%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.394565/  0.865697, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9328%\n",
      "layer   2  Sparsity: 76.6844%\n",
      "layer   3  Sparsity: 76.7600%\n",
      "total_backward_count 1928630 real_backward_count 88473   4.587%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.391924/  0.895451, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9476%\n",
      "layer   2  Sparsity: 76.7210%\n",
      "layer   3  Sparsity: 76.4936%\n",
      "total_backward_count 1938420 real_backward_count 88621   4.572%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.391600/  0.865270, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9281%\n",
      "layer   2  Sparsity: 76.6873%\n",
      "layer   3  Sparsity: 76.5942%\n",
      "total_backward_count 1948210 real_backward_count 88745   4.555%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.387800/  0.861254, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.9382%\n",
      "layer   2  Sparsity: 76.6462%\n",
      "layer   3  Sparsity: 76.5410%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ff816e1016415bb403701b6cdf1a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.3878</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>0.86125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-sweep-74</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/64t53hx0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/64t53hx0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_140524-64t53hx0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6ebtsqu3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_182901-6ebtsqu3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ebtsqu3' target=\"_blank\">devoted-sweep-81</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ebtsqu3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ebtsqu3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251115_182910_262', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "lif layer 1 self.abs_max_v: 1338.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 254.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1426.0\n",
      "lif layer 1 self.abs_max_v: 1854.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "lif layer 2 self.abs_max_v: 662.5\n",
      "lif layer 1 self.abs_max_v: 1877.0\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "lif layer 2 self.abs_max_v: 1179.5\n",
      "lif layer 1 self.abs_max_v: 2282.5\n",
      "fc layer 1 self.abs_max_out: 1952.0\n",
      "lif layer 1 self.abs_max_v: 2442.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "lif layer 2 self.abs_max_v: 1489.0\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "fc layer 2 self.abs_max_out: 1196.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 1591.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 2192.0\n",
      "fc layer 1 self.abs_max_out: 2755.0\n",
      "lif layer 1 self.abs_max_v: 2755.0\n",
      "fc layer 1 self.abs_max_out: 2820.0\n",
      "lif layer 1 self.abs_max_v: 2820.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 1634.0\n",
      "fc layer 1 self.abs_max_out: 3334.0\n",
      "lif layer 1 self.abs_max_v: 3334.0\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "lif layer 2 self.abs_max_v: 2124.0\n",
      "fc layer 1 self.abs_max_out: 3481.0\n",
      "lif layer 1 self.abs_max_v: 3481.0\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "lif layer 2 self.abs_max_v: 2444.5\n",
      "fc layer 1 self.abs_max_out: 3715.0\n",
      "lif layer 1 self.abs_max_v: 3715.0\n",
      "lif layer 2 self.abs_max_v: 2571.5\n",
      "fc layer 1 self.abs_max_out: 3779.0\n",
      "lif layer 1 self.abs_max_v: 3779.0\n",
      "fc layer 1 self.abs_max_out: 4264.0\n",
      "lif layer 1 self.abs_max_v: 4264.0\n",
      "fc layer 1 self.abs_max_out: 4603.0\n",
      "lif layer 1 self.abs_max_v: 4603.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "lif layer 2 self.abs_max_v: 2594.5\n",
      "lif layer 2 self.abs_max_v: 2638.5\n",
      "fc layer 1 self.abs_max_out: 4953.0\n",
      "lif layer 1 self.abs_max_v: 4953.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 1 self.abs_max_out: 5173.0\n",
      "lif layer 1 self.abs_max_v: 5173.0\n",
      "fc layer 1 self.abs_max_out: 5291.0\n",
      "lif layer 1 self.abs_max_v: 5291.0\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "lif layer 2 self.abs_max_v: 2674.0\n",
      "lif layer 2 self.abs_max_v: 2685.0\n",
      "fc layer 2 self.abs_max_out: 1765.0\n",
      "lif layer 2 self.abs_max_v: 3107.5\n",
      "fc layer 2 self.abs_max_out: 1805.0\n",
      "lif layer 2 self.abs_max_v: 3351.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "lif layer 2 self.abs_max_v: 3370.5\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "lif layer 2 self.abs_max_v: 3608.5\n",
      "fc layer 2 self.abs_max_out: 2144.0\n",
      "fc layer 2 self.abs_max_out: 2325.0\n",
      "lif layer 2 self.abs_max_v: 3711.0\n",
      "fc layer 2 self.abs_max_out: 2526.0\n",
      "lif layer 1 self.abs_max_v: 5429.0\n",
      "fc layer 1 self.abs_max_out: 5310.0\n",
      "fc layer 2 self.abs_max_out: 2659.0\n",
      "fc layer 1 self.abs_max_out: 6119.0\n",
      "lif layer 1 self.abs_max_v: 6119.0\n",
      "fc layer 1 self.abs_max_out: 6310.0\n",
      "lif layer 1 self.abs_max_v: 6310.0\n",
      "lif layer 2 self.abs_max_v: 3994.0\n",
      "fc layer 2 self.abs_max_out: 2778.0\n",
      "fc layer 2 self.abs_max_out: 2873.0\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "fc layer 2 self.abs_max_out: 3036.0\n",
      "fc layer 2 self.abs_max_out: 3185.0\n",
      "lif layer 2 self.abs_max_v: 4006.0\n",
      "fc layer 1 self.abs_max_out: 6369.0\n",
      "lif layer 1 self.abs_max_v: 6369.0\n",
      "fc layer 1 self.abs_max_out: 7059.0\n",
      "lif layer 1 self.abs_max_v: 7059.0\n",
      "fc layer 2 self.abs_max_out: 3191.0\n",
      "lif layer 2 self.abs_max_v: 4384.0\n",
      "lif layer 2 self.abs_max_v: 4591.0\n",
      "lif layer 2 self.abs_max_v: 4716.5\n",
      "fc layer 1 self.abs_max_out: 7375.0\n",
      "lif layer 1 self.abs_max_v: 7375.0\n",
      "lif layer 2 self.abs_max_v: 4771.5\n",
      "lif layer 2 self.abs_max_v: 4982.0\n",
      "lif layer 2 self.abs_max_v: 5106.0\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "fc layer 2 self.abs_max_out: 3448.0\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 2 self.abs_max_out: 3513.0\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "lif layer 2 self.abs_max_v: 5264.5\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "fc layer 1 self.abs_max_out: 7705.0\n",
      "lif layer 1 self.abs_max_v: 7705.0\n",
      "fc layer 2 self.abs_max_out: 3558.0\n",
      "fc layer 2 self.abs_max_out: 3566.0\n",
      "lif layer 2 self.abs_max_v: 5564.5\n",
      "lif layer 2 self.abs_max_v: 5582.5\n",
      "lif layer 2 self.abs_max_v: 5844.5\n",
      "lif layer 2 self.abs_max_v: 5959.5\n",
      "lif layer 2 self.abs_max_v: 6061.0\n",
      "lif layer 1 self.abs_max_v: 8057.0\n",
      "fc layer 2 self.abs_max_out: 3635.0\n",
      "fc layer 3 self.abs_max_out: 836.0\n",
      "lif layer 1 self.abs_max_v: 8275.0\n",
      "fc layer 2 self.abs_max_out: 3638.0\n",
      "fc layer 2 self.abs_max_out: 3714.0\n",
      "fc layer 2 self.abs_max_out: 3967.0\n",
      "fc layer 2 self.abs_max_out: 3971.0\n",
      "fc layer 2 self.abs_max_out: 4009.0\n",
      "lif layer 1 self.abs_max_v: 8670.5\n",
      "lif layer 1 self.abs_max_v: 9052.5\n",
      "lif layer 1 self.abs_max_v: 9296.5\n",
      "lif layer 1 self.abs_max_v: 9570.5\n",
      "fc layer 2 self.abs_max_out: 4066.0\n",
      "fc layer 2 self.abs_max_out: 4228.0\n",
      "lif layer 1 self.abs_max_v: 9657.0\n",
      "lif layer 2 self.abs_max_v: 6150.5\n",
      "lif layer 1 self.abs_max_v: 10497.5\n",
      "lif layer 2 self.abs_max_v: 6458.5\n",
      "lif layer 2 self.abs_max_v: 6646.0\n",
      "lif layer 2 self.abs_max_v: 6738.0\n",
      "fc layer 3 self.abs_max_out: 865.0\n",
      "fc layer 3 self.abs_max_out: 898.0\n",
      "fc layer 1 self.abs_max_out: 7772.0\n",
      "lif layer 1 self.abs_max_v: 10775.0\n",
      "fc layer 2 self.abs_max_out: 4475.0\n",
      "fc layer 1 self.abs_max_out: 7787.0\n",
      "lif layer 1 self.abs_max_v: 11151.5\n",
      "lif layer 1 self.abs_max_v: 11676.5\n",
      "lif layer 1 self.abs_max_v: 12804.5\n",
      "fc layer 1 self.abs_max_out: 8273.0\n",
      "lif layer 1 self.abs_max_v: 13522.0\n",
      "lif layer 1 self.abs_max_v: 14047.0\n",
      "lif layer 1 self.abs_max_v: 14787.0\n",
      "lif layer 1 self.abs_max_v: 15470.5\n",
      "lif layer 1 self.abs_max_v: 15776.5\n",
      "fc layer 1 self.abs_max_out: 8309.0\n",
      "fc layer 1 self.abs_max_out: 8950.0\n",
      "fc layer 2 self.abs_max_out: 4542.0\n",
      "fc layer 1 self.abs_max_out: 9327.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.061637/  2.112937, val:  41.67%, val_best:  41.67%, tr:  73.65%, tr_best:  73.65%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 67.4411%\n",
      "layer   3  Sparsity: 78.7397%\n",
      "total_backward_count 9790 real_backward_count 4341  44.341%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "lif layer 2 self.abs_max_v: 6750.5\n",
      "lif layer 1 self.abs_max_v: 16092.0\n",
      "lif layer 1 self.abs_max_v: 16169.0\n",
      "lif layer 1 self.abs_max_v: 16854.5\n",
      "fc layer 1 self.abs_max_out: 9651.0\n",
      "fc layer 1 self.abs_max_out: 10523.0\n",
      "lif layer 2 self.abs_max_v: 6937.0\n",
      "lif layer 2 self.abs_max_v: 7076.5\n",
      "lif layer 1 self.abs_max_v: 17851.0\n",
      "lif layer 1 self.abs_max_v: 18123.5\n",
      "fc layer 2 self.abs_max_out: 4594.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.053519/  2.114756, val:  49.58%, val_best:  49.58%, tr:  90.30%, tr_best:  90.30%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 72.3823%\n",
      "layer   3  Sparsity: 80.1534%\n",
      "total_backward_count 19580 real_backward_count 7344  37.508%\n",
      "lif layer 2 self.abs_max_v: 7136.5\n",
      "fc layer 2 self.abs_max_out: 4790.0\n",
      "lif layer 2 self.abs_max_v: 7215.0\n",
      "lif layer 2 self.abs_max_v: 7222.5\n",
      "lif layer 2 self.abs_max_v: 7287.0\n",
      "fc layer 1 self.abs_max_out: 10616.0\n",
      "fc layer 1 self.abs_max_out: 10818.0\n",
      "lif layer 1 self.abs_max_v: 18759.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.071353/  2.141417, val:  50.00%, val_best:  50.00%, tr:  94.59%, tr_best:  94.59%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 72.9884%\n",
      "layer   3  Sparsity: 79.8553%\n",
      "total_backward_count 29370 real_backward_count 9875  33.623%\n",
      "lif layer 2 self.abs_max_v: 7453.0\n",
      "lif layer 2 self.abs_max_v: 7625.5\n",
      "lif layer 2 self.abs_max_v: 7658.0\n",
      "fc layer 1 self.abs_max_out: 11459.0\n",
      "lif layer 1 self.abs_max_v: 19394.5\n",
      "lif layer 1 self.abs_max_v: 20482.5\n",
      "fc layer 1 self.abs_max_out: 11528.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.075636/  2.132857, val:  46.25%, val_best:  50.00%, tr:  95.81%, tr_best:  95.81%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2417%\n",
      "layer   3  Sparsity: 79.6067%\n",
      "total_backward_count 39160 real_backward_count 12298  31.404%\n",
      "fc layer 2 self.abs_max_out: 4918.0\n",
      "fc layer 2 self.abs_max_out: 5039.0\n",
      "fc layer 2 self.abs_max_out: 5098.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.084738/  2.145395, val:  53.75%, val_best:  53.75%, tr:  96.94%, tr_best:  96.94%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5225%\n",
      "layer   3  Sparsity: 79.2896%\n",
      "total_backward_count 48950 real_backward_count 14568  29.761%\n",
      "fc layer 2 self.abs_max_out: 5300.0\n",
      "lif layer 2 self.abs_max_v: 7904.5\n",
      "fc layer 1 self.abs_max_out: 12400.0\n",
      "lif layer 1 self.abs_max_v: 20909.0\n",
      "lif layer 1 self.abs_max_v: 21914.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.080264/  2.147431, val:  43.33%, val_best:  53.75%, tr:  96.83%, tr_best:  96.94%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4348%\n",
      "layer   3  Sparsity: 78.4511%\n",
      "total_backward_count 58740 real_backward_count 16764  28.539%\n",
      "fc layer 1 self.abs_max_out: 13118.0\n",
      "lif layer 1 self.abs_max_v: 22467.5\n",
      "lif layer 1 self.abs_max_v: 23137.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.083667/  2.143895, val:  54.17%, val_best:  54.17%, tr:  98.06%, tr_best:  98.06%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.9525%\n",
      "layer   3  Sparsity: 78.8182%\n",
      "total_backward_count 68530 real_backward_count 18857  27.516%\n",
      "lif layer 2 self.abs_max_v: 7966.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.083243/  2.129394, val:  48.75%, val_best:  54.17%, tr:  98.16%, tr_best:  98.16%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.9781%\n",
      "layer   3  Sparsity: 78.7731%\n",
      "total_backward_count 78320 real_backward_count 20737  26.477%\n",
      "lif layer 2 self.abs_max_v: 8051.0\n",
      "fc layer 2 self.abs_max_out: 5427.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.091049/  2.136512, val:  55.83%, val_best:  55.83%, tr:  98.16%, tr_best:  98.16%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 76.0163%\n",
      "layer   3  Sparsity: 79.5169%\n",
      "total_backward_count 88110 real_backward_count 22731  25.798%\n",
      "fc layer 1 self.abs_max_out: 13721.0\n",
      "lif layer 1 self.abs_max_v: 23399.5\n",
      "lif layer 2 self.abs_max_v: 8107.0\n",
      "lif layer 2 self.abs_max_v: 8108.5\n",
      "lif layer 2 self.abs_max_v: 8202.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.097047/  2.158810, val:  50.42%, val_best:  55.83%, tr:  97.96%, tr_best:  98.16%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 76.1808%\n",
      "layer   3  Sparsity: 79.9855%\n",
      "total_backward_count 97900 real_backward_count 24538  25.064%\n",
      "lif layer 2 self.abs_max_v: 8355.5\n",
      "lif layer 2 self.abs_max_v: 8407.0\n",
      "fc layer 1 self.abs_max_out: 14051.0\n",
      "lif layer 1 self.abs_max_v: 23741.0\n",
      "lif layer 1 self.abs_max_v: 24238.5\n",
      "lif layer 2 self.abs_max_v: 8432.5\n",
      "lif layer 2 self.abs_max_v: 8533.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.102614/  2.162199, val:  46.25%, val_best:  55.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.5610%\n",
      "layer   3  Sparsity: 80.7109%\n",
      "total_backward_count 107690 real_backward_count 26342  24.461%\n",
      "lif layer 2 self.abs_max_v: 8813.0\n",
      "lif layer 2 self.abs_max_v: 9383.5\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.113321/  2.153507, val:  51.67%, val_best:  55.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.2982%\n",
      "layer   3  Sparsity: 81.0517%\n",
      "total_backward_count 117480 real_backward_count 28092  23.912%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.112878/  2.160738, val:  49.58%, val_best:  55.83%, tr:  99.39%, tr_best:  99.39%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.9412%\n",
      "layer   3  Sparsity: 81.2651%\n",
      "total_backward_count 127270 real_backward_count 29723  23.354%\n",
      "fc layer 1 self.abs_max_out: 14514.0\n",
      "fc layer 1 self.abs_max_out: 14952.0\n",
      "lif layer 1 self.abs_max_v: 25406.5\n",
      "lif layer 1 self.abs_max_v: 25514.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.122797/  2.175964, val:  49.58%, val_best:  55.83%, tr:  99.08%, tr_best:  99.39%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.6881%\n",
      "layer   3  Sparsity: 81.2514%\n",
      "total_backward_count 137060 real_backward_count 31401  22.910%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.130510/  2.176896, val:  54.17%, val_best:  55.83%, tr:  99.28%, tr_best:  99.39%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 76.4879%\n",
      "layer   3  Sparsity: 81.9072%\n",
      "total_backward_count 146850 real_backward_count 33111  22.547%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.138578/  2.180048, val:  43.33%, val_best:  55.83%, tr:  99.18%, tr_best:  99.39%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 76.2548%\n",
      "layer   3  Sparsity: 82.3043%\n",
      "total_backward_count 156640 real_backward_count 34766  22.195%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.133286/  2.174559, val:  56.67%, val_best:  56.67%, tr:  99.28%, tr_best:  99.39%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 76.0857%\n",
      "layer   3  Sparsity: 82.3176%\n",
      "total_backward_count 166430 real_backward_count 36307  21.815%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.133485/  2.177355, val:  57.50%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.8757%\n",
      "layer   3  Sparsity: 82.2496%\n",
      "total_backward_count 176220 real_backward_count 37880  21.496%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.140322/  2.168246, val:  61.25%, val_best:  61.25%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.7461%\n",
      "layer   3  Sparsity: 82.2210%\n",
      "total_backward_count 186010 real_backward_count 39453  21.210%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.138945/  2.188656, val:  45.42%, val_best:  61.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 76.0086%\n",
      "layer   3  Sparsity: 82.6961%\n",
      "total_backward_count 195800 real_backward_count 40920  20.899%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.137793/  2.187208, val:  46.67%, val_best:  61.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.8075%\n",
      "layer   3  Sparsity: 82.6708%\n",
      "total_backward_count 205590 real_backward_count 42379  20.613%\n",
      "fc layer 1 self.abs_max_out: 15117.0\n",
      "lif layer 2 self.abs_max_v: 9437.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.143464/  2.190126, val:  56.25%, val_best:  61.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.5002%\n",
      "layer   3  Sparsity: 83.2193%\n",
      "total_backward_count 215380 real_backward_count 43864  20.366%\n",
      "fc layer 1 self.abs_max_out: 15537.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.150803/  2.182219, val:  54.58%, val_best:  61.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.3525%\n",
      "layer   3  Sparsity: 83.1481%\n",
      "total_backward_count 225170 real_backward_count 45340  20.136%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.149327/  2.190267, val:  56.25%, val_best:  61.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.4767%\n",
      "layer   3  Sparsity: 83.6803%\n",
      "total_backward_count 234960 real_backward_count 46833  19.932%\n",
      "fc layer 1 self.abs_max_out: 15554.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.151107/  2.183318, val:  54.58%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.1817%\n",
      "layer   3  Sparsity: 83.7323%\n",
      "total_backward_count 244750 real_backward_count 48288  19.730%\n",
      "lif layer 1 self.abs_max_v: 25623.0\n",
      "fc layer 1 self.abs_max_out: 15579.0\n",
      "fc layer 1 self.abs_max_out: 15708.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.149596/  2.188462, val:  65.00%, val_best:  65.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.2169%\n",
      "layer   3  Sparsity: 83.6782%\n",
      "total_backward_count 254540 real_backward_count 49762  19.550%\n",
      "lif layer 1 self.abs_max_v: 26033.5\n",
      "lif layer 1 self.abs_max_v: 26494.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.157224/  2.194098, val:  55.00%, val_best:  65.00%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.9493%\n",
      "layer   3  Sparsity: 83.6486%\n",
      "total_backward_count 264330 real_backward_count 51218  19.377%\n",
      "fc layer 1 self.abs_max_out: 15731.0\n",
      "fc layer 1 self.abs_max_out: 15805.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.154155/  2.185744, val:  61.67%, val_best:  65.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 75.0953%\n",
      "layer   3  Sparsity: 83.8495%\n",
      "total_backward_count 274120 real_backward_count 52598  19.188%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.154954/  2.192199, val:  61.25%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.7125%\n",
      "layer   3  Sparsity: 83.9871%\n",
      "total_backward_count 283910 real_backward_count 53940  18.999%\n",
      "fc layer 1 self.abs_max_out: 15904.0\n",
      "lif layer 1 self.abs_max_v: 26719.5\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.158971/  2.198295, val:  47.50%, val_best:  65.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4802%\n",
      "layer   3  Sparsity: 83.8376%\n",
      "total_backward_count 293700 real_backward_count 55333  18.840%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.159139/  2.198636, val:  54.58%, val_best:  65.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1579%\n",
      "layer   3  Sparsity: 83.6398%\n",
      "total_backward_count 303490 real_backward_count 56671  18.673%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.161642/  2.205144, val:  57.92%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5562%\n",
      "layer   3  Sparsity: 84.5644%\n",
      "total_backward_count 313280 real_backward_count 58002  18.514%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.163620/  2.202865, val:  59.17%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.7191%\n",
      "layer   3  Sparsity: 84.6972%\n",
      "total_backward_count 323070 real_backward_count 59270  18.346%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.168701/  2.206764, val:  58.33%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.9612%\n",
      "layer   3  Sparsity: 85.0031%\n",
      "total_backward_count 332860 real_backward_count 60558  18.193%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.166751/  2.205886, val:  57.08%, val_best:  65.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4144%\n",
      "layer   3  Sparsity: 84.8674%\n",
      "total_backward_count 342650 real_backward_count 61846  18.049%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.168581/  2.210346, val:  57.08%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3573%\n",
      "layer   3  Sparsity: 85.0906%\n",
      "total_backward_count 352440 real_backward_count 63131  17.913%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.169496/  2.199169, val:  58.75%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3569%\n",
      "layer   3  Sparsity: 85.3509%\n",
      "total_backward_count 362230 real_backward_count 64460  17.795%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.167532/  2.208611, val:  54.58%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3132%\n",
      "layer   3  Sparsity: 84.9747%\n",
      "total_backward_count 372020 real_backward_count 65700  17.660%\n",
      "fc layer 1 self.abs_max_out: 16657.0\n",
      "lif layer 1 self.abs_max_v: 26733.5\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.162696/  2.205184, val:  55.83%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4942%\n",
      "layer   3  Sparsity: 84.9656%\n",
      "total_backward_count 381810 real_backward_count 67008  17.550%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.168908/  2.209692, val:  51.67%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3206%\n",
      "layer   3  Sparsity: 85.4390%\n",
      "total_backward_count 391600 real_backward_count 68267  17.433%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.166233/  2.195723, val:  56.67%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1334%\n",
      "layer   3  Sparsity: 85.1680%\n",
      "total_backward_count 401390 real_backward_count 69555  17.329%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.166354/  2.198259, val:  62.08%, val_best:  65.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1890%\n",
      "layer   3  Sparsity: 85.2295%\n",
      "total_backward_count 411180 real_backward_count 70877  17.237%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.166576/  2.204558, val:  57.50%, val_best:  65.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2064%\n",
      "layer   3  Sparsity: 85.6016%\n",
      "total_backward_count 420970 real_backward_count 72118  17.131%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.172166/  2.206998, val:  64.17%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4617%\n",
      "layer   3  Sparsity: 85.6650%\n",
      "total_backward_count 430760 real_backward_count 73325  17.022%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.174159/  2.212311, val:  57.50%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2721%\n",
      "layer   3  Sparsity: 85.9792%\n",
      "total_backward_count 440550 real_backward_count 74576  16.928%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.175251/  2.208684, val:  55.00%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1906%\n",
      "layer   3  Sparsity: 85.8426%\n",
      "total_backward_count 450340 real_backward_count 75782  16.828%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.176283/  2.206533, val:  57.92%, val_best:  65.00%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.5552%\n",
      "layer   3  Sparsity: 85.4680%\n",
      "total_backward_count 460130 real_backward_count 77024  16.740%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.173299/  2.208013, val:  59.17%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7955%\n",
      "layer   3  Sparsity: 85.6912%\n",
      "total_backward_count 469920 real_backward_count 78212  16.644%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.179834/  2.212466, val:  69.17%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2523%\n",
      "layer   3  Sparsity: 86.0776%\n",
      "total_backward_count 479710 real_backward_count 79385  16.549%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.178267/  2.210526, val:  61.25%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2534%\n",
      "layer   3  Sparsity: 86.0012%\n",
      "total_backward_count 489500 real_backward_count 80601  16.466%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.179910/  2.213662, val:  52.50%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.5690%\n",
      "layer   3  Sparsity: 85.9064%\n",
      "total_backward_count 499290 real_backward_count 81803  16.384%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.176792/  2.212485, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0155%\n",
      "layer   3  Sparsity: 85.9474%\n",
      "total_backward_count 509080 real_backward_count 82940  16.292%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.179425/  2.213058, val:  62.50%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0434%\n",
      "layer   3  Sparsity: 86.1691%\n",
      "total_backward_count 518870 real_backward_count 84120  16.212%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.176763/  2.207492, val:  62.92%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9724%\n",
      "layer   3  Sparsity: 86.2434%\n",
      "total_backward_count 528660 real_backward_count 85326  16.140%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.170030/  2.220330, val:  56.25%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7689%\n",
      "layer   3  Sparsity: 86.3895%\n",
      "total_backward_count 538450 real_backward_count 86537  16.072%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.178200/  2.220906, val:  60.42%, val_best:  69.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7331%\n",
      "layer   3  Sparsity: 86.5128%\n",
      "total_backward_count 548240 real_backward_count 87763  16.008%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.178476/  2.210818, val:  53.33%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8923%\n",
      "layer   3  Sparsity: 86.4274%\n",
      "total_backward_count 558030 real_backward_count 88949  15.940%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.177691/  2.218214, val:  62.08%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8561%\n",
      "layer   3  Sparsity: 86.6299%\n",
      "total_backward_count 567820 real_backward_count 90049  15.859%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.176568/  2.206028, val:  58.75%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7679%\n",
      "layer   3  Sparsity: 86.8280%\n",
      "total_backward_count 577610 real_backward_count 91156  15.782%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.174006/  2.215435, val:  61.25%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6884%\n",
      "layer   3  Sparsity: 86.5827%\n",
      "total_backward_count 587400 real_backward_count 92308  15.715%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.173487/  2.208078, val:  60.83%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2546%\n",
      "layer   3  Sparsity: 86.3884%\n",
      "total_backward_count 597190 real_backward_count 93453  15.649%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.172402/  2.201301, val:  61.67%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0361%\n",
      "layer   3  Sparsity: 86.5532%\n",
      "total_backward_count 606980 real_backward_count 94575  15.581%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.176737/  2.216721, val:  57.50%, val_best:  69.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0894%\n",
      "layer   3  Sparsity: 86.8023%\n",
      "total_backward_count 616770 real_backward_count 95743  15.523%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.181365/  2.212708, val:  59.17%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7238%\n",
      "layer   3  Sparsity: 86.7523%\n",
      "total_backward_count 626560 real_backward_count 96847  15.457%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.175684/  2.214977, val:  58.33%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6505%\n",
      "layer   3  Sparsity: 86.5389%\n",
      "total_backward_count 636350 real_backward_count 97993  15.399%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.180819/  2.213265, val:  57.08%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6255%\n",
      "layer   3  Sparsity: 86.8238%\n",
      "total_backward_count 646140 real_backward_count 99102  15.338%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.178071/  2.215702, val:  61.25%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7512%\n",
      "layer   3  Sparsity: 86.3617%\n",
      "total_backward_count 655930 real_backward_count 100190  15.274%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.177195/  2.205824, val:  60.42%, val_best:  69.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8662%\n",
      "layer   3  Sparsity: 86.5318%\n",
      "total_backward_count 665720 real_backward_count 101334  15.222%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.175939/  2.206154, val:  64.58%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9838%\n",
      "layer   3  Sparsity: 86.8283%\n",
      "total_backward_count 675510 real_backward_count 102435  15.164%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.179300/  2.221436, val:  48.75%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.2789%\n",
      "layer   3  Sparsity: 86.7784%\n",
      "total_backward_count 685300 real_backward_count 103497  15.102%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.180183/  2.211039, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7861%\n",
      "layer   3  Sparsity: 86.8879%\n",
      "total_backward_count 695090 real_backward_count 104631  15.053%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.180857/  2.215638, val:  55.83%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9252%\n",
      "layer   3  Sparsity: 86.9368%\n",
      "total_backward_count 704880 real_backward_count 105728  14.999%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.181585/  2.216621, val:  59.58%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8985%\n",
      "layer   3  Sparsity: 86.9016%\n",
      "total_backward_count 714670 real_backward_count 106812  14.946%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.180668/  2.220358, val:  60.83%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.5142%\n",
      "layer   3  Sparsity: 87.0379%\n",
      "total_backward_count 724460 real_backward_count 107882  14.891%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.185683/  2.224721, val:  59.58%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8248%\n",
      "layer   3  Sparsity: 87.0234%\n",
      "total_backward_count 734250 real_backward_count 108994  14.844%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.181926/  2.218100, val:  63.75%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8370%\n",
      "layer   3  Sparsity: 86.8734%\n",
      "total_backward_count 744040 real_backward_count 110064  14.793%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.183955/  2.213441, val:  66.25%, val_best:  69.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9988%\n",
      "layer   3  Sparsity: 87.0328%\n",
      "total_backward_count 753830 real_backward_count 111170  14.747%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.184008/  2.224838, val:  52.08%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9680%\n",
      "layer   3  Sparsity: 86.9722%\n",
      "total_backward_count 763620 real_backward_count 112221  14.696%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.185004/  2.221954, val:  60.00%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7329%\n",
      "layer   3  Sparsity: 87.0236%\n",
      "total_backward_count 773410 real_backward_count 113304  14.650%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.187615/  2.226827, val:  58.75%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0408%\n",
      "layer   3  Sparsity: 87.1626%\n",
      "total_backward_count 783200 real_backward_count 114372  14.603%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.190274/  2.221465, val:  62.08%, val_best:  69.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9928%\n",
      "layer   3  Sparsity: 87.2229%\n",
      "total_backward_count 792990 real_backward_count 115449  14.559%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.193123/  2.230638, val:  55.42%, val_best:  69.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9786%\n",
      "layer   3  Sparsity: 87.5087%\n",
      "total_backward_count 802780 real_backward_count 116505  14.513%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.188820/  2.220718, val:  66.67%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1800%\n",
      "layer   3  Sparsity: 86.9964%\n",
      "total_backward_count 812570 real_backward_count 117575  14.470%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.182507/  2.224799, val:  59.58%, val_best:  69.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3363%\n",
      "layer   3  Sparsity: 87.2527%\n",
      "total_backward_count 822360 real_backward_count 118645  14.427%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.181709/  2.215832, val:  61.67%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8861%\n",
      "layer   3  Sparsity: 87.1861%\n",
      "total_backward_count 832150 real_backward_count 119700  14.384%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.183043/  2.218340, val:  65.83%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8793%\n",
      "layer   3  Sparsity: 87.2687%\n",
      "total_backward_count 841940 real_backward_count 120730  14.340%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.181599/  2.211031, val:  58.33%, val_best:  69.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9064%\n",
      "layer   3  Sparsity: 87.2977%\n",
      "total_backward_count 851730 real_backward_count 121765  14.296%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.176105/  2.213016, val:  64.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2336%\n",
      "layer   3  Sparsity: 87.2193%\n",
      "total_backward_count 861520 real_backward_count 122872  14.262%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.177176/  2.210798, val:  62.08%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7219%\n",
      "layer   3  Sparsity: 86.6270%\n",
      "total_backward_count 871310 real_backward_count 123955  14.226%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.178563/  2.206673, val:  61.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0148%\n",
      "layer   3  Sparsity: 86.5451%\n",
      "total_backward_count 881100 real_backward_count 125008  14.188%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.177361/  2.215838, val:  58.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6759%\n",
      "layer   3  Sparsity: 86.9239%\n",
      "total_backward_count 890890 real_backward_count 126070  14.151%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.183315/  2.213279, val:  66.25%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2491%\n",
      "layer   3  Sparsity: 87.0401%\n",
      "total_backward_count 900680 real_backward_count 127087  14.110%\n",
      "lif layer 1 self.abs_max_v: 27366.0\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.178899/  2.214454, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0110%\n",
      "layer   3  Sparsity: 86.9410%\n",
      "total_backward_count 910470 real_backward_count 128097  14.069%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.176349/  2.213367, val:  61.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7952%\n",
      "layer   3  Sparsity: 87.0416%\n",
      "total_backward_count 920260 real_backward_count 129113  14.030%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.184383/  2.218729, val:  60.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9379%\n",
      "layer   3  Sparsity: 87.0133%\n",
      "total_backward_count 930050 real_backward_count 130148  13.994%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.182097/  2.224812, val:  62.50%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7187%\n",
      "layer   3  Sparsity: 87.1237%\n",
      "total_backward_count 939840 real_backward_count 131204  13.960%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.184756/  2.219850, val:  54.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8667%\n",
      "layer   3  Sparsity: 86.9161%\n",
      "total_backward_count 949630 real_backward_count 132251  13.927%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.180847/  2.213003, val:  65.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2639%\n",
      "layer   3  Sparsity: 87.2671%\n",
      "total_backward_count 959420 real_backward_count 133316  13.895%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.186598/  2.230807, val:  54.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7512%\n",
      "layer   3  Sparsity: 87.0085%\n",
      "total_backward_count 969210 real_backward_count 134343  13.861%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.188811/  2.219976, val:  58.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0805%\n",
      "layer   3  Sparsity: 87.3258%\n",
      "total_backward_count 979000 real_backward_count 135378  13.828%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.188426/  2.220467, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0423%\n",
      "layer   3  Sparsity: 87.4347%\n",
      "total_backward_count 988790 real_backward_count 136381  13.793%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.189051/  2.223082, val:  64.58%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9742%\n",
      "layer   3  Sparsity: 87.3640%\n",
      "total_backward_count 998580 real_backward_count 137373  13.757%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.178504/  2.209295, val:  63.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9036%\n",
      "layer   3  Sparsity: 87.0004%\n",
      "total_backward_count 1008370 real_backward_count 138431  13.728%\n",
      "lif layer 2 self.abs_max_v: 9636.0\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.174369/  2.215527, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7612%\n",
      "layer   3  Sparsity: 86.8656%\n",
      "total_backward_count 1018160 real_backward_count 139475  13.699%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.174234/  2.213829, val:  59.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2247%\n",
      "layer   3  Sparsity: 86.9024%\n",
      "total_backward_count 1027950 real_backward_count 140453  13.663%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.179890/  2.217078, val:  57.92%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8144%\n",
      "layer   3  Sparsity: 86.9189%\n",
      "total_backward_count 1037740 real_backward_count 141461  13.632%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.179593/  2.214344, val:  59.58%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0398%\n",
      "layer   3  Sparsity: 87.3070%\n",
      "total_backward_count 1047530 real_backward_count 142477  13.601%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.178957/  2.217304, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0279%\n",
      "layer   3  Sparsity: 87.2377%\n",
      "total_backward_count 1057320 real_backward_count 143482  13.570%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.182344/  2.224368, val:  56.25%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8828%\n",
      "layer   3  Sparsity: 87.1609%\n",
      "total_backward_count 1067110 real_backward_count 144511  13.542%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.183507/  2.219352, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6342%\n",
      "layer   3  Sparsity: 87.4037%\n",
      "total_backward_count 1076900 real_backward_count 145508  13.512%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.187878/  2.219479, val:  65.83%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6377%\n",
      "layer   3  Sparsity: 87.1938%\n",
      "total_backward_count 1086690 real_backward_count 146526  13.484%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.187492/  2.221830, val:  63.75%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7637%\n",
      "layer   3  Sparsity: 87.0007%\n",
      "total_backward_count 1096480 real_backward_count 147488  13.451%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.186363/  2.218448, val:  66.67%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9792%\n",
      "layer   3  Sparsity: 87.1792%\n",
      "total_backward_count 1106270 real_backward_count 148435  13.418%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.183838/  2.222019, val:  59.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0585%\n",
      "layer   3  Sparsity: 87.3912%\n",
      "total_backward_count 1116060 real_backward_count 149379  13.384%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.182003/  2.215626, val:  62.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6969%\n",
      "layer   3  Sparsity: 86.8355%\n",
      "total_backward_count 1125850 real_backward_count 150381  13.357%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.182683/  2.210870, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0381%\n",
      "layer   3  Sparsity: 86.8770%\n",
      "total_backward_count 1135640 real_backward_count 151351  13.327%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.179618/  2.215796, val:  61.67%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1822%\n",
      "layer   3  Sparsity: 86.8345%\n",
      "total_backward_count 1145430 real_backward_count 152352  13.301%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.180657/  2.211334, val:  60.00%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6397%\n",
      "layer   3  Sparsity: 86.9557%\n",
      "total_backward_count 1155220 real_backward_count 153387  13.278%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.182062/  2.219276, val:  57.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.5539%\n",
      "layer   3  Sparsity: 86.8342%\n",
      "total_backward_count 1165010 real_backward_count 154401  13.253%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.181092/  2.209606, val:  63.33%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.5785%\n",
      "layer   3  Sparsity: 86.8617%\n",
      "total_backward_count 1174800 real_backward_count 155420  13.229%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.178301/  2.221237, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9822%\n",
      "layer   3  Sparsity: 87.1234%\n",
      "total_backward_count 1184590 real_backward_count 156393  13.202%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.184423/  2.221114, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0600%\n",
      "layer   3  Sparsity: 87.0838%\n",
      "total_backward_count 1194380 real_backward_count 157355  13.175%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.183772/  2.220622, val:  56.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9763%\n",
      "layer   3  Sparsity: 86.8968%\n",
      "total_backward_count 1204170 real_backward_count 158359  13.151%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.179651/  2.218086, val:  63.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8765%\n",
      "layer   3  Sparsity: 86.9172%\n",
      "total_backward_count 1213960 real_backward_count 159351  13.127%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.182859/  2.211199, val:  64.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7402%\n",
      "layer   3  Sparsity: 86.9059%\n",
      "total_backward_count 1223750 real_backward_count 160372  13.105%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.183041/  2.219814, val:  62.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1187%\n",
      "layer   3  Sparsity: 87.1228%\n",
      "total_backward_count 1233540 real_backward_count 161367  13.082%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.183771/  2.220097, val:  57.50%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2376%\n",
      "layer   3  Sparsity: 87.0920%\n",
      "total_backward_count 1243330 real_backward_count 162333  13.056%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.176834/  2.214627, val:  64.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1224%\n",
      "layer   3  Sparsity: 86.9286%\n",
      "total_backward_count 1253120 real_backward_count 163309  13.032%\n",
      "lif layer 1 self.abs_max_v: 27877.5\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.176073/  2.216409, val:  60.83%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8233%\n",
      "layer   3  Sparsity: 86.9374%\n",
      "total_backward_count 1262910 real_backward_count 164273  13.007%\n",
      "lif layer 1 self.abs_max_v: 27940.0\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.177546/  2.214436, val:  62.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1923%\n",
      "layer   3  Sparsity: 86.8184%\n",
      "total_backward_count 1272700 real_backward_count 165202  12.980%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.179814/  2.218361, val:  53.75%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0366%\n",
      "layer   3  Sparsity: 87.1134%\n",
      "total_backward_count 1282490 real_backward_count 166185  12.958%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.178999/  2.210592, val:  61.67%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3389%\n",
      "layer   3  Sparsity: 87.1291%\n",
      "total_backward_count 1292280 real_backward_count 167215  12.940%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.179894/  2.220289, val:  61.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1096%\n",
      "layer   3  Sparsity: 87.2274%\n",
      "total_backward_count 1302070 real_backward_count 168166  12.915%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.181551/  2.219713, val:  52.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1010%\n",
      "layer   3  Sparsity: 87.0075%\n",
      "total_backward_count 1311860 real_backward_count 169184  12.896%\n",
      "lif layer 1 self.abs_max_v: 28139.0\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.181569/  2.213451, val:  59.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0467%\n",
      "layer   3  Sparsity: 86.8532%\n",
      "total_backward_count 1321650 real_backward_count 170159  12.875%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.178279/  2.214568, val:  65.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.7616%\n",
      "layer   3  Sparsity: 86.8253%\n",
      "total_backward_count 1331440 real_backward_count 171131  12.853%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.179619/  2.212250, val:  57.50%, val_best:  69.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9341%\n",
      "layer   3  Sparsity: 87.2011%\n",
      "total_backward_count 1341230 real_backward_count 172099  12.831%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.177762/  2.216480, val:  59.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9650%\n",
      "layer   3  Sparsity: 86.8991%\n",
      "total_backward_count 1351020 real_backward_count 173054  12.809%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.177041/  2.214181, val:  61.25%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1590%\n",
      "layer   3  Sparsity: 87.0897%\n",
      "total_backward_count 1360810 real_backward_count 174034  12.789%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.182019/  2.218962, val:  59.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2765%\n",
      "layer   3  Sparsity: 87.1270%\n",
      "total_backward_count 1370600 real_backward_count 175036  12.771%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.183927/  2.222405, val:  60.83%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2622%\n",
      "layer   3  Sparsity: 86.9471%\n",
      "total_backward_count 1380390 real_backward_count 175971  12.748%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.182117/  2.216710, val:  55.00%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2607%\n",
      "layer   3  Sparsity: 86.9759%\n",
      "total_backward_count 1390180 real_backward_count 176900  12.725%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.180562/  2.217504, val:  65.42%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1156%\n",
      "layer   3  Sparsity: 87.2790%\n",
      "total_backward_count 1399970 real_backward_count 177924  12.709%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.176590/  2.218929, val:  57.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0960%\n",
      "layer   3  Sparsity: 87.2147%\n",
      "total_backward_count 1409760 real_backward_count 178930  12.692%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.181133/  2.217840, val:  60.42%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0869%\n",
      "layer   3  Sparsity: 87.3553%\n",
      "total_backward_count 1419550 real_backward_count 179861  12.670%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.183309/  2.221126, val:  52.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3258%\n",
      "layer   3  Sparsity: 86.9974%\n",
      "total_backward_count 1429340 real_backward_count 180822  12.651%\n",
      "lif layer 1 self.abs_max_v: 28548.5\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.182361/  2.215699, val:  65.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5134%\n",
      "layer   3  Sparsity: 87.1376%\n",
      "total_backward_count 1439130 real_backward_count 181789  12.632%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.183559/  2.229307, val:  62.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2875%\n",
      "layer   3  Sparsity: 87.4231%\n",
      "total_backward_count 1448920 real_backward_count 182681  12.608%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.182491/  2.216590, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0844%\n",
      "layer   3  Sparsity: 87.0033%\n",
      "total_backward_count 1458710 real_backward_count 183636  12.589%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.180101/  2.218983, val:  62.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3831%\n",
      "layer   3  Sparsity: 87.5155%\n",
      "total_backward_count 1468500 real_backward_count 184608  12.571%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.181842/  2.226027, val:  55.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4267%\n",
      "layer   3  Sparsity: 87.1740%\n",
      "total_backward_count 1478290 real_backward_count 185521  12.550%\n",
      "lif layer 1 self.abs_max_v: 28886.5\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.186599/  2.218119, val:  62.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0920%\n",
      "layer   3  Sparsity: 87.1964%\n",
      "total_backward_count 1488080 real_backward_count 186510  12.534%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.180212/  2.220920, val:  54.58%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4387%\n",
      "layer   3  Sparsity: 86.9825%\n",
      "total_backward_count 1497870 real_backward_count 187444  12.514%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.181102/  2.213418, val:  64.58%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5131%\n",
      "layer   3  Sparsity: 87.2370%\n",
      "total_backward_count 1507660 real_backward_count 188420  12.498%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.180364/  2.222323, val:  54.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2471%\n",
      "layer   3  Sparsity: 86.7936%\n",
      "total_backward_count 1517450 real_backward_count 189325  12.477%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.175049/  2.215291, val:  64.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.6470%\n",
      "layer   3  Sparsity: 87.0762%\n",
      "total_backward_count 1527240 real_backward_count 190264  12.458%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.182134/  2.222552, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2221%\n",
      "layer   3  Sparsity: 87.1685%\n",
      "total_backward_count 1537030 real_backward_count 191194  12.439%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.180552/  2.218604, val:  62.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3493%\n",
      "layer   3  Sparsity: 87.1738%\n",
      "total_backward_count 1546820 real_backward_count 192110  12.420%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.182341/  2.220945, val:  59.58%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9900%\n",
      "layer   3  Sparsity: 87.1704%\n",
      "total_backward_count 1556610 real_backward_count 193118  12.406%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.181550/  2.216032, val:  62.50%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4320%\n",
      "layer   3  Sparsity: 87.2265%\n",
      "total_backward_count 1566400 real_backward_count 194033  12.387%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.180819/  2.218222, val:  60.83%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9596%\n",
      "layer   3  Sparsity: 87.2100%\n",
      "total_backward_count 1576190 real_backward_count 194996  12.371%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.187629/  2.228704, val:  61.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.8475%\n",
      "layer   3  Sparsity: 87.2553%\n",
      "total_backward_count 1585980 real_backward_count 196010  12.359%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.186789/  2.222969, val:  64.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5112%\n",
      "layer   3  Sparsity: 87.1554%\n",
      "total_backward_count 1595770 real_backward_count 196944  12.342%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.181127/  2.221399, val:  66.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3955%\n",
      "layer   3  Sparsity: 87.0374%\n",
      "total_backward_count 1605560 real_backward_count 197880  12.325%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.186498/  2.222192, val:  61.25%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.6449%\n",
      "layer   3  Sparsity: 87.4646%\n",
      "total_backward_count 1615350 real_backward_count 198823  12.308%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.182900/  2.218905, val:  62.08%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2832%\n",
      "layer   3  Sparsity: 87.2660%\n",
      "total_backward_count 1625140 real_backward_count 199752  12.291%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.181237/  2.224752, val:  65.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5709%\n",
      "layer   3  Sparsity: 87.2583%\n",
      "total_backward_count 1634930 real_backward_count 200614  12.270%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.183050/  2.218786, val:  58.75%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2275%\n",
      "layer   3  Sparsity: 87.0609%\n",
      "total_backward_count 1644720 real_backward_count 201565  12.255%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.178640/  2.218853, val:  64.17%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5311%\n",
      "layer   3  Sparsity: 87.2311%\n",
      "total_backward_count 1654510 real_backward_count 202490  12.239%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.177298/  2.222091, val:  60.83%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4557%\n",
      "layer   3  Sparsity: 86.9648%\n",
      "total_backward_count 1664300 real_backward_count 203396  12.221%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.178829/  2.218289, val:  63.75%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2219%\n",
      "layer   3  Sparsity: 87.2892%\n",
      "total_backward_count 1674090 real_backward_count 204290  12.203%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.180171/  2.220132, val:  49.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3511%\n",
      "layer   3  Sparsity: 86.8622%\n",
      "total_backward_count 1683880 real_backward_count 205193  12.186%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.179657/  2.214352, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4580%\n",
      "layer   3  Sparsity: 86.9658%\n",
      "total_backward_count 1693670 real_backward_count 206125  12.170%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.184944/  2.221901, val:  61.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.9828%\n",
      "layer   3  Sparsity: 87.1787%\n",
      "total_backward_count 1703460 real_backward_count 207033  12.154%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.182884/  2.220603, val:  60.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5148%\n",
      "layer   3  Sparsity: 87.2641%\n",
      "total_backward_count 1713250 real_backward_count 207961  12.138%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.181432/  2.216781, val:  67.50%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0589%\n",
      "layer   3  Sparsity: 86.8941%\n",
      "total_backward_count 1723040 real_backward_count 208933  12.126%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.184353/  2.221243, val:  64.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3699%\n",
      "layer   3  Sparsity: 87.2583%\n",
      "total_backward_count 1732830 real_backward_count 209911  12.114%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.186228/  2.220973, val:  69.17%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3828%\n",
      "layer   3  Sparsity: 87.2376%\n",
      "total_backward_count 1742620 real_backward_count 210775  12.095%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.183212/  2.218542, val:  61.25%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.0197%\n",
      "layer   3  Sparsity: 86.8525%\n",
      "total_backward_count 1752410 real_backward_count 211693  12.080%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.184410/  2.222056, val:  63.33%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4024%\n",
      "layer   3  Sparsity: 86.7994%\n",
      "total_backward_count 1762200 real_backward_count 212644  12.067%\n",
      "fc layer 1 self.abs_max_out: 16677.0\n",
      "fc layer 1 self.abs_max_out: 17251.0\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.182266/  2.223156, val:  60.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2599%\n",
      "layer   3  Sparsity: 86.9091%\n",
      "total_backward_count 1771990 real_backward_count 213596  12.054%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.180819/  2.215824, val:  64.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3049%\n",
      "layer   3  Sparsity: 86.9721%\n",
      "total_backward_count 1781780 real_backward_count 214452  12.036%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.183712/  2.225640, val:  62.92%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.6386%\n",
      "layer   3  Sparsity: 87.1945%\n",
      "total_backward_count 1791570 real_backward_count 215343  12.020%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.184706/  2.221451, val:  60.83%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1244%\n",
      "layer   3  Sparsity: 87.0954%\n",
      "total_backward_count 1801360 real_backward_count 216235  12.004%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.185595/  2.223071, val:  60.42%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4427%\n",
      "layer   3  Sparsity: 87.3047%\n",
      "total_backward_count 1811150 real_backward_count 217151  11.990%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.183307/  2.216575, val:  62.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4081%\n",
      "layer   3  Sparsity: 86.9135%\n",
      "total_backward_count 1820940 real_backward_count 218120  11.978%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.183387/  2.218446, val:  57.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1878%\n",
      "layer   3  Sparsity: 86.8719%\n",
      "total_backward_count 1830730 real_backward_count 219032  11.964%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.182886/  2.217184, val:  63.75%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.4034%\n",
      "layer   3  Sparsity: 86.8380%\n",
      "total_backward_count 1840520 real_backward_count 219995  11.953%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.179589/  2.220262, val:  63.33%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9596%\n",
      "layer   3  Sparsity: 86.6433%\n",
      "total_backward_count 1850310 real_backward_count 220909  11.939%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.178603/  2.221228, val:  61.67%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9143%\n",
      "layer   3  Sparsity: 86.9891%\n",
      "total_backward_count 1860100 real_backward_count 221794  11.924%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.183868/  2.220076, val:  65.83%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3374%\n",
      "layer   3  Sparsity: 87.1158%\n",
      "total_backward_count 1869890 real_backward_count 222710  11.910%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.181011/  2.223742, val:  54.58%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.5707%\n",
      "layer   3  Sparsity: 87.0117%\n",
      "total_backward_count 1879680 real_backward_count 223702  11.901%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.182335/  2.215172, val:  59.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.9142%\n",
      "layer   3  Sparsity: 87.2503%\n",
      "total_backward_count 1889470 real_backward_count 224658  11.890%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.181042/  2.228522, val:  58.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 73.6530%\n",
      "layer   3  Sparsity: 87.1574%\n",
      "total_backward_count 1899260 real_backward_count 225558  11.876%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.180756/  2.215892, val:  62.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.1301%\n",
      "layer   3  Sparsity: 86.9822%\n",
      "total_backward_count 1909050 real_backward_count 226501  11.865%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.181128/  2.229800, val:  58.75%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.3554%\n",
      "layer   3  Sparsity: 87.1411%\n",
      "total_backward_count 1918840 real_backward_count 227442  11.853%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.180946/  2.219312, val:  60.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2368%\n",
      "layer   3  Sparsity: 87.1143%\n",
      "total_backward_count 1928630 real_backward_count 228363  11.841%\n",
      "lif layer 2 self.abs_max_v: 9655.0\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.179177/  2.220292, val:  57.92%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2774%\n",
      "layer   3  Sparsity: 87.0558%\n",
      "total_backward_count 1938420 real_backward_count 229268  11.828%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.184223/  2.222319, val:  60.00%, val_best:  69.17%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2741%\n",
      "layer   3  Sparsity: 87.0763%\n",
      "total_backward_count 1948210 real_backward_count 230227  11.817%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.186892/  2.219946, val:  62.50%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 80.4392%\n",
      "layer   2  Sparsity: 74.2155%\n",
      "layer   3  Sparsity: 86.9019%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d88929c8b1468ba8177953f05fdc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99591</td></tr><tr><td>tr_epoch_loss</td><td>2.18689</td></tr><tr><td>val_acc_best</td><td>0.69167</td></tr><tr><td>val_acc_now</td><td>0.625</td></tr><tr><td>val_loss</td><td>2.21995</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-sweep-81</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ebtsqu3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6ebtsqu3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_182901-6ebtsqu3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bfqe9kmr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_225253-bfqe9kmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfqe9kmr' target=\"_blank\">eager-sweep-87</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfqe9kmr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfqe9kmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251115_225302_637', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "lif layer 2 self.abs_max_v: 1228.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 318.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 525.5\n",
      "lif layer 2 self.abs_max_v: 1326.0\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "lif layer 2 self.abs_max_v: 1605.0\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "lif layer 1 self.abs_max_v: 712.5\n",
      "fc layer 1 self.abs_max_out: 837.0\n",
      "lif layer 1 self.abs_max_v: 1010.0\n",
      "fc layer 1 self.abs_max_out: 1000.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 1827.5\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 1165.5\n",
      "fc layer 1 self.abs_max_out: 1015.0\n",
      "lif layer 1 self.abs_max_v: 1209.5\n",
      "lif layer 1 self.abs_max_v: 1343.5\n",
      "lif layer 1 self.abs_max_v: 1359.0\n",
      "lif layer 1 self.abs_max_v: 1432.5\n",
      "fc layer 1 self.abs_max_out: 1085.0\n",
      "lif layer 1 self.abs_max_v: 1450.5\n",
      "fc layer 3 self.abs_max_out: 710.0\n",
      "fc layer 2 self.abs_max_out: 1314.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "fc layer 1 self.abs_max_out: 1192.0\n",
      "lif layer 1 self.abs_max_v: 1668.0\n",
      "fc layer 2 self.abs_max_out: 1365.0\n",
      "lif layer 2 self.abs_max_v: 1890.5\n",
      "fc layer 1 self.abs_max_out: 1797.0\n",
      "lif layer 1 self.abs_max_v: 1797.0\n",
      "lif layer 2 self.abs_max_v: 1983.5\n",
      "fc layer 1 self.abs_max_out: 1844.0\n",
      "lif layer 1 self.abs_max_v: 2508.0\n",
      "fc layer 1 self.abs_max_out: 1883.0\n",
      "lif layer 1 self.abs_max_v: 2758.5\n",
      "lif layer 1 self.abs_max_v: 2807.5\n",
      "fc layer 1 self.abs_max_out: 1901.0\n",
      "lif layer 1 self.abs_max_v: 2971.0\n",
      "lif layer 2 self.abs_max_v: 2008.5\n",
      "fc layer 1 self.abs_max_out: 2432.0\n",
      "fc layer 2 self.abs_max_out: 1481.0\n",
      "lif layer 2 self.abs_max_v: 2411.0\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "fc layer 2 self.abs_max_out: 1539.0\n",
      "fc layer 2 self.abs_max_out: 1595.0\n",
      "lif layer 2 self.abs_max_v: 2502.5\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "fc layer 2 self.abs_max_out: 1636.0\n",
      "lif layer 2 self.abs_max_v: 2747.5\n",
      "fc layer 2 self.abs_max_out: 1663.0\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "lif layer 1 self.abs_max_v: 3081.5\n",
      "lif layer 1 self.abs_max_v: 3472.5\n",
      "lif layer 1 self.abs_max_v: 4012.5\n",
      "lif layer 1 self.abs_max_v: 4021.5\n",
      "lif layer 1 self.abs_max_v: 4102.0\n",
      "fc layer 2 self.abs_max_out: 1957.0\n",
      "lif layer 2 self.abs_max_v: 2748.0\n",
      "lif layer 2 self.abs_max_v: 3027.0\n",
      "lif layer 2 self.abs_max_v: 3161.0\n",
      "lif layer 2 self.abs_max_v: 3171.5\n",
      "lif layer 2 self.abs_max_v: 3249.0\n",
      "lif layer 2 self.abs_max_v: 3521.5\n",
      "lif layer 2 self.abs_max_v: 3588.5\n",
      "fc layer 2 self.abs_max_out: 1974.0\n",
      "lif layer 2 self.abs_max_v: 3768.5\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "fc layer 2 self.abs_max_out: 2049.0\n",
      "lif layer 2 self.abs_max_v: 3840.0\n",
      "fc layer 1 self.abs_max_out: 2450.0\n",
      "lif layer 1 self.abs_max_v: 4247.0\n",
      "fc layer 1 self.abs_max_out: 2544.0\n",
      "fc layer 1 self.abs_max_out: 2549.0\n",
      "lif layer 1 self.abs_max_v: 4537.5\n",
      "fc layer 1 self.abs_max_out: 2848.0\n",
      "lif layer 1 self.abs_max_v: 4977.5\n",
      "fc layer 1 self.abs_max_out: 3229.0\n",
      "lif layer 1 self.abs_max_v: 5358.5\n",
      "fc layer 3 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 2056.0\n",
      "lif layer 2 self.abs_max_v: 3935.0\n",
      "fc layer 2 self.abs_max_out: 2059.0\n",
      "lif layer 2 self.abs_max_v: 3985.5\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "lif layer 1 self.abs_max_v: 5686.0\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "lif layer 1 self.abs_max_v: 5770.5\n",
      "fc layer 1 self.abs_max_out: 3297.0\n",
      "lif layer 1 self.abs_max_v: 5784.5\n",
      "lif layer 1 self.abs_max_v: 5962.5\n",
      "fc layer 1 self.abs_max_out: 3308.0\n",
      "fc layer 1 self.abs_max_out: 3490.0\n",
      "fc layer 3 self.abs_max_out: 1031.0\n",
      "fc layer 3 self.abs_max_out: 1159.0\n",
      "fc layer 1 self.abs_max_out: 3526.0\n",
      "lif layer 1 self.abs_max_v: 6086.5\n",
      "fc layer 1 self.abs_max_out: 3650.0\n",
      "lif layer 1 self.abs_max_v: 6675.5\n",
      "lif layer 1 self.abs_max_v: 6806.0\n",
      "fc layer 1 self.abs_max_out: 3708.0\n",
      "lif layer 1 self.abs_max_v: 7111.0\n",
      "fc layer 3 self.abs_max_out: 1163.0\n",
      "fc layer 3 self.abs_max_out: 1289.0\n",
      "fc layer 3 self.abs_max_out: 1353.0\n",
      "fc layer 1 self.abs_max_out: 4031.0\n",
      "fc layer 1 self.abs_max_out: 4145.0\n",
      "lif layer 1 self.abs_max_v: 7525.0\n",
      "fc layer 3 self.abs_max_out: 1363.0\n",
      "fc layer 3 self.abs_max_out: 1383.0\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "fc layer 2 self.abs_max_out: 2233.0\n",
      "lif layer 1 self.abs_max_v: 7628.0\n",
      "fc layer 2 self.abs_max_out: 2292.0\n",
      "fc layer 2 self.abs_max_out: 2298.0\n",
      "fc layer 2 self.abs_max_out: 2417.0\n",
      "fc layer 1 self.abs_max_out: 4264.0\n",
      "fc layer 1 self.abs_max_out: 4538.0\n",
      "lif layer 1 self.abs_max_v: 7810.5\n",
      "fc layer 1 self.abs_max_out: 4895.0\n",
      "lif layer 1 self.abs_max_v: 8800.5\n",
      "lif layer 2 self.abs_max_v: 3993.5\n",
      "lif layer 2 self.abs_max_v: 4013.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.391951/  1.870806, val:  33.33%, val_best:  33.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 68.3602%\n",
      "layer   3  Sparsity: 57.5196%\n",
      "total_backward_count 9790 real_backward_count 1235  12.615%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 5139.0\n",
      "lif layer 1 self.abs_max_v: 9216.5\n",
      "fc layer 3 self.abs_max_out: 1450.0\n",
      "fc layer 3 self.abs_max_out: 1507.0\n",
      "fc layer 2 self.abs_max_out: 2494.0\n",
      "fc layer 2 self.abs_max_out: 2519.0\n",
      "fc layer 2 self.abs_max_out: 2520.0\n",
      "fc layer 3 self.abs_max_out: 1510.0\n",
      "lif layer 2 self.abs_max_v: 4112.0\n",
      "lif layer 2 self.abs_max_v: 4401.0\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "fc layer 2 self.abs_max_out: 2649.0\n",
      "fc layer 3 self.abs_max_out: 1535.0\n",
      "fc layer 3 self.abs_max_out: 1572.0\n",
      "lif layer 2 self.abs_max_v: 4521.0\n",
      "lif layer 2 self.abs_max_v: 4665.0\n",
      "lif layer 2 self.abs_max_v: 4817.5\n",
      "fc layer 1 self.abs_max_out: 5252.0\n",
      "lif layer 1 self.abs_max_v: 9438.5\n",
      "fc layer 1 self.abs_max_out: 5274.0\n",
      "lif layer 1 self.abs_max_v: 9529.5\n",
      "lif layer 1 self.abs_max_v: 9616.0\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "fc layer 2 self.abs_max_out: 2731.0\n",
      "fc layer 1 self.abs_max_out: 5282.0\n",
      "lif layer 1 self.abs_max_v: 9773.0\n",
      "fc layer 2 self.abs_max_out: 2796.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.229964/  1.775326, val:  36.67%, val_best:  36.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.2592%\n",
      "layer   3  Sparsity: 59.3725%\n",
      "total_backward_count 19580 real_backward_count 2433  12.426%\n",
      "fc layer 3 self.abs_max_out: 1692.0\n",
      "fc layer 3 self.abs_max_out: 1695.0\n",
      "fc layer 3 self.abs_max_out: 1708.0\n",
      "fc layer 3 self.abs_max_out: 1811.0\n",
      "fc layer 1 self.abs_max_out: 5546.0\n",
      "lif layer 1 self.abs_max_v: 10016.0\n",
      "fc layer 1 self.abs_max_out: 5632.0\n",
      "fc layer 1 self.abs_max_out: 6049.0\n",
      "lif layer 1 self.abs_max_v: 10433.0\n",
      "lif layer 1 self.abs_max_v: 10685.5\n",
      "lif layer 2 self.abs_max_v: 4865.5\n",
      "lif layer 2 self.abs_max_v: 4990.0\n",
      "lif layer 1 self.abs_max_v: 10920.5\n",
      "lif layer 1 self.abs_max_v: 11298.5\n",
      "lif layer 1 self.abs_max_v: 11663.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.153892/  1.623463, val:  41.67%, val_best:  41.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2121%\n",
      "layer   3  Sparsity: 59.6568%\n",
      "total_backward_count 29370 real_backward_count 3596  12.244%\n",
      "fc layer 2 self.abs_max_out: 2813.0\n",
      "fc layer 2 self.abs_max_out: 2828.0\n",
      "fc layer 2 self.abs_max_out: 3031.0\n",
      "fc layer 3 self.abs_max_out: 1816.0\n",
      "fc layer 1 self.abs_max_out: 6303.0\n",
      "lif layer 2 self.abs_max_v: 5081.0\n",
      "fc layer 1 self.abs_max_out: 6306.0\n",
      "lif layer 1 self.abs_max_v: 11983.5\n",
      "fc layer 1 self.abs_max_out: 6470.0\n",
      "lif layer 1 self.abs_max_v: 12172.0\n",
      "lif layer 1 self.abs_max_v: 12396.0\n",
      "fc layer 1 self.abs_max_out: 6549.0\n",
      "lif layer 1 self.abs_max_v: 12747.0\n",
      "fc layer 1 self.abs_max_out: 6960.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.085423/  1.795621, val:  28.75%, val_best:  41.67%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.8181%\n",
      "layer   3  Sparsity: 58.9152%\n",
      "total_backward_count 39160 real_backward_count 4748  12.125%\n",
      "lif layer 2 self.abs_max_v: 5130.0\n",
      "lif layer 2 self.abs_max_v: 5202.0\n",
      "lif layer 2 self.abs_max_v: 5205.0\n",
      "fc layer 3 self.abs_max_out: 1929.0\n",
      "lif layer 2 self.abs_max_v: 5255.0\n",
      "fc layer 1 self.abs_max_out: 7130.0\n",
      "lif layer 2 self.abs_max_v: 5388.0\n",
      "lif layer 2 self.abs_max_v: 5691.0\n",
      "fc layer 2 self.abs_max_out: 3098.0\n",
      "lif layer 2 self.abs_max_v: 5943.5\n",
      "fc layer 3 self.abs_max_out: 1956.0\n",
      "fc layer 3 self.abs_max_out: 1988.0\n",
      "fc layer 3 self.abs_max_out: 2024.0\n",
      "fc layer 2 self.abs_max_out: 3178.0\n",
      "fc layer 3 self.abs_max_out: 2030.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.035444/  1.571901, val:  46.67%, val_best:  46.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.7162%\n",
      "layer   3  Sparsity: 59.9973%\n",
      "total_backward_count 48950 real_backward_count 5909  12.072%\n",
      "fc layer 2 self.abs_max_out: 3250.0\n",
      "fc layer 2 self.abs_max_out: 3336.0\n",
      "fc layer 2 self.abs_max_out: 3395.0\n",
      "lif layer 2 self.abs_max_v: 5998.0\n",
      "lif layer 2 self.abs_max_v: 6011.5\n",
      "lif layer 2 self.abs_max_v: 6053.5\n",
      "lif layer 2 self.abs_max_v: 6155.5\n",
      "lif layer 2 self.abs_max_v: 6384.0\n",
      "lif layer 1 self.abs_max_v: 12761.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.023188/  1.645816, val:  47.92%, val_best:  47.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.5155%\n",
      "layer   3  Sparsity: 61.0161%\n",
      "total_backward_count 58740 real_backward_count 7060  12.019%\n",
      "fc layer 2 self.abs_max_out: 3483.0\n",
      "lif layer 1 self.abs_max_v: 12858.0\n",
      "fc layer 3 self.abs_max_out: 2424.0\n",
      "fc layer 3 self.abs_max_out: 2427.0\n",
      "fc layer 3 self.abs_max_out: 2437.0\n",
      "fc layer 1 self.abs_max_out: 7239.0\n",
      "lif layer 1 self.abs_max_v: 13334.0\n",
      "fc layer 1 self.abs_max_out: 7311.0\n",
      "lif layer 1 self.abs_max_v: 13446.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  0.997864/  1.610158, val:  49.58%, val_best:  49.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.2555%\n",
      "layer   3  Sparsity: 62.1438%\n",
      "total_backward_count 68530 real_backward_count 8200  11.966%\n",
      "fc layer 1 self.abs_max_out: 7436.0\n",
      "lif layer 1 self.abs_max_v: 13492.0\n",
      "lif layer 1 self.abs_max_v: 13715.5\n",
      "lif layer 1 self.abs_max_v: 13956.0\n",
      "fc layer 1 self.abs_max_out: 7618.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  0.967609/  1.402805, val:  54.58%, val_best:  54.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.5212%\n",
      "layer   3  Sparsity: 62.5234%\n",
      "total_backward_count 78320 real_backward_count 9310  11.887%\n",
      "lif layer 2 self.abs_max_v: 6512.5\n",
      "fc layer 2 self.abs_max_out: 3513.0\n",
      "lif layer 2 self.abs_max_v: 6567.0\n",
      "lif layer 2 self.abs_max_v: 6636.0\n",
      "fc layer 2 self.abs_max_out: 3622.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  0.952512/  1.379530, val:  58.33%, val_best:  58.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.9230%\n",
      "layer   3  Sparsity: 63.9250%\n",
      "total_backward_count 88110 real_backward_count 10478  11.892%\n",
      "fc layer 2 self.abs_max_out: 3746.0\n",
      "fc layer 1 self.abs_max_out: 7724.0\n",
      "fc layer 1 self.abs_max_out: 7782.0\n",
      "fc layer 1 self.abs_max_out: 8240.0\n",
      "lif layer 1 self.abs_max_v: 14859.5\n",
      "lif layer 1 self.abs_max_v: 15636.0\n",
      "fc layer 1 self.abs_max_out: 8815.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.968761/  1.554030, val:  47.08%, val_best:  58.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.7820%\n",
      "layer   3  Sparsity: 65.4596%\n",
      "total_backward_count 97900 real_backward_count 11657  11.907%\n",
      "lif layer 1 self.abs_max_v: 16119.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.945425/  1.501196, val:  52.50%, val_best:  58.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.2195%\n",
      "layer   3  Sparsity: 64.8495%\n",
      "total_backward_count 107690 real_backward_count 12757  11.846%\n",
      "lif layer 2 self.abs_max_v: 6656.0\n",
      "lif layer 2 self.abs_max_v: 6745.5\n",
      "lif layer 2 self.abs_max_v: 6802.0\n",
      "lif layer 2 self.abs_max_v: 7049.0\n",
      "fc layer 2 self.abs_max_out: 3768.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.922208/  1.403388, val:  58.75%, val_best:  58.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.0510%\n",
      "layer   3  Sparsity: 63.8852%\n",
      "total_backward_count 117480 real_backward_count 13898  11.830%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.908451/  1.566405, val:  42.50%, val_best:  58.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.3237%\n",
      "layer   3  Sparsity: 64.9334%\n",
      "total_backward_count 127270 real_backward_count 14960  11.755%\n",
      "fc layer 2 self.abs_max_out: 3793.0\n",
      "fc layer 2 self.abs_max_out: 3841.0\n",
      "lif layer 2 self.abs_max_v: 7084.5\n",
      "fc layer 2 self.abs_max_out: 3854.0\n",
      "fc layer 2 self.abs_max_out: 3895.0\n",
      "lif layer 1 self.abs_max_v: 16518.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.934241/  1.683927, val:  37.50%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.6362%\n",
      "layer   3  Sparsity: 64.1527%\n",
      "total_backward_count 137060 real_backward_count 16033  11.698%\n",
      "fc layer 1 self.abs_max_out: 8834.0\n",
      "fc layer 2 self.abs_max_out: 3946.0\n",
      "fc layer 2 self.abs_max_out: 4260.0\n",
      "lif layer 2 self.abs_max_v: 7573.5\n",
      "fc layer 1 self.abs_max_out: 8988.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.935391/  1.543659, val:  46.25%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2603%\n",
      "layer   3  Sparsity: 66.2465%\n",
      "total_backward_count 146850 real_backward_count 17063  11.619%\n",
      "lif layer 2 self.abs_max_v: 7633.0\n",
      "lif layer 1 self.abs_max_v: 16692.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.933115/  1.401343, val:  55.00%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.2972%\n",
      "layer   3  Sparsity: 66.9200%\n",
      "total_backward_count 156640 real_backward_count 18149  11.586%\n",
      "fc layer 1 self.abs_max_out: 9272.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.914401/  1.362984, val:  61.25%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.7292%\n",
      "layer   3  Sparsity: 65.6370%\n",
      "total_backward_count 166430 real_backward_count 19164  11.515%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.902273/  1.440745, val:  50.42%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.7267%\n",
      "layer   3  Sparsity: 66.6215%\n",
      "total_backward_count 176220 real_backward_count 20179  11.451%\n",
      "lif layer 1 self.abs_max_v: 16921.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.887028/  1.360264, val:  53.75%, val_best:  61.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2553%\n",
      "layer   3  Sparsity: 67.0856%\n",
      "total_backward_count 186010 real_backward_count 21191  11.392%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.869318/  1.499763, val:  46.67%, val_best:  61.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.5409%\n",
      "layer   3  Sparsity: 67.9139%\n",
      "total_backward_count 195800 real_backward_count 22220  11.348%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.828624/  1.313649, val:  62.92%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2561%\n",
      "layer   3  Sparsity: 66.5290%\n",
      "total_backward_count 205590 real_backward_count 23201  11.285%\n",
      "fc layer 1 self.abs_max_out: 9552.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.837832/  1.432445, val:  49.17%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2516%\n",
      "layer   3  Sparsity: 66.8520%\n",
      "total_backward_count 215380 real_backward_count 24212  11.242%\n",
      "fc layer 1 self.abs_max_out: 9579.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.834723/  1.372967, val:  46.67%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.3440%\n",
      "layer   3  Sparsity: 67.4198%\n",
      "total_backward_count 225170 real_backward_count 25235  11.207%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.817577/  1.309563, val:  57.08%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.6947%\n",
      "layer   3  Sparsity: 66.3422%\n",
      "total_backward_count 234960 real_backward_count 26225  11.161%\n",
      "fc layer 3 self.abs_max_out: 2438.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.824808/  1.294133, val:  62.08%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.9552%\n",
      "layer   3  Sparsity: 66.9902%\n",
      "total_backward_count 244750 real_backward_count 27196  11.112%\n",
      "fc layer 1 self.abs_max_out: 9705.0\n",
      "lif layer 1 self.abs_max_v: 17222.0\n",
      "lif layer 1 self.abs_max_v: 17318.0\n",
      "lif layer 1 self.abs_max_v: 17622.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.781044/  1.289122, val:  59.17%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2610%\n",
      "layer   3  Sparsity: 66.6253%\n",
      "total_backward_count 254540 real_backward_count 28179  11.071%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.762427/  1.309867, val:  60.00%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.9874%\n",
      "layer   3  Sparsity: 66.5668%\n",
      "total_backward_count 264330 real_backward_count 29123  11.018%\n",
      "fc layer 3 self.abs_max_out: 2503.0\n",
      "fc layer 1 self.abs_max_out: 9832.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.774057/  1.296737, val:  67.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.0397%\n",
      "layer   3  Sparsity: 66.5372%\n",
      "total_backward_count 274120 real_backward_count 30074  10.971%\n",
      "lif layer 2 self.abs_max_v: 7686.0\n",
      "fc layer 1 self.abs_max_out: 10133.0\n",
      "lif layer 2 self.abs_max_v: 7707.5\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.746991/  1.292742, val:  59.58%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.3927%\n",
      "layer   3  Sparsity: 66.4542%\n",
      "total_backward_count 283910 real_backward_count 31007  10.921%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.785143/  1.380597, val:  56.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.3550%\n",
      "layer   3  Sparsity: 67.5835%\n",
      "total_backward_count 293700 real_backward_count 31931  10.872%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.765558/  1.306476, val:  60.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.7240%\n",
      "layer   3  Sparsity: 66.9627%\n",
      "total_backward_count 303490 real_backward_count 32865  10.829%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.764015/  1.330970, val:  56.67%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2453%\n",
      "layer   3  Sparsity: 66.6331%\n",
      "total_backward_count 313280 real_backward_count 33840  10.802%\n",
      "fc layer 3 self.abs_max_out: 2504.0\n",
      "fc layer 3 self.abs_max_out: 2530.0\n",
      "fc layer 3 self.abs_max_out: 2682.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.714139/  1.284135, val:  57.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.6736%\n",
      "layer   3  Sparsity: 67.1115%\n",
      "total_backward_count 323070 real_backward_count 34750  10.756%\n",
      "fc layer 3 self.abs_max_out: 2704.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.714919/  1.224184, val:  59.58%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.0655%\n",
      "layer   3  Sparsity: 67.0030%\n",
      "total_backward_count 332860 real_backward_count 35679  10.719%\n",
      "fc layer 3 self.abs_max_out: 2820.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.705315/  1.275042, val:  57.08%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 72.6090%\n",
      "layer   3  Sparsity: 66.1689%\n",
      "total_backward_count 342650 real_backward_count 36572  10.673%\n",
      "fc layer 1 self.abs_max_out: 10143.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.715051/  1.298262, val:  56.25%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 72.7730%\n",
      "layer   3  Sparsity: 65.7064%\n",
      "total_backward_count 352440 real_backward_count 37473  10.632%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.696792/  1.167480, val:  67.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.7141%\n",
      "layer   3  Sparsity: 66.5591%\n",
      "total_backward_count 362230 real_backward_count 38360  10.590%\n",
      "lif layer 1 self.abs_max_v: 17880.5\n",
      "fc layer 1 self.abs_max_out: 10222.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.639149/  1.308700, val:  60.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.8139%\n",
      "layer   3  Sparsity: 65.5591%\n",
      "total_backward_count 372020 real_backward_count 39183  10.532%\n",
      "lif layer 1 self.abs_max_v: 18489.0\n",
      "lif layer 1 self.abs_max_v: 18504.5\n",
      "lif layer 1 self.abs_max_v: 18967.5\n",
      "fc layer 1 self.abs_max_out: 11048.0\n",
      "lif layer 1 self.abs_max_v: 20532.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.667721/  1.307160, val:  58.75%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.8452%\n",
      "layer   3  Sparsity: 66.4667%\n",
      "total_backward_count 381810 real_backward_count 40080  10.497%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.651281/  1.302048, val:  62.92%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.7671%\n",
      "layer   3  Sparsity: 66.0495%\n",
      "total_backward_count 391600 real_backward_count 40914  10.448%\n",
      "fc layer 1 self.abs_max_out: 11068.0\n",
      "lif layer 1 self.abs_max_v: 20581.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.647624/  1.200457, val:  66.25%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.7146%\n",
      "layer   3  Sparsity: 65.3637%\n",
      "total_backward_count 401390 real_backward_count 41757  10.403%\n",
      "lif layer 2 self.abs_max_v: 7828.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.650421/  1.239469, val:  58.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.7774%\n",
      "layer   3  Sparsity: 66.0458%\n",
      "total_backward_count 411180 real_backward_count 42602  10.361%\n",
      "fc layer 2 self.abs_max_out: 4497.0\n",
      "lif layer 2 self.abs_max_v: 8011.5\n",
      "lif layer 2 self.abs_max_v: 8210.5\n",
      "fc layer 3 self.abs_max_out: 2850.0\n",
      "fc layer 1 self.abs_max_out: 11158.0\n",
      "lif layer 1 self.abs_max_v: 20759.5\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.633035/  1.204904, val:  59.58%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.6034%\n",
      "layer   3  Sparsity: 65.2896%\n",
      "total_backward_count 420970 real_backward_count 43399  10.309%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.619500/  1.196145, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.0456%\n",
      "layer   3  Sparsity: 66.1519%\n",
      "total_backward_count 430760 real_backward_count 44188  10.258%\n",
      "fc layer 1 self.abs_max_out: 11244.0\n",
      "lif layer 1 self.abs_max_v: 20938.5\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.643896/  1.277502, val:  57.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.8678%\n",
      "layer   3  Sparsity: 66.6977%\n",
      "total_backward_count 440550 real_backward_count 45010  10.217%\n",
      "fc layer 1 self.abs_max_out: 11364.0\n",
      "lif layer 1 self.abs_max_v: 21193.5\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.624068/  1.185775, val:  61.67%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.1192%\n",
      "layer   3  Sparsity: 65.6807%\n",
      "total_backward_count 450340 real_backward_count 45828  10.176%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.611329/  1.250621, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.3346%\n",
      "layer   3  Sparsity: 64.6540%\n",
      "total_backward_count 460130 real_backward_count 46588  10.125%\n",
      "lif layer 2 self.abs_max_v: 8235.5\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.608131/  1.305876, val:  60.42%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.4642%\n",
      "layer   3  Sparsity: 64.6766%\n",
      "total_backward_count 469920 real_backward_count 47405  10.088%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.571917/  1.089366, val:  72.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2441%\n",
      "layer   3  Sparsity: 64.6881%\n",
      "total_backward_count 479710 real_backward_count 48170  10.041%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.548117/  1.187099, val:  60.83%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.5963%\n",
      "layer   3  Sparsity: 63.6119%\n",
      "total_backward_count 489500 real_backward_count 48924   9.995%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.590551/  1.252325, val:  59.58%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.0887%\n",
      "layer   3  Sparsity: 64.6433%\n",
      "total_backward_count 499290 real_backward_count 49710   9.956%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.579670/  1.100817, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2419%\n",
      "layer   3  Sparsity: 65.7232%\n",
      "total_backward_count 509080 real_backward_count 50490   9.918%\n",
      "fc layer 1 self.abs_max_out: 11424.0\n",
      "lif layer 1 self.abs_max_v: 21327.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.579017/  1.205186, val:  56.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.6505%\n",
      "layer   3  Sparsity: 66.4326%\n",
      "total_backward_count 518870 real_backward_count 51249   9.877%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.573130/  1.141713, val:  67.08%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.4412%\n",
      "layer   3  Sparsity: 67.1235%\n",
      "total_backward_count 528660 real_backward_count 52008   9.838%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.567207/  1.164043, val:  64.58%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.2472%\n",
      "layer   3  Sparsity: 67.8955%\n",
      "total_backward_count 538450 real_backward_count 52699   9.787%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.589343/  1.147244, val:  69.17%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.3338%\n",
      "layer   3  Sparsity: 67.4918%\n",
      "total_backward_count 548240 real_backward_count 53463   9.752%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.585578/  1.227486, val:  61.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.8059%\n",
      "layer   3  Sparsity: 67.9060%\n",
      "total_backward_count 558030 real_backward_count 54132   9.701%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.567067/  1.185552, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.2991%\n",
      "layer   3  Sparsity: 67.1629%\n",
      "total_backward_count 567820 real_backward_count 54851   9.660%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.555999/  1.132124, val:  68.75%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.5896%\n",
      "layer   3  Sparsity: 66.3625%\n",
      "total_backward_count 577610 real_backward_count 55546   9.617%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.542463/  1.138199, val:  63.75%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.9251%\n",
      "layer   3  Sparsity: 65.9298%\n",
      "total_backward_count 587400 real_backward_count 56199   9.567%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.549522/  1.198277, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.7069%\n",
      "layer   3  Sparsity: 66.2485%\n",
      "total_backward_count 597190 real_backward_count 56899   9.528%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.547987/  1.213104, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.1125%\n",
      "layer   3  Sparsity: 66.2201%\n",
      "total_backward_count 606980 real_backward_count 57576   9.486%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.534975/  1.189710, val:  57.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.2401%\n",
      "layer   3  Sparsity: 65.6520%\n",
      "total_backward_count 616770 real_backward_count 58292   9.451%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.536312/  1.123696, val:  67.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.1002%\n",
      "layer   3  Sparsity: 65.7560%\n",
      "total_backward_count 626560 real_backward_count 58929   9.405%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.524375/  1.153047, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.5594%\n",
      "layer   3  Sparsity: 65.3126%\n",
      "total_backward_count 636350 real_backward_count 59588   9.364%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.516488/  1.184739, val:  65.42%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8082%\n",
      "layer   3  Sparsity: 65.9130%\n",
      "total_backward_count 646140 real_backward_count 60222   9.320%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.539611/  1.160514, val:  63.33%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.6818%\n",
      "layer   3  Sparsity: 66.6248%\n",
      "total_backward_count 655930 real_backward_count 60896   9.284%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.522416/  1.161096, val:  61.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.2842%\n",
      "layer   3  Sparsity: 65.6198%\n",
      "total_backward_count 665720 real_backward_count 61519   9.241%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.498182/  1.078760, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.9756%\n",
      "layer   3  Sparsity: 65.9879%\n",
      "total_backward_count 675510 real_backward_count 62147   9.200%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.472356/  1.185778, val:  56.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.6420%\n",
      "layer   3  Sparsity: 65.3057%\n",
      "total_backward_count 685300 real_backward_count 62706   9.150%\n",
      "fc layer 3 self.abs_max_out: 2940.0\n",
      "fc layer 3 self.abs_max_out: 2998.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.501819/  1.166172, val:  65.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.3623%\n",
      "layer   3  Sparsity: 66.3028%\n",
      "total_backward_count 695090 real_backward_count 63378   9.118%\n",
      "fc layer 3 self.abs_max_out: 3192.0\n",
      "fc layer 1 self.abs_max_out: 11655.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.506628/  1.178047, val:  62.50%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.9140%\n",
      "layer   3  Sparsity: 66.4941%\n",
      "total_backward_count 704880 real_backward_count 64018   9.082%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.507350/  1.154559, val:  67.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.9549%\n",
      "layer   3  Sparsity: 66.2395%\n",
      "total_backward_count 714670 real_backward_count 64677   9.050%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.491224/  1.128722, val:  67.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.3950%\n",
      "layer   3  Sparsity: 66.3088%\n",
      "total_backward_count 724460 real_backward_count 65287   9.012%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.505939/  1.110847, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.8555%\n",
      "layer   3  Sparsity: 66.8295%\n",
      "total_backward_count 734250 real_backward_count 65940   8.981%\n",
      "lif layer 1 self.abs_max_v: 21587.5\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.516792/  1.106534, val:  69.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 73.9851%\n",
      "layer   3  Sparsity: 65.6238%\n",
      "total_backward_count 744040 real_backward_count 66571   8.947%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.502649/  1.193300, val:  62.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.4233%\n",
      "layer   3  Sparsity: 64.9936%\n",
      "total_backward_count 753830 real_backward_count 67224   8.918%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.497273/  1.312926, val:  56.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.8158%\n",
      "layer   3  Sparsity: 65.3987%\n",
      "total_backward_count 763620 real_backward_count 67822   8.882%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.511227/  1.158639, val:  67.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 74.8626%\n",
      "layer   3  Sparsity: 64.9985%\n",
      "total_backward_count 773410 real_backward_count 68437   8.849%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.492633/  1.087449, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.3541%\n",
      "layer   3  Sparsity: 65.1629%\n",
      "total_backward_count 783200 real_backward_count 69057   8.817%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.501526/  1.117481, val:  67.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4339%\n",
      "layer   3  Sparsity: 66.4203%\n",
      "total_backward_count 792990 real_backward_count 69659   8.784%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.468453/  1.180595, val:  57.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.5841%\n",
      "layer   3  Sparsity: 66.5693%\n",
      "total_backward_count 802780 real_backward_count 70237   8.749%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.489228/  1.173133, val:  65.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.4475%\n",
      "layer   3  Sparsity: 66.4617%\n",
      "total_backward_count 812570 real_backward_count 70825   8.716%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.479893/  1.265827, val:  58.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.9011%\n",
      "layer   3  Sparsity: 67.4090%\n",
      "total_backward_count 822360 real_backward_count 71407   8.683%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.484547/  1.105807, val:  71.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.3484%\n",
      "layer   3  Sparsity: 66.8675%\n",
      "total_backward_count 832150 real_backward_count 72007   8.653%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.468047/  1.170157, val:  67.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.6053%\n",
      "layer   3  Sparsity: 66.2607%\n",
      "total_backward_count 841940 real_backward_count 72590   8.622%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.472048/  1.092237, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8654%\n",
      "layer   3  Sparsity: 66.8226%\n",
      "total_backward_count 851730 real_backward_count 73204   8.595%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.486887/  1.203650, val:  65.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.7961%\n",
      "layer   3  Sparsity: 68.1407%\n",
      "total_backward_count 861520 real_backward_count 73830   8.570%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.469406/  1.037912, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.7246%\n",
      "layer   3  Sparsity: 67.9773%\n",
      "total_backward_count 871310 real_backward_count 74373   8.536%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.488341/  1.066775, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.1278%\n",
      "layer   3  Sparsity: 67.5653%\n",
      "total_backward_count 881100 real_backward_count 74957   8.507%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.483175/  1.139915, val:  66.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.4339%\n",
      "layer   3  Sparsity: 67.6596%\n",
      "total_backward_count 890890 real_backward_count 75519   8.477%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.477078/  1.187097, val:  68.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.4453%\n",
      "layer   3  Sparsity: 67.5011%\n",
      "total_backward_count 900680 real_backward_count 76027   8.441%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.464054/  1.073001, val:  70.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8388%\n",
      "layer   3  Sparsity: 67.2301%\n",
      "total_backward_count 910470 real_backward_count 76558   8.409%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.451052/  1.070159, val:  70.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.8330%\n",
      "layer   3  Sparsity: 66.3367%\n",
      "total_backward_count 920260 real_backward_count 77077   8.376%\n",
      "lif layer 1 self.abs_max_v: 21609.5\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.420979/  1.072411, val:  67.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.9081%\n",
      "layer   3  Sparsity: 66.8679%\n",
      "total_backward_count 930050 real_backward_count 77623   8.346%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.434198/  1.072694, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.3170%\n",
      "layer   3  Sparsity: 67.0317%\n",
      "total_backward_count 939840 real_backward_count 78175   8.318%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.448596/  1.281558, val:  60.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.7603%\n",
      "layer   3  Sparsity: 67.2229%\n",
      "total_backward_count 949630 real_backward_count 78723   8.290%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.440236/  1.134542, val:  71.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8219%\n",
      "layer   3  Sparsity: 67.6882%\n",
      "total_backward_count 959420 real_backward_count 79241   8.259%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.438706/  1.199233, val:  65.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.2877%\n",
      "layer   3  Sparsity: 67.4738%\n",
      "total_backward_count 969210 real_backward_count 79757   8.229%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.446444/  1.092417, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.3151%\n",
      "layer   3  Sparsity: 68.0984%\n",
      "total_backward_count 979000 real_backward_count 80265   8.199%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.436415/  1.136184, val:  70.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.1680%\n",
      "layer   3  Sparsity: 67.1397%\n",
      "total_backward_count 988790 real_backward_count 80771   8.169%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.452464/  1.120409, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.8813%\n",
      "layer   3  Sparsity: 67.1311%\n",
      "total_backward_count 998580 real_backward_count 81307   8.142%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.449120/  1.127937, val:  70.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 75.6648%\n",
      "layer   3  Sparsity: 66.1683%\n",
      "total_backward_count 1008370 real_backward_count 81812   8.113%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.430293/  1.243715, val:  58.75%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.4927%\n",
      "layer   3  Sparsity: 66.9764%\n",
      "total_backward_count 1018160 real_backward_count 82317   8.085%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.429059/  1.139008, val:  63.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.5767%\n",
      "layer   3  Sparsity: 68.8360%\n",
      "total_backward_count 1027950 real_backward_count 82767   8.052%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.438828/  1.088749, val:  66.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.6395%\n",
      "layer   3  Sparsity: 68.8668%\n",
      "total_backward_count 1037740 real_backward_count 83255   8.023%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.426359/  1.077052, val:  70.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 76.7808%\n",
      "layer   3  Sparsity: 67.7379%\n",
      "total_backward_count 1047530 real_backward_count 83723   7.992%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.425638/  1.059232, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.0034%\n",
      "layer   3  Sparsity: 67.7649%\n",
      "total_backward_count 1057320 real_backward_count 84190   7.963%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.403816/  1.113560, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.0959%\n",
      "layer   3  Sparsity: 66.8171%\n",
      "total_backward_count 1067110 real_backward_count 84644   7.932%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.413338/  1.046716, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.1432%\n",
      "layer   3  Sparsity: 67.5645%\n",
      "total_backward_count 1076900 real_backward_count 85098   7.902%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.411367/  1.110157, val:  63.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2322%\n",
      "layer   3  Sparsity: 66.9997%\n",
      "total_backward_count 1086690 real_backward_count 85581   7.875%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.408657/  1.052512, val:  69.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.1701%\n",
      "layer   3  Sparsity: 68.0335%\n",
      "total_backward_count 1096480 real_backward_count 86009   7.844%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.406884/  1.092113, val:  65.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4042%\n",
      "layer   3  Sparsity: 68.0869%\n",
      "total_backward_count 1106270 real_backward_count 86418   7.812%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.414378/  1.162450, val:  66.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2647%\n",
      "layer   3  Sparsity: 67.9486%\n",
      "total_backward_count 1116060 real_backward_count 86891   7.786%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.405999/  1.072698, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4984%\n",
      "layer   3  Sparsity: 69.1610%\n",
      "total_backward_count 1125850 real_backward_count 87368   7.760%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.410010/  1.071868, val:  70.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8580%\n",
      "layer   3  Sparsity: 69.5182%\n",
      "total_backward_count 1135640 real_backward_count 87838   7.735%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.403341/  1.120030, val:  64.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0018%\n",
      "layer   3  Sparsity: 70.4083%\n",
      "total_backward_count 1145430 real_backward_count 88279   7.707%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.405315/  0.999651, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8858%\n",
      "layer   3  Sparsity: 69.4769%\n",
      "total_backward_count 1155220 real_backward_count 88727   7.681%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.400546/  1.063736, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.5425%\n",
      "layer   3  Sparsity: 69.1917%\n",
      "total_backward_count 1165010 real_backward_count 89169   7.654%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.380524/  1.111585, val:  69.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4250%\n",
      "layer   3  Sparsity: 69.1159%\n",
      "total_backward_count 1174800 real_backward_count 89586   7.626%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.358844/  1.088725, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3655%\n",
      "layer   3  Sparsity: 68.9838%\n",
      "total_backward_count 1184590 real_backward_count 89968   7.595%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.374136/  1.103635, val:  70.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2414%\n",
      "layer   3  Sparsity: 68.5984%\n",
      "total_backward_count 1194380 real_backward_count 90396   7.568%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.402093/  1.055789, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2356%\n",
      "layer   3  Sparsity: 69.1855%\n",
      "total_backward_count 1204170 real_backward_count 90829   7.543%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.387564/  1.116566, val:  68.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.0685%\n",
      "layer   3  Sparsity: 68.6068%\n",
      "total_backward_count 1213960 real_backward_count 91238   7.516%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.391433/  1.004006, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3212%\n",
      "layer   3  Sparsity: 69.3713%\n",
      "total_backward_count 1223750 real_backward_count 91687   7.492%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.397279/  1.101667, val:  70.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2916%\n",
      "layer   3  Sparsity: 69.8718%\n",
      "total_backward_count 1233540 real_backward_count 92097   7.466%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.398142/  1.096666, val:  71.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2372%\n",
      "layer   3  Sparsity: 70.1403%\n",
      "total_backward_count 1243330 real_backward_count 92485   7.438%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.383007/  1.030703, val:  71.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3081%\n",
      "layer   3  Sparsity: 70.2171%\n",
      "total_backward_count 1253120 real_backward_count 92852   7.410%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.389379/  1.103454, val:  66.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2826%\n",
      "layer   3  Sparsity: 69.1568%\n",
      "total_backward_count 1262910 real_backward_count 93217   7.381%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.401291/  1.101764, val:  64.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.1057%\n",
      "layer   3  Sparsity: 70.6258%\n",
      "total_backward_count 1272700 real_backward_count 93619   7.356%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.391849/  1.133560, val:  72.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.2517%\n",
      "layer   3  Sparsity: 70.0411%\n",
      "total_backward_count 1282490 real_backward_count 94020   7.331%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.398431/  1.087740, val:  68.75%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.3553%\n",
      "layer   3  Sparsity: 69.4041%\n",
      "total_backward_count 1292280 real_backward_count 94418   7.306%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.367582/  1.106724, val:  70.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.7779%\n",
      "layer   3  Sparsity: 68.5537%\n",
      "total_backward_count 1302070 real_backward_count 94753   7.277%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.371814/  1.058340, val:  71.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0033%\n",
      "layer   3  Sparsity: 68.3218%\n",
      "total_backward_count 1311860 real_backward_count 95135   7.252%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.356414/  1.132298, val:  63.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2708%\n",
      "layer   3  Sparsity: 67.9486%\n",
      "total_backward_count 1321650 real_backward_count 95502   7.226%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.381480/  1.138410, val:  66.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0717%\n",
      "layer   3  Sparsity: 67.5537%\n",
      "total_backward_count 1331440 real_backward_count 95907   7.203%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.358438/  0.991125, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.8445%\n",
      "layer   3  Sparsity: 68.3210%\n",
      "total_backward_count 1341230 real_backward_count 96263   7.177%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.350694/  1.006765, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.4149%\n",
      "layer   3  Sparsity: 68.4136%\n",
      "total_backward_count 1351020 real_backward_count 96624   7.152%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.360840/  0.978450, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.5326%\n",
      "layer   3  Sparsity: 67.6194%\n",
      "total_backward_count 1360810 real_backward_count 96970   7.126%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.352049/  1.002396, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.0331%\n",
      "layer   3  Sparsity: 67.8489%\n",
      "total_backward_count 1370600 real_backward_count 97353   7.103%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.324794/  1.026423, val:  70.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9289%\n",
      "layer   3  Sparsity: 68.4771%\n",
      "total_backward_count 1380390 real_backward_count 97665   7.075%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.323535/  1.015059, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2130%\n",
      "layer   3  Sparsity: 68.4742%\n",
      "total_backward_count 1390180 real_backward_count 97962   7.047%\n",
      "fc layer 3 self.abs_max_out: 3238.0\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.334363/  1.023892, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3254%\n",
      "layer   3  Sparsity: 68.1226%\n",
      "total_backward_count 1399970 real_backward_count 98303   7.022%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.328345/  1.151474, val:  68.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2704%\n",
      "layer   3  Sparsity: 68.3051%\n",
      "total_backward_count 1409760 real_backward_count 98604   6.994%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.345346/  1.004305, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.1751%\n",
      "layer   3  Sparsity: 69.5976%\n",
      "total_backward_count 1419550 real_backward_count 98918   6.968%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.342459/  1.069551, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2938%\n",
      "layer   3  Sparsity: 68.9394%\n",
      "total_backward_count 1429340 real_backward_count 99253   6.944%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.346662/  1.046708, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.2728%\n",
      "layer   3  Sparsity: 69.4811%\n",
      "total_backward_count 1439130 real_backward_count 99607   6.921%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.345091/  1.045362, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 77.9340%\n",
      "layer   3  Sparsity: 69.6760%\n",
      "total_backward_count 1448920 real_backward_count 99940   6.898%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.355091/  1.037954, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.4059%\n",
      "layer   3  Sparsity: 69.2038%\n",
      "total_backward_count 1458710 real_backward_count 100288   6.875%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.353478/  1.057064, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6138%\n",
      "layer   3  Sparsity: 69.5324%\n",
      "total_backward_count 1468500 real_backward_count 100648   6.854%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.339965/  0.982918, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7203%\n",
      "layer   3  Sparsity: 70.0897%\n",
      "total_backward_count 1478290 real_backward_count 100955   6.829%\n",
      "fc layer 3 self.abs_max_out: 3295.0\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.342487/  1.083836, val:  70.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5052%\n",
      "layer   3  Sparsity: 68.8257%\n",
      "total_backward_count 1488080 real_backward_count 101283   6.806%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.324586/  0.978819, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5459%\n",
      "layer   3  Sparsity: 68.6333%\n",
      "total_backward_count 1497870 real_backward_count 101583   6.782%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.330910/  1.006113, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6023%\n",
      "layer   3  Sparsity: 68.2340%\n",
      "total_backward_count 1507660 real_backward_count 101915   6.760%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.304531/  1.043298, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3475%\n",
      "layer   3  Sparsity: 68.3385%\n",
      "total_backward_count 1517450 real_backward_count 102220   6.736%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.305722/  1.083876, val:  67.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.9532%\n",
      "layer   3  Sparsity: 68.4956%\n",
      "total_backward_count 1527240 real_backward_count 102531   6.713%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.315268/  0.999660, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1511%\n",
      "layer   3  Sparsity: 68.7549%\n",
      "total_backward_count 1537030 real_backward_count 102860   6.692%\n",
      "fc layer 3 self.abs_max_out: 3345.0\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.313746/  1.012168, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0261%\n",
      "layer   3  Sparsity: 69.6924%\n",
      "total_backward_count 1546820 real_backward_count 103134   6.667%\n",
      "fc layer 3 self.abs_max_out: 3400.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.326877/  1.060736, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5473%\n",
      "layer   3  Sparsity: 68.8718%\n",
      "total_backward_count 1556610 real_backward_count 103418   6.644%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.327759/  1.024969, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7036%\n",
      "layer   3  Sparsity: 68.6616%\n",
      "total_backward_count 1566400 real_backward_count 103712   6.621%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.321559/  0.976973, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6475%\n",
      "layer   3  Sparsity: 69.0025%\n",
      "total_backward_count 1576190 real_backward_count 104017   6.599%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.326770/  0.971522, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5247%\n",
      "layer   3  Sparsity: 69.7172%\n",
      "total_backward_count 1585980 real_backward_count 104313   6.577%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.317049/  0.984180, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7419%\n",
      "layer   3  Sparsity: 70.0448%\n",
      "total_backward_count 1595770 real_backward_count 104590   6.554%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.321071/  0.984583, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6783%\n",
      "layer   3  Sparsity: 69.6270%\n",
      "total_backward_count 1605560 real_backward_count 104874   6.532%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.307127/  0.982020, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6808%\n",
      "layer   3  Sparsity: 69.8574%\n",
      "total_backward_count 1615350 real_backward_count 105140   6.509%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.313731/  0.981350, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.0999%\n",
      "layer   3  Sparsity: 68.5182%\n",
      "total_backward_count 1625140 real_backward_count 105407   6.486%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.307936/  1.033319, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1481%\n",
      "layer   3  Sparsity: 69.2895%\n",
      "total_backward_count 1634930 real_backward_count 105694   6.465%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.312007/  1.114753, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.1242%\n",
      "layer   3  Sparsity: 70.5632%\n",
      "total_backward_count 1644720 real_backward_count 105976   6.443%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.321931/  1.004127, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.7233%\n",
      "layer   3  Sparsity: 69.8545%\n",
      "total_backward_count 1654510 real_backward_count 106256   6.422%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.309015/  0.992903, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.6433%\n",
      "layer   3  Sparsity: 69.3744%\n",
      "total_backward_count 1664300 real_backward_count 106507   6.400%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.320484/  1.029427, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.3603%\n",
      "layer   3  Sparsity: 69.8184%\n",
      "total_backward_count 1674090 real_backward_count 106752   6.377%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.312581/  0.989471, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5172%\n",
      "layer   3  Sparsity: 70.0424%\n",
      "total_backward_count 1683880 real_backward_count 106990   6.354%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.296034/  0.990166, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 78.5424%\n",
      "layer   3  Sparsity: 70.3126%\n",
      "total_backward_count 1693670 real_backward_count 107196   6.329%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.320920/  1.029783, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.2188%\n",
      "layer   3  Sparsity: 69.9206%\n",
      "total_backward_count 1703460 real_backward_count 107440   6.307%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.318096/  1.013447, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.2366%\n",
      "layer   3  Sparsity: 70.7327%\n",
      "total_backward_count 1713250 real_backward_count 107679   6.285%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.339096/  1.055237, val:  71.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.4772%\n",
      "layer   3  Sparsity: 69.7890%\n",
      "total_backward_count 1723040 real_backward_count 107995   6.268%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.322601/  1.001769, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.6051%\n",
      "layer   3  Sparsity: 68.8358%\n",
      "total_backward_count 1732830 real_backward_count 108278   6.249%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.312675/  0.969476, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.8694%\n",
      "layer   3  Sparsity: 70.1234%\n",
      "total_backward_count 1742620 real_backward_count 108568   6.230%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.294749/  1.053195, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.8659%\n",
      "layer   3  Sparsity: 70.9626%\n",
      "total_backward_count 1752410 real_backward_count 108823   6.210%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.313094/  0.977708, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.7676%\n",
      "layer   3  Sparsity: 69.3302%\n",
      "total_backward_count 1762200 real_backward_count 109094   6.191%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.305162/  1.003179, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.6543%\n",
      "layer   3  Sparsity: 69.3435%\n",
      "total_backward_count 1771990 real_backward_count 109378   6.173%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.302424/  0.970160, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.8904%\n",
      "layer   3  Sparsity: 68.9053%\n",
      "total_backward_count 1781780 real_backward_count 109614   6.152%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.292178/  1.007631, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.6957%\n",
      "layer   3  Sparsity: 69.8619%\n",
      "total_backward_count 1791570 real_backward_count 109829   6.130%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.298789/  1.027604, val:  70.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.5728%\n",
      "layer   3  Sparsity: 69.4155%\n",
      "total_backward_count 1801360 real_backward_count 110060   6.110%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.291758/  0.938426, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.7397%\n",
      "layer   3  Sparsity: 69.6456%\n",
      "total_backward_count 1811150 real_backward_count 110285   6.089%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.289743/  0.995168, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.5294%\n",
      "layer   3  Sparsity: 70.3284%\n",
      "total_backward_count 1820940 real_backward_count 110486   6.068%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.303324/  0.960125, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.5123%\n",
      "layer   3  Sparsity: 69.7366%\n",
      "total_backward_count 1830730 real_backward_count 110731   6.048%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.285056/  0.969352, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.7653%\n",
      "layer   3  Sparsity: 69.2564%\n",
      "total_backward_count 1840520 real_backward_count 110988   6.030%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.289452/  1.009248, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.7363%\n",
      "layer   3  Sparsity: 69.9655%\n",
      "total_backward_count 1850310 real_backward_count 111226   6.011%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.281128/  0.983396, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 80.0074%\n",
      "layer   3  Sparsity: 69.9207%\n",
      "total_backward_count 1860100 real_backward_count 111457   5.992%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.284061/  1.004898, val:  75.00%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.9171%\n",
      "layer   3  Sparsity: 69.9884%\n",
      "total_backward_count 1869890 real_backward_count 111666   5.972%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.283805/  1.025328, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.8298%\n",
      "layer   3  Sparsity: 69.9409%\n",
      "total_backward_count 1879680 real_backward_count 111895   5.953%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.289096/  1.011951, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.9535%\n",
      "layer   3  Sparsity: 69.6813%\n",
      "total_backward_count 1889470 real_backward_count 112124   5.934%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.281970/  1.064045, val:  71.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.8721%\n",
      "layer   3  Sparsity: 69.4651%\n",
      "total_backward_count 1899260 real_backward_count 112347   5.915%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.293102/  1.189064, val:  62.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 80.0376%\n",
      "layer   3  Sparsity: 69.3910%\n",
      "total_backward_count 1909050 real_backward_count 112606   5.899%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.298788/  1.040505, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.7756%\n",
      "layer   3  Sparsity: 69.4473%\n",
      "total_backward_count 1918840 real_backward_count 112828   5.880%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.293116/  1.046596, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.5613%\n",
      "layer   3  Sparsity: 69.1000%\n",
      "total_backward_count 1928630 real_backward_count 113087   5.864%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.304457/  0.979863, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.2490%\n",
      "layer   3  Sparsity: 69.8412%\n",
      "total_backward_count 1938420 real_backward_count 113362   5.848%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.279530/  0.992783, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.5571%\n",
      "layer   3  Sparsity: 69.7374%\n",
      "total_backward_count 1948210 real_backward_count 113556   5.829%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.268659/  1.077971, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.6055%\n",
      "layer   2  Sparsity: 79.6865%\n",
      "layer   3  Sparsity: 70.5085%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3764e3970a3f42bfabfe2a7a957643cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.26866</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.69583</td></tr><tr><td>val_loss</td><td>1.07797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-87</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfqe9kmr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfqe9kmr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_225253-bfqe9kmr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9j5rk0t3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_031540-9j5rk0t3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9j5rk0t3' target=\"_blank\">denim-sweep-93</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9j5rk0t3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9j5rk0t3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251116_031549_490', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 696.0\n",
      "lif layer 1 self.abs_max_v: 696.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 254.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 826.0\n",
      "lif layer 1 self.abs_max_v: 1075.5\n",
      "fc layer 2 self.abs_max_out: 804.0\n",
      "lif layer 2 self.abs_max_v: 832.5\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 2 self.abs_max_out: 884.0\n",
      "lif layer 2 self.abs_max_v: 1134.5\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 1 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 1079.5\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "lif layer 2 self.abs_max_v: 1347.5\n",
      "lif layer 1 self.abs_max_v: 1247.0\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "lif layer 2 self.abs_max_v: 1518.0\n",
      "fc layer 1 self.abs_max_out: 1202.0\n",
      "lif layer 1 self.abs_max_v: 1294.0\n",
      "fc layer 1 self.abs_max_out: 1570.0\n",
      "lif layer 1 self.abs_max_v: 1602.0\n",
      "lif layer 2 self.abs_max_v: 1592.0\n",
      "fc layer 1 self.abs_max_out: 1697.0\n",
      "lif layer 1 self.abs_max_v: 1697.0\n",
      "lif layer 2 self.abs_max_v: 1762.0\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 3 self.abs_max_out: 321.0\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "fc layer 2 self.abs_max_out: 1481.0\n",
      "lif layer 2 self.abs_max_v: 1816.5\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "lif layer 2 self.abs_max_v: 1966.0\n",
      "fc layer 1 self.abs_max_out: 2029.0\n",
      "lif layer 1 self.abs_max_v: 2029.0\n",
      "lif layer 2 self.abs_max_v: 2128.5\n",
      "lif layer 2 self.abs_max_v: 2388.5\n",
      "lif layer 2 self.abs_max_v: 2390.5\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "lif layer 1 self.abs_max_v: 2243.0\n",
      "lif layer 2 self.abs_max_v: 2516.5\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 1 self.abs_max_out: 2530.0\n",
      "lif layer 1 self.abs_max_v: 2530.0\n",
      "lif layer 2 self.abs_max_v: 2560.5\n",
      "fc layer 1 self.abs_max_out: 2726.0\n",
      "lif layer 1 self.abs_max_v: 2726.0\n",
      "fc layer 1 self.abs_max_out: 2913.0\n",
      "lif layer 1 self.abs_max_v: 2913.0\n",
      "fc layer 3 self.abs_max_out: 859.0\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "fc layer 2 self.abs_max_out: 2199.0\n",
      "fc layer 2 self.abs_max_out: 2334.0\n",
      "lif layer 1 self.abs_max_v: 3013.0\n",
      "lif layer 2 self.abs_max_v: 2661.0\n",
      "fc layer 1 self.abs_max_out: 2943.0\n",
      "lif layer 2 self.abs_max_v: 2849.0\n",
      "lif layer 2 self.abs_max_v: 3141.0\n",
      "lif layer 2 self.abs_max_v: 3203.5\n",
      "lif layer 2 self.abs_max_v: 3414.5\n",
      "lif layer 1 self.abs_max_v: 3960.5\n",
      "fc layer 1 self.abs_max_out: 3019.0\n",
      "fc layer 1 self.abs_max_out: 3238.0\n",
      "fc layer 1 self.abs_max_out: 3396.0\n",
      "lif layer 1 self.abs_max_v: 3994.5\n",
      "fc layer 1 self.abs_max_out: 4411.0\n",
      "lif layer 1 self.abs_max_v: 4411.0\n",
      "fc layer 1 self.abs_max_out: 4679.0\n",
      "lif layer 1 self.abs_max_v: 4746.5\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "lif layer 2 self.abs_max_v: 3476.0\n",
      "lif layer 2 self.abs_max_v: 3588.5\n",
      "lif layer 2 self.abs_max_v: 3972.5\n",
      "lif layer 1 self.abs_max_v: 4909.0\n",
      "lif layer 2 self.abs_max_v: 3988.5\n",
      "fc layer 2 self.abs_max_out: 2773.0\n",
      "lif layer 1 self.abs_max_v: 5502.5\n",
      "fc layer 2 self.abs_max_out: 2893.0\n",
      "lif layer 1 self.abs_max_v: 5968.5\n",
      "lif layer 1 self.abs_max_v: 6146.0\n",
      "lif layer 2 self.abs_max_v: 4377.0\n",
      "fc layer 2 self.abs_max_out: 2940.0\n",
      "fc layer 2 self.abs_max_out: 3054.0\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "fc layer 3 self.abs_max_out: 1118.0\n",
      "lif layer 1 self.abs_max_v: 6234.5\n",
      "lif layer 1 self.abs_max_v: 7114.5\n",
      "fc layer 1 self.abs_max_out: 4721.0\n",
      "fc layer 1 self.abs_max_out: 4907.0\n",
      "lif layer 2 self.abs_max_v: 4383.0\n",
      "lif layer 2 self.abs_max_v: 4396.0\n",
      "fc layer 3 self.abs_max_out: 1129.0\n",
      "fc layer 1 self.abs_max_out: 4913.0\n",
      "fc layer 1 self.abs_max_out: 5285.0\n",
      "lif layer 1 self.abs_max_v: 7207.0\n",
      "fc layer 1 self.abs_max_out: 5630.0\n",
      "lif layer 1 self.abs_max_v: 7883.5\n",
      "fc layer 2 self.abs_max_out: 3069.0\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "lif layer 2 self.abs_max_v: 4825.5\n",
      "lif layer 2 self.abs_max_v: 5230.0\n",
      "fc layer 1 self.abs_max_out: 6181.0\n",
      "fc layer 2 self.abs_max_out: 3161.0\n",
      "lif layer 1 self.abs_max_v: 8092.5\n",
      "fc layer 2 self.abs_max_out: 3203.0\n",
      "lif layer 1 self.abs_max_v: 8298.0\n",
      "fc layer 2 self.abs_max_out: 3415.0\n",
      "fc layer 2 self.abs_max_out: 3577.0\n",
      "fc layer 2 self.abs_max_out: 3720.0\n",
      "fc layer 2 self.abs_max_out: 3796.0\n",
      "fc layer 2 self.abs_max_out: 3839.0\n",
      "lif layer 1 self.abs_max_v: 8642.0\n",
      "lif layer 1 self.abs_max_v: 9538.0\n",
      "fc layer 1 self.abs_max_out: 6234.0\n",
      "lif layer 1 self.abs_max_v: 9930.5\n",
      "lif layer 1 self.abs_max_v: 10011.5\n",
      "fc layer 1 self.abs_max_out: 6872.0\n",
      "fc layer 1 self.abs_max_out: 6914.0\n",
      "fc layer 2 self.abs_max_out: 3866.0\n",
      "fc layer 2 self.abs_max_out: 3867.0\n",
      "fc layer 2 self.abs_max_out: 4224.0\n",
      "fc layer 2 self.abs_max_out: 4250.0\n",
      "lif layer 2 self.abs_max_v: 5508.5\n",
      "lif layer 2 self.abs_max_v: 5667.5\n",
      "lif layer 1 self.abs_max_v: 10081.0\n",
      "fc layer 3 self.abs_max_out: 1130.0\n",
      "fc layer 3 self.abs_max_out: 1165.0\n",
      "fc layer 3 self.abs_max_out: 1241.0\n",
      "lif layer 1 self.abs_max_v: 10527.0\n",
      "lif layer 1 self.abs_max_v: 10881.5\n",
      "lif layer 1 self.abs_max_v: 11042.0\n",
      "lif layer 1 self.abs_max_v: 11562.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.861921/  1.987807, val:  33.33%, val_best:  33.33%, tr:  96.63%, tr_best:  96.63%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.7450%\n",
      "layer   3  Sparsity: 77.7365%\n",
      "total_backward_count 9790 real_backward_count 2225  22.727%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 5724.5\n",
      "lif layer 1 self.abs_max_v: 12033.0\n",
      "fc layer 1 self.abs_max_out: 6989.0\n",
      "fc layer 1 self.abs_max_out: 7107.0\n",
      "fc layer 1 self.abs_max_out: 7122.0\n",
      "fc layer 2 self.abs_max_out: 4317.0\n",
      "fc layer 1 self.abs_max_out: 7614.0\n",
      "fc layer 3 self.abs_max_out: 1353.0\n",
      "fc layer 3 self.abs_max_out: 1469.0\n",
      "fc layer 3 self.abs_max_out: 1488.0\n",
      "fc layer 3 self.abs_max_out: 1547.0\n",
      "lif layer 2 self.abs_max_v: 5991.0\n",
      "lif layer 2 self.abs_max_v: 6049.5\n",
      "fc layer 1 self.abs_max_out: 7871.0\n",
      "lif layer 1 self.abs_max_v: 12899.0\n",
      "lif layer 1 self.abs_max_v: 13327.5\n",
      "lif layer 1 self.abs_max_v: 13505.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.766706/  1.960887, val:  45.42%, val_best:  45.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0089%\n",
      "layer   3  Sparsity: 75.1407%\n",
      "total_backward_count 19580 real_backward_count 3805  19.433%\n",
      "lif layer 1 self.abs_max_v: 13992.0\n",
      "fc layer 1 self.abs_max_out: 7902.0\n",
      "fc layer 1 self.abs_max_out: 8143.0\n",
      "lif layer 2 self.abs_max_v: 6215.5\n",
      "lif layer 2 self.abs_max_v: 6360.0\n",
      "lif layer 2 self.abs_max_v: 6371.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.743922/  1.955438, val:  41.67%, val_best:  45.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6237%\n",
      "layer   3  Sparsity: 75.2872%\n",
      "total_backward_count 29370 real_backward_count 5368  18.277%\n",
      "lif layer 2 self.abs_max_v: 6396.5\n",
      "fc layer 1 self.abs_max_out: 8367.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.724427/  1.935293, val:  33.33%, val_best:  45.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.6555%\n",
      "layer   3  Sparsity: 74.1589%\n",
      "total_backward_count 39160 real_backward_count 6878  17.564%\n",
      "fc layer 1 self.abs_max_out: 8473.0\n",
      "fc layer 1 self.abs_max_out: 8598.0\n",
      "fc layer 1 self.abs_max_out: 8816.0\n",
      "lif layer 1 self.abs_max_v: 15544.0\n",
      "lif layer 1 self.abs_max_v: 16009.0\n",
      "fc layer 1 self.abs_max_out: 9006.0\n",
      "fc layer 1 self.abs_max_out: 9745.0\n",
      "lif layer 1 self.abs_max_v: 16980.0\n",
      "fc layer 2 self.abs_max_out: 4335.0\n",
      "fc layer 2 self.abs_max_out: 4477.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.697939/  1.933167, val:  34.58%, val_best:  45.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.3794%\n",
      "layer   3  Sparsity: 73.9921%\n",
      "total_backward_count 48950 real_backward_count 8315  16.987%\n",
      "fc layer 1 self.abs_max_out: 10173.0\n",
      "lif layer 1 self.abs_max_v: 17360.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.701854/  1.924498, val:  52.50%, val_best:  52.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.5789%\n",
      "layer   3  Sparsity: 76.6082%\n",
      "total_backward_count 58740 real_backward_count 9736  16.575%\n",
      "fc layer 3 self.abs_max_out: 1597.0\n",
      "lif layer 2 self.abs_max_v: 6515.0\n",
      "lif layer 2 self.abs_max_v: 7143.5\n",
      "fc layer 3 self.abs_max_out: 1614.0\n",
      "fc layer 2 self.abs_max_out: 4528.0\n",
      "fc layer 1 self.abs_max_out: 10240.0\n",
      "lif layer 1 self.abs_max_v: 17383.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.685068/  1.855947, val:  57.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0586%\n",
      "layer   3  Sparsity: 76.7959%\n",
      "total_backward_count 68530 real_backward_count 11130  16.241%\n",
      "lif layer 2 self.abs_max_v: 7277.0\n",
      "fc layer 1 self.abs_max_out: 10401.0\n",
      "lif layer 1 self.abs_max_v: 17673.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.669826/  1.862401, val:  55.83%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.2083%\n",
      "layer   3  Sparsity: 77.4313%\n",
      "total_backward_count 78320 real_backward_count 12483  15.938%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.687833/  1.889578, val:  55.42%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.8901%\n",
      "layer   3  Sparsity: 78.3236%\n",
      "total_backward_count 88110 real_backward_count 13864  15.735%\n",
      "fc layer 1 self.abs_max_out: 10417.0\n",
      "lif layer 1 self.abs_max_v: 18324.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.693291/  1.897440, val:  53.33%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1133%\n",
      "layer   3  Sparsity: 78.8721%\n",
      "total_backward_count 97900 real_backward_count 15194  15.520%\n",
      "fc layer 2 self.abs_max_out: 4675.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.691602/  1.910619, val:  50.42%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.2599%\n",
      "layer   3  Sparsity: 78.3335%\n",
      "total_backward_count 107690 real_backward_count 16491  15.313%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.695153/  1.863286, val:  55.42%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.9950%\n",
      "layer   3  Sparsity: 79.0883%\n",
      "total_backward_count 117480 real_backward_count 17802  15.153%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.681080/  1.881008, val:  55.42%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.6919%\n",
      "layer   3  Sparsity: 79.6646%\n",
      "total_backward_count 127270 real_backward_count 19035  14.956%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.683611/  1.876684, val:  48.75%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.6412%\n",
      "layer   3  Sparsity: 79.5452%\n",
      "total_backward_count 137060 real_backward_count 20279  14.796%\n",
      "lif layer 1 self.abs_max_v: 18787.5\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.668896/  1.852354, val:  57.08%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1635%\n",
      "layer   3  Sparsity: 80.2010%\n",
      "total_backward_count 146850 real_backward_count 21460  14.614%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.686572/  1.884291, val:  50.83%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1079%\n",
      "layer   3  Sparsity: 80.5439%\n",
      "total_backward_count 156640 real_backward_count 22645  14.457%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.680536/  1.865550, val:  62.92%, val_best:  62.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4588%\n",
      "layer   3  Sparsity: 80.5566%\n",
      "total_backward_count 166430 real_backward_count 23781  14.289%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.669338/  1.839266, val:  55.83%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.3587%\n",
      "layer   3  Sparsity: 79.6581%\n",
      "total_backward_count 176220 real_backward_count 24939  14.152%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.662888/  1.831866, val:  54.17%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.9978%\n",
      "layer   3  Sparsity: 80.0201%\n",
      "total_backward_count 186010 real_backward_count 26087  14.025%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.646707/  1.856888, val:  51.67%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1567%\n",
      "layer   3  Sparsity: 80.0641%\n",
      "total_backward_count 195800 real_backward_count 27161  13.872%\n",
      "fc layer 3 self.abs_max_out: 1676.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.654454/  1.849167, val:  59.17%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5397%\n",
      "layer   3  Sparsity: 80.5560%\n",
      "total_backward_count 205590 real_backward_count 28217  13.725%\n",
      "fc layer 1 self.abs_max_out: 10840.0\n",
      "lif layer 1 self.abs_max_v: 19372.5\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.670624/  1.898955, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8825%\n",
      "layer   3  Sparsity: 81.7987%\n",
      "total_backward_count 215380 real_backward_count 29288  13.598%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.689170/  1.800869, val:  57.50%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4634%\n",
      "layer   3  Sparsity: 81.2783%\n",
      "total_backward_count 225170 real_backward_count 30409  13.505%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.655780/  1.831717, val:  67.50%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5043%\n",
      "layer   3  Sparsity: 80.7675%\n",
      "total_backward_count 234960 real_backward_count 31452  13.386%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.636717/  1.782717, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5298%\n",
      "layer   3  Sparsity: 80.4762%\n",
      "total_backward_count 244750 real_backward_count 32490  13.275%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.634685/  1.810546, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4053%\n",
      "layer   3  Sparsity: 81.7774%\n",
      "total_backward_count 254540 real_backward_count 33484  13.155%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.629922/  1.800336, val:  73.75%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4977%\n",
      "layer   3  Sparsity: 80.7901%\n",
      "total_backward_count 264330 real_backward_count 34474  13.042%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.632380/  1.832547, val:  60.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.3602%\n",
      "layer   3  Sparsity: 81.0975%\n",
      "total_backward_count 274120 real_backward_count 35417  12.920%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.623453/  1.790128, val:  68.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1346%\n",
      "layer   3  Sparsity: 80.6323%\n",
      "total_backward_count 283910 real_backward_count 36375  12.812%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.638846/  1.828182, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.3571%\n",
      "layer   3  Sparsity: 81.8729%\n",
      "total_backward_count 293700 real_backward_count 37313  12.704%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.658071/  1.846394, val:  71.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4383%\n",
      "layer   3  Sparsity: 82.2729%\n",
      "total_backward_count 303490 real_backward_count 38223  12.594%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.655769/  1.835162, val:  67.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7662%\n",
      "layer   3  Sparsity: 82.0158%\n",
      "total_backward_count 313280 real_backward_count 39101  12.481%\n",
      "lif layer 2 self.abs_max_v: 7400.5\n",
      "lif layer 2 self.abs_max_v: 7552.5\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.659720/  1.843706, val:  62.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4978%\n",
      "layer   3  Sparsity: 81.9111%\n",
      "total_backward_count 323070 real_backward_count 40005  12.383%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.659132/  1.807858, val:  78.75%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5479%\n",
      "layer   3  Sparsity: 82.2434%\n",
      "total_backward_count 332860 real_backward_count 40879  12.281%\n",
      "lif layer 2 self.abs_max_v: 7596.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.655731/  1.819154, val:  65.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1270%\n",
      "layer   3  Sparsity: 82.3664%\n",
      "total_backward_count 342650 real_backward_count 41741  12.182%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.639906/  1.813668, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.3974%\n",
      "layer   3  Sparsity: 82.2308%\n",
      "total_backward_count 352440 real_backward_count 42584  12.083%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.625865/  1.778641, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4898%\n",
      "layer   3  Sparsity: 82.1042%\n",
      "total_backward_count 362230 real_backward_count 43402  11.982%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.596640/  1.872102, val:  60.42%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.3518%\n",
      "layer   3  Sparsity: 81.5771%\n",
      "total_backward_count 372020 real_backward_count 44208  11.883%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.615848/  1.795282, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.9357%\n",
      "layer   3  Sparsity: 82.2484%\n",
      "total_backward_count 381810 real_backward_count 45000  11.786%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.603069/  1.814007, val:  66.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0997%\n",
      "layer   3  Sparsity: 81.2993%\n",
      "total_backward_count 391600 real_backward_count 45746  11.682%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.602311/  1.788422, val:  75.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.9552%\n",
      "layer   3  Sparsity: 81.6722%\n",
      "total_backward_count 401390 real_backward_count 46505  11.586%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.605848/  1.815298, val:  63.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.3965%\n",
      "layer   3  Sparsity: 81.9779%\n",
      "total_backward_count 411180 real_backward_count 47222  11.485%\n",
      "fc layer 3 self.abs_max_out: 1693.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.591958/  1.778671, val:  70.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0561%\n",
      "layer   3  Sparsity: 81.7829%\n",
      "total_backward_count 420970 real_backward_count 47982  11.398%\n",
      "fc layer 3 self.abs_max_out: 1748.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.587921/  1.790408, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0365%\n",
      "layer   3  Sparsity: 81.7232%\n",
      "total_backward_count 430760 real_backward_count 48711  11.308%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.593962/  1.781604, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0224%\n",
      "layer   3  Sparsity: 81.5350%\n",
      "total_backward_count 440550 real_backward_count 49456  11.226%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.599375/  1.791226, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.2114%\n",
      "layer   3  Sparsity: 81.9974%\n",
      "total_backward_count 450340 real_backward_count 50127  11.131%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.629825/  1.811090, val:  67.50%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0032%\n",
      "layer   3  Sparsity: 82.9073%\n",
      "total_backward_count 460130 real_backward_count 50834  11.048%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.610925/  1.802272, val:  69.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5398%\n",
      "layer   3  Sparsity: 82.7310%\n",
      "total_backward_count 469920 real_backward_count 51530  10.966%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.588698/  1.752744, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6699%\n",
      "layer   3  Sparsity: 81.6559%\n",
      "total_backward_count 479710 real_backward_count 52243  10.891%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.591387/  1.757799, val:  75.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8939%\n",
      "layer   3  Sparsity: 81.7879%\n",
      "total_backward_count 489500 real_backward_count 52881  10.803%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.611936/  1.803595, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7589%\n",
      "layer   3  Sparsity: 83.0160%\n",
      "total_backward_count 499290 real_backward_count 53529  10.721%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.607614/  1.843423, val:  75.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6227%\n",
      "layer   3  Sparsity: 82.7347%\n",
      "total_backward_count 509080 real_backward_count 54170  10.641%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.622303/  1.809213, val:  70.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6247%\n",
      "layer   3  Sparsity: 82.4434%\n",
      "total_backward_count 518870 real_backward_count 54834  10.568%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.602631/  1.753628, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6498%\n",
      "layer   3  Sparsity: 81.5890%\n",
      "total_backward_count 528660 real_backward_count 55456  10.490%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.590935/  1.799150, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8059%\n",
      "layer   3  Sparsity: 81.9274%\n",
      "total_backward_count 538450 real_backward_count 56119  10.422%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.613410/  1.797158, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7018%\n",
      "layer   3  Sparsity: 82.6470%\n",
      "total_backward_count 548240 real_backward_count 56766  10.354%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.594421/  1.786738, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0922%\n",
      "layer   3  Sparsity: 82.4409%\n",
      "total_backward_count 558030 real_backward_count 57362  10.279%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.593429/  1.777960, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7441%\n",
      "layer   3  Sparsity: 82.1017%\n",
      "total_backward_count 567820 real_backward_count 57951  10.206%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.587404/  1.780460, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5703%\n",
      "layer   3  Sparsity: 82.2175%\n",
      "total_backward_count 577610 real_backward_count 58525  10.132%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.583044/  1.803373, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8644%\n",
      "layer   3  Sparsity: 82.6589%\n",
      "total_backward_count 587400 real_backward_count 59080  10.058%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.581548/  1.777761, val:  69.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9847%\n",
      "layer   3  Sparsity: 82.2905%\n",
      "total_backward_count 597190 real_backward_count 59683   9.994%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.591987/  1.793479, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8709%\n",
      "layer   3  Sparsity: 82.9011%\n",
      "total_backward_count 606980 real_backward_count 60276   9.930%\n",
      "fc layer 3 self.abs_max_out: 1806.0\n",
      "fc layer 3 self.abs_max_out: 1835.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.579697/  1.792666, val:  74.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0286%\n",
      "layer   3  Sparsity: 82.5911%\n",
      "total_backward_count 616770 real_backward_count 60846   9.865%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.586022/  1.787488, val:  75.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5195%\n",
      "layer   3  Sparsity: 82.6526%\n",
      "total_backward_count 626560 real_backward_count 61388   9.798%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.587801/  1.797325, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4873%\n",
      "layer   3  Sparsity: 82.5867%\n",
      "total_backward_count 636350 real_backward_count 61946   9.735%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.584828/  1.786465, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5343%\n",
      "layer   3  Sparsity: 82.0625%\n",
      "total_backward_count 646140 real_backward_count 62475   9.669%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.577756/  1.760306, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6372%\n",
      "layer   3  Sparsity: 82.1651%\n",
      "total_backward_count 655930 real_backward_count 62990   9.603%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.576825/  1.777373, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7321%\n",
      "layer   3  Sparsity: 81.9299%\n",
      "total_backward_count 665720 real_backward_count 63524   9.542%\n",
      "fc layer 1 self.abs_max_out: 10912.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.566852/  1.764584, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5883%\n",
      "layer   3  Sparsity: 83.0960%\n",
      "total_backward_count 675510 real_backward_count 64027   9.478%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.566618/  1.770438, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5582%\n",
      "layer   3  Sparsity: 82.7267%\n",
      "total_backward_count 685300 real_backward_count 64472   9.408%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.563596/  1.749539, val:  69.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5454%\n",
      "layer   3  Sparsity: 82.5575%\n",
      "total_backward_count 695090 real_backward_count 64943   9.343%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.561266/  1.762254, val:  71.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7956%\n",
      "layer   3  Sparsity: 82.4507%\n",
      "total_backward_count 704880 real_backward_count 65417   9.281%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.574257/  1.766050, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6912%\n",
      "layer   3  Sparsity: 82.4553%\n",
      "total_backward_count 714670 real_backward_count 65913   9.223%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.555730/  1.748522, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7634%\n",
      "layer   3  Sparsity: 82.6922%\n",
      "total_backward_count 724460 real_backward_count 66380   9.163%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.566681/  1.755020, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7693%\n",
      "layer   3  Sparsity: 82.2687%\n",
      "total_backward_count 734250 real_backward_count 66877   9.108%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.550732/  1.738350, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4490%\n",
      "layer   3  Sparsity: 81.3823%\n",
      "total_backward_count 744040 real_backward_count 67351   9.052%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.551791/  1.768725, val:  70.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6065%\n",
      "layer   3  Sparsity: 81.6217%\n",
      "total_backward_count 753830 real_backward_count 67842   9.000%\n",
      "fc layer 3 self.abs_max_out: 1887.0\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.553531/  1.764536, val:  72.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6416%\n",
      "layer   3  Sparsity: 82.2390%\n",
      "total_backward_count 763620 real_backward_count 68313   8.946%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.508192/  1.723967, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9173%\n",
      "layer   3  Sparsity: 81.7292%\n",
      "total_backward_count 773410 real_backward_count 68786   8.894%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.510284/  1.732995, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.1405%\n",
      "layer   3  Sparsity: 81.5405%\n",
      "total_backward_count 783200 real_backward_count 69220   8.838%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.536505/  1.744657, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7429%\n",
      "layer   3  Sparsity: 82.0667%\n",
      "total_backward_count 792990 real_backward_count 69656   8.784%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.532248/  1.759015, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7316%\n",
      "layer   3  Sparsity: 82.4276%\n",
      "total_backward_count 802780 real_backward_count 70070   8.728%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.531126/  1.725991, val:  72.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7220%\n",
      "layer   3  Sparsity: 81.4805%\n",
      "total_backward_count 812570 real_backward_count 70504   8.677%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.515679/  1.734953, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7555%\n",
      "layer   3  Sparsity: 81.8973%\n",
      "total_backward_count 822360 real_backward_count 70981   8.631%\n",
      "fc layer 3 self.abs_max_out: 1899.0\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.511865/  1.710300, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8313%\n",
      "layer   3  Sparsity: 81.7074%\n",
      "total_backward_count 832150 real_backward_count 71399   8.580%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.515294/  1.732535, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9386%\n",
      "layer   3  Sparsity: 81.7609%\n",
      "total_backward_count 841940 real_backward_count 71849   8.534%\n",
      "fc layer 3 self.abs_max_out: 1961.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.516673/  1.724076, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7263%\n",
      "layer   3  Sparsity: 81.6568%\n",
      "total_backward_count 851730 real_backward_count 72262   8.484%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.522679/  1.733263, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5082%\n",
      "layer   3  Sparsity: 81.7489%\n",
      "total_backward_count 861520 real_backward_count 72688   8.437%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.519294/  1.732768, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4327%\n",
      "layer   3  Sparsity: 82.1559%\n",
      "total_backward_count 871310 real_backward_count 73096   8.389%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.503611/  1.703058, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5031%\n",
      "layer   3  Sparsity: 81.0127%\n",
      "total_backward_count 881100 real_backward_count 73505   8.342%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.480413/  1.715183, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4991%\n",
      "layer   3  Sparsity: 81.7790%\n",
      "total_backward_count 890890 real_backward_count 73928   8.298%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.498418/  1.700319, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7326%\n",
      "layer   3  Sparsity: 81.5793%\n",
      "total_backward_count 900680 real_backward_count 74328   8.252%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.471589/  1.713396, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6302%\n",
      "layer   3  Sparsity: 81.3711%\n",
      "total_backward_count 910470 real_backward_count 74706   8.205%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.486708/  1.711926, val:  75.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8731%\n",
      "layer   3  Sparsity: 81.5169%\n",
      "total_backward_count 920260 real_backward_count 75094   8.160%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.491987/  1.702962, val:  75.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9235%\n",
      "layer   3  Sparsity: 81.8639%\n",
      "total_backward_count 930050 real_backward_count 75465   8.114%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.481930/  1.693882, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6326%\n",
      "layer   3  Sparsity: 81.5204%\n",
      "total_backward_count 939840 real_backward_count 75856   8.071%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.475856/  1.730881, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6427%\n",
      "layer   3  Sparsity: 81.6186%\n",
      "total_backward_count 949630 real_backward_count 76176   8.022%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.519817/  1.725437, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4136%\n",
      "layer   3  Sparsity: 82.4037%\n",
      "total_backward_count 959420 real_backward_count 76563   7.980%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.512186/  1.764850, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7148%\n",
      "layer   3  Sparsity: 82.3263%\n",
      "total_backward_count 969210 real_backward_count 76905   7.935%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.505579/  1.696947, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6783%\n",
      "layer   3  Sparsity: 82.1622%\n",
      "total_backward_count 979000 real_backward_count 77257   7.891%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.489028/  1.700914, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7987%\n",
      "layer   3  Sparsity: 81.9146%\n",
      "total_backward_count 988790 real_backward_count 77614   7.849%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.488899/  1.727903, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0583%\n",
      "layer   3  Sparsity: 81.7663%\n",
      "total_backward_count 998580 real_backward_count 77975   7.809%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.505268/  1.722080, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7802%\n",
      "layer   3  Sparsity: 81.8866%\n",
      "total_backward_count 1008370 real_backward_count 78313   7.766%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.484338/  1.721688, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8335%\n",
      "layer   3  Sparsity: 81.2375%\n",
      "total_backward_count 1018160 real_backward_count 78659   7.726%\n",
      "fc layer 3 self.abs_max_out: 2043.0\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.498180/  1.726444, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7543%\n",
      "layer   3  Sparsity: 81.7202%\n",
      "total_backward_count 1027950 real_backward_count 79048   7.690%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.487976/  1.698982, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6980%\n",
      "layer   3  Sparsity: 81.2065%\n",
      "total_backward_count 1037740 real_backward_count 79365   7.648%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.474142/  1.702241, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6241%\n",
      "layer   3  Sparsity: 81.6640%\n",
      "total_backward_count 1047530 real_backward_count 79698   7.608%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.476084/  1.719961, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5167%\n",
      "layer   3  Sparsity: 82.0600%\n",
      "total_backward_count 1057320 real_backward_count 80014   7.568%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.486764/  1.701033, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7897%\n",
      "layer   3  Sparsity: 82.3187%\n",
      "total_backward_count 1067110 real_backward_count 80313   7.526%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.486777/  1.712244, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5984%\n",
      "layer   3  Sparsity: 82.4826%\n",
      "total_backward_count 1076900 real_backward_count 80619   7.486%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.493738/  1.714567, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7383%\n",
      "layer   3  Sparsity: 82.4700%\n",
      "total_backward_count 1086690 real_backward_count 80917   7.446%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.476288/  1.695559, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7137%\n",
      "layer   3  Sparsity: 82.4171%\n",
      "total_backward_count 1096480 real_backward_count 81201   7.406%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.490602/  1.708010, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4021%\n",
      "layer   3  Sparsity: 82.5157%\n",
      "total_backward_count 1106270 real_backward_count 81538   7.371%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.477132/  1.730148, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4767%\n",
      "layer   3  Sparsity: 82.5204%\n",
      "total_backward_count 1116060 real_backward_count 81822   7.331%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.494305/  1.697174, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4490%\n",
      "layer   3  Sparsity: 82.7808%\n",
      "total_backward_count 1125850 real_backward_count 82131   7.295%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.480716/  1.691153, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6307%\n",
      "layer   3  Sparsity: 81.5876%\n",
      "total_backward_count 1135640 real_backward_count 82475   7.262%\n",
      "fc layer 1 self.abs_max_out: 10917.0\n",
      "lif layer 1 self.abs_max_v: 19450.0\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.480114/  1.697372, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7418%\n",
      "layer   3  Sparsity: 82.2857%\n",
      "total_backward_count 1145430 real_backward_count 82828   7.231%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.490669/  1.723888, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5771%\n",
      "layer   3  Sparsity: 82.4040%\n",
      "total_backward_count 1155220 real_backward_count 83171   7.200%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.483535/  1.693342, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7037%\n",
      "layer   3  Sparsity: 82.2295%\n",
      "total_backward_count 1165010 real_backward_count 83535   7.170%\n",
      "fc layer 2 self.abs_max_out: 4677.0\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.477656/  1.723262, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8318%\n",
      "layer   3  Sparsity: 82.6071%\n",
      "total_backward_count 1174800 real_backward_count 83846   7.137%\n",
      "fc layer 2 self.abs_max_out: 4690.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.478990/  1.712635, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9174%\n",
      "layer   3  Sparsity: 82.1361%\n",
      "total_backward_count 1184590 real_backward_count 84146   7.103%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.470787/  1.664039, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7662%\n",
      "layer   3  Sparsity: 81.2997%\n",
      "total_backward_count 1194380 real_backward_count 84437   7.070%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.451423/  1.686743, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9166%\n",
      "layer   3  Sparsity: 81.7828%\n",
      "total_backward_count 1204170 real_backward_count 84733   7.037%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.450069/  1.663730, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7158%\n",
      "layer   3  Sparsity: 81.1458%\n",
      "total_backward_count 1213960 real_backward_count 85018   7.003%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.460794/  1.681099, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8587%\n",
      "layer   3  Sparsity: 82.3497%\n",
      "total_backward_count 1223750 real_backward_count 85333   6.973%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.459609/  1.696799, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8089%\n",
      "layer   3  Sparsity: 82.1165%\n",
      "total_backward_count 1233540 real_backward_count 85628   6.942%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.473614/  1.691385, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8910%\n",
      "layer   3  Sparsity: 82.3192%\n",
      "total_backward_count 1243330 real_backward_count 85897   6.909%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.466711/  1.681681, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5641%\n",
      "layer   3  Sparsity: 82.7169%\n",
      "total_backward_count 1253120 real_backward_count 86179   6.877%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.454694/  1.671616, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7919%\n",
      "layer   3  Sparsity: 82.1146%\n",
      "total_backward_count 1262910 real_backward_count 86438   6.844%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.459364/  1.674393, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7274%\n",
      "layer   3  Sparsity: 82.2359%\n",
      "total_backward_count 1272700 real_backward_count 86705   6.813%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.458935/  1.681940, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9186%\n",
      "layer   3  Sparsity: 82.0810%\n",
      "total_backward_count 1282490 real_backward_count 86954   6.780%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.460171/  1.665082, val:  75.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7626%\n",
      "layer   3  Sparsity: 81.8660%\n",
      "total_backward_count 1292280 real_backward_count 87243   6.751%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.420292/  1.669880, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8694%\n",
      "layer   3  Sparsity: 80.5304%\n",
      "total_backward_count 1302070 real_backward_count 87522   6.722%\n",
      "fc layer 3 self.abs_max_out: 2090.0\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.435091/  1.677606, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8101%\n",
      "layer   3  Sparsity: 81.1379%\n",
      "total_backward_count 1311860 real_backward_count 87820   6.694%\n",
      "fc layer 3 self.abs_max_out: 2096.0\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.430523/  1.667031, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7075%\n",
      "layer   3  Sparsity: 81.6550%\n",
      "total_backward_count 1321650 real_backward_count 88048   6.662%\n",
      "fc layer 1 self.abs_max_out: 10939.0\n",
      "lif layer 1 self.abs_max_v: 19599.5\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.437589/  1.708010, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.1433%\n",
      "layer   3  Sparsity: 82.3964%\n",
      "total_backward_count 1331440 real_backward_count 88274   6.630%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.450117/  1.677507, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9086%\n",
      "layer   3  Sparsity: 82.2224%\n",
      "total_backward_count 1341230 real_backward_count 88518   6.600%\n",
      "fc layer 1 self.abs_max_out: 10945.0\n",
      "lif layer 1 self.abs_max_v: 19704.5\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.440290/  1.673205, val:  74.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9862%\n",
      "layer   3  Sparsity: 82.3868%\n",
      "total_backward_count 1351020 real_backward_count 88757   6.570%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.439451/  1.670460, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5959%\n",
      "layer   3  Sparsity: 81.9950%\n",
      "total_backward_count 1360810 real_backward_count 88990   6.539%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.455691/  1.680944, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6886%\n",
      "layer   3  Sparsity: 82.2944%\n",
      "total_backward_count 1370600 real_backward_count 89252   6.512%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.436974/  1.671562, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6853%\n",
      "layer   3  Sparsity: 82.1545%\n",
      "total_backward_count 1380390 real_backward_count 89501   6.484%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.426590/  1.664573, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8057%\n",
      "layer   3  Sparsity: 81.3235%\n",
      "total_backward_count 1390180 real_backward_count 89756   6.456%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.422696/  1.677989, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9453%\n",
      "layer   3  Sparsity: 81.3833%\n",
      "total_backward_count 1399970 real_backward_count 89994   6.428%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.433354/  1.668112, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9340%\n",
      "layer   3  Sparsity: 81.3407%\n",
      "total_backward_count 1409760 real_backward_count 90255   6.402%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.430667/  1.680311, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9964%\n",
      "layer   3  Sparsity: 81.5961%\n",
      "total_backward_count 1419550 real_backward_count 90507   6.376%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.434264/  1.656682, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8607%\n",
      "layer   3  Sparsity: 82.0599%\n",
      "total_backward_count 1429340 real_backward_count 90723   6.347%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.438973/  1.648188, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6207%\n",
      "layer   3  Sparsity: 81.5031%\n",
      "total_backward_count 1439130 real_backward_count 90999   6.323%\n",
      "fc layer 1 self.abs_max_out: 10991.0\n",
      "lif layer 1 self.abs_max_v: 19857.5\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.420222/  1.659763, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.7719%\n",
      "layer   3  Sparsity: 81.7605%\n",
      "total_backward_count 1448920 real_backward_count 91239   6.297%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.425916/  1.675059, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6491%\n",
      "layer   3  Sparsity: 82.0340%\n",
      "total_backward_count 1458710 real_backward_count 91466   6.270%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.451000/  1.687833, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.5993%\n",
      "layer   3  Sparsity: 81.7929%\n",
      "total_backward_count 1468500 real_backward_count 91720   6.246%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.425190/  1.663325, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9089%\n",
      "layer   3  Sparsity: 81.8769%\n",
      "total_backward_count 1478290 real_backward_count 91938   6.219%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.429041/  1.674680, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8422%\n",
      "layer   3  Sparsity: 82.1228%\n",
      "total_backward_count 1488080 real_backward_count 92162   6.193%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.432249/  1.694802, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8545%\n",
      "layer   3  Sparsity: 82.1998%\n",
      "total_backward_count 1497870 real_backward_count 92409   6.169%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.432096/  1.674375, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9514%\n",
      "layer   3  Sparsity: 82.2736%\n",
      "total_backward_count 1507660 real_backward_count 92647   6.145%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.425528/  1.672165, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8934%\n",
      "layer   3  Sparsity: 82.7863%\n",
      "total_backward_count 1517450 real_backward_count 92829   6.117%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.433600/  1.683756, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8547%\n",
      "layer   3  Sparsity: 82.4186%\n",
      "total_backward_count 1527240 real_backward_count 93035   6.092%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.426933/  1.690132, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8342%\n",
      "layer   3  Sparsity: 82.3427%\n",
      "total_backward_count 1537030 real_backward_count 93239   6.066%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.442689/  1.716715, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8666%\n",
      "layer   3  Sparsity: 82.7858%\n",
      "total_backward_count 1546820 real_backward_count 93420   6.039%\n",
      "fc layer 1 self.abs_max_out: 11019.0\n",
      "lif layer 1 self.abs_max_v: 19997.5\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.437372/  1.671627, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0641%\n",
      "layer   3  Sparsity: 82.6263%\n",
      "total_backward_count 1556610 real_backward_count 93640   6.016%\n",
      "fc layer 1 self.abs_max_out: 11030.0\n",
      "lif layer 1 self.abs_max_v: 20050.5\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.419397/  1.665240, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8155%\n",
      "layer   3  Sparsity: 82.1311%\n",
      "total_backward_count 1566400 real_backward_count 93846   5.991%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.419818/  1.655457, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8349%\n",
      "layer   3  Sparsity: 82.4210%\n",
      "total_backward_count 1576190 real_backward_count 94080   5.969%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.435736/  1.669216, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6858%\n",
      "layer   3  Sparsity: 82.6593%\n",
      "total_backward_count 1585980 real_backward_count 94284   5.945%\n",
      "fc layer 1 self.abs_max_out: 11041.0\n",
      "lif layer 1 self.abs_max_v: 20103.5\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.437727/  1.675305, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.6980%\n",
      "layer   3  Sparsity: 82.5592%\n",
      "total_backward_count 1595770 real_backward_count 94489   5.921%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.424495/  1.672186, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9424%\n",
      "layer   3  Sparsity: 82.2926%\n",
      "total_backward_count 1605560 real_backward_count 94665   5.896%\n",
      "fc layer 3 self.abs_max_out: 2139.0\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.422618/  1.689111, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9838%\n",
      "layer   3  Sparsity: 82.2861%\n",
      "total_backward_count 1615350 real_backward_count 94891   5.874%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.446001/  1.673260, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9067%\n",
      "layer   3  Sparsity: 82.6680%\n",
      "total_backward_count 1625140 real_backward_count 95095   5.851%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.434596/  1.695090, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0801%\n",
      "layer   3  Sparsity: 82.6314%\n",
      "total_backward_count 1634930 real_backward_count 95273   5.827%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.437200/  1.661826, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0355%\n",
      "layer   3  Sparsity: 82.0383%\n",
      "total_backward_count 1644720 real_backward_count 95497   5.806%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.417565/  1.661669, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0443%\n",
      "layer   3  Sparsity: 81.7542%\n",
      "total_backward_count 1654510 real_backward_count 95688   5.783%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.418573/  1.641179, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2276%\n",
      "layer   3  Sparsity: 81.9669%\n",
      "total_backward_count 1664300 real_backward_count 95877   5.761%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.423121/  1.670113, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.1602%\n",
      "layer   3  Sparsity: 81.7989%\n",
      "total_backward_count 1674090 real_backward_count 96105   5.741%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.399706/  1.666861, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2629%\n",
      "layer   3  Sparsity: 82.1534%\n",
      "total_backward_count 1683880 real_backward_count 96318   5.720%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.409432/  1.664140, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.3560%\n",
      "layer   3  Sparsity: 82.8153%\n",
      "total_backward_count 1693670 real_backward_count 96506   5.698%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.426603/  1.676137, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.3345%\n",
      "layer   3  Sparsity: 82.7847%\n",
      "total_backward_count 1703460 real_backward_count 96695   5.676%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.405129/  1.652663, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.3457%\n",
      "layer   3  Sparsity: 81.9660%\n",
      "total_backward_count 1713250 real_backward_count 96904   5.656%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.412624/  1.666939, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2615%\n",
      "layer   3  Sparsity: 81.5394%\n",
      "total_backward_count 1723040 real_backward_count 97101   5.635%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.407737/  1.660863, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2554%\n",
      "layer   3  Sparsity: 81.4248%\n",
      "total_backward_count 1732830 real_backward_count 97310   5.616%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.400113/  1.676255, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2050%\n",
      "layer   3  Sparsity: 81.6070%\n",
      "total_backward_count 1742620 real_backward_count 97529   5.597%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.419758/  1.664633, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2423%\n",
      "layer   3  Sparsity: 82.4858%\n",
      "total_backward_count 1752410 real_backward_count 97724   5.577%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.407065/  1.644278, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2317%\n",
      "layer   3  Sparsity: 82.1873%\n",
      "total_backward_count 1762200 real_backward_count 97925   5.557%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.417357/  1.664969, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0910%\n",
      "layer   3  Sparsity: 82.2950%\n",
      "total_backward_count 1771990 real_backward_count 98118   5.537%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.431197/  1.661345, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.3883%\n",
      "layer   3  Sparsity: 82.3908%\n",
      "total_backward_count 1781780 real_backward_count 98326   5.518%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.431978/  1.688248, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.2088%\n",
      "layer   3  Sparsity: 82.2174%\n",
      "total_backward_count 1791570 real_backward_count 98538   5.500%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.422089/  1.660784, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.1158%\n",
      "layer   3  Sparsity: 81.9749%\n",
      "total_backward_count 1801360 real_backward_count 98731   5.481%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.409824/  1.644345, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0464%\n",
      "layer   3  Sparsity: 81.9246%\n",
      "total_backward_count 1811150 real_backward_count 98936   5.463%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.406130/  1.653821, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0405%\n",
      "layer   3  Sparsity: 82.0134%\n",
      "total_backward_count 1820940 real_backward_count 99129   5.444%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.412561/  1.665663, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.1402%\n",
      "layer   3  Sparsity: 82.5448%\n",
      "total_backward_count 1830730 real_backward_count 99264   5.422%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.409023/  1.644494, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9900%\n",
      "layer   3  Sparsity: 82.6824%\n",
      "total_backward_count 1840520 real_backward_count 99437   5.403%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.423119/  1.678942, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8318%\n",
      "layer   3  Sparsity: 83.1240%\n",
      "total_backward_count 1850310 real_backward_count 99601   5.383%\n",
      "fc layer 3 self.abs_max_out: 2147.0\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.430010/  1.678350, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9740%\n",
      "layer   3  Sparsity: 82.8171%\n",
      "total_backward_count 1860100 real_backward_count 99781   5.364%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.430969/  1.693436, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8854%\n",
      "layer   3  Sparsity: 82.9586%\n",
      "total_backward_count 1869890 real_backward_count 99967   5.346%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.426891/  1.693450, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9659%\n",
      "layer   3  Sparsity: 83.1742%\n",
      "total_backward_count 1879680 real_backward_count 100116   5.326%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.435880/  1.682369, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0623%\n",
      "layer   3  Sparsity: 82.9301%\n",
      "total_backward_count 1889470 real_backward_count 100281   5.307%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.425290/  1.682884, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0485%\n",
      "layer   3  Sparsity: 82.5383%\n",
      "total_backward_count 1899260 real_backward_count 100426   5.288%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.422150/  1.670681, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.9089%\n",
      "layer   3  Sparsity: 82.5085%\n",
      "total_backward_count 1909050 real_backward_count 100598   5.270%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.415988/  1.692626, val:  73.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0370%\n",
      "layer   3  Sparsity: 82.5252%\n",
      "total_backward_count 1918840 real_backward_count 100779   5.252%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.426059/  1.676121, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0424%\n",
      "layer   3  Sparsity: 82.7362%\n",
      "total_backward_count 1928630 real_backward_count 100966   5.235%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.422738/  1.668599, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.1345%\n",
      "layer   3  Sparsity: 82.3548%\n",
      "total_backward_count 1938420 real_backward_count 101141   5.218%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.407247/  1.636503, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 77.0814%\n",
      "layer   3  Sparsity: 82.6009%\n",
      "total_backward_count 1948210 real_backward_count 101304   5.200%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.399359/  1.672081, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8845%\n",
      "layer   3  Sparsity: 82.2371%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edab740104ad4f7c94f48d40f9f68f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.39936</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.84167</td></tr><tr><td>val_loss</td><td>1.67208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-sweep-93</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9j5rk0t3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9j5rk0t3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_031540-9j5rk0t3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gfmadcvd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_073806-gfmadcvd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gfmadcvd' target=\"_blank\">stellar-sweep-99</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gfmadcvd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gfmadcvd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251116_073815_315', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 168.0\n",
      "lif layer 1 self.abs_max_v: 168.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 186.0\n",
      "lif layer 1 self.abs_max_v: 211.0\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 269.0\n",
      "fc layer 2 self.abs_max_out: 68.0\n",
      "lif layer 2 self.abs_max_v: 68.0\n",
      "lif layer 1 self.abs_max_v: 302.5\n",
      "fc layer 1 self.abs_max_out: 237.0\n",
      "lif layer 1 self.abs_max_v: 350.5\n",
      "fc layer 2 self.abs_max_out: 106.0\n",
      "lif layer 2 self.abs_max_v: 120.0\n",
      "fc layer 1 self.abs_max_out: 357.0\n",
      "lif layer 1 self.abs_max_v: 484.5\n",
      "fc layer 1 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 561.0\n",
      "fc layer 2 self.abs_max_out: 147.0\n",
      "lif layer 2 self.abs_max_v: 181.5\n",
      "fc layer 1 self.abs_max_out: 576.0\n",
      "lif layer 1 self.abs_max_v: 576.0\n",
      "fc layer 2 self.abs_max_out: 166.0\n",
      "lif layer 2 self.abs_max_v: 244.5\n",
      "fc layer 1 self.abs_max_out: 625.0\n",
      "lif layer 1 self.abs_max_v: 625.0\n",
      "lif layer 2 self.abs_max_v: 248.5\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 294.5\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 2 self.abs_max_out: 248.0\n",
      "lif layer 2 self.abs_max_v: 326.5\n",
      "fc layer 3 self.abs_max_out: 39.0\n",
      "fc layer 1 self.abs_max_out: 737.0\n",
      "lif layer 1 self.abs_max_v: 737.0\n",
      "fc layer 2 self.abs_max_out: 291.0\n",
      "lif layer 2 self.abs_max_v: 390.0\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 749.0\n",
      "fc layer 2 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 400.0\n",
      "fc layer 1 self.abs_max_out: 1131.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "fc layer 1 self.abs_max_out: 1376.0\n",
      "lif layer 1 self.abs_max_v: 1376.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "fc layer 2 self.abs_max_out: 372.0\n",
      "lif layer 2 self.abs_max_v: 407.0\n",
      "fc layer 3 self.abs_max_out: 48.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 2 self.abs_max_out: 407.0\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 2 self.abs_max_out: 441.0\n",
      "lif layer 2 self.abs_max_v: 441.0\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 2 self.abs_max_out: 452.0\n",
      "lif layer 2 self.abs_max_v: 452.0\n",
      "fc layer 2 self.abs_max_out: 460.0\n",
      "lif layer 2 self.abs_max_v: 460.0\n",
      "lif layer 2 self.abs_max_v: 468.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "lif layer 2 self.abs_max_v: 470.5\n",
      "lif layer 2 self.abs_max_v: 490.0\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "lif layer 2 self.abs_max_v: 544.0\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "lif layer 2 self.abs_max_v: 587.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "lif layer 2 self.abs_max_v: 602.5\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "lif layer 2 self.abs_max_v: 656.0\n",
      "lif layer 2 self.abs_max_v: 666.0\n",
      "lif layer 2 self.abs_max_v: 711.0\n",
      "lif layer 2 self.abs_max_v: 739.0\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "fc layer 3 self.abs_max_out: 97.0\n",
      "fc layer 1 self.abs_max_out: 1735.0\n",
      "lif layer 1 self.abs_max_v: 1735.0\n",
      "lif layer 2 self.abs_max_v: 753.0\n",
      "fc layer 2 self.abs_max_out: 726.0\n",
      "fc layer 3 self.abs_max_out: 100.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "lif layer 2 self.abs_max_v: 787.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "lif layer 2 self.abs_max_v: 793.0\n",
      "lif layer 2 self.abs_max_v: 795.5\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 875.5\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 1853.0\n",
      "lif layer 1 self.abs_max_v: 1853.0\n",
      "lif layer 2 self.abs_max_v: 886.0\n",
      "lif layer 2 self.abs_max_v: 971.0\n",
      "lif layer 2 self.abs_max_v: 1053.5\n",
      "lif layer 2 self.abs_max_v: 1082.0\n",
      "fc layer 2 self.abs_max_out: 800.0\n",
      "fc layer 2 self.abs_max_out: 812.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "fc layer 2 self.abs_max_out: 824.0\n",
      "fc layer 2 self.abs_max_out: 881.0\n",
      "fc layer 1 self.abs_max_out: 1902.0\n",
      "lif layer 1 self.abs_max_v: 1902.0\n",
      "fc layer 2 self.abs_max_out: 891.0\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "fc layer 2 self.abs_max_out: 924.0\n",
      "fc layer 2 self.abs_max_out: 983.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "lif layer 2 self.abs_max_v: 1104.0\n",
      "fc layer 2 self.abs_max_out: 1130.0\n",
      "lif layer 2 self.abs_max_v: 1130.0\n",
      "lif layer 1 self.abs_max_v: 1902.5\n",
      "fc layer 1 self.abs_max_out: 2223.0\n",
      "lif layer 1 self.abs_max_v: 2223.0\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "lif layer 2 self.abs_max_v: 1138.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "lif layer 2 self.abs_max_v: 1234.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 3 self.abs_max_out: 213.0\n",
      "lif layer 1 self.abs_max_v: 2251.0\n",
      "lif layer 1 self.abs_max_v: 2363.5\n",
      "lif layer 1 self.abs_max_v: 2537.0\n",
      "lif layer 2 self.abs_max_v: 1251.5\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 1 self.abs_max_out: 2225.0\n",
      "fc layer 1 self.abs_max_out: 2262.0\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "lif layer 1 self.abs_max_v: 2549.0\n",
      "fc layer 1 self.abs_max_out: 2332.0\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "lif layer 1 self.abs_max_v: 2584.0\n",
      "lif layer 1 self.abs_max_v: 2591.0\n",
      "fc layer 1 self.abs_max_out: 2341.0\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "fc layer 1 self.abs_max_out: 2485.0\n",
      "lif layer 1 self.abs_max_v: 2751.0\n",
      "fc layer 1 self.abs_max_out: 2527.0\n",
      "fc layer 1 self.abs_max_out: 2551.0\n",
      "fc layer 1 self.abs_max_out: 2579.0\n",
      "fc layer 1 self.abs_max_out: 2607.0\n",
      "fc layer 1 self.abs_max_out: 2652.0\n",
      "lif layer 2 self.abs_max_v: 1314.5\n",
      "lif layer 1 self.abs_max_v: 2805.5\n",
      "fc layer 1 self.abs_max_out: 2890.0\n",
      "lif layer 1 self.abs_max_v: 2890.0\n",
      "fc layer 2 self.abs_max_out: 1246.0\n",
      "fc layer 2 self.abs_max_out: 1278.0\n",
      "fc layer 2 self.abs_max_out: 1325.0\n",
      "lif layer 2 self.abs_max_v: 1325.0\n",
      "fc layer 2 self.abs_max_out: 1442.0\n",
      "lif layer 2 self.abs_max_v: 1442.0\n",
      "lif layer 1 self.abs_max_v: 3168.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.978717/  2.069798, val:  34.58%, val_best:  34.58%, tr:  96.32%, tr_best:  96.32%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0304%\n",
      "layer   2  Sparsity: 86.2276%\n",
      "layer   3  Sparsity: 90.1334%\n",
      "total_backward_count 9790 real_backward_count 2440  24.923%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "lif layer 1 self.abs_max_v: 3304.5\n",
      "fc layer 1 self.abs_max_out: 2941.0\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "fc layer 1 self.abs_max_out: 3042.0\n",
      "fc layer 1 self.abs_max_out: 3336.0\n",
      "lif layer 1 self.abs_max_v: 3336.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "lif layer 1 self.abs_max_v: 3359.0\n",
      "lif layer 2 self.abs_max_v: 1447.0\n",
      "lif layer 1 self.abs_max_v: 3529.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "lif layer 2 self.abs_max_v: 1454.5\n",
      "lif layer 1 self.abs_max_v: 3569.5\n",
      "lif layer 1 self.abs_max_v: 3663.0\n",
      "lif layer 1 self.abs_max_v: 3757.5\n",
      "lif layer 1 self.abs_max_v: 3887.5\n",
      "lif layer 1 self.abs_max_v: 4128.0\n",
      "lif layer 2 self.abs_max_v: 1488.0\n",
      "lif layer 2 self.abs_max_v: 1504.0\n",
      "fc layer 1 self.abs_max_out: 3451.0\n",
      "lif layer 2 self.abs_max_v: 1566.5\n",
      "fc layer 1 self.abs_max_out: 3610.0\n",
      "lif layer 2 self.abs_max_v: 1579.0\n",
      "lif layer 2 self.abs_max_v: 1708.5\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "lif layer 1 self.abs_max_v: 4203.0\n",
      "fc layer 2 self.abs_max_out: 1507.0\n",
      "lif layer 1 self.abs_max_v: 4262.0\n",
      "lif layer 1 self.abs_max_v: 4469.0\n",
      "lif layer 1 self.abs_max_v: 4600.5\n",
      "lif layer 1 self.abs_max_v: 4767.5\n",
      "lif layer 1 self.abs_max_v: 4813.0\n",
      "lif layer 2 self.abs_max_v: 1734.0\n",
      "lif layer 2 self.abs_max_v: 1865.0\n",
      "lif layer 2 self.abs_max_v: 1893.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.850864/  2.033871, val:  40.00%, val_best:  40.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0457%\n",
      "layer   2  Sparsity: 84.3975%\n",
      "layer   3  Sparsity: 87.9968%\n",
      "total_backward_count 19580 real_backward_count 4263  21.772%\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "lif layer 2 self.abs_max_v: 2113.0\n",
      "fc layer 1 self.abs_max_out: 3728.0\n",
      "lif layer 1 self.abs_max_v: 4839.5\n",
      "lif layer 2 self.abs_max_v: 2122.5\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "lif layer 1 self.abs_max_v: 5150.5\n",
      "lif layer 1 self.abs_max_v: 5440.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.813690/  2.013677, val:  33.75%, val_best:  40.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0520%\n",
      "layer   2  Sparsity: 84.0167%\n",
      "layer   3  Sparsity: 87.0494%\n",
      "total_backward_count 29370 real_backward_count 6065  20.650%\n",
      "fc layer 2 self.abs_max_out: 1602.0\n",
      "fc layer 2 self.abs_max_out: 1638.0\n",
      "fc layer 1 self.abs_max_out: 4003.0\n",
      "fc layer 2 self.abs_max_out: 1639.0\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "fc layer 2 self.abs_max_out: 1725.0\n",
      "fc layer 1 self.abs_max_out: 4058.0\n",
      "fc layer 2 self.abs_max_out: 1727.0\n",
      "fc layer 2 self.abs_max_out: 1794.0\n",
      "fc layer 2 self.abs_max_out: 1810.0\n",
      "fc layer 2 self.abs_max_out: 1879.0\n",
      "fc layer 1 self.abs_max_out: 4151.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.744332/  1.952349, val:  37.50%, val_best:  40.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0327%\n",
      "layer   2  Sparsity: 83.1712%\n",
      "layer   3  Sparsity: 86.0566%\n",
      "total_backward_count 39160 real_backward_count 7639  19.507%\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "fc layer 1 self.abs_max_out: 4280.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "lif layer 2 self.abs_max_v: 2229.5\n",
      "lif layer 2 self.abs_max_v: 2255.5\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "lif layer 2 self.abs_max_v: 2495.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.725354/  1.936779, val:  44.58%, val_best:  44.58%, tr:  99.18%, tr_best:  99.80%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0311%\n",
      "layer   2  Sparsity: 82.6468%\n",
      "layer   3  Sparsity: 85.0730%\n",
      "total_backward_count 48950 real_backward_count 9195  18.784%\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 4356.0\n",
      "fc layer 1 self.abs_max_out: 4669.0\n",
      "lif layer 2 self.abs_max_v: 2536.0\n",
      "lif layer 2 self.abs_max_v: 2555.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.663792/  1.956442, val:  53.75%, val_best:  53.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0271%\n",
      "layer   2  Sparsity: 82.7975%\n",
      "layer   3  Sparsity: 84.9976%\n",
      "total_backward_count 58740 real_backward_count 10775  18.344%\n",
      "fc layer 2 self.abs_max_out: 1922.0\n",
      "lif layer 1 self.abs_max_v: 5642.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.696476/  1.921489, val:  50.42%, val_best:  53.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0215%\n",
      "layer   2  Sparsity: 82.7256%\n",
      "layer   3  Sparsity: 85.4094%\n",
      "total_backward_count 68530 real_backward_count 12286  17.928%\n",
      "fc layer 1 self.abs_max_out: 4882.0\n",
      "lif layer 1 self.abs_max_v: 5729.0\n",
      "lif layer 1 self.abs_max_v: 5807.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.637354/  1.877911, val:  45.42%, val_best:  53.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0392%\n",
      "layer   2  Sparsity: 83.2958%\n",
      "layer   3  Sparsity: 84.3976%\n",
      "total_backward_count 78320 real_backward_count 13737  17.540%\n",
      "lif layer 2 self.abs_max_v: 2582.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "lif layer 1 self.abs_max_v: 6061.5\n",
      "lif layer 1 self.abs_max_v: 6177.0\n",
      "lif layer 2 self.abs_max_v: 2599.0\n",
      "lif layer 1 self.abs_max_v: 6179.5\n",
      "lif layer 2 self.abs_max_v: 2628.0\n",
      "lif layer 2 self.abs_max_v: 2735.0\n",
      "lif layer 1 self.abs_max_v: 6333.0\n",
      "lif layer 1 self.abs_max_v: 6895.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.628776/  1.842717, val:  52.92%, val_best:  53.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0553%\n",
      "layer   2  Sparsity: 83.8057%\n",
      "layer   3  Sparsity: 84.6510%\n",
      "total_backward_count 88110 real_backward_count 15266  17.326%\n",
      "lif layer 2 self.abs_max_v: 2935.5\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 2942.0\n",
      "lif layer 2 self.abs_max_v: 3015.5\n",
      "lif layer 1 self.abs_max_v: 7367.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.622637/  1.900971, val:  41.25%, val_best:  53.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0407%\n",
      "layer   2  Sparsity: 83.9311%\n",
      "layer   3  Sparsity: 85.0247%\n",
      "total_backward_count 97900 real_backward_count 16775  17.135%\n",
      "fc layer 1 self.abs_max_out: 4969.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "lif layer 2 self.abs_max_v: 3083.5\n",
      "lif layer 2 self.abs_max_v: 3275.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.596212/  1.881184, val:  50.83%, val_best:  53.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0425%\n",
      "layer   2  Sparsity: 83.5831%\n",
      "layer   3  Sparsity: 84.7805%\n",
      "total_backward_count 107690 real_backward_count 18240  16.938%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.645999/  1.929208, val:  35.42%, val_best:  53.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.0065%\n",
      "layer   2  Sparsity: 83.7303%\n",
      "layer   3  Sparsity: 85.6349%\n",
      "total_backward_count 117480 real_backward_count 19724  16.789%\n",
      "lif layer 1 self.abs_max_v: 7732.0\n",
      "lif layer 1 self.abs_max_v: 8250.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.641568/  1.899231, val:  35.42%, val_best:  53.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0361%\n",
      "layer   2  Sparsity: 83.6912%\n",
      "layer   3  Sparsity: 85.9464%\n",
      "total_backward_count 127270 real_backward_count 21163  16.628%\n",
      "fc layer 2 self.abs_max_out: 1948.0\n",
      "fc layer 2 self.abs_max_out: 1996.0\n",
      "fc layer 2 self.abs_max_out: 2015.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.626659/  1.914899, val:  37.92%, val_best:  53.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0451%\n",
      "layer   2  Sparsity: 82.8253%\n",
      "layer   3  Sparsity: 85.7473%\n",
      "total_backward_count 137060 real_backward_count 22594  16.485%\n",
      "fc layer 1 self.abs_max_out: 5188.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.629945/  1.839372, val:  55.00%, val_best:  55.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0166%\n",
      "layer   2  Sparsity: 83.0784%\n",
      "layer   3  Sparsity: 85.5831%\n",
      "total_backward_count 146850 real_backward_count 23967  16.321%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.624182/  1.869745, val:  49.17%, val_best:  55.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0242%\n",
      "layer   2  Sparsity: 83.5952%\n",
      "layer   3  Sparsity: 86.0941%\n",
      "total_backward_count 156640 real_backward_count 25381  16.203%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.619989/  1.869540, val:  50.83%, val_best:  55.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0363%\n",
      "layer   2  Sparsity: 83.3909%\n",
      "layer   3  Sparsity: 85.5134%\n",
      "total_backward_count 166430 real_backward_count 26776  16.088%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.600856/  1.814078, val:  62.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0047%\n",
      "layer   2  Sparsity: 83.4594%\n",
      "layer   3  Sparsity: 85.5565%\n",
      "total_backward_count 176220 real_backward_count 28160  15.980%\n",
      "fc layer 1 self.abs_max_out: 5221.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 495.0\n",
      "lif layer 1 self.abs_max_v: 8523.5\n",
      "lif layer 1 self.abs_max_v: 8980.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.581481/  1.794297, val:  42.92%, val_best:  62.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0153%\n",
      "layer   2  Sparsity: 83.2644%\n",
      "layer   3  Sparsity: 85.6945%\n",
      "total_backward_count 186010 real_backward_count 29635  15.932%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.578725/  1.873411, val:  40.00%, val_best:  62.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0113%\n",
      "layer   2  Sparsity: 83.2898%\n",
      "layer   3  Sparsity: 85.9080%\n",
      "total_backward_count 195800 real_backward_count 30991  15.828%\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "lif layer 2 self.abs_max_v: 3321.5\n",
      "lif layer 2 self.abs_max_v: 3360.0\n",
      "lif layer 2 self.abs_max_v: 3427.0\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.579632/  1.857011, val:  47.50%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0237%\n",
      "layer   2  Sparsity: 83.2007%\n",
      "layer   3  Sparsity: 85.7577%\n",
      "total_backward_count 205590 real_backward_count 32333  15.727%\n",
      "fc layer 1 self.abs_max_out: 5573.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.592302/  1.830859, val:  39.58%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0325%\n",
      "layer   2  Sparsity: 83.1974%\n",
      "layer   3  Sparsity: 85.5336%\n",
      "total_backward_count 215380 real_backward_count 33683  15.639%\n",
      "fc layer 2 self.abs_max_out: 2124.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.585427/  1.805171, val:  60.00%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0207%\n",
      "layer   2  Sparsity: 83.2841%\n",
      "layer   3  Sparsity: 85.5643%\n",
      "total_backward_count 225170 real_backward_count 35071  15.575%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 2 self.abs_max_out: 2134.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.554196/  1.771236, val:  56.67%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9946%\n",
      "layer   2  Sparsity: 83.0823%\n",
      "layer   3  Sparsity: 84.7221%\n",
      "total_backward_count 234960 real_backward_count 36424  15.502%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.541699/  1.774952, val:  65.42%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0406%\n",
      "layer   2  Sparsity: 82.6718%\n",
      "layer   3  Sparsity: 84.9002%\n",
      "total_backward_count 244750 real_backward_count 37763  15.429%\n",
      "lif layer 1 self.abs_max_v: 9076.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.546236/  1.746483, val:  72.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0225%\n",
      "layer   2  Sparsity: 82.7193%\n",
      "layer   3  Sparsity: 85.1014%\n",
      "total_backward_count 254540 real_backward_count 39180  15.392%\n",
      "lif layer 1 self.abs_max_v: 9312.0\n",
      "lif layer 1 self.abs_max_v: 9468.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.548007/  1.786286, val:  48.33%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0248%\n",
      "layer   2  Sparsity: 82.6609%\n",
      "layer   3  Sparsity: 85.4361%\n",
      "total_backward_count 264330 real_backward_count 40493  15.319%\n",
      "fc layer 1 self.abs_max_out: 5639.0\n",
      "lif layer 1 self.abs_max_v: 9554.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.534768/  1.757874, val:  57.08%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9914%\n",
      "layer   2  Sparsity: 82.8005%\n",
      "layer   3  Sparsity: 85.1317%\n",
      "total_backward_count 274120 real_backward_count 41799  15.248%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.539168/  1.834225, val:  42.50%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0174%\n",
      "layer   2  Sparsity: 82.6432%\n",
      "layer   3  Sparsity: 85.7853%\n",
      "total_backward_count 283910 real_backward_count 43045  15.161%\n",
      "lif layer 2 self.abs_max_v: 3503.0\n",
      "lif layer 2 self.abs_max_v: 3584.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.550707/  1.789026, val:  55.42%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0224%\n",
      "layer   2  Sparsity: 82.4292%\n",
      "layer   3  Sparsity: 85.5300%\n",
      "total_backward_count 293700 real_backward_count 44321  15.091%\n",
      "fc layer 1 self.abs_max_out: 5816.0\n",
      "fc layer 2 self.abs_max_out: 2142.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.544437/  1.726188, val:  61.25%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0496%\n",
      "layer   2  Sparsity: 82.4703%\n",
      "layer   3  Sparsity: 85.4528%\n",
      "total_backward_count 303490 real_backward_count 45596  15.024%\n",
      "fc layer 2 self.abs_max_out: 2179.0\n",
      "fc layer 2 self.abs_max_out: 2207.0\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.516885/  1.756550, val:  51.67%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0054%\n",
      "layer   2  Sparsity: 82.6189%\n",
      "layer   3  Sparsity: 84.8362%\n",
      "total_backward_count 313280 real_backward_count 46838  14.951%\n",
      "fc layer 2 self.abs_max_out: 2282.0\n",
      "fc layer 2 self.abs_max_out: 2283.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.493347/  1.767275, val:  59.58%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0209%\n",
      "layer   2  Sparsity: 82.1011%\n",
      "layer   3  Sparsity: 84.4555%\n",
      "total_backward_count 323070 real_backward_count 48038  14.869%\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "fc layer 3 self.abs_max_out: 578.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.476594/  1.704454, val:  61.25%, val_best:  72.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0500%\n",
      "layer   2  Sparsity: 82.6617%\n",
      "layer   3  Sparsity: 84.4074%\n",
      "total_backward_count 332860 real_backward_count 49327  14.819%\n",
      "fc layer 1 self.abs_max_out: 5912.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.465386/  1.725643, val:  57.92%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.0250%\n",
      "layer   2  Sparsity: 82.4079%\n",
      "layer   3  Sparsity: 84.5101%\n",
      "total_backward_count 342650 real_backward_count 50551  14.753%\n",
      "fc layer 1 self.abs_max_out: 6054.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.472478/  1.711691, val:  62.50%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0162%\n",
      "layer   2  Sparsity: 82.5243%\n",
      "layer   3  Sparsity: 84.7030%\n",
      "total_backward_count 352440 real_backward_count 51809  14.700%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.471030/  1.715141, val:  69.17%, val_best:  72.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0513%\n",
      "layer   2  Sparsity: 82.4524%\n",
      "layer   3  Sparsity: 84.6992%\n",
      "total_backward_count 362230 real_backward_count 53035  14.641%\n",
      "fc layer 1 self.abs_max_out: 6114.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.482357/  1.737653, val:  58.75%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0475%\n",
      "layer   2  Sparsity: 82.1695%\n",
      "layer   3  Sparsity: 85.1251%\n",
      "total_backward_count 372020 real_backward_count 54246  14.581%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.474602/  1.711470, val:  61.25%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9942%\n",
      "layer   2  Sparsity: 82.0515%\n",
      "layer   3  Sparsity: 84.6626%\n",
      "total_backward_count 381810 real_backward_count 55474  14.529%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.475649/  1.741719, val:  60.42%, val_best:  72.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0365%\n",
      "layer   2  Sparsity: 81.8438%\n",
      "layer   3  Sparsity: 85.0378%\n",
      "total_backward_count 391600 real_backward_count 56647  14.466%\n",
      "fc layer 1 self.abs_max_out: 6657.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.472526/  1.696845, val:  62.08%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0147%\n",
      "layer   2  Sparsity: 81.6699%\n",
      "layer   3  Sparsity: 85.0733%\n",
      "total_backward_count 401390 real_backward_count 57881  14.420%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.451360/  1.656305, val:  60.42%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0269%\n",
      "layer   2  Sparsity: 81.4043%\n",
      "layer   3  Sparsity: 84.3463%\n",
      "total_backward_count 411180 real_backward_count 59067  14.365%\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.417982/  1.699331, val:  55.00%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0514%\n",
      "layer   2  Sparsity: 81.6367%\n",
      "layer   3  Sparsity: 84.0309%\n",
      "total_backward_count 420970 real_backward_count 60240  14.310%\n",
      "lif layer 1 self.abs_max_v: 9597.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.425592/  1.675705, val:  61.67%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0566%\n",
      "layer   2  Sparsity: 81.6853%\n",
      "layer   3  Sparsity: 84.3967%\n",
      "total_backward_count 430760 real_backward_count 61398  14.253%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.401274/  1.663484, val:  71.67%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0703%\n",
      "layer   2  Sparsity: 81.6315%\n",
      "layer   3  Sparsity: 84.1078%\n",
      "total_backward_count 440550 real_backward_count 62588  14.207%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.389454/  1.636646, val:  70.00%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0111%\n",
      "layer   2  Sparsity: 81.5700%\n",
      "layer   3  Sparsity: 83.7562%\n",
      "total_backward_count 450340 real_backward_count 63779  14.162%\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 2334.0\n",
      "fc layer 2 self.abs_max_out: 2345.0\n",
      "fc layer 2 self.abs_max_out: 2403.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.395296/  1.650883, val:  63.75%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0121%\n",
      "layer   2  Sparsity: 81.4333%\n",
      "layer   3  Sparsity: 82.9674%\n",
      "total_backward_count 460130 real_backward_count 64936  14.113%\n",
      "fc layer 2 self.abs_max_out: 2455.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.337619/  1.656392, val:  55.00%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0516%\n",
      "layer   2  Sparsity: 81.5734%\n",
      "layer   3  Sparsity: 82.5495%\n",
      "total_backward_count 469920 real_backward_count 66003  14.046%\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "fc layer 2 self.abs_max_out: 2460.0\n",
      "fc layer 2 self.abs_max_out: 2500.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.344972/  1.633002, val:  60.83%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9984%\n",
      "layer   2  Sparsity: 81.5789%\n",
      "layer   3  Sparsity: 82.9041%\n",
      "total_backward_count 479710 real_backward_count 67093  13.986%\n",
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "lif layer 1 self.abs_max_v: 9719.0\n",
      "lif layer 1 self.abs_max_v: 9987.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.351358/  1.577031, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0127%\n",
      "layer   2  Sparsity: 81.3769%\n",
      "layer   3  Sparsity: 82.5985%\n",
      "total_backward_count 489500 real_backward_count 68219  13.936%\n",
      "lif layer 1 self.abs_max_v: 10013.0\n",
      "lif layer 1 self.abs_max_v: 10439.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.346087/  1.631847, val:  60.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0346%\n",
      "layer   2  Sparsity: 81.3216%\n",
      "layer   3  Sparsity: 83.1691%\n",
      "total_backward_count 499290 real_backward_count 69297  13.879%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.365500/  1.616956, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0005%\n",
      "layer   2  Sparsity: 81.5060%\n",
      "layer   3  Sparsity: 83.7229%\n",
      "total_backward_count 509080 real_backward_count 70356  13.820%\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.354873/  1.604354, val:  66.25%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0270%\n",
      "layer   2  Sparsity: 81.1123%\n",
      "layer   3  Sparsity: 83.1455%\n",
      "total_backward_count 518870 real_backward_count 71435  13.767%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.332456/  1.621525, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0177%\n",
      "layer   2  Sparsity: 80.8834%\n",
      "layer   3  Sparsity: 83.4671%\n",
      "total_backward_count 528660 real_backward_count 72501  13.714%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.364685/  1.598496, val:  72.92%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0287%\n",
      "layer   2  Sparsity: 81.3421%\n",
      "layer   3  Sparsity: 83.4745%\n",
      "total_backward_count 538450 real_backward_count 73518  13.654%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.378531/  1.654148, val:  60.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0368%\n",
      "layer   2  Sparsity: 81.5217%\n",
      "layer   3  Sparsity: 83.4184%\n",
      "total_backward_count 548240 real_backward_count 74621  13.611%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.365179/  1.628085, val:  66.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0069%\n",
      "layer   2  Sparsity: 81.1874%\n",
      "layer   3  Sparsity: 83.5076%\n",
      "total_backward_count 558030 real_backward_count 75702  13.566%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.345858/  1.602963, val:  71.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0396%\n",
      "layer   2  Sparsity: 80.9468%\n",
      "layer   3  Sparsity: 83.1311%\n",
      "total_backward_count 567820 real_backward_count 76755  13.517%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.340109/  1.580029, val:  66.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0286%\n",
      "layer   2  Sparsity: 80.8641%\n",
      "layer   3  Sparsity: 83.1308%\n",
      "total_backward_count 577610 real_backward_count 77768  13.464%\n",
      "lif layer 2 self.abs_max_v: 3639.5\n",
      "lif layer 2 self.abs_max_v: 3740.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.325516/  1.618916, val:  49.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0384%\n",
      "layer   2  Sparsity: 80.6235%\n",
      "layer   3  Sparsity: 82.4015%\n",
      "total_backward_count 587400 real_backward_count 78784  13.412%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.293679/  1.590240, val:  55.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0343%\n",
      "layer   2  Sparsity: 80.7134%\n",
      "layer   3  Sparsity: 82.6496%\n",
      "total_backward_count 597190 real_backward_count 79860  13.373%\n",
      "fc layer 2 self.abs_max_out: 2545.0\n",
      "fc layer 2 self.abs_max_out: 2549.0\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.302721/  1.521944, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0094%\n",
      "layer   2  Sparsity: 80.9721%\n",
      "layer   3  Sparsity: 82.6308%\n",
      "total_backward_count 606980 real_backward_count 80904  13.329%\n",
      "fc layer 1 self.abs_max_out: 6916.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.284304/  1.577516, val:  71.25%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0258%\n",
      "layer   2  Sparsity: 81.1017%\n",
      "layer   3  Sparsity: 82.9692%\n",
      "total_backward_count 616770 real_backward_count 81899  13.279%\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.303709/  1.587859, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0501%\n",
      "layer   2  Sparsity: 81.0683%\n",
      "layer   3  Sparsity: 82.1651%\n",
      "total_backward_count 626560 real_backward_count 82837  13.221%\n",
      "fc layer 2 self.abs_max_out: 2673.0\n",
      "fc layer 2 self.abs_max_out: 2700.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.309861/  1.582448, val:  65.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0498%\n",
      "layer   2  Sparsity: 80.7659%\n",
      "layer   3  Sparsity: 82.7195%\n",
      "total_backward_count 636350 real_backward_count 83838  13.175%\n",
      "fc layer 2 self.abs_max_out: 2732.0\n",
      "lif layer 1 self.abs_max_v: 10452.0\n",
      "lif layer 1 self.abs_max_v: 10654.5\n",
      "lif layer 1 self.abs_max_v: 10741.5\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.298853/  1.578124, val:  65.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0277%\n",
      "layer   2  Sparsity: 80.8547%\n",
      "layer   3  Sparsity: 82.7848%\n",
      "total_backward_count 646140 real_backward_count 84830  13.129%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.280721/  1.521817, val:  71.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0229%\n",
      "layer   2  Sparsity: 81.1902%\n",
      "layer   3  Sparsity: 83.0060%\n",
      "total_backward_count 655930 real_backward_count 85830  13.085%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.254704/  1.497833, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0088%\n",
      "layer   2  Sparsity: 80.9681%\n",
      "layer   3  Sparsity: 82.7680%\n",
      "total_backward_count 665720 real_backward_count 86838  13.044%\n",
      "lif layer 1 self.abs_max_v: 10874.5\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.241499/  1.504373, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0115%\n",
      "layer   2  Sparsity: 81.3211%\n",
      "layer   3  Sparsity: 82.9139%\n",
      "total_backward_count 675510 real_backward_count 87845  13.004%\n",
      "fc layer 1 self.abs_max_out: 6963.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.230025/  1.513968, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0218%\n",
      "layer   2  Sparsity: 81.2234%\n",
      "layer   3  Sparsity: 82.6691%\n",
      "total_backward_count 685300 real_backward_count 88818  12.960%\n",
      "fc layer 1 self.abs_max_out: 7069.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.203794/  1.512755, val:  62.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0326%\n",
      "layer   2  Sparsity: 81.1551%\n",
      "layer   3  Sparsity: 82.0335%\n",
      "total_backward_count 695090 real_backward_count 89822  12.922%\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.196311/  1.530178, val:  68.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0173%\n",
      "layer   2  Sparsity: 81.1467%\n",
      "layer   3  Sparsity: 82.0758%\n",
      "total_backward_count 704880 real_backward_count 90749  12.874%\n",
      "fc layer 1 self.abs_max_out: 7235.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.214543/  1.474115, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0294%\n",
      "layer   2  Sparsity: 80.9694%\n",
      "layer   3  Sparsity: 82.8546%\n",
      "total_backward_count 714670 real_backward_count 91739  12.837%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.206575/  1.526060, val:  68.33%, val_best:  80.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0262%\n",
      "layer   2  Sparsity: 80.8251%\n",
      "layer   3  Sparsity: 82.6597%\n",
      "total_backward_count 724460 real_backward_count 92653  12.789%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.250222/  1.508461, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0396%\n",
      "layer   2  Sparsity: 80.7512%\n",
      "layer   3  Sparsity: 82.6870%\n",
      "total_backward_count 734250 real_backward_count 93665  12.757%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.239167/  1.478182, val:  79.17%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0153%\n",
      "layer   2  Sparsity: 80.9093%\n",
      "layer   3  Sparsity: 82.6400%\n",
      "total_backward_count 744040 real_backward_count 94560  12.709%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.231124/  1.481875, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0384%\n",
      "layer   2  Sparsity: 80.7658%\n",
      "layer   3  Sparsity: 82.0379%\n",
      "total_backward_count 753830 real_backward_count 95465  12.664%\n",
      "fc layer 3 self.abs_max_out: 757.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.213238/  1.492591, val:  61.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0387%\n",
      "layer   2  Sparsity: 80.9544%\n",
      "layer   3  Sparsity: 81.6917%\n",
      "total_backward_count 763620 real_backward_count 96372  12.620%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.194137/  1.538046, val:  74.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0466%\n",
      "layer   2  Sparsity: 80.9772%\n",
      "layer   3  Sparsity: 82.2597%\n",
      "total_backward_count 773410 real_backward_count 97283  12.578%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.229136/  1.507075, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0240%\n",
      "layer   2  Sparsity: 81.1437%\n",
      "layer   3  Sparsity: 82.9164%\n",
      "total_backward_count 783200 real_backward_count 98158  12.533%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.240210/  1.518745, val:  60.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0051%\n",
      "layer   2  Sparsity: 80.7309%\n",
      "layer   3  Sparsity: 82.6969%\n",
      "total_backward_count 792990 real_backward_count 99043  12.490%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.222155/  1.514744, val:  75.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0370%\n",
      "layer   2  Sparsity: 80.6946%\n",
      "layer   3  Sparsity: 82.0335%\n",
      "total_backward_count 802780 real_backward_count 99957  12.451%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.215015/  1.457234, val:  65.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0244%\n",
      "layer   2  Sparsity: 80.7329%\n",
      "layer   3  Sparsity: 82.2183%\n",
      "total_backward_count 812570 real_backward_count 100820  12.408%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.199791/  1.527709, val:  70.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0024%\n",
      "layer   2  Sparsity: 81.0786%\n",
      "layer   3  Sparsity: 82.4815%\n",
      "total_backward_count 822360 real_backward_count 101738  12.371%\n",
      "fc layer 2 self.abs_max_out: 2760.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.240963/  1.487334, val:  70.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0259%\n",
      "layer   2  Sparsity: 81.0058%\n",
      "layer   3  Sparsity: 82.8772%\n",
      "total_backward_count 832150 real_backward_count 102685  12.340%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.219536/  1.497149, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0337%\n",
      "layer   2  Sparsity: 80.9442%\n",
      "layer   3  Sparsity: 82.8758%\n",
      "total_backward_count 841940 real_backward_count 103592  12.304%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.207695/  1.477398, val:  76.25%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0158%\n",
      "layer   2  Sparsity: 80.8550%\n",
      "layer   3  Sparsity: 82.2262%\n",
      "total_backward_count 851730 real_backward_count 104486  12.268%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.201388/  1.501590, val:  73.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9859%\n",
      "layer   2  Sparsity: 80.9517%\n",
      "layer   3  Sparsity: 82.5256%\n",
      "total_backward_count 861520 real_backward_count 105406  12.235%\n",
      "lif layer 2 self.abs_max_v: 3787.0\n",
      "lif layer 2 self.abs_max_v: 3944.5\n",
      "lif layer 2 self.abs_max_v: 3964.5\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.226834/  1.483985, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0060%\n",
      "layer   2  Sparsity: 80.8039%\n",
      "layer   3  Sparsity: 82.9058%\n",
      "total_backward_count 871310 real_backward_count 106241  12.193%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.227083/  1.474207, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0392%\n",
      "layer   2  Sparsity: 80.9899%\n",
      "layer   3  Sparsity: 82.8131%\n",
      "total_backward_count 881100 real_backward_count 107155  12.162%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.234491/  1.535529, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0318%\n",
      "layer   2  Sparsity: 81.1173%\n",
      "layer   3  Sparsity: 83.0662%\n",
      "total_backward_count 890890 real_backward_count 108027  12.126%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.235095/  1.509241, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0000%\n",
      "layer   2  Sparsity: 80.7875%\n",
      "layer   3  Sparsity: 83.0918%\n",
      "total_backward_count 900680 real_backward_count 108857  12.086%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.235964/  1.500730, val:  75.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0490%\n",
      "layer   2  Sparsity: 80.6861%\n",
      "layer   3  Sparsity: 82.9023%\n",
      "total_backward_count 910470 real_backward_count 109726  12.052%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.216329/  1.494316, val:  76.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0396%\n",
      "layer   2  Sparsity: 80.8048%\n",
      "layer   3  Sparsity: 83.0852%\n",
      "total_backward_count 920260 real_backward_count 110608  12.019%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.226362/  1.488597, val:  74.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0473%\n",
      "layer   2  Sparsity: 80.7688%\n",
      "layer   3  Sparsity: 82.7329%\n",
      "total_backward_count 930050 real_backward_count 111473  11.986%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.233030/  1.484144, val:  73.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0458%\n",
      "layer   2  Sparsity: 80.8952%\n",
      "layer   3  Sparsity: 82.6996%\n",
      "total_backward_count 939840 real_backward_count 112311  11.950%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.228671/  1.506451, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9997%\n",
      "layer   2  Sparsity: 80.9518%\n",
      "layer   3  Sparsity: 83.1618%\n",
      "total_backward_count 949630 real_backward_count 113154  11.916%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.214086/  1.470806, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0367%\n",
      "layer   2  Sparsity: 80.9833%\n",
      "layer   3  Sparsity: 82.8396%\n",
      "total_backward_count 959420 real_backward_count 113989  11.881%\n",
      "lif layer 1 self.abs_max_v: 11162.5\n",
      "lif layer 1 self.abs_max_v: 11375.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.219882/  1.525251, val:  74.58%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0367%\n",
      "layer   2  Sparsity: 81.0723%\n",
      "layer   3  Sparsity: 82.9963%\n",
      "total_backward_count 969210 real_backward_count 114851  11.850%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.218138/  1.471589, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0316%\n",
      "layer   2  Sparsity: 81.0700%\n",
      "layer   3  Sparsity: 83.1662%\n",
      "total_backward_count 979000 real_backward_count 115681  11.816%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.204756/  1.493748, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0232%\n",
      "layer   2  Sparsity: 81.2724%\n",
      "layer   3  Sparsity: 82.7770%\n",
      "total_backward_count 988790 real_backward_count 116520  11.784%\n",
      "lif layer 1 self.abs_max_v: 11660.0\n",
      "lif layer 1 self.abs_max_v: 11843.0\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.225173/  1.494143, val:  76.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0196%\n",
      "layer   2  Sparsity: 81.2973%\n",
      "layer   3  Sparsity: 82.8665%\n",
      "total_backward_count 998580 real_backward_count 117426  11.759%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.203530/  1.454710, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0689%\n",
      "layer   2  Sparsity: 81.3755%\n",
      "layer   3  Sparsity: 83.2342%\n",
      "total_backward_count 1008370 real_backward_count 118236  11.725%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.188312/  1.482633, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0217%\n",
      "layer   2  Sparsity: 81.3436%\n",
      "layer   3  Sparsity: 83.2092%\n",
      "total_backward_count 1018160 real_backward_count 119084  11.696%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.218899/  1.483353, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0503%\n",
      "layer   2  Sparsity: 81.2349%\n",
      "layer   3  Sparsity: 83.2465%\n",
      "total_backward_count 1027950 real_backward_count 119922  11.666%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.223082/  1.509812, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0231%\n",
      "layer   2  Sparsity: 81.4107%\n",
      "layer   3  Sparsity: 83.5482%\n",
      "total_backward_count 1037740 real_backward_count 120753  11.636%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.230959/  1.493029, val:  76.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0337%\n",
      "layer   2  Sparsity: 81.3828%\n",
      "layer   3  Sparsity: 83.8420%\n",
      "total_backward_count 1047530 real_backward_count 121585  11.607%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.221079/  1.518453, val:  71.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0288%\n",
      "layer   2  Sparsity: 81.3522%\n",
      "layer   3  Sparsity: 83.0350%\n",
      "total_backward_count 1057320 real_backward_count 122460  11.582%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.237524/  1.505478, val:  84.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0223%\n",
      "layer   2  Sparsity: 81.2637%\n",
      "layer   3  Sparsity: 83.4841%\n",
      "total_backward_count 1067110 real_backward_count 123312  11.556%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.225549/  1.503693, val:  82.08%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9967%\n",
      "layer   2  Sparsity: 81.2998%\n",
      "layer   3  Sparsity: 83.6033%\n",
      "total_backward_count 1076900 real_backward_count 124120  11.526%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.211427/  1.450329, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0427%\n",
      "layer   2  Sparsity: 81.1132%\n",
      "layer   3  Sparsity: 83.0197%\n",
      "total_backward_count 1086690 real_backward_count 124926  11.496%\n",
      "lif layer 2 self.abs_max_v: 3986.5\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.196901/  1.455136, val:  75.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0459%\n",
      "layer   2  Sparsity: 81.2373%\n",
      "layer   3  Sparsity: 82.7375%\n",
      "total_backward_count 1096480 real_backward_count 125700  11.464%\n",
      "fc layer 2 self.abs_max_out: 2761.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.193399/  1.483710, val:  82.08%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0429%\n",
      "layer   2  Sparsity: 81.3749%\n",
      "layer   3  Sparsity: 82.8674%\n",
      "total_backward_count 1106270 real_backward_count 126485  11.433%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.177060/  1.468141, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9891%\n",
      "layer   2  Sparsity: 81.2635%\n",
      "layer   3  Sparsity: 82.5324%\n",
      "total_backward_count 1116060 real_backward_count 127293  11.406%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.177188/  1.476248, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0383%\n",
      "layer   2  Sparsity: 81.2906%\n",
      "layer   3  Sparsity: 82.7517%\n",
      "total_backward_count 1125850 real_backward_count 128112  11.379%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.214575/  1.534816, val:  72.92%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0313%\n",
      "layer   2  Sparsity: 81.0315%\n",
      "layer   3  Sparsity: 82.8398%\n",
      "total_backward_count 1135640 real_backward_count 128933  11.353%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.213243/  1.467259, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0232%\n",
      "layer   2  Sparsity: 80.9985%\n",
      "layer   3  Sparsity: 83.0009%\n",
      "total_backward_count 1145430 real_backward_count 129686  11.322%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.193324/  1.471200, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0416%\n",
      "layer   2  Sparsity: 81.2819%\n",
      "layer   3  Sparsity: 83.1057%\n",
      "total_backward_count 1155220 real_backward_count 130471  11.294%\n",
      "fc layer 1 self.abs_max_out: 7375.0\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.195426/  1.509701, val:  70.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.0095%\n",
      "layer   2  Sparsity: 80.9501%\n",
      "layer   3  Sparsity: 82.9744%\n",
      "total_backward_count 1165010 real_backward_count 131277  11.268%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.192016/  1.450344, val:  82.50%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0178%\n",
      "layer   2  Sparsity: 80.7303%\n",
      "layer   3  Sparsity: 82.7740%\n",
      "total_backward_count 1174800 real_backward_count 132059  11.241%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.200068/  1.493787, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0120%\n",
      "layer   2  Sparsity: 80.5945%\n",
      "layer   3  Sparsity: 83.1219%\n",
      "total_backward_count 1184590 real_backward_count 132850  11.215%\n",
      "fc layer 1 self.abs_max_out: 7403.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.216071/  1.477502, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0344%\n",
      "layer   2  Sparsity: 80.6043%\n",
      "layer   3  Sparsity: 82.8565%\n",
      "total_backward_count 1194380 real_backward_count 133622  11.188%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.207638/  1.464929, val:  82.08%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0159%\n",
      "layer   2  Sparsity: 81.1141%\n",
      "layer   3  Sparsity: 83.1008%\n",
      "total_backward_count 1204170 real_backward_count 134430  11.164%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.188605/  1.432130, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0233%\n",
      "layer   2  Sparsity: 80.9562%\n",
      "layer   3  Sparsity: 83.3927%\n",
      "total_backward_count 1213960 real_backward_count 135217  11.139%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.175617/  1.448294, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0447%\n",
      "layer   2  Sparsity: 80.9898%\n",
      "layer   3  Sparsity: 83.1007%\n",
      "total_backward_count 1223750 real_backward_count 135990  11.113%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.203079/  1.456465, val:  80.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9984%\n",
      "layer   2  Sparsity: 81.2906%\n",
      "layer   3  Sparsity: 83.0179%\n",
      "total_backward_count 1233540 real_backward_count 136752  11.086%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.192723/  1.465182, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0503%\n",
      "layer   2  Sparsity: 81.1947%\n",
      "layer   3  Sparsity: 83.4138%\n",
      "total_backward_count 1243330 real_backward_count 137488  11.058%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.177297/  1.449241, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0457%\n",
      "layer   2  Sparsity: 81.5137%\n",
      "layer   3  Sparsity: 83.0007%\n",
      "total_backward_count 1253120 real_backward_count 138238  11.032%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.191336/  1.454901, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0097%\n",
      "layer   2  Sparsity: 81.5028%\n",
      "layer   3  Sparsity: 83.0917%\n",
      "total_backward_count 1262910 real_backward_count 138975  11.004%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.160887/  1.428502, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0246%\n",
      "layer   2  Sparsity: 81.3536%\n",
      "layer   3  Sparsity: 83.0094%\n",
      "total_backward_count 1272700 real_backward_count 139735  10.979%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.172210/  1.452518, val:  74.58%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0137%\n",
      "layer   2  Sparsity: 81.2050%\n",
      "layer   3  Sparsity: 83.3238%\n",
      "total_backward_count 1282490 real_backward_count 140504  10.956%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.169481/  1.466035, val:  72.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0201%\n",
      "layer   2  Sparsity: 81.2408%\n",
      "layer   3  Sparsity: 83.4808%\n",
      "total_backward_count 1292280 real_backward_count 141223  10.928%\n",
      "fc layer 1 self.abs_max_out: 7420.0\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.169832/  1.445659, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0380%\n",
      "layer   2  Sparsity: 81.3395%\n",
      "layer   3  Sparsity: 83.2133%\n",
      "total_backward_count 1302070 real_backward_count 141951  10.902%\n",
      "fc layer 1 self.abs_max_out: 7427.0\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.207972/  1.475088, val:  79.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0339%\n",
      "layer   2  Sparsity: 81.2913%\n",
      "layer   3  Sparsity: 83.2814%\n",
      "total_backward_count 1311860 real_backward_count 142705  10.878%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.175695/  1.467376, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0215%\n",
      "layer   2  Sparsity: 81.2852%\n",
      "layer   3  Sparsity: 83.2753%\n",
      "total_backward_count 1321650 real_backward_count 143454  10.854%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.191545/  1.457003, val:  79.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0084%\n",
      "layer   2  Sparsity: 81.1127%\n",
      "layer   3  Sparsity: 83.2544%\n",
      "total_backward_count 1331440 real_backward_count 144244  10.834%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.175127/  1.446079, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0293%\n",
      "layer   2  Sparsity: 80.9759%\n",
      "layer   3  Sparsity: 83.0652%\n",
      "total_backward_count 1341230 real_backward_count 145026  10.813%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.201542/  1.478974, val:  69.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0485%\n",
      "layer   2  Sparsity: 80.7194%\n",
      "layer   3  Sparsity: 83.5101%\n",
      "total_backward_count 1351020 real_backward_count 145771  10.790%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.178489/  1.447499, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0129%\n",
      "layer   2  Sparsity: 81.0630%\n",
      "layer   3  Sparsity: 83.3641%\n",
      "total_backward_count 1360810 real_backward_count 146524  10.767%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.194890/  1.467598, val:  73.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0321%\n",
      "layer   2  Sparsity: 80.9135%\n",
      "layer   3  Sparsity: 83.4082%\n",
      "total_backward_count 1370600 real_backward_count 147272  10.745%\n",
      "fc layer 1 self.abs_max_out: 7456.0\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.184987/  1.498905, val:  70.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9967%\n",
      "layer   2  Sparsity: 80.7999%\n",
      "layer   3  Sparsity: 83.2222%\n",
      "total_backward_count 1380390 real_backward_count 147982  10.720%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.192496/  1.486568, val:  73.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0319%\n",
      "layer   2  Sparsity: 80.9388%\n",
      "layer   3  Sparsity: 83.2889%\n",
      "total_backward_count 1390180 real_backward_count 148753  10.700%\n",
      "fc layer 1 self.abs_max_out: 7594.0\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.193405/  1.436922, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0101%\n",
      "layer   2  Sparsity: 80.9909%\n",
      "layer   3  Sparsity: 83.3362%\n",
      "total_backward_count 1399970 real_backward_count 149533  10.681%\n",
      "fc layer 1 self.abs_max_out: 7647.0\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.176754/  1.453181, val:  79.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0067%\n",
      "layer   2  Sparsity: 81.1963%\n",
      "layer   3  Sparsity: 83.2348%\n",
      "total_backward_count 1409760 real_backward_count 150256  10.658%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.175703/  1.437438, val:  78.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0375%\n",
      "layer   2  Sparsity: 81.3778%\n",
      "layer   3  Sparsity: 83.3583%\n",
      "total_backward_count 1419550 real_backward_count 151021  10.639%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.171382/  1.428408, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0462%\n",
      "layer   2  Sparsity: 81.4655%\n",
      "layer   3  Sparsity: 83.6486%\n",
      "total_backward_count 1429340 real_backward_count 151734  10.616%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.178862/  1.459229, val:  72.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0412%\n",
      "layer   2  Sparsity: 81.5958%\n",
      "layer   3  Sparsity: 83.3604%\n",
      "total_backward_count 1439130 real_backward_count 152445  10.593%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.150729/  1.452911, val:  70.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0222%\n",
      "layer   2  Sparsity: 81.5876%\n",
      "layer   3  Sparsity: 83.6668%\n",
      "total_backward_count 1448920 real_backward_count 153068  10.564%\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.166349/  1.428547, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0359%\n",
      "layer   2  Sparsity: 81.5100%\n",
      "layer   3  Sparsity: 83.9699%\n",
      "total_backward_count 1458710 real_backward_count 153779  10.542%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.169254/  1.464686, val:  79.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0245%\n",
      "layer   2  Sparsity: 81.3836%\n",
      "layer   3  Sparsity: 84.0173%\n",
      "total_backward_count 1468500 real_backward_count 154493  10.520%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.179989/  1.428015, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0677%\n",
      "layer   2  Sparsity: 81.3144%\n",
      "layer   3  Sparsity: 83.6882%\n",
      "total_backward_count 1478290 real_backward_count 155209  10.499%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.179481/  1.436112, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0343%\n",
      "layer   2  Sparsity: 81.3496%\n",
      "layer   3  Sparsity: 83.6702%\n",
      "total_backward_count 1488080 real_backward_count 155915  10.478%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.187236/  1.456094, val:  78.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0360%\n",
      "layer   2  Sparsity: 81.2510%\n",
      "layer   3  Sparsity: 83.8404%\n",
      "total_backward_count 1497870 real_backward_count 156604  10.455%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.179008/  1.422709, val:  79.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0173%\n",
      "layer   2  Sparsity: 81.1233%\n",
      "layer   3  Sparsity: 83.5591%\n",
      "total_backward_count 1507660 real_backward_count 157371  10.438%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.149647/  1.409155, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0150%\n",
      "layer   2  Sparsity: 81.0740%\n",
      "layer   3  Sparsity: 83.3511%\n",
      "total_backward_count 1517450 real_backward_count 158060  10.416%\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.141639/  1.434721, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0423%\n",
      "layer   2  Sparsity: 81.3534%\n",
      "layer   3  Sparsity: 83.6491%\n",
      "total_backward_count 1527240 real_backward_count 158736  10.394%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.165107/  1.453712, val:  77.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0328%\n",
      "layer   2  Sparsity: 81.2585%\n",
      "layer   3  Sparsity: 84.2320%\n",
      "total_backward_count 1537030 real_backward_count 159416  10.372%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.158969/  1.407257, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0147%\n",
      "layer   2  Sparsity: 81.1897%\n",
      "layer   3  Sparsity: 83.8291%\n",
      "total_backward_count 1546820 real_backward_count 160085  10.349%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.155376/  1.436695, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0485%\n",
      "layer   2  Sparsity: 81.3845%\n",
      "layer   3  Sparsity: 83.6516%\n",
      "total_backward_count 1556610 real_backward_count 160810  10.331%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.141245/  1.432569, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0413%\n",
      "layer   2  Sparsity: 81.5670%\n",
      "layer   3  Sparsity: 83.2715%\n",
      "total_backward_count 1566400 real_backward_count 161440  10.306%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.141869/  1.410532, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0495%\n",
      "layer   2  Sparsity: 81.5050%\n",
      "layer   3  Sparsity: 83.0657%\n",
      "total_backward_count 1576190 real_backward_count 162131  10.286%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.141392/  1.418368, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0286%\n",
      "layer   2  Sparsity: 81.3231%\n",
      "layer   3  Sparsity: 83.1637%\n",
      "total_backward_count 1585980 real_backward_count 162801  10.265%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.144382/  1.453606, val:  79.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0211%\n",
      "layer   2  Sparsity: 81.2807%\n",
      "layer   3  Sparsity: 83.8828%\n",
      "total_backward_count 1595770 real_backward_count 163451  10.243%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.124530/  1.452119, val:  72.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0255%\n",
      "layer   2  Sparsity: 81.1379%\n",
      "layer   3  Sparsity: 83.8117%\n",
      "total_backward_count 1605560 real_backward_count 164123  10.222%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.140961/  1.420573, val:  78.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0160%\n",
      "layer   2  Sparsity: 81.2002%\n",
      "layer   3  Sparsity: 83.0034%\n",
      "total_backward_count 1615350 real_backward_count 164878  10.207%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.147178/  1.406702, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0322%\n",
      "layer   2  Sparsity: 81.1009%\n",
      "layer   3  Sparsity: 83.0328%\n",
      "total_backward_count 1625140 real_backward_count 165586  10.189%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.137750/  1.464778, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0400%\n",
      "layer   2  Sparsity: 81.1523%\n",
      "layer   3  Sparsity: 82.8796%\n",
      "total_backward_count 1634930 real_backward_count 166261  10.169%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.139763/  1.396442, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.0132%\n",
      "layer   2  Sparsity: 80.9976%\n",
      "layer   3  Sparsity: 82.7048%\n",
      "total_backward_count 1644720 real_backward_count 166986  10.153%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.113387/  1.386452, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0573%\n",
      "layer   2  Sparsity: 81.1755%\n",
      "layer   3  Sparsity: 82.5978%\n",
      "total_backward_count 1654510 real_backward_count 167694  10.136%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.127800/  1.386418, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0309%\n",
      "layer   2  Sparsity: 81.3510%\n",
      "layer   3  Sparsity: 83.1424%\n",
      "total_backward_count 1664300 real_backward_count 168330  10.114%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.129268/  1.417497, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0336%\n",
      "layer   2  Sparsity: 81.2154%\n",
      "layer   3  Sparsity: 83.1189%\n",
      "total_backward_count 1674090 real_backward_count 169050  10.098%\n",
      "fc layer 1 self.abs_max_out: 7687.0\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.130417/  1.450987, val:  78.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0031%\n",
      "layer   2  Sparsity: 81.2198%\n",
      "layer   3  Sparsity: 82.8298%\n",
      "total_backward_count 1683880 real_backward_count 169722  10.079%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.126514/  1.380554, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0183%\n",
      "layer   2  Sparsity: 81.3633%\n",
      "layer   3  Sparsity: 82.8991%\n",
      "total_backward_count 1693670 real_backward_count 170405  10.061%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.106214/  1.403509, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0111%\n",
      "layer   2  Sparsity: 81.4641%\n",
      "layer   3  Sparsity: 83.4853%\n",
      "total_backward_count 1703460 real_backward_count 171034  10.040%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.107957/  1.423737, val:  80.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0132%\n",
      "layer   2  Sparsity: 81.4180%\n",
      "layer   3  Sparsity: 83.5660%\n",
      "total_backward_count 1713250 real_backward_count 171681  10.021%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.126627/  1.393906, val:  88.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9939%\n",
      "layer   2  Sparsity: 81.5524%\n",
      "layer   3  Sparsity: 83.1839%\n",
      "total_backward_count 1723040 real_backward_count 172373  10.004%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.151871/  1.434370, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.0272%\n",
      "layer   2  Sparsity: 81.5840%\n",
      "layer   3  Sparsity: 83.6294%\n",
      "total_backward_count 1732830 real_backward_count 172980   9.983%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.129828/  1.396843, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0250%\n",
      "layer   2  Sparsity: 81.4929%\n",
      "layer   3  Sparsity: 82.9256%\n",
      "total_backward_count 1742620 real_backward_count 173640   9.964%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.107015/  1.441720, val:  70.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0198%\n",
      "layer   2  Sparsity: 81.4096%\n",
      "layer   3  Sparsity: 82.5539%\n",
      "total_backward_count 1752410 real_backward_count 174356   9.949%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.099752/  1.374115, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0354%\n",
      "layer   2  Sparsity: 81.3191%\n",
      "layer   3  Sparsity: 82.7246%\n",
      "total_backward_count 1762200 real_backward_count 175035   9.933%\n",
      "fc layer 1 self.abs_max_out: 7796.0\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.104791/  1.389625, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0071%\n",
      "layer   2  Sparsity: 81.2740%\n",
      "layer   3  Sparsity: 83.0787%\n",
      "total_backward_count 1771990 real_backward_count 175679   9.914%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.108545/  1.401469, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0344%\n",
      "layer   2  Sparsity: 81.4123%\n",
      "layer   3  Sparsity: 83.4044%\n",
      "total_backward_count 1781780 real_backward_count 176316   9.895%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.115654/  1.384352, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.9972%\n",
      "layer   2  Sparsity: 81.1776%\n",
      "layer   3  Sparsity: 83.6793%\n",
      "total_backward_count 1791570 real_backward_count 176984   9.879%\n",
      "fc layer 1 self.abs_max_out: 7835.0\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.112037/  1.422555, val:  75.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0207%\n",
      "layer   2  Sparsity: 81.2877%\n",
      "layer   3  Sparsity: 82.8232%\n",
      "total_backward_count 1801360 real_backward_count 177647   9.862%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.128083/  1.420835, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0294%\n",
      "layer   2  Sparsity: 81.3351%\n",
      "layer   3  Sparsity: 83.0377%\n",
      "total_backward_count 1811150 real_backward_count 178273   9.843%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.114254/  1.389889, val:  77.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0018%\n",
      "layer   2  Sparsity: 81.3677%\n",
      "layer   3  Sparsity: 83.2363%\n",
      "total_backward_count 1820940 real_backward_count 178935   9.827%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.070658/  1.358225, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0118%\n",
      "layer   2  Sparsity: 81.4890%\n",
      "layer   3  Sparsity: 83.0606%\n",
      "total_backward_count 1830730 real_backward_count 179538   9.807%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.067985/  1.426237, val:  75.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.0261%\n",
      "layer   2  Sparsity: 81.5265%\n",
      "layer   3  Sparsity: 83.6839%\n",
      "total_backward_count 1840520 real_backward_count 180177   9.789%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.084363/  1.374902, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0204%\n",
      "layer   2  Sparsity: 81.3688%\n",
      "layer   3  Sparsity: 83.2648%\n",
      "total_backward_count 1850310 real_backward_count 180851   9.774%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.081428/  1.373831, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9993%\n",
      "layer   2  Sparsity: 81.3493%\n",
      "layer   3  Sparsity: 82.9388%\n",
      "total_backward_count 1860100 real_backward_count 181483   9.757%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.104964/  1.382553, val:  78.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0091%\n",
      "layer   2  Sparsity: 81.0916%\n",
      "layer   3  Sparsity: 83.8189%\n",
      "total_backward_count 1869890 real_backward_count 182109   9.739%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.127952/  1.415942, val:  72.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0194%\n",
      "layer   2  Sparsity: 81.2175%\n",
      "layer   3  Sparsity: 83.3697%\n",
      "total_backward_count 1879680 real_backward_count 182781   9.724%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.097319/  1.400254, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0128%\n",
      "layer   2  Sparsity: 81.3183%\n",
      "layer   3  Sparsity: 82.8542%\n",
      "total_backward_count 1889470 real_backward_count 183443   9.709%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.100564/  1.436483, val:  76.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0402%\n",
      "layer   2  Sparsity: 81.4497%\n",
      "layer   3  Sparsity: 83.0197%\n",
      "total_backward_count 1899260 real_backward_count 184071   9.692%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.121549/  1.375597, val:  84.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0292%\n",
      "layer   2  Sparsity: 81.4119%\n",
      "layer   3  Sparsity: 83.2395%\n",
      "total_backward_count 1909050 real_backward_count 184717   9.676%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.096423/  1.379739, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.9908%\n",
      "layer   2  Sparsity: 81.1198%\n",
      "layer   3  Sparsity: 82.6653%\n",
      "total_backward_count 1918840 real_backward_count 185380   9.661%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.058460/  1.336753, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0382%\n",
      "layer   2  Sparsity: 81.2703%\n",
      "layer   3  Sparsity: 82.8374%\n",
      "total_backward_count 1928630 real_backward_count 185945   9.641%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.072347/  1.351981, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.0784%\n",
      "layer   2  Sparsity: 81.2407%\n",
      "layer   3  Sparsity: 83.1413%\n",
      "total_backward_count 1938420 real_backward_count 186555   9.624%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.043897/  1.372007, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0168%\n",
      "layer   2  Sparsity: 81.2097%\n",
      "layer   3  Sparsity: 83.0486%\n",
      "total_backward_count 1948210 real_backward_count 187146   9.606%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.041359/  1.344586, val:  79.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.0336%\n",
      "layer   2  Sparsity: 81.4317%\n",
      "layer   3  Sparsity: 83.6032%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f655538b0104204839fde924c692697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.04136</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>1.34459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-99</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gfmadcvd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gfmadcvd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_073806-gfmadcvd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1uvq5b6k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_120128-1uvq5b6k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1uvq5b6k' target=\"_blank\">misunderstood-sweep-105</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1uvq5b6k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1uvq5b6k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251116_120137_376', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 654.0\n",
      "lif layer 1 self.abs_max_v: 654.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "lif layer 2 self.abs_max_v: 1021.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 741.0\n",
      "lif layer 1 self.abs_max_v: 832.0\n",
      "fc layer 2 self.abs_max_out: 1456.0\n",
      "lif layer 2 self.abs_max_v: 1850.5\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "fc layer 1 self.abs_max_out: 838.0\n",
      "lif layer 1 self.abs_max_v: 1082.5\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 2 self.abs_max_v: 2405.5\n",
      "fc layer 2 self.abs_max_out: 1630.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 1 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 1211.0\n",
      "fc layer 1 self.abs_max_out: 1114.0\n",
      "lif layer 1 self.abs_max_v: 1265.5\n",
      "fc layer 1 self.abs_max_out: 1334.0\n",
      "lif layer 1 self.abs_max_v: 1735.0\n",
      "fc layer 2 self.abs_max_out: 1646.0\n",
      "lif layer 2 self.abs_max_v: 2518.5\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "fc layer 2 self.abs_max_out: 1735.0\n",
      "fc layer 2 self.abs_max_out: 1962.0\n",
      "lif layer 2 self.abs_max_v: 3003.5\n",
      "fc layer 1 self.abs_max_out: 1344.0\n",
      "lif layer 1 self.abs_max_v: 1755.0\n",
      "fc layer 3 self.abs_max_out: 1082.0\n",
      "lif layer 1 self.abs_max_v: 2040.0\n",
      "lif layer 1 self.abs_max_v: 2280.0\n",
      "fc layer 1 self.abs_max_out: 1708.0\n",
      "fc layer 1 self.abs_max_out: 1762.0\n",
      "fc layer 1 self.abs_max_out: 1875.0\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "lif layer 2 self.abs_max_v: 3022.0\n",
      "lif layer 2 self.abs_max_v: 3056.5\n",
      "fc layer 1 self.abs_max_out: 1876.0\n",
      "lif layer 1 self.abs_max_v: 2598.5\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "fc layer 1 self.abs_max_out: 1932.0\n",
      "lif layer 2 self.abs_max_v: 3244.0\n",
      "lif layer 2 self.abs_max_v: 3395.0\n",
      "fc layer 1 self.abs_max_out: 2042.0\n",
      "fc layer 2 self.abs_max_out: 2283.0\n",
      "fc layer 2 self.abs_max_out: 2373.0\n",
      "fc layer 1 self.abs_max_out: 2325.0\n",
      "lif layer 1 self.abs_max_v: 2737.5\n",
      "lif layer 2 self.abs_max_v: 3714.5\n",
      "fc layer 1 self.abs_max_out: 2360.0\n",
      "fc layer 2 self.abs_max_out: 2427.0\n",
      "fc layer 1 self.abs_max_out: 2418.0\n",
      "lif layer 2 self.abs_max_v: 3752.0\n",
      "lif layer 2 self.abs_max_v: 3878.0\n",
      "fc layer 3 self.abs_max_out: 1109.0\n",
      "fc layer 2 self.abs_max_out: 2428.0\n",
      "fc layer 2 self.abs_max_out: 2641.0\n",
      "fc layer 3 self.abs_max_out: 1300.0\n",
      "fc layer 1 self.abs_max_out: 2848.0\n",
      "lif layer 1 self.abs_max_v: 2848.0\n",
      "lif layer 1 self.abs_max_v: 3109.0\n",
      "lif layer 1 self.abs_max_v: 3302.5\n",
      "fc layer 1 self.abs_max_out: 2937.0\n",
      "lif layer 1 self.abs_max_v: 3563.5\n",
      "fc layer 1 self.abs_max_out: 3212.0\n",
      "lif layer 1 self.abs_max_v: 3672.0\n",
      "lif layer 1 self.abs_max_v: 4055.5\n",
      "lif layer 1 self.abs_max_v: 4531.5\n",
      "lif layer 2 self.abs_max_v: 3948.5\n",
      "lif layer 1 self.abs_max_v: 4580.0\n",
      "lif layer 2 self.abs_max_v: 3958.5\n",
      "fc layer 2 self.abs_max_out: 2679.0\n",
      "lif layer 2 self.abs_max_v: 4453.5\n",
      "lif layer 2 self.abs_max_v: 4624.0\n",
      "fc layer 1 self.abs_max_out: 3268.0\n",
      "fc layer 2 self.abs_max_out: 2763.0\n",
      "fc layer 2 self.abs_max_out: 2886.0\n",
      "fc layer 1 self.abs_max_out: 3321.0\n",
      "fc layer 2 self.abs_max_out: 3092.0\n",
      "lif layer 2 self.abs_max_v: 4762.5\n",
      "lif layer 2 self.abs_max_v: 5207.5\n",
      "lif layer 1 self.abs_max_v: 4773.0\n",
      "lif layer 2 self.abs_max_v: 5340.0\n",
      "lif layer 1 self.abs_max_v: 5410.5\n",
      "fc layer 1 self.abs_max_out: 3518.0\n",
      "fc layer 2 self.abs_max_out: 3150.0\n",
      "lif layer 1 self.abs_max_v: 5888.5\n",
      "fc layer 1 self.abs_max_out: 3823.0\n",
      "fc layer 3 self.abs_max_out: 1367.0\n",
      "fc layer 2 self.abs_max_out: 3215.0\n",
      "lif layer 1 self.abs_max_v: 5957.0\n",
      "lif layer 1 self.abs_max_v: 5966.5\n",
      "lif layer 1 self.abs_max_v: 6328.5\n",
      "fc layer 1 self.abs_max_out: 4108.0\n",
      "fc layer 1 self.abs_max_out: 4425.0\n",
      "lif layer 1 self.abs_max_v: 6477.5\n",
      "lif layer 1 self.abs_max_v: 6570.5\n",
      "lif layer 1 self.abs_max_v: 7107.5\n",
      "lif layer 1 self.abs_max_v: 7464.5\n",
      "fc layer 1 self.abs_max_out: 4542.0\n",
      "lif layer 1 self.abs_max_v: 7935.5\n",
      "fc layer 1 self.abs_max_out: 4677.0\n",
      "fc layer 3 self.abs_max_out: 1495.0\n",
      "fc layer 3 self.abs_max_out: 1577.0\n",
      "fc layer 3 self.abs_max_out: 1614.0\n",
      "fc layer 1 self.abs_max_out: 4795.0\n",
      "lif layer 2 self.abs_max_v: 5462.0\n",
      "fc layer 1 self.abs_max_out: 5134.0\n",
      "lif layer 1 self.abs_max_v: 8176.0\n",
      "lif layer 1 self.abs_max_v: 8426.5\n",
      "fc layer 3 self.abs_max_out: 1622.0\n",
      "fc layer 2 self.abs_max_out: 3221.0\n",
      "fc layer 1 self.abs_max_out: 5201.0\n",
      "fc layer 1 self.abs_max_out: 5331.0\n",
      "fc layer 2 self.abs_max_out: 3252.0\n",
      "fc layer 2 self.abs_max_out: 3255.0\n",
      "lif layer 1 self.abs_max_v: 8646.0\n",
      "lif layer 1 self.abs_max_v: 8673.0\n",
      "fc layer 2 self.abs_max_out: 3402.0\n",
      "fc layer 2 self.abs_max_out: 3462.0\n",
      "fc layer 1 self.abs_max_out: 5372.0\n",
      "fc layer 1 self.abs_max_out: 6012.0\n",
      "lif layer 1 self.abs_max_v: 8900.5\n",
      "lif layer 1 self.abs_max_v: 9227.5\n",
      "lif layer 1 self.abs_max_v: 9483.0\n",
      "lif layer 1 self.abs_max_v: 9499.0\n",
      "lif layer 1 self.abs_max_v: 9702.0\n",
      "lif layer 1 self.abs_max_v: 10089.0\n",
      "lif layer 1 self.abs_max_v: 10174.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.741462/  1.955959, val:  33.75%, val_best:  33.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.0854%\n",
      "layer   3  Sparsity: 68.9023%\n",
      "total_backward_count 9790 real_backward_count 1905  19.459%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3516.0\n",
      "lif layer 2 self.abs_max_v: 5505.5\n",
      "fc layer 3 self.abs_max_out: 1634.0\n",
      "lif layer 2 self.abs_max_v: 5899.5\n",
      "fc layer 3 self.abs_max_out: 1648.0\n",
      "fc layer 2 self.abs_max_out: 3520.0\n",
      "fc layer 3 self.abs_max_out: 1670.0\n",
      "fc layer 3 self.abs_max_out: 1704.0\n",
      "fc layer 2 self.abs_max_out: 3538.0\n",
      "fc layer 1 self.abs_max_out: 6140.0\n",
      "lif layer 2 self.abs_max_v: 6065.0\n",
      "fc layer 3 self.abs_max_out: 1767.0\n",
      "fc layer 1 self.abs_max_out: 6153.0\n",
      "lif layer 1 self.abs_max_v: 10383.0\n",
      "fc layer 3 self.abs_max_out: 1781.0\n",
      "fc layer 2 self.abs_max_out: 3719.0\n",
      "lif layer 1 self.abs_max_v: 10399.5\n",
      "lif layer 1 self.abs_max_v: 11142.0\n",
      "lif layer 1 self.abs_max_v: 11572.0\n",
      "lif layer 1 self.abs_max_v: 11638.0\n",
      "lif layer 1 self.abs_max_v: 11836.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.691753/  1.942798, val:  41.25%, val_best:  41.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5490%\n",
      "layer   3  Sparsity: 69.7032%\n",
      "total_backward_count 19580 real_backward_count 3399  17.360%\n",
      "lif layer 2 self.abs_max_v: 6284.0\n",
      "fc layer 1 self.abs_max_out: 6195.0\n",
      "fc layer 1 self.abs_max_out: 6383.0\n",
      "fc layer 3 self.abs_max_out: 1850.0\n",
      "fc layer 1 self.abs_max_out: 6484.0\n",
      "fc layer 1 self.abs_max_out: 7049.0\n",
      "fc layer 1 self.abs_max_out: 7618.0\n",
      "lif layer 1 self.abs_max_v: 13263.0\n",
      "fc layer 1 self.abs_max_out: 7750.0\n",
      "lif layer 1 self.abs_max_v: 13665.5\n",
      "lif layer 1 self.abs_max_v: 13789.0\n",
      "lif layer 1 self.abs_max_v: 14247.5\n",
      "lif layer 1 self.abs_max_v: 14689.0\n",
      "lif layer 1 self.abs_max_v: 14864.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.655306/  1.870416, val:  46.67%, val_best:  46.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.9327%\n",
      "layer   3  Sparsity: 71.0327%\n",
      "total_backward_count 29370 real_backward_count 4827  16.435%\n",
      "fc layer 2 self.abs_max_out: 4027.0\n",
      "lif layer 2 self.abs_max_v: 6341.5\n",
      "lif layer 2 self.abs_max_v: 6397.0\n",
      "fc layer 3 self.abs_max_out: 1875.0\n",
      "fc layer 3 self.abs_max_out: 1907.0\n",
      "fc layer 3 self.abs_max_out: 1917.0\n",
      "fc layer 3 self.abs_max_out: 2035.0\n",
      "fc layer 2 self.abs_max_out: 4080.0\n",
      "fc layer 2 self.abs_max_out: 4236.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.630882/  1.908437, val:  40.42%, val_best:  46.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4116%\n",
      "layer   3  Sparsity: 69.7952%\n",
      "total_backward_count 39160 real_backward_count 6236  15.924%\n",
      "fc layer 1 self.abs_max_out: 8059.0\n",
      "fc layer 1 self.abs_max_out: 8768.0\n",
      "lif layer 1 self.abs_max_v: 16199.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.582319/  1.832153, val:  44.58%, val_best:  46.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.9012%\n",
      "layer   3  Sparsity: 68.7708%\n",
      "total_backward_count 48950 real_backward_count 7549  15.422%\n",
      "fc layer 3 self.abs_max_out: 2129.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.563179/  1.841602, val:  49.58%, val_best:  49.58%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.2378%\n",
      "layer   3  Sparsity: 68.5353%\n",
      "total_backward_count 58740 real_backward_count 8859  15.082%\n",
      "fc layer 3 self.abs_max_out: 2165.0\n",
      "fc layer 3 self.abs_max_out: 2237.0\n",
      "lif layer 2 self.abs_max_v: 6685.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.552663/  1.810374, val:  58.33%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.1308%\n",
      "layer   3  Sparsity: 69.3773%\n",
      "total_backward_count 68530 real_backward_count 10185  14.862%\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.549707/  1.821550, val:  48.75%, val_best:  58.33%, tr:  99.39%, tr_best: 100.00%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.8227%\n",
      "layer   3  Sparsity: 69.8123%\n",
      "total_backward_count 78320 real_backward_count 11400  14.556%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.544864/  1.779426, val:  55.83%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.6163%\n",
      "layer   3  Sparsity: 70.9217%\n",
      "total_backward_count 88110 real_backward_count 12642  14.348%\n",
      "fc layer 3 self.abs_max_out: 2317.0\n",
      "fc layer 3 self.abs_max_out: 2332.0\n",
      "lif layer 2 self.abs_max_v: 7005.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.503795/  1.781052, val:  57.92%, val_best:  58.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 75.0265%\n",
      "layer   3  Sparsity: 71.8467%\n",
      "total_backward_count 97900 real_backward_count 13878  14.176%\n",
      "fc layer 2 self.abs_max_out: 4324.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.487972/  1.782869, val:  47.08%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.7002%\n",
      "layer   3  Sparsity: 70.3162%\n",
      "total_backward_count 107690 real_backward_count 15105  14.026%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.487984/  1.732843, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4452%\n",
      "layer   3  Sparsity: 69.8925%\n",
      "total_backward_count 117480 real_backward_count 16258  13.839%\n",
      "fc layer 3 self.abs_max_out: 2347.0\n",
      "fc layer 3 self.abs_max_out: 2367.0\n",
      "fc layer 1 self.abs_max_out: 9595.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.446176/  1.736102, val:  55.00%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0758%\n",
      "layer   3  Sparsity: 70.6792%\n",
      "total_backward_count 127270 real_backward_count 17333  13.619%\n",
      "lif layer 1 self.abs_max_v: 17147.5\n",
      "lif layer 1 self.abs_max_v: 17430.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.446936/  1.750686, val:  46.25%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8959%\n",
      "layer   3  Sparsity: 71.0905%\n",
      "total_backward_count 137060 real_backward_count 18451  13.462%\n",
      "fc layer 3 self.abs_max_out: 2381.0\n",
      "fc layer 3 self.abs_max_out: 2466.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.450899/  1.711790, val:  57.92%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3346%\n",
      "layer   3  Sparsity: 71.2761%\n",
      "total_backward_count 146850 real_backward_count 19470  13.258%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.434480/  1.755961, val:  46.67%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2610%\n",
      "layer   3  Sparsity: 71.4040%\n",
      "total_backward_count 156640 real_backward_count 20552  13.121%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.431823/  1.706986, val:  60.83%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1608%\n",
      "layer   3  Sparsity: 72.3847%\n",
      "total_backward_count 166430 real_backward_count 21589  12.972%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.428487/  1.644632, val:  64.17%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0354%\n",
      "layer   3  Sparsity: 71.8708%\n",
      "total_backward_count 176220 real_backward_count 22552  12.798%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.411812/  1.676448, val:  56.25%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2646%\n",
      "layer   3  Sparsity: 72.1435%\n",
      "total_backward_count 186010 real_backward_count 23564  12.668%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.381943/  1.694114, val:  60.83%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1190%\n",
      "layer   3  Sparsity: 71.7464%\n",
      "total_backward_count 195800 real_backward_count 24529  12.528%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.381948/  1.682383, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6104%\n",
      "layer   3  Sparsity: 72.2634%\n",
      "total_backward_count 205590 real_backward_count 25471  12.389%\n",
      "fc layer 2 self.abs_max_out: 4511.0\n",
      "fc layer 2 self.abs_max_out: 4779.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.381041/  1.665614, val:  61.67%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6745%\n",
      "layer   3  Sparsity: 72.2338%\n",
      "total_backward_count 215380 real_backward_count 26509  12.308%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.382622/  1.658313, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5808%\n",
      "layer   3  Sparsity: 72.5171%\n",
      "total_backward_count 225170 real_backward_count 27451  12.191%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.359821/  1.618187, val:  66.25%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8629%\n",
      "layer   3  Sparsity: 73.2492%\n",
      "total_backward_count 234960 real_backward_count 28279  12.036%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.350451/  1.608089, val:  65.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6901%\n",
      "layer   3  Sparsity: 73.1579%\n",
      "total_backward_count 244750 real_backward_count 29099  11.889%\n",
      "fc layer 1 self.abs_max_out: 9986.0\n",
      "lif layer 1 self.abs_max_v: 18110.5\n",
      "lif layer 1 self.abs_max_v: 18576.5\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.361682/  1.598931, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6270%\n",
      "layer   3  Sparsity: 72.9692%\n",
      "total_backward_count 254540 real_backward_count 29967  11.773%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.328740/  1.612397, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8831%\n",
      "layer   3  Sparsity: 73.4843%\n",
      "total_backward_count 264330 real_backward_count 30786  11.647%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.338545/  1.586888, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7108%\n",
      "layer   3  Sparsity: 74.1392%\n",
      "total_backward_count 274120 real_backward_count 31593  11.525%\n",
      "lif layer 2 self.abs_max_v: 7012.5\n",
      "lif layer 2 self.abs_max_v: 7189.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.353060/  1.646366, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.3614%\n",
      "layer   3  Sparsity: 74.2497%\n",
      "total_backward_count 283910 real_backward_count 32393  11.410%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.366001/  1.606433, val:  67.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.1126%\n",
      "layer   3  Sparsity: 73.6960%\n",
      "total_backward_count 293700 real_backward_count 33190  11.301%\n",
      "fc layer 3 self.abs_max_out: 2505.0\n",
      "fc layer 3 self.abs_max_out: 2519.0\n",
      "fc layer 3 self.abs_max_out: 2527.0\n",
      "fc layer 3 self.abs_max_out: 2601.0\n",
      "fc layer 3 self.abs_max_out: 2611.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.331741/  1.582131, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.3597%\n",
      "layer   3  Sparsity: 73.8186%\n",
      "total_backward_count 303490 real_backward_count 34004  11.204%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.327306/  1.626575, val:  68.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9473%\n",
      "layer   3  Sparsity: 74.5940%\n",
      "total_backward_count 313280 real_backward_count 34766  11.097%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.326095/  1.604507, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5205%\n",
      "layer   3  Sparsity: 74.6748%\n",
      "total_backward_count 323070 real_backward_count 35484  10.983%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.339254/  1.570285, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.3468%\n",
      "layer   3  Sparsity: 74.6263%\n",
      "total_backward_count 332860 real_backward_count 36202  10.876%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.336222/  1.615275, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6055%\n",
      "layer   3  Sparsity: 74.8551%\n",
      "total_backward_count 342650 real_backward_count 36918  10.774%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.351148/  1.612461, val:  60.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5206%\n",
      "layer   3  Sparsity: 75.3808%\n",
      "total_backward_count 352440 real_backward_count 37578  10.662%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.329656/  1.563564, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9619%\n",
      "layer   3  Sparsity: 75.6120%\n",
      "total_backward_count 362230 real_backward_count 38225  10.553%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.305147/  1.608318, val:  69.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7841%\n",
      "layer   3  Sparsity: 75.2072%\n",
      "total_backward_count 372020 real_backward_count 38860  10.446%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.319367/  1.595244, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8642%\n",
      "layer   3  Sparsity: 75.2098%\n",
      "total_backward_count 381810 real_backward_count 39505  10.347%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.304068/  1.597870, val:  71.67%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6336%\n",
      "layer   3  Sparsity: 75.5995%\n",
      "total_backward_count 391600 real_backward_count 40142  10.251%\n",
      "fc layer 2 self.abs_max_out: 4814.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.299492/  1.523337, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5253%\n",
      "layer   3  Sparsity: 75.3078%\n",
      "total_backward_count 401390 real_backward_count 40752  10.153%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.295126/  1.576126, val:  70.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.3610%\n",
      "layer   3  Sparsity: 74.8662%\n",
      "total_backward_count 411180 real_backward_count 41325  10.050%\n",
      "lif layer 2 self.abs_max_v: 7208.5\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.301785/  1.564640, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5045%\n",
      "layer   3  Sparsity: 75.2779%\n",
      "total_backward_count 420970 real_backward_count 41943   9.963%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.302311/  1.555266, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.4448%\n",
      "layer   3  Sparsity: 76.2571%\n",
      "total_backward_count 430760 real_backward_count 42542   9.876%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.287572/  1.557797, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8743%\n",
      "layer   3  Sparsity: 75.9650%\n",
      "total_backward_count 440550 real_backward_count 43119   9.788%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.282744/  1.571294, val:  72.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1419%\n",
      "layer   3  Sparsity: 75.7259%\n",
      "total_backward_count 450340 real_backward_count 43651   9.693%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.277574/  1.561598, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2602%\n",
      "layer   3  Sparsity: 76.5167%\n",
      "total_backward_count 460130 real_backward_count 44211   9.608%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.277972/  1.568458, val:  67.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0716%\n",
      "layer   3  Sparsity: 75.1820%\n",
      "total_backward_count 469920 real_backward_count 44758   9.525%\n",
      "fc layer 2 self.abs_max_out: 4939.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.287755/  1.580292, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2969%\n",
      "layer   3  Sparsity: 75.5097%\n",
      "total_backward_count 479710 real_backward_count 45296   9.442%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.291006/  1.543952, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9218%\n",
      "layer   3  Sparsity: 76.4484%\n",
      "total_backward_count 489500 real_backward_count 45849   9.366%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.291593/  1.590860, val:  75.00%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1579%\n",
      "layer   3  Sparsity: 76.4795%\n",
      "total_backward_count 499290 real_backward_count 46431   9.299%\n",
      "fc layer 3 self.abs_max_out: 2861.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.269847/  1.527112, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1289%\n",
      "layer   3  Sparsity: 75.8098%\n",
      "total_backward_count 509080 real_backward_count 46959   9.224%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.249817/  1.542197, val:  68.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2758%\n",
      "layer   3  Sparsity: 75.3063%\n",
      "total_backward_count 518870 real_backward_count 47458   9.146%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.257274/  1.556952, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.5321%\n",
      "layer   3  Sparsity: 76.0458%\n",
      "total_backward_count 528660 real_backward_count 47933   9.067%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.283807/  1.554789, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2826%\n",
      "layer   3  Sparsity: 76.2886%\n",
      "total_backward_count 538450 real_backward_count 48420   8.992%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.265847/  1.556744, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2259%\n",
      "layer   3  Sparsity: 76.0836%\n",
      "total_backward_count 548240 real_backward_count 48931   8.925%\n",
      "fc layer 2 self.abs_max_out: 5014.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.255455/  1.573237, val:  70.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8461%\n",
      "layer   3  Sparsity: 75.3779%\n",
      "total_backward_count 558030 real_backward_count 49460   8.863%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.254062/  1.533114, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2235%\n",
      "layer   3  Sparsity: 75.6838%\n",
      "total_backward_count 567820 real_backward_count 49921   8.792%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.240717/  1.511198, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1485%\n",
      "layer   3  Sparsity: 75.1068%\n",
      "total_backward_count 577610 real_backward_count 50394   8.725%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.261106/  1.581449, val:  68.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2136%\n",
      "layer   3  Sparsity: 75.1711%\n",
      "total_backward_count 587400 real_backward_count 50901   8.665%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.248513/  1.535706, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2248%\n",
      "layer   3  Sparsity: 76.1753%\n",
      "total_backward_count 597190 real_backward_count 51355   8.599%\n",
      "fc layer 2 self.abs_max_out: 5124.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.241542/  1.522118, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0610%\n",
      "layer   3  Sparsity: 76.3804%\n",
      "total_backward_count 606980 real_backward_count 51813   8.536%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.253102/  1.536748, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0161%\n",
      "layer   3  Sparsity: 76.5085%\n",
      "total_backward_count 616770 real_backward_count 52253   8.472%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.261694/  1.567133, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0150%\n",
      "layer   3  Sparsity: 76.4807%\n",
      "total_backward_count 626560 real_backward_count 52695   8.410%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.265366/  1.540373, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9461%\n",
      "layer   3  Sparsity: 76.9244%\n",
      "total_backward_count 636350 real_backward_count 53108   8.346%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.255876/  1.550269, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2267%\n",
      "layer   3  Sparsity: 76.6979%\n",
      "total_backward_count 646140 real_backward_count 53542   8.286%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.248109/  1.561351, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4462%\n",
      "layer   3  Sparsity: 76.7120%\n",
      "total_backward_count 655930 real_backward_count 53958   8.226%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.247133/  1.502302, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.6262%\n",
      "layer   3  Sparsity: 76.4888%\n",
      "total_backward_count 665720 real_backward_count 54322   8.160%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.227544/  1.488854, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4528%\n",
      "layer   3  Sparsity: 76.1054%\n",
      "total_backward_count 675510 real_backward_count 54707   8.099%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.219577/  1.482569, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1037%\n",
      "layer   3  Sparsity: 76.7392%\n",
      "total_backward_count 685300 real_backward_count 55099   8.040%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.220799/  1.486417, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1372%\n",
      "layer   3  Sparsity: 75.8898%\n",
      "total_backward_count 695090 real_backward_count 55486   7.983%\n",
      "fc layer 2 self.abs_max_out: 5153.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.200920/  1.463694, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1373%\n",
      "layer   3  Sparsity: 76.4628%\n",
      "total_backward_count 704880 real_backward_count 55869   7.926%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.184630/  1.480337, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.5443%\n",
      "layer   3  Sparsity: 76.2609%\n",
      "total_backward_count 714670 real_backward_count 56211   7.865%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.178834/  1.488675, val:  70.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2156%\n",
      "layer   3  Sparsity: 76.8927%\n",
      "total_backward_count 724460 real_backward_count 56551   7.806%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.194688/  1.471157, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4135%\n",
      "layer   3  Sparsity: 76.8862%\n",
      "total_backward_count 734250 real_backward_count 56910   7.751%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.209144/  1.499970, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0253%\n",
      "layer   3  Sparsity: 76.3626%\n",
      "total_backward_count 744040 real_backward_count 57281   7.699%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.201524/  1.472467, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9959%\n",
      "layer   3  Sparsity: 75.6571%\n",
      "total_backward_count 753830 real_backward_count 57631   7.645%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.197238/  1.496401, val:  73.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1346%\n",
      "layer   3  Sparsity: 76.0684%\n",
      "total_backward_count 763620 real_backward_count 57983   7.593%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.192151/  1.487607, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3417%\n",
      "layer   3  Sparsity: 76.4382%\n",
      "total_backward_count 773410 real_backward_count 58300   7.538%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.181856/  1.487243, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.6146%\n",
      "layer   3  Sparsity: 76.2578%\n",
      "total_backward_count 783200 real_backward_count 58623   7.485%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.192743/  1.481013, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3498%\n",
      "layer   3  Sparsity: 76.4158%\n",
      "total_backward_count 792990 real_backward_count 58954   7.434%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.188516/  1.492594, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1744%\n",
      "layer   3  Sparsity: 76.3347%\n",
      "total_backward_count 802780 real_backward_count 59294   7.386%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.215805/  1.511163, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8068%\n",
      "layer   3  Sparsity: 76.8473%\n",
      "total_backward_count 812570 real_backward_count 59605   7.335%\n",
      "fc layer 2 self.abs_max_out: 5286.0\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.195896/  1.497490, val:  72.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.3968%\n",
      "layer   3  Sparsity: 76.7653%\n",
      "total_backward_count 822360 real_backward_count 59923   7.287%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.175670/  1.445840, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7685%\n",
      "layer   3  Sparsity: 76.3295%\n",
      "total_backward_count 832150 real_backward_count 60232   7.238%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.179419/  1.487791, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.6114%\n",
      "layer   3  Sparsity: 76.7495%\n",
      "total_backward_count 841940 real_backward_count 60524   7.189%\n",
      "fc layer 2 self.abs_max_out: 5409.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.177704/  1.470148, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0242%\n",
      "layer   3  Sparsity: 76.5190%\n",
      "total_backward_count 851730 real_backward_count 60823   7.141%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.186648/  1.495452, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7446%\n",
      "layer   3  Sparsity: 76.7512%\n",
      "total_backward_count 861520 real_backward_count 61135   7.096%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.182922/  1.462416, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7566%\n",
      "layer   3  Sparsity: 76.6534%\n",
      "total_backward_count 871310 real_backward_count 61408   7.048%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.170196/  1.479017, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9960%\n",
      "layer   3  Sparsity: 75.7747%\n",
      "total_backward_count 881100 real_backward_count 61701   7.003%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.162491/  1.459525, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8331%\n",
      "layer   3  Sparsity: 76.3668%\n",
      "total_backward_count 890890 real_backward_count 62001   6.959%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.160974/  1.461156, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0683%\n",
      "layer   3  Sparsity: 76.3082%\n",
      "total_backward_count 900680 real_backward_count 62269   6.914%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.159279/  1.450142, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1950%\n",
      "layer   3  Sparsity: 75.7731%\n",
      "total_backward_count 910470 real_backward_count 62563   6.872%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.159435/  1.469677, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0905%\n",
      "layer   3  Sparsity: 76.4212%\n",
      "total_backward_count 920260 real_backward_count 62829   6.827%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.161790/  1.462847, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1507%\n",
      "layer   3  Sparsity: 76.4861%\n",
      "total_backward_count 930050 real_backward_count 63102   6.785%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.145657/  1.434287, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8279%\n",
      "layer   3  Sparsity: 76.6267%\n",
      "total_backward_count 939840 real_backward_count 63374   6.743%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.149608/  1.456199, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9830%\n",
      "layer   3  Sparsity: 76.7249%\n",
      "total_backward_count 949630 real_backward_count 63628   6.700%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.151983/  1.436102, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2221%\n",
      "layer   3  Sparsity: 76.5128%\n",
      "total_backward_count 959420 real_backward_count 63878   6.658%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.120310/  1.458722, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2455%\n",
      "layer   3  Sparsity: 76.2231%\n",
      "total_backward_count 969210 real_backward_count 64136   6.617%\n",
      "fc layer 3 self.abs_max_out: 2936.0\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.134928/  1.429855, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9979%\n",
      "layer   3  Sparsity: 76.1886%\n",
      "total_backward_count 979000 real_backward_count 64401   6.578%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.139585/  1.431329, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9176%\n",
      "layer   3  Sparsity: 76.6444%\n",
      "total_backward_count 988790 real_backward_count 64656   6.539%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.161705/  1.451590, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9245%\n",
      "layer   3  Sparsity: 77.2532%\n",
      "total_backward_count 998580 real_backward_count 64895   6.499%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.145472/  1.427023, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9539%\n",
      "layer   3  Sparsity: 76.4724%\n",
      "total_backward_count 1008370 real_backward_count 65173   6.463%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.143327/  1.456580, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9437%\n",
      "layer   3  Sparsity: 76.7041%\n",
      "total_backward_count 1018160 real_backward_count 65430   6.426%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.152705/  1.477459, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8976%\n",
      "layer   3  Sparsity: 76.9265%\n",
      "total_backward_count 1027950 real_backward_count 65681   6.390%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.155081/  1.455871, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1050%\n",
      "layer   3  Sparsity: 77.4199%\n",
      "total_backward_count 1037740 real_backward_count 65902   6.351%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.154366/  1.469918, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0995%\n",
      "layer   3  Sparsity: 77.3708%\n",
      "total_backward_count 1047530 real_backward_count 66137   6.314%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.144183/  1.473623, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0199%\n",
      "layer   3  Sparsity: 76.9011%\n",
      "total_backward_count 1057320 real_backward_count 66363   6.277%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.152733/  1.463034, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.5507%\n",
      "layer   3  Sparsity: 77.5882%\n",
      "total_backward_count 1067110 real_backward_count 66600   6.241%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.158154/  1.472303, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7684%\n",
      "layer   3  Sparsity: 77.5400%\n",
      "total_backward_count 1076900 real_backward_count 66835   6.206%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.140762/  1.448168, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8548%\n",
      "layer   3  Sparsity: 76.8950%\n",
      "total_backward_count 1086690 real_backward_count 67069   6.172%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.140794/  1.500569, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0634%\n",
      "layer   3  Sparsity: 77.5794%\n",
      "total_backward_count 1096480 real_backward_count 67278   6.136%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.155406/  1.462781, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9218%\n",
      "layer   3  Sparsity: 77.1200%\n",
      "total_backward_count 1106270 real_backward_count 67525   6.104%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.125341/  1.430925, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8908%\n",
      "layer   3  Sparsity: 76.7954%\n",
      "total_backward_count 1116060 real_backward_count 67729   6.069%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.129423/  1.452963, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1118%\n",
      "layer   3  Sparsity: 77.5870%\n",
      "total_backward_count 1125850 real_backward_count 67984   6.038%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.123206/  1.429986, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0574%\n",
      "layer   3  Sparsity: 77.5372%\n",
      "total_backward_count 1135640 real_backward_count 68189   6.004%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.118193/  1.434693, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1570%\n",
      "layer   3  Sparsity: 76.8216%\n",
      "total_backward_count 1145430 real_backward_count 68399   5.971%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.127244/  1.454662, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9555%\n",
      "layer   3  Sparsity: 77.5535%\n",
      "total_backward_count 1155220 real_backward_count 68607   5.939%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.125559/  1.429227, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2363%\n",
      "layer   3  Sparsity: 78.0984%\n",
      "total_backward_count 1165010 real_backward_count 68818   5.907%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.129486/  1.442815, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9347%\n",
      "layer   3  Sparsity: 77.6039%\n",
      "total_backward_count 1174800 real_backward_count 69032   5.876%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.128546/  1.436986, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8352%\n",
      "layer   3  Sparsity: 77.1518%\n",
      "total_backward_count 1184590 real_backward_count 69238   5.845%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.126415/  1.437464, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0649%\n",
      "layer   3  Sparsity: 77.9338%\n",
      "total_backward_count 1194380 real_backward_count 69434   5.813%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.124972/  1.434769, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9582%\n",
      "layer   3  Sparsity: 77.2238%\n",
      "total_backward_count 1204170 real_backward_count 69648   5.784%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.111559/  1.400766, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8637%\n",
      "layer   3  Sparsity: 77.5570%\n",
      "total_backward_count 1213960 real_backward_count 69896   5.758%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.111442/  1.442173, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9754%\n",
      "layer   3  Sparsity: 77.7985%\n",
      "total_backward_count 1223750 real_backward_count 70125   5.730%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.118505/  1.425900, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9875%\n",
      "layer   3  Sparsity: 77.0543%\n",
      "total_backward_count 1233540 real_backward_count 70351   5.703%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.116382/  1.426709, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0881%\n",
      "layer   3  Sparsity: 77.6625%\n",
      "total_backward_count 1243330 real_backward_count 70541   5.674%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.104823/  1.402345, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8932%\n",
      "layer   3  Sparsity: 77.3961%\n",
      "total_backward_count 1253120 real_backward_count 70755   5.646%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.117973/  1.430890, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8783%\n",
      "layer   3  Sparsity: 77.6073%\n",
      "total_backward_count 1262910 real_backward_count 70929   5.616%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.111289/  1.395006, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9455%\n",
      "layer   3  Sparsity: 77.3108%\n",
      "total_backward_count 1272700 real_backward_count 71093   5.586%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.112183/  1.433981, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9233%\n",
      "layer   3  Sparsity: 76.8691%\n",
      "total_backward_count 1282490 real_backward_count 71279   5.558%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.117218/  1.448298, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1108%\n",
      "layer   3  Sparsity: 77.4210%\n",
      "total_backward_count 1292280 real_backward_count 71454   5.529%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.124107/  1.428980, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1653%\n",
      "layer   3  Sparsity: 78.2366%\n",
      "total_backward_count 1302070 real_backward_count 71612   5.500%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.123901/  1.409434, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2400%\n",
      "layer   3  Sparsity: 78.0467%\n",
      "total_backward_count 1311860 real_backward_count 71791   5.472%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.131416/  1.410900, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1801%\n",
      "layer   3  Sparsity: 77.3283%\n",
      "total_backward_count 1321650 real_backward_count 71943   5.443%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.133070/  1.433998, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4158%\n",
      "layer   3  Sparsity: 77.4779%\n",
      "total_backward_count 1331440 real_backward_count 72123   5.417%\n",
      "fc layer 3 self.abs_max_out: 2978.0\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.105955/  1.434084, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4123%\n",
      "layer   3  Sparsity: 78.0491%\n",
      "total_backward_count 1341230 real_backward_count 72293   5.390%\n",
      "fc layer 3 self.abs_max_out: 3077.0\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.121853/  1.451058, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3233%\n",
      "layer   3  Sparsity: 77.7648%\n",
      "total_backward_count 1351020 real_backward_count 72449   5.363%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.125932/  1.458758, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2920%\n",
      "layer   3  Sparsity: 77.5622%\n",
      "total_backward_count 1360810 real_backward_count 72614   5.336%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.146101/  1.449580, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3683%\n",
      "layer   3  Sparsity: 77.4357%\n",
      "total_backward_count 1370600 real_backward_count 72815   5.313%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.132831/  1.444633, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3884%\n",
      "layer   3  Sparsity: 77.9714%\n",
      "total_backward_count 1380390 real_backward_count 72995   5.288%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.131403/  1.429426, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1578%\n",
      "layer   3  Sparsity: 76.9784%\n",
      "total_backward_count 1390180 real_backward_count 73167   5.263%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.113225/  1.413318, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9437%\n",
      "layer   3  Sparsity: 76.9632%\n",
      "total_backward_count 1399970 real_backward_count 73326   5.238%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.105535/  1.390566, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8220%\n",
      "layer   3  Sparsity: 76.7372%\n",
      "total_backward_count 1409760 real_backward_count 73490   5.213%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.093191/  1.403249, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7633%\n",
      "layer   3  Sparsity: 76.5665%\n",
      "total_backward_count 1419550 real_backward_count 73669   5.190%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.081630/  1.435020, val:  75.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8421%\n",
      "layer   3  Sparsity: 77.2724%\n",
      "total_backward_count 1429340 real_backward_count 73820   5.165%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.079648/  1.398600, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3130%\n",
      "layer   3  Sparsity: 77.3020%\n",
      "total_backward_count 1439130 real_backward_count 73972   5.140%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.084524/  1.427134, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0823%\n",
      "layer   3  Sparsity: 77.7134%\n",
      "total_backward_count 1448920 real_backward_count 74095   5.114%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.090305/  1.410287, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2049%\n",
      "layer   3  Sparsity: 77.3996%\n",
      "total_backward_count 1458710 real_backward_count 74278   5.092%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.084857/  1.422751, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2026%\n",
      "layer   3  Sparsity: 77.5741%\n",
      "total_backward_count 1468500 real_backward_count 74429   5.068%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.090236/  1.410581, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0702%\n",
      "layer   3  Sparsity: 77.0424%\n",
      "total_backward_count 1478290 real_backward_count 74620   5.048%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.073984/  1.382317, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1899%\n",
      "layer   3  Sparsity: 76.4829%\n",
      "total_backward_count 1488080 real_backward_count 74787   5.026%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.088762/  1.416414, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0389%\n",
      "layer   3  Sparsity: 76.4831%\n",
      "total_backward_count 1497870 real_backward_count 74951   5.004%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.073747/  1.370727, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1194%\n",
      "layer   3  Sparsity: 76.7956%\n",
      "total_backward_count 1507660 real_backward_count 75130   4.983%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.054917/  1.390037, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2327%\n",
      "layer   3  Sparsity: 77.0789%\n",
      "total_backward_count 1517450 real_backward_count 75287   4.961%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.053754/  1.396672, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1861%\n",
      "layer   3  Sparsity: 77.5659%\n",
      "total_backward_count 1527240 real_backward_count 75419   4.938%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.059460/  1.402439, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1138%\n",
      "layer   3  Sparsity: 77.6261%\n",
      "total_backward_count 1537030 real_backward_count 75536   4.914%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.067012/  1.364690, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2565%\n",
      "layer   3  Sparsity: 77.9194%\n",
      "total_backward_count 1546820 real_backward_count 75698   4.894%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.043449/  1.357961, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.3844%\n",
      "layer   3  Sparsity: 78.0681%\n",
      "total_backward_count 1556610 real_backward_count 75812   4.870%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.037342/  1.364033, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.4191%\n",
      "layer   3  Sparsity: 78.4320%\n",
      "total_backward_count 1566400 real_backward_count 75929   4.847%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.047770/  1.385067, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2885%\n",
      "layer   3  Sparsity: 78.5358%\n",
      "total_backward_count 1576190 real_backward_count 76040   4.824%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.057681/  1.411532, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1085%\n",
      "layer   3  Sparsity: 78.1900%\n",
      "total_backward_count 1585980 real_backward_count 76224   4.806%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.069274/  1.390738, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0857%\n",
      "layer   3  Sparsity: 78.0877%\n",
      "total_backward_count 1595770 real_backward_count 76374   4.786%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.059736/  1.377645, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2723%\n",
      "layer   3  Sparsity: 77.8707%\n",
      "total_backward_count 1605560 real_backward_count 76511   4.765%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.051460/  1.369018, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0405%\n",
      "layer   3  Sparsity: 77.2838%\n",
      "total_backward_count 1615350 real_backward_count 76662   4.746%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.040812/  1.389470, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9892%\n",
      "layer   3  Sparsity: 76.9544%\n",
      "total_backward_count 1625140 real_backward_count 76777   4.724%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.051216/  1.394808, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0865%\n",
      "layer   3  Sparsity: 76.8485%\n",
      "total_backward_count 1634930 real_backward_count 76928   4.705%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.046159/  1.374330, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1400%\n",
      "layer   3  Sparsity: 77.3433%\n",
      "total_backward_count 1644720 real_backward_count 77043   4.684%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.037280/  1.377218, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2228%\n",
      "layer   3  Sparsity: 77.1395%\n",
      "total_backward_count 1654510 real_backward_count 77156   4.663%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.043544/  1.416595, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1782%\n",
      "layer   3  Sparsity: 77.5645%\n",
      "total_backward_count 1664300 real_backward_count 77269   4.643%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.053627/  1.364281, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2016%\n",
      "layer   3  Sparsity: 77.4559%\n",
      "total_backward_count 1674090 real_backward_count 77418   4.624%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.059527/  1.388978, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1882%\n",
      "layer   3  Sparsity: 78.0603%\n",
      "total_backward_count 1683880 real_backward_count 77569   4.607%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.045420/  1.385705, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9634%\n",
      "layer   3  Sparsity: 77.5528%\n",
      "total_backward_count 1693670 real_backward_count 77727   4.589%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.041112/  1.359568, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9616%\n",
      "layer   3  Sparsity: 78.1667%\n",
      "total_backward_count 1703460 real_backward_count 77820   4.568%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.047124/  1.373888, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8667%\n",
      "layer   3  Sparsity: 77.9768%\n",
      "total_backward_count 1713250 real_backward_count 77914   4.548%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.042795/  1.377881, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7071%\n",
      "layer   3  Sparsity: 78.7282%\n",
      "total_backward_count 1723040 real_backward_count 78008   4.527%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.043011/  1.369149, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.7014%\n",
      "layer   3  Sparsity: 78.3041%\n",
      "total_backward_count 1732830 real_backward_count 78135   4.509%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.041042/  1.383369, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9530%\n",
      "layer   3  Sparsity: 78.0301%\n",
      "total_backward_count 1742620 real_backward_count 78249   4.490%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.034716/  1.362393, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0660%\n",
      "layer   3  Sparsity: 77.6251%\n",
      "total_backward_count 1752410 real_backward_count 78372   4.472%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.045269/  1.379050, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0018%\n",
      "layer   3  Sparsity: 78.0178%\n",
      "total_backward_count 1762200 real_backward_count 78515   4.456%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.047701/  1.365404, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1174%\n",
      "layer   3  Sparsity: 77.9061%\n",
      "total_backward_count 1771990 real_backward_count 78650   4.439%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.026362/  1.350306, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8397%\n",
      "layer   3  Sparsity: 77.6110%\n",
      "total_backward_count 1781780 real_backward_count 78790   4.422%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.024893/  1.359474, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8461%\n",
      "layer   3  Sparsity: 77.2730%\n",
      "total_backward_count 1791570 real_backward_count 78936   4.406%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.040253/  1.354545, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9519%\n",
      "layer   3  Sparsity: 77.7973%\n",
      "total_backward_count 1801360 real_backward_count 79077   4.390%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.034286/  1.349999, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0072%\n",
      "layer   3  Sparsity: 77.6374%\n",
      "total_backward_count 1811150 real_backward_count 79199   4.373%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.049846/  1.381634, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9404%\n",
      "layer   3  Sparsity: 77.7546%\n",
      "total_backward_count 1820940 real_backward_count 79358   4.358%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.053869/  1.359518, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0162%\n",
      "layer   3  Sparsity: 77.3326%\n",
      "total_backward_count 1830730 real_backward_count 79492   4.342%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.040534/  1.364965, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0938%\n",
      "layer   3  Sparsity: 77.4205%\n",
      "total_backward_count 1840520 real_backward_count 79638   4.327%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.038356/  1.359608, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1527%\n",
      "layer   3  Sparsity: 77.3465%\n",
      "total_backward_count 1850310 real_backward_count 79763   4.311%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.037185/  1.362420, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2405%\n",
      "layer   3  Sparsity: 76.9952%\n",
      "total_backward_count 1860100 real_backward_count 79874   4.294%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.039683/  1.384273, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1992%\n",
      "layer   3  Sparsity: 77.9000%\n",
      "total_backward_count 1869890 real_backward_count 80031   4.280%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.048083/  1.377030, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.0884%\n",
      "layer   3  Sparsity: 77.8979%\n",
      "total_backward_count 1879680 real_backward_count 80138   4.263%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.040222/  1.385692, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.2658%\n",
      "layer   3  Sparsity: 77.9655%\n",
      "total_backward_count 1889470 real_backward_count 80249   4.247%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.051189/  1.399117, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.9512%\n",
      "layer   3  Sparsity: 78.5512%\n",
      "total_backward_count 1899260 real_backward_count 80364   4.231%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.038528/  1.367725, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 73.8537%\n",
      "layer   3  Sparsity: 78.3578%\n",
      "total_backward_count 1909050 real_backward_count 80493   4.216%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.033064/  1.373566, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.1398%\n",
      "layer   3  Sparsity: 77.7048%\n",
      "total_backward_count 1918840 real_backward_count 80636   4.202%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.026320/  1.346699, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.5656%\n",
      "layer   3  Sparsity: 77.7875%\n",
      "total_backward_count 1928630 real_backward_count 80755   4.187%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.023745/  1.376872, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.5991%\n",
      "layer   3  Sparsity: 77.8915%\n",
      "total_backward_count 1938420 real_backward_count 80871   4.172%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.029412/  1.384933, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.8667%\n",
      "layer   3  Sparsity: 77.5498%\n",
      "total_backward_count 1948210 real_backward_count 80986   4.157%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.035458/  1.353858, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.4549%\n",
      "layer   2  Sparsity: 74.8854%\n",
      "layer   3  Sparsity: 77.5767%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9784cf311c4da581c351c52508a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.03546</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>1.35386</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-sweep-105</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1uvq5b6k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1uvq5b6k</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_120128-1uvq5b6k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j0my51tv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_162613-j0my51tv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0my51tv' target=\"_blank\">bright-sweep-111</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0my51tv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0my51tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251116_162622_003', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 352.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 482.0\n",
      "lif layer 1 self.abs_max_v: 536.0\n",
      "fc layer 2 self.abs_max_out: 137.0\n",
      "lif layer 2 self.abs_max_v: 137.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 764.5\n",
      "fc layer 2 self.abs_max_out: 230.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "fc layer 1 self.abs_max_out: 738.0\n",
      "lif layer 1 self.abs_max_v: 949.0\n",
      "fc layer 2 self.abs_max_out: 397.0\n",
      "lif layer 2 self.abs_max_v: 479.5\n",
      "fc layer 1 self.abs_max_out: 798.0\n",
      "fc layer 1 self.abs_max_out: 1132.0\n",
      "lif layer 1 self.abs_max_v: 1132.0\n",
      "lif layer 2 self.abs_max_v: 551.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 1133.0\n",
      "lif layer 2 self.abs_max_v: 620.5\n",
      "fc layer 1 self.abs_max_out: 1301.0\n",
      "lif layer 1 self.abs_max_v: 1301.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "fc layer 3 self.abs_max_out: 90.0\n",
      "fc layer 2 self.abs_max_out: 529.0\n",
      "lif layer 2 self.abs_max_v: 668.5\n",
      "fc layer 1 self.abs_max_out: 1512.0\n",
      "lif layer 1 self.abs_max_v: 1512.0\n",
      "fc layer 2 self.abs_max_out: 575.0\n",
      "lif layer 2 self.abs_max_v: 753.5\n",
      "lif layer 2 self.abs_max_v: 824.0\n",
      "lif layer 2 self.abs_max_v: 858.0\n",
      "fc layer 1 self.abs_max_out: 1789.0\n",
      "lif layer 1 self.abs_max_v: 1789.0\n",
      "fc layer 1 self.abs_max_out: 1875.0\n",
      "lif layer 1 self.abs_max_v: 1875.0\n",
      "lif layer 2 self.abs_max_v: 890.5\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 1 self.abs_max_out: 2072.0\n",
      "lif layer 1 self.abs_max_v: 2072.0\n",
      "fc layer 1 self.abs_max_out: 2180.0\n",
      "lif layer 1 self.abs_max_v: 2180.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 828.0\n",
      "fc layer 1 self.abs_max_out: 2500.0\n",
      "lif layer 1 self.abs_max_v: 2500.0\n",
      "lif layer 2 self.abs_max_v: 917.5\n",
      "lif layer 2 self.abs_max_v: 1078.5\n",
      "fc layer 2 self.abs_max_out: 835.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 1080.0\n",
      "fc layer 2 self.abs_max_out: 838.0\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "lif layer 2 self.abs_max_v: 1118.0\n",
      "fc layer 2 self.abs_max_out: 980.0\n",
      "lif layer 2 self.abs_max_v: 1226.0\n",
      "fc layer 2 self.abs_max_out: 1114.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "lif layer 2 self.abs_max_v: 1378.5\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 1398.0\n",
      "lif layer 2 self.abs_max_v: 1423.5\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 1 self.abs_max_out: 2820.0\n",
      "lif layer 1 self.abs_max_v: 2820.0\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "lif layer 1 self.abs_max_v: 2922.0\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "lif layer 2 self.abs_max_v: 1434.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "lif layer 2 self.abs_max_v: 1554.0\n",
      "lif layer 2 self.abs_max_v: 1578.0\n",
      "fc layer 2 self.abs_max_out: 1258.0\n",
      "fc layer 1 self.abs_max_out: 2945.0\n",
      "lif layer 1 self.abs_max_v: 2945.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 2 self.abs_max_out: 1283.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "fc layer 1 self.abs_max_out: 2958.0\n",
      "lif layer 1 self.abs_max_v: 2958.0\n",
      "fc layer 2 self.abs_max_out: 1303.0\n",
      "fc layer 1 self.abs_max_out: 3710.0\n",
      "lif layer 1 self.abs_max_v: 3710.0\n",
      "fc layer 2 self.abs_max_out: 1405.0\n",
      "lif layer 2 self.abs_max_v: 1630.5\n",
      "lif layer 2 self.abs_max_v: 1732.0\n",
      "fc layer 2 self.abs_max_out: 1463.0\n",
      "fc layer 2 self.abs_max_out: 1478.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 1777.0\n",
      "lif layer 2 self.abs_max_v: 1795.5\n",
      "lif layer 2 self.abs_max_v: 1802.5\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 2 self.abs_max_out: 1641.0\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "fc layer 1 self.abs_max_out: 4054.0\n",
      "lif layer 1 self.abs_max_v: 4054.0\n",
      "fc layer 2 self.abs_max_out: 1723.0\n",
      "fc layer 2 self.abs_max_out: 1795.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "lif layer 1 self.abs_max_v: 4307.5\n",
      "lif layer 1 self.abs_max_v: 4326.5\n",
      "lif layer 1 self.abs_max_v: 4416.5\n",
      "lif layer 1 self.abs_max_v: 4450.0\n",
      "lif layer 1 self.abs_max_v: 4493.0\n",
      "lif layer 1 self.abs_max_v: 4709.5\n",
      "lif layer 1 self.abs_max_v: 5205.0\n",
      "lif layer 2 self.abs_max_v: 1859.5\n",
      "fc layer 2 self.abs_max_out: 1804.0\n",
      "fc layer 1 self.abs_max_out: 4152.0\n",
      "lif layer 1 self.abs_max_v: 5205.5\n",
      "lif layer 1 self.abs_max_v: 5467.5\n",
      "lif layer 1 self.abs_max_v: 5475.5\n",
      "lif layer 1 self.abs_max_v: 5876.5\n",
      "lif layer 1 self.abs_max_v: 6429.5\n",
      "fc layer 2 self.abs_max_out: 1836.0\n",
      "fc layer 1 self.abs_max_out: 4272.0\n",
      "fc layer 3 self.abs_max_out: 436.0\n",
      "fc layer 2 self.abs_max_out: 1870.0\n",
      "lif layer 2 self.abs_max_v: 1870.0\n",
      "fc layer 1 self.abs_max_out: 4467.0\n",
      "lif layer 2 self.abs_max_v: 1909.0\n",
      "lif layer 2 self.abs_max_v: 1946.5\n",
      "fc layer 1 self.abs_max_out: 4502.0\n",
      "fc layer 1 self.abs_max_out: 4521.0\n",
      "fc layer 1 self.abs_max_out: 4684.0\n",
      "fc layer 1 self.abs_max_out: 4862.0\n",
      "fc layer 1 self.abs_max_out: 4865.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.073400/  2.138004, val:  38.33%, val_best:  38.33%, tr:  89.58%, tr_best:  89.58%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2497%\n",
      "layer   2  Sparsity: 86.2248%\n",
      "layer   3  Sparsity: 89.5779%\n",
      "total_backward_count 9790 real_backward_count 3151  32.186%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 6528.5\n",
      "lif layer 1 self.abs_max_v: 6635.0\n",
      "lif layer 1 self.abs_max_v: 7573.5\n",
      "lif layer 1 self.abs_max_v: 7748.0\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "lif layer 2 self.abs_max_v: 2034.0\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "lif layer 2 self.abs_max_v: 2189.5\n",
      "fc layer 1 self.abs_max_out: 5122.0\n",
      "fc layer 2 self.abs_max_out: 1994.0\n",
      "fc layer 2 self.abs_max_out: 2017.0\n",
      "fc layer 1 self.abs_max_out: 5147.0\n",
      "fc layer 1 self.abs_max_out: 5195.0\n",
      "fc layer 1 self.abs_max_out: 5595.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.016350/  2.132403, val:  43.75%, val_best:  43.75%, tr:  98.47%, tr_best:  98.47%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2805%\n",
      "layer   2  Sparsity: 85.6887%\n",
      "layer   3  Sparsity: 88.0587%\n",
      "total_backward_count 19580 real_backward_count 5229  26.706%\n",
      "fc layer 2 self.abs_max_out: 2126.0\n",
      "lif layer 2 self.abs_max_v: 2274.5\n",
      "lif layer 2 self.abs_max_v: 2275.0\n",
      "fc layer 2 self.abs_max_out: 2162.0\n",
      "fc layer 2 self.abs_max_out: 2281.0\n",
      "lif layer 2 self.abs_max_v: 2281.0\n",
      "fc layer 2 self.abs_max_out: 2342.0\n",
      "lif layer 2 self.abs_max_v: 2342.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "lif layer 2 self.abs_max_v: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2615.0\n",
      "lif layer 2 self.abs_max_v: 2615.0\n",
      "fc layer 1 self.abs_max_out: 5599.0\n",
      "fc layer 1 self.abs_max_out: 6104.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  2.019292/  2.111151, val:  47.92%, val_best:  47.92%, tr:  98.47%, tr_best:  98.47%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2880%\n",
      "layer   2  Sparsity: 85.0112%\n",
      "layer   3  Sparsity: 87.2215%\n",
      "total_backward_count 29370 real_backward_count 7137  24.300%\n",
      "lif layer 1 self.abs_max_v: 8278.5\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "lif layer 1 self.abs_max_v: 8296.0\n",
      "lif layer 1 self.abs_max_v: 8316.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  2.003807/  2.121296, val:  37.92%, val_best:  47.92%, tr:  98.98%, tr_best:  98.98%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2374%\n",
      "layer   2  Sparsity: 84.8437%\n",
      "layer   3  Sparsity: 86.9096%\n",
      "total_backward_count 39160 real_backward_count 8973  22.914%\n",
      "lif layer 1 self.abs_max_v: 8635.5\n",
      "lif layer 1 self.abs_max_v: 8812.0\n",
      "lif layer 1 self.abs_max_v: 9425.5\n",
      "lif layer 1 self.abs_max_v: 9564.0\n",
      "fc layer 1 self.abs_max_out: 6334.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  2.009207/  2.114705, val:  53.75%, val_best:  53.75%, tr:  99.28%, tr_best:  99.28%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2544%\n",
      "layer   2  Sparsity: 84.8805%\n",
      "layer   3  Sparsity: 86.8942%\n",
      "total_backward_count 48950 real_backward_count 10685  21.828%\n",
      "lif layer 2 self.abs_max_v: 2647.0\n",
      "lif layer 2 self.abs_max_v: 2705.0\n",
      "lif layer 2 self.abs_max_v: 2814.0\n",
      "lif layer 2 self.abs_max_v: 2816.0\n",
      "lif layer 2 self.abs_max_v: 2900.0\n",
      "lif layer 2 self.abs_max_v: 3040.0\n",
      "fc layer 1 self.abs_max_out: 6359.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  2.007420/  2.125630, val:  54.58%, val_best:  54.58%, tr:  99.18%, tr_best:  99.28%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2420%\n",
      "layer   2  Sparsity: 84.9425%\n",
      "layer   3  Sparsity: 87.2581%\n",
      "total_backward_count 58740 real_backward_count 12445  21.187%\n",
      "lif layer 1 self.abs_max_v: 9733.0\n",
      "lif layer 1 self.abs_max_v: 9882.0\n",
      "lif layer 1 self.abs_max_v: 9992.0\n",
      "lif layer 1 self.abs_max_v: 10707.0\n",
      "lif layer 1 self.abs_max_v: 10765.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.998318/  2.082224, val:  53.75%, val_best:  54.58%, tr:  99.28%, tr_best:  99.28%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2451%\n",
      "layer   2  Sparsity: 84.7160%\n",
      "layer   3  Sparsity: 86.4680%\n",
      "total_backward_count 68530 real_backward_count 14075  20.538%\n",
      "fc layer 3 self.abs_max_out: 456.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 1 self.abs_max_out: 6526.0\n",
      "fc layer 2 self.abs_max_out: 2632.0\n",
      "fc layer 2 self.abs_max_out: 2673.0\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "fc layer 1 self.abs_max_out: 6798.0\n",
      "lif layer 1 self.abs_max_v: 11053.0\n",
      "lif layer 1 self.abs_max_v: 11299.5\n",
      "lif layer 1 self.abs_max_v: 11998.0\n",
      "lif layer 1 self.abs_max_v: 12308.0\n",
      "lif layer 2 self.abs_max_v: 3052.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.972072/  2.056406, val:  52.08%, val_best:  54.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2611%\n",
      "layer   2  Sparsity: 84.5371%\n",
      "layer   3  Sparsity: 85.8891%\n",
      "total_backward_count 78320 real_backward_count 15700  20.046%\n",
      "fc layer 2 self.abs_max_out: 2710.0\n",
      "lif layer 2 self.abs_max_v: 3155.0\n",
      "lif layer 2 self.abs_max_v: 3160.0\n",
      "lif layer 2 self.abs_max_v: 3170.0\n",
      "lif layer 2 self.abs_max_v: 3177.0\n",
      "lif layer 2 self.abs_max_v: 3209.0\n",
      "lif layer 2 self.abs_max_v: 3243.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "lif layer 2 self.abs_max_v: 3256.5\n",
      "lif layer 2 self.abs_max_v: 3262.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.976387/  2.071637, val:  50.83%, val_best:  54.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2736%\n",
      "layer   2  Sparsity: 83.5861%\n",
      "layer   3  Sparsity: 85.5534%\n",
      "total_backward_count 88110 real_backward_count 17309  19.645%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.962631/  2.079114, val:  50.42%, val_best:  54.58%, tr:  99.39%, tr_best:  99.69%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2699%\n",
      "layer   2  Sparsity: 83.9602%\n",
      "layer   3  Sparsity: 86.0010%\n",
      "total_backward_count 97900 real_backward_count 18875  19.280%\n",
      "lif layer 2 self.abs_max_v: 3342.5\n",
      "fc layer 2 self.abs_max_out: 2799.0\n",
      "lif layer 1 self.abs_max_v: 12385.0\n",
      "lif layer 1 self.abs_max_v: 12499.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.957453/  2.089865, val:  38.33%, val_best:  54.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2458%\n",
      "layer   2  Sparsity: 84.0087%\n",
      "layer   3  Sparsity: 85.8136%\n",
      "total_backward_count 107690 real_backward_count 20452  18.992%\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "fc layer 2 self.abs_max_out: 2834.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.974293/  2.083511, val:  51.25%, val_best:  54.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2246%\n",
      "layer   2  Sparsity: 84.1274%\n",
      "layer   3  Sparsity: 85.9219%\n",
      "total_backward_count 117480 real_backward_count 21975  18.705%\n",
      "fc layer 2 self.abs_max_out: 2853.0\n",
      "fc layer 1 self.abs_max_out: 6830.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.963623/  2.094138, val:  41.67%, val_best:  54.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2386%\n",
      "layer   2  Sparsity: 84.4011%\n",
      "layer   3  Sparsity: 85.9801%\n",
      "total_backward_count 127270 real_backward_count 23467  18.439%\n",
      "fc layer 2 self.abs_max_out: 3087.0\n",
      "fc layer 1 self.abs_max_out: 6913.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.966638/  2.086380, val:  33.33%, val_best:  54.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2702%\n",
      "layer   2  Sparsity: 84.5519%\n",
      "layer   3  Sparsity: 85.2829%\n",
      "total_backward_count 137060 real_backward_count 24999  18.239%\n",
      "lif layer 2 self.abs_max_v: 3469.5\n",
      "lif layer 2 self.abs_max_v: 3748.0\n",
      "fc layer 1 self.abs_max_out: 7010.0\n",
      "fc layer 1 self.abs_max_out: 7262.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.956330/  2.050608, val:  55.00%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2315%\n",
      "layer   2  Sparsity: 83.9440%\n",
      "layer   3  Sparsity: 85.6494%\n",
      "total_backward_count 146850 real_backward_count 26466  18.022%\n",
      "fc layer 1 self.abs_max_out: 7645.0\n",
      "fc layer 1 self.abs_max_out: 8028.0\n",
      "fc layer 1 self.abs_max_out: 8251.0\n",
      "fc layer 1 self.abs_max_out: 8574.0\n",
      "fc layer 1 self.abs_max_out: 9110.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.958849/  2.068498, val:  41.67%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2386%\n",
      "layer   2  Sparsity: 83.1555%\n",
      "layer   3  Sparsity: 85.5756%\n",
      "total_backward_count 156640 real_backward_count 27962  17.851%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.932174/  2.034442, val:  56.25%, val_best:  56.25%, tr:  99.18%, tr_best:  99.80%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2558%\n",
      "layer   2  Sparsity: 82.9480%\n",
      "layer   3  Sparsity: 85.2208%\n",
      "total_backward_count 166430 real_backward_count 29436  17.687%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.954435/  2.043767, val:  62.92%, val_best:  62.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2150%\n",
      "layer   2  Sparsity: 82.8998%\n",
      "layer   3  Sparsity: 85.7128%\n",
      "total_backward_count 176220 real_backward_count 30955  17.566%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.942716/  2.061080, val:  50.42%, val_best:  62.92%, tr:  99.18%, tr_best:  99.80%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2377%\n",
      "layer   2  Sparsity: 83.5878%\n",
      "layer   3  Sparsity: 85.3947%\n",
      "total_backward_count 186010 real_backward_count 32479  17.461%\n",
      "fc layer 2 self.abs_max_out: 3146.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.942616/  2.060856, val:  39.17%, val_best:  62.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2412%\n",
      "layer   2  Sparsity: 83.6117%\n",
      "layer   3  Sparsity: 85.4598%\n",
      "total_backward_count 195800 real_backward_count 33895  17.311%\n",
      "lif layer 1 self.abs_max_v: 12656.5\n",
      "lif layer 1 self.abs_max_v: 12731.5\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.923037/  2.054982, val:  47.50%, val_best:  62.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2451%\n",
      "layer   2  Sparsity: 83.8485%\n",
      "layer   3  Sparsity: 85.4275%\n",
      "total_backward_count 205590 real_backward_count 35376  17.207%\n",
      "lif layer 1 self.abs_max_v: 12927.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.915766/  2.040203, val:  35.83%, val_best:  62.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2572%\n",
      "layer   2  Sparsity: 83.3063%\n",
      "layer   3  Sparsity: 84.5089%\n",
      "total_backward_count 215380 real_backward_count 36907  17.136%\n",
      "fc layer 2 self.abs_max_out: 3149.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.903961/  2.003195, val:  57.92%, val_best:  62.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2532%\n",
      "layer   2  Sparsity: 83.1532%\n",
      "layer   3  Sparsity: 84.4740%\n",
      "total_backward_count 225170 real_backward_count 38334  17.024%\n",
      "fc layer 2 self.abs_max_out: 3150.0\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.909124/  2.018973, val:  57.92%, val_best:  62.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2130%\n",
      "layer   2  Sparsity: 83.3296%\n",
      "layer   3  Sparsity: 84.7534%\n",
      "total_backward_count 234960 real_backward_count 39887  16.976%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.912139/  2.025558, val:  62.50%, val_best:  62.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2745%\n",
      "layer   2  Sparsity: 83.1688%\n",
      "layer   3  Sparsity: 85.0075%\n",
      "total_backward_count 244750 real_backward_count 41324  16.884%\n",
      "lif layer 1 self.abs_max_v: 12948.0\n",
      "lif layer 1 self.abs_max_v: 13698.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.922925/  2.034781, val:  60.83%, val_best:  62.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2377%\n",
      "layer   2  Sparsity: 83.6389%\n",
      "layer   3  Sparsity: 85.5040%\n",
      "total_backward_count 254540 real_backward_count 42837  16.829%\n",
      "fc layer 1 self.abs_max_out: 9144.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.925473/  2.029754, val:  53.75%, val_best:  62.92%, tr:  99.08%, tr_best:  99.80%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2474%\n",
      "layer   2  Sparsity: 83.4809%\n",
      "layer   3  Sparsity: 85.5322%\n",
      "total_backward_count 264330 real_backward_count 44305  16.761%\n",
      "lif layer 1 self.abs_max_v: 13778.5\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.911443/  2.032775, val:  55.83%, val_best:  62.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.1948%\n",
      "layer   2  Sparsity: 83.6775%\n",
      "layer   3  Sparsity: 85.0267%\n",
      "total_backward_count 274120 real_backward_count 45808  16.711%\n",
      "lif layer 2 self.abs_max_v: 3765.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.912518/  2.031452, val:  50.42%, val_best:  62.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2461%\n",
      "layer   2  Sparsity: 83.5110%\n",
      "layer   3  Sparsity: 85.2241%\n",
      "total_backward_count 283910 real_backward_count 47196  16.624%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.927716/  2.032316, val:  56.25%, val_best:  62.92%, tr:  98.98%, tr_best:  99.80%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2318%\n",
      "layer   2  Sparsity: 83.4709%\n",
      "layer   3  Sparsity: 86.1307%\n",
      "total_backward_count 293700 real_backward_count 48647  16.564%\n",
      "lif layer 2 self.abs_max_v: 4073.0\n",
      "fc layer 3 self.abs_max_out: 611.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.913196/  2.036065, val:  60.42%, val_best:  62.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2846%\n",
      "layer   2  Sparsity: 83.4271%\n",
      "layer   3  Sparsity: 85.3669%\n",
      "total_backward_count 303490 real_backward_count 50076  16.500%\n",
      "fc layer 2 self.abs_max_out: 3167.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.923054/  2.042243, val:  52.08%, val_best:  62.92%, tr:  99.08%, tr_best:  99.80%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2285%\n",
      "layer   2  Sparsity: 83.4918%\n",
      "layer   3  Sparsity: 85.3866%\n",
      "total_backward_count 313280 real_backward_count 51524  16.447%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.915533/  2.037085, val:  60.00%, val_best:  62.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2352%\n",
      "layer   2  Sparsity: 83.5016%\n",
      "layer   3  Sparsity: 85.0190%\n",
      "total_backward_count 323070 real_backward_count 52910  16.377%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.909144/  2.037368, val:  56.67%, val_best:  62.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2752%\n",
      "layer   2  Sparsity: 82.7921%\n",
      "layer   3  Sparsity: 84.7395%\n",
      "total_backward_count 332860 real_backward_count 54306  16.315%\n",
      "lif layer 1 self.abs_max_v: 13927.0\n",
      "fc layer 2 self.abs_max_out: 3177.0\n",
      "lif layer 1 self.abs_max_v: 14530.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.878466/  1.992325, val:  55.42%, val_best:  62.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2354%\n",
      "layer   2  Sparsity: 82.7254%\n",
      "layer   3  Sparsity: 84.4338%\n",
      "total_backward_count 342650 real_backward_count 55649  16.241%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.875216/  2.009862, val:  54.58%, val_best:  62.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2253%\n",
      "layer   2  Sparsity: 82.6243%\n",
      "layer   3  Sparsity: 84.4553%\n",
      "total_backward_count 352440 real_backward_count 56971  16.165%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.882965/  1.997146, val:  53.75%, val_best:  62.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2869%\n",
      "layer   2  Sparsity: 82.7633%\n",
      "layer   3  Sparsity: 85.1386%\n",
      "total_backward_count 362230 real_backward_count 58335  16.104%\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.875266/  2.019596, val:  54.58%, val_best:  62.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2782%\n",
      "layer   2  Sparsity: 82.2388%\n",
      "layer   3  Sparsity: 84.6382%\n",
      "total_backward_count 372020 real_backward_count 59642  16.032%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.877161/  1.999103, val:  56.67%, val_best:  62.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.1937%\n",
      "layer   2  Sparsity: 82.2365%\n",
      "layer   3  Sparsity: 84.5198%\n",
      "total_backward_count 381810 real_backward_count 60993  15.975%\n",
      "lif layer 2 self.abs_max_v: 4098.5\n",
      "fc layer 1 self.abs_max_out: 9155.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.885180/  2.012287, val:  56.25%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2664%\n",
      "layer   2  Sparsity: 82.6194%\n",
      "layer   3  Sparsity: 85.0418%\n",
      "total_backward_count 391600 real_backward_count 62311  15.912%\n",
      "lif layer 2 self.abs_max_v: 4099.5\n",
      "lif layer 2 self.abs_max_v: 4146.0\n",
      "fc layer 1 self.abs_max_out: 9294.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.902266/  2.012606, val:  55.00%, val_best:  62.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2168%\n",
      "layer   2  Sparsity: 82.6994%\n",
      "layer   3  Sparsity: 85.1891%\n",
      "total_backward_count 401390 real_backward_count 63683  15.866%\n",
      "lif layer 1 self.abs_max_v: 14981.5\n",
      "fc layer 1 self.abs_max_out: 9625.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.883141/  2.011997, val:  60.00%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2376%\n",
      "layer   2  Sparsity: 82.6057%\n",
      "layer   3  Sparsity: 84.6482%\n",
      "total_backward_count 411180 real_backward_count 65038  15.817%\n",
      "lif layer 1 self.abs_max_v: 15212.0\n",
      "lif layer 2 self.abs_max_v: 4287.0\n",
      "lif layer 2 self.abs_max_v: 4356.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.867861/  1.991227, val:  60.42%, val_best:  62.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2956%\n",
      "layer   2  Sparsity: 82.4616%\n",
      "layer   3  Sparsity: 85.0210%\n",
      "total_backward_count 420970 real_backward_count 66394  15.772%\n",
      "lif layer 1 self.abs_max_v: 16068.5\n",
      "fc layer 1 self.abs_max_out: 9815.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.873451/  2.002041, val:  62.08%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3006%\n",
      "layer   2  Sparsity: 82.1141%\n",
      "layer   3  Sparsity: 85.3923%\n",
      "total_backward_count 430760 real_backward_count 67705  15.718%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.889035/  2.009009, val:  58.33%, val_best:  62.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3063%\n",
      "layer   2  Sparsity: 82.0265%\n",
      "layer   3  Sparsity: 85.0088%\n",
      "total_backward_count 440550 real_backward_count 69109  15.687%\n",
      "lif layer 2 self.abs_max_v: 4412.0\n",
      "lif layer 2 self.abs_max_v: 4722.0\n",
      "fc layer 2 self.abs_max_out: 3179.0\n",
      "lif layer 2 self.abs_max_v: 4844.5\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.870225/  1.974142, val:  57.92%, val_best:  62.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2127%\n",
      "layer   2  Sparsity: 82.6983%\n",
      "layer   3  Sparsity: 84.8586%\n",
      "total_backward_count 450340 real_backward_count 70448  15.643%\n",
      "fc layer 2 self.abs_max_out: 3276.0\n",
      "lif layer 1 self.abs_max_v: 16445.5\n",
      "lif layer 1 self.abs_max_v: 16687.0\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.861440/  2.020550, val:  53.75%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2151%\n",
      "layer   2  Sparsity: 82.7858%\n",
      "layer   3  Sparsity: 84.1635%\n",
      "total_backward_count 460130 real_backward_count 71765  15.597%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.870048/  2.026796, val:  55.00%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3026%\n",
      "layer   2  Sparsity: 82.9172%\n",
      "layer   3  Sparsity: 84.4134%\n",
      "total_backward_count 469920 real_backward_count 73038  15.543%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.861940/  2.013778, val:  57.08%, val_best:  62.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2046%\n",
      "layer   2  Sparsity: 82.7000%\n",
      "layer   3  Sparsity: 84.2824%\n",
      "total_backward_count 479710 real_backward_count 74391  15.507%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.843701/  1.961951, val:  62.08%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2389%\n",
      "layer   2  Sparsity: 82.6795%\n",
      "layer   3  Sparsity: 84.4413%\n",
      "total_backward_count 489500 real_backward_count 75745  15.474%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.850771/  1.975317, val:  67.50%, val_best:  67.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2661%\n",
      "layer   2  Sparsity: 82.4859%\n",
      "layer   3  Sparsity: 84.9559%\n",
      "total_backward_count 499290 real_backward_count 77090  15.440%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.841092/  1.993605, val:  60.83%, val_best:  67.50%, tr:  98.88%, tr_best:  99.90%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2145%\n",
      "layer   2  Sparsity: 82.8742%\n",
      "layer   3  Sparsity: 84.6833%\n",
      "total_backward_count 509080 real_backward_count 78404  15.401%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.873505/  2.014195, val:  60.00%, val_best:  67.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2591%\n",
      "layer   2  Sparsity: 83.0413%\n",
      "layer   3  Sparsity: 84.7223%\n",
      "total_backward_count 518870 real_backward_count 79794  15.378%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.849558/  1.983300, val:  65.42%, val_best:  67.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2464%\n",
      "layer   2  Sparsity: 82.8273%\n",
      "layer   3  Sparsity: 84.3152%\n",
      "total_backward_count 528660 real_backward_count 81083  15.337%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.838101/  1.969831, val:  60.00%, val_best:  67.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2710%\n",
      "layer   2  Sparsity: 82.5787%\n",
      "layer   3  Sparsity: 83.7289%\n",
      "total_backward_count 538450 real_backward_count 82369  15.297%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.849433/  1.980363, val:  60.00%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2692%\n",
      "layer   2  Sparsity: 82.7361%\n",
      "layer   3  Sparsity: 84.0559%\n",
      "total_backward_count 548240 real_backward_count 83743  15.275%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.844465/  1.984867, val:  49.17%, val_best:  67.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2223%\n",
      "layer   2  Sparsity: 82.6447%\n",
      "layer   3  Sparsity: 83.8898%\n",
      "total_backward_count 558030 real_backward_count 85064  15.244%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.840374/  1.973897, val:  60.42%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2706%\n",
      "layer   2  Sparsity: 82.7270%\n",
      "layer   3  Sparsity: 84.0518%\n",
      "total_backward_count 567820 real_backward_count 86330  15.204%\n",
      "fc layer 2 self.abs_max_out: 3309.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.852469/  1.985034, val:  64.17%, val_best:  67.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2631%\n",
      "layer   2  Sparsity: 82.6928%\n",
      "layer   3  Sparsity: 84.8004%\n",
      "total_backward_count 577610 real_backward_count 87633  15.172%\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.858497/  1.986516, val:  55.83%, val_best:  67.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2591%\n",
      "layer   2  Sparsity: 82.7442%\n",
      "layer   3  Sparsity: 84.8359%\n",
      "total_backward_count 587400 real_backward_count 88894  15.133%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.857201/  1.973811, val:  58.75%, val_best:  67.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2710%\n",
      "layer   2  Sparsity: 82.7214%\n",
      "layer   3  Sparsity: 84.4585%\n",
      "total_backward_count 597190 real_backward_count 90205  15.105%\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.842177/  1.959670, val:  67.08%, val_best:  67.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2203%\n",
      "layer   2  Sparsity: 83.1669%\n",
      "layer   3  Sparsity: 84.6528%\n",
      "total_backward_count 606980 real_backward_count 91507  15.076%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.833252/  1.988104, val:  55.42%, val_best:  67.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2555%\n",
      "layer   2  Sparsity: 82.9711%\n",
      "layer   3  Sparsity: 84.1041%\n",
      "total_backward_count 616770 real_backward_count 92744  15.037%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.836790/  2.002996, val:  54.17%, val_best:  67.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2763%\n",
      "layer   2  Sparsity: 82.6311%\n",
      "layer   3  Sparsity: 85.0135%\n",
      "total_backward_count 626560 real_backward_count 93983  15.000%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.859756/  1.996978, val:  55.00%, val_best:  67.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2541%\n",
      "layer   2  Sparsity: 82.1233%\n",
      "layer   3  Sparsity: 85.0014%\n",
      "total_backward_count 636350 real_backward_count 95269  14.971%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.855140/  1.986708, val:  55.83%, val_best:  67.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2547%\n",
      "layer   2  Sparsity: 82.0438%\n",
      "layer   3  Sparsity: 84.8242%\n",
      "total_backward_count 646140 real_backward_count 96555  14.943%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.827638/  1.973414, val:  72.08%, val_best:  72.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2344%\n",
      "layer   2  Sparsity: 82.2394%\n",
      "layer   3  Sparsity: 84.0141%\n",
      "total_backward_count 655930 real_backward_count 97832  14.915%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.818050/  1.976275, val:  54.58%, val_best:  72.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2181%\n",
      "layer   2  Sparsity: 82.1217%\n",
      "layer   3  Sparsity: 84.2368%\n",
      "total_backward_count 665720 real_backward_count 99070  14.882%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.813735/  1.938387, val:  60.42%, val_best:  72.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2316%\n",
      "layer   2  Sparsity: 82.3452%\n",
      "layer   3  Sparsity: 83.7959%\n",
      "total_backward_count 675510 real_backward_count 100293  14.847%\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.804597/  1.972708, val:  62.50%, val_best:  72.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2417%\n",
      "layer   2  Sparsity: 82.7008%\n",
      "layer   3  Sparsity: 84.5994%\n",
      "total_backward_count 685300 real_backward_count 101565  14.821%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.819811/  1.964645, val:  60.42%, val_best:  72.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2634%\n",
      "layer   2  Sparsity: 82.5293%\n",
      "layer   3  Sparsity: 84.5503%\n",
      "total_backward_count 695090 real_backward_count 102824  14.793%\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.809718/  1.959007, val:  56.67%, val_best:  72.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2408%\n",
      "layer   2  Sparsity: 81.9920%\n",
      "layer   3  Sparsity: 83.7023%\n",
      "total_backward_count 704880 real_backward_count 104041  14.760%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.823122/  1.938378, val:  64.58%, val_best:  72.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2504%\n",
      "layer   2  Sparsity: 82.0147%\n",
      "layer   3  Sparsity: 84.0286%\n",
      "total_backward_count 714670 real_backward_count 105321  14.737%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.814020/  1.959909, val:  62.50%, val_best:  72.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2373%\n",
      "layer   2  Sparsity: 81.8903%\n",
      "layer   3  Sparsity: 84.0065%\n",
      "total_backward_count 724460 real_backward_count 106593  14.713%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.818830/  1.956926, val:  51.25%, val_best:  72.08%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2526%\n",
      "layer   2  Sparsity: 81.7089%\n",
      "layer   3  Sparsity: 83.7978%\n",
      "total_backward_count 734250 real_backward_count 107921  14.698%\n",
      "fc layer 2 self.abs_max_out: 3376.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.813364/  1.947929, val:  62.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2447%\n",
      "layer   2  Sparsity: 81.2823%\n",
      "layer   3  Sparsity: 83.5865%\n",
      "total_backward_count 744040 real_backward_count 109106  14.664%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.806713/  1.958837, val:  62.92%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2655%\n",
      "layer   2  Sparsity: 81.5428%\n",
      "layer   3  Sparsity: 84.0648%\n",
      "total_backward_count 753830 real_backward_count 110351  14.639%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.801367/  1.956329, val:  54.58%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2668%\n",
      "layer   2  Sparsity: 81.5870%\n",
      "layer   3  Sparsity: 83.9356%\n",
      "total_backward_count 763620 real_backward_count 111554  14.609%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.797001/  1.970611, val:  54.58%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2868%\n",
      "layer   2  Sparsity: 81.7022%\n",
      "layer   3  Sparsity: 83.9883%\n",
      "total_backward_count 773410 real_backward_count 112786  14.583%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.809616/  1.946913, val:  63.75%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2409%\n",
      "layer   2  Sparsity: 81.3211%\n",
      "layer   3  Sparsity: 84.2074%\n",
      "total_backward_count 783200 real_backward_count 114017  14.558%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.798004/  1.945388, val:  58.33%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2288%\n",
      "layer   2  Sparsity: 81.1376%\n",
      "layer   3  Sparsity: 83.5979%\n",
      "total_backward_count 792990 real_backward_count 115285  14.538%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.801889/  1.961760, val:  52.50%, val_best:  72.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2431%\n",
      "layer   2  Sparsity: 81.1705%\n",
      "layer   3  Sparsity: 84.3749%\n",
      "total_backward_count 802780 real_backward_count 116505  14.513%\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.779251/  1.928003, val:  66.25%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2389%\n",
      "layer   2  Sparsity: 81.1374%\n",
      "layer   3  Sparsity: 83.0120%\n",
      "total_backward_count 812570 real_backward_count 117767  14.493%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.782251/  1.930566, val:  55.83%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2129%\n",
      "layer   2  Sparsity: 81.5311%\n",
      "layer   3  Sparsity: 83.1437%\n",
      "total_backward_count 822360 real_backward_count 119008  14.472%\n",
      "fc layer 1 self.abs_max_out: 9920.0\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.764553/  1.913595, val:  60.83%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2381%\n",
      "layer   2  Sparsity: 81.2253%\n",
      "layer   3  Sparsity: 82.5628%\n",
      "total_backward_count 832150 real_backward_count 120246  14.450%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.761590/  1.925239, val:  66.25%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2697%\n",
      "layer   2  Sparsity: 81.0089%\n",
      "layer   3  Sparsity: 82.8681%\n",
      "total_backward_count 841940 real_backward_count 121510  14.432%\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.751599/  1.908280, val:  65.83%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2390%\n",
      "layer   2  Sparsity: 80.5619%\n",
      "layer   3  Sparsity: 82.7744%\n",
      "total_backward_count 851730 real_backward_count 122670  14.402%\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.756850/  1.903739, val:  65.42%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.1934%\n",
      "layer   2  Sparsity: 80.5054%\n",
      "layer   3  Sparsity: 82.4097%\n",
      "total_backward_count 861520 real_backward_count 123890  14.380%\n",
      "fc layer 1 self.abs_max_out: 10114.0\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.756842/  1.921616, val:  60.00%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2125%\n",
      "layer   2  Sparsity: 80.8524%\n",
      "layer   3  Sparsity: 82.4231%\n",
      "total_backward_count 871310 real_backward_count 125079  14.355%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.760180/  1.924262, val:  64.17%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2607%\n",
      "layer   2  Sparsity: 80.8764%\n",
      "layer   3  Sparsity: 82.5837%\n",
      "total_backward_count 881100 real_backward_count 126250  14.329%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.785529/  1.942727, val:  58.33%, val_best:  72.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2469%\n",
      "layer   2  Sparsity: 80.8278%\n",
      "layer   3  Sparsity: 82.6661%\n",
      "total_backward_count 890890 real_backward_count 127488  14.310%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.765286/  1.895975, val:  72.08%, val_best:  72.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2102%\n",
      "layer   2  Sparsity: 81.0173%\n",
      "layer   3  Sparsity: 82.8295%\n",
      "total_backward_count 900680 real_backward_count 128639  14.282%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.762544/  1.898748, val:  70.83%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2585%\n",
      "layer   2  Sparsity: 80.7474%\n",
      "layer   3  Sparsity: 82.9472%\n",
      "total_backward_count 910470 real_backward_count 129817  14.258%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.760641/  1.909665, val:  71.67%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2547%\n",
      "layer   2  Sparsity: 81.0680%\n",
      "layer   3  Sparsity: 82.8925%\n",
      "total_backward_count 920260 real_backward_count 130986  14.234%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.766109/  1.910753, val:  63.33%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2693%\n",
      "layer   2  Sparsity: 81.2048%\n",
      "layer   3  Sparsity: 83.4333%\n",
      "total_backward_count 930050 real_backward_count 132123  14.206%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.768674/  1.911148, val:  71.67%, val_best:  72.08%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2700%\n",
      "layer   2  Sparsity: 81.0889%\n",
      "layer   3  Sparsity: 83.5761%\n",
      "total_backward_count 939840 real_backward_count 133358  14.189%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.778324/  1.946209, val:  55.42%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2132%\n",
      "layer   2  Sparsity: 81.3326%\n",
      "layer   3  Sparsity: 84.1444%\n",
      "total_backward_count 949630 real_backward_count 134565  14.170%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.777599/  1.908793, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2555%\n",
      "layer   2  Sparsity: 81.2438%\n",
      "layer   3  Sparsity: 83.8844%\n",
      "total_backward_count 959420 real_backward_count 135713  14.145%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.765859/  1.947242, val:  57.92%, val_best:  73.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2590%\n",
      "layer   2  Sparsity: 81.3973%\n",
      "layer   3  Sparsity: 83.3807%\n",
      "total_backward_count 969210 real_backward_count 136857  14.120%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.782509/  1.922337, val:  73.33%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2312%\n",
      "layer   2  Sparsity: 81.2325%\n",
      "layer   3  Sparsity: 83.7761%\n",
      "total_backward_count 979000 real_backward_count 138022  14.098%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.763759/  1.927281, val:  62.92%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2396%\n",
      "layer   2  Sparsity: 81.4047%\n",
      "layer   3  Sparsity: 83.4139%\n",
      "total_backward_count 988790 real_backward_count 139210  14.079%\n",
      "lif layer 2 self.abs_max_v: 4934.0\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.765194/  1.880641, val:  67.50%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2351%\n",
      "layer   2  Sparsity: 81.5530%\n",
      "layer   3  Sparsity: 82.9334%\n",
      "total_backward_count 998580 real_backward_count 140346  14.055%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.750218/  1.917849, val:  59.58%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3042%\n",
      "layer   2  Sparsity: 81.3820%\n",
      "layer   3  Sparsity: 83.4729%\n",
      "total_backward_count 1008370 real_backward_count 141494  14.032%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.769104/  1.935680, val:  67.08%, val_best:  73.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2359%\n",
      "layer   2  Sparsity: 81.1342%\n",
      "layer   3  Sparsity: 83.5273%\n",
      "total_backward_count 1018160 real_backward_count 142665  14.012%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.776622/  1.914513, val:  63.75%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2787%\n",
      "layer   2  Sparsity: 80.9555%\n",
      "layer   3  Sparsity: 83.3787%\n",
      "total_backward_count 1027950 real_backward_count 143839  13.993%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.775433/  1.921311, val:  69.17%, val_best:  73.33%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2527%\n",
      "layer   2  Sparsity: 81.0953%\n",
      "layer   3  Sparsity: 83.4262%\n",
      "total_backward_count 1037740 real_backward_count 145030  13.976%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.762935/  1.907809, val:  54.17%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2411%\n",
      "layer   2  Sparsity: 81.2416%\n",
      "layer   3  Sparsity: 83.5052%\n",
      "total_backward_count 1047530 real_backward_count 146113  13.948%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.777072/  1.937368, val:  60.00%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2501%\n",
      "layer   2  Sparsity: 81.4316%\n",
      "layer   3  Sparsity: 83.5661%\n",
      "total_backward_count 1057320 real_backward_count 147275  13.929%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.764076/  1.919290, val:  65.42%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2403%\n",
      "layer   2  Sparsity: 81.0113%\n",
      "layer   3  Sparsity: 83.5228%\n",
      "total_backward_count 1067110 real_backward_count 148436  13.910%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.752722/  1.935719, val:  64.17%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.1978%\n",
      "layer   2  Sparsity: 81.0208%\n",
      "layer   3  Sparsity: 83.2783%\n",
      "total_backward_count 1076900 real_backward_count 149575  13.889%\n",
      "fc layer 1 self.abs_max_out: 10305.0\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.754354/  1.870765, val:  69.17%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2865%\n",
      "layer   2  Sparsity: 81.2893%\n",
      "layer   3  Sparsity: 83.6358%\n",
      "total_backward_count 1086690 real_backward_count 150786  13.876%\n",
      "fc layer 2 self.abs_max_out: 3377.0\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.731129/  1.901029, val:  64.58%, val_best:  73.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2732%\n",
      "layer   2  Sparsity: 80.9486%\n",
      "layer   3  Sparsity: 83.2704%\n",
      "total_backward_count 1096480 real_backward_count 151902  13.854%\n",
      "lif layer 2 self.abs_max_v: 5156.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.749700/  1.902282, val:  74.17%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2693%\n",
      "layer   2  Sparsity: 80.8265%\n",
      "layer   3  Sparsity: 83.7436%\n",
      "total_backward_count 1106270 real_backward_count 153014  13.832%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.743578/  1.908110, val:  67.92%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.1675%\n",
      "layer   2  Sparsity: 80.9681%\n",
      "layer   3  Sparsity: 83.6932%\n",
      "total_backward_count 1116060 real_backward_count 154171  13.814%\n",
      "fc layer 2 self.abs_max_out: 3394.0\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.756816/  1.931769, val:  57.08%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2473%\n",
      "layer   2  Sparsity: 80.8653%\n",
      "layer   3  Sparsity: 83.4868%\n",
      "total_backward_count 1125850 real_backward_count 155304  13.794%\n",
      "lif layer 2 self.abs_max_v: 5193.5\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.750503/  1.926835, val:  64.17%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2538%\n",
      "layer   2  Sparsity: 81.1235%\n",
      "layer   3  Sparsity: 83.1485%\n",
      "total_backward_count 1135640 real_backward_count 156414  13.773%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.741753/  1.906885, val:  59.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2424%\n",
      "layer   2  Sparsity: 80.9443%\n",
      "layer   3  Sparsity: 82.9683%\n",
      "total_backward_count 1145430 real_backward_count 157550  13.755%\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.748286/  1.901303, val:  75.42%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2837%\n",
      "layer   2  Sparsity: 80.7703%\n",
      "layer   3  Sparsity: 82.9599%\n",
      "total_backward_count 1155220 real_backward_count 158624  13.731%\n",
      "lif layer 2 self.abs_max_v: 5216.0\n",
      "lif layer 2 self.abs_max_v: 5605.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.731923/  1.875565, val:  63.75%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2291%\n",
      "layer   2  Sparsity: 80.6824%\n",
      "layer   3  Sparsity: 82.5535%\n",
      "total_backward_count 1165010 real_backward_count 159706  13.709%\n",
      "fc layer 2 self.abs_max_out: 3462.0\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.714177/  1.867238, val:  71.67%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2355%\n",
      "layer   2  Sparsity: 80.6866%\n",
      "layer   3  Sparsity: 82.2969%\n",
      "total_backward_count 1174800 real_backward_count 160827  13.690%\n",
      "lif layer 1 self.abs_max_v: 16866.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.713426/  1.876502, val:  68.75%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2345%\n",
      "layer   2  Sparsity: 80.6756%\n",
      "layer   3  Sparsity: 82.2696%\n",
      "total_backward_count 1184590 real_backward_count 161849  13.663%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.707987/  1.882632, val:  65.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2703%\n",
      "layer   2  Sparsity: 80.5583%\n",
      "layer   3  Sparsity: 82.7759%\n",
      "total_backward_count 1194380 real_backward_count 162971  13.645%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.702816/  1.871913, val:  67.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2390%\n",
      "layer   2  Sparsity: 80.7390%\n",
      "layer   3  Sparsity: 82.7063%\n",
      "total_backward_count 1204170 real_backward_count 164126  13.630%\n",
      "lif layer 2 self.abs_max_v: 5634.5\n",
      "lif layer 2 self.abs_max_v: 5860.5\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.701643/  1.855428, val:  62.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2314%\n",
      "layer   2  Sparsity: 80.4186%\n",
      "layer   3  Sparsity: 82.1952%\n",
      "total_backward_count 1213960 real_backward_count 165273  13.614%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.692071/  1.833593, val:  72.92%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2580%\n",
      "layer   2  Sparsity: 80.2887%\n",
      "layer   3  Sparsity: 82.8217%\n",
      "total_backward_count 1223750 real_backward_count 166345  13.593%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.703849/  1.877531, val:  59.58%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2099%\n",
      "layer   2  Sparsity: 80.6258%\n",
      "layer   3  Sparsity: 82.6701%\n",
      "total_backward_count 1233540 real_backward_count 167409  13.571%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.690339/  1.878425, val:  65.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2678%\n",
      "layer   2  Sparsity: 80.6436%\n",
      "layer   3  Sparsity: 81.6872%\n",
      "total_backward_count 1243330 real_backward_count 168476  13.550%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.682749/  1.862040, val:  69.17%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2795%\n",
      "layer   2  Sparsity: 80.6577%\n",
      "layer   3  Sparsity: 82.0217%\n",
      "total_backward_count 1253120 real_backward_count 169507  13.527%\n",
      "fc layer 3 self.abs_max_out: 812.0\n",
      "fc layer 3 self.abs_max_out: 864.0\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.684476/  1.868179, val:  74.17%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2284%\n",
      "layer   2  Sparsity: 80.6119%\n",
      "layer   3  Sparsity: 82.5471%\n",
      "total_backward_count 1262910 real_backward_count 170582  13.507%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.686908/  1.859856, val:  73.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2419%\n",
      "layer   2  Sparsity: 80.6255%\n",
      "layer   3  Sparsity: 82.9774%\n",
      "total_backward_count 1272700 real_backward_count 171555  13.480%\n",
      "lif layer 2 self.abs_max_v: 6082.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.718081/  1.875878, val:  57.92%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2263%\n",
      "layer   2  Sparsity: 80.6337%\n",
      "layer   3  Sparsity: 83.1172%\n",
      "total_backward_count 1282490 real_backward_count 172657  13.463%\n",
      "lif layer 2 self.abs_max_v: 6116.5\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.695573/  1.855780, val:  60.42%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2350%\n",
      "layer   2  Sparsity: 80.3048%\n",
      "layer   3  Sparsity: 82.5152%\n",
      "total_backward_count 1292280 real_backward_count 173831  13.451%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.694328/  1.870064, val:  51.25%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2620%\n",
      "layer   2  Sparsity: 80.4882%\n",
      "layer   3  Sparsity: 82.4580%\n",
      "total_backward_count 1302070 real_backward_count 174891  13.432%\n",
      "lif layer 2 self.abs_max_v: 6207.0\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.702347/  1.879498, val:  66.67%, val_best:  75.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2515%\n",
      "layer   2  Sparsity: 80.2597%\n",
      "layer   3  Sparsity: 82.3629%\n",
      "total_backward_count 1311860 real_backward_count 175970  13.414%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.690513/  1.852664, val:  62.50%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2520%\n",
      "layer   2  Sparsity: 80.5276%\n",
      "layer   3  Sparsity: 82.4379%\n",
      "total_backward_count 1321650 real_backward_count 177042  13.396%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.703466/  1.850627, val:  63.75%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2085%\n",
      "layer   2  Sparsity: 80.9712%\n",
      "layer   3  Sparsity: 82.9066%\n",
      "total_backward_count 1331440 real_backward_count 178113  13.377%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.707632/  1.872904, val:  74.17%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2508%\n",
      "layer   2  Sparsity: 81.0822%\n",
      "layer   3  Sparsity: 82.7219%\n",
      "total_backward_count 1341230 real_backward_count 179251  13.365%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.722876/  1.866209, val:  68.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2765%\n",
      "layer   2  Sparsity: 80.9255%\n",
      "layer   3  Sparsity: 83.1185%\n",
      "total_backward_count 1351020 real_backward_count 180334  13.348%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.702845/  1.860391, val:  72.92%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2303%\n",
      "layer   2  Sparsity: 80.8013%\n",
      "layer   3  Sparsity: 82.5397%\n",
      "total_backward_count 1360810 real_backward_count 181485  13.337%\n",
      "fc layer 2 self.abs_max_out: 3636.0\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.704610/  1.843600, val:  70.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2575%\n",
      "layer   2  Sparsity: 80.8821%\n",
      "layer   3  Sparsity: 82.3026%\n",
      "total_backward_count 1370600 real_backward_count 182611  13.323%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.677320/  1.871632, val:  63.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.1957%\n",
      "layer   2  Sparsity: 80.5934%\n",
      "layer   3  Sparsity: 81.6171%\n",
      "total_backward_count 1380390 real_backward_count 183717  13.309%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.689941/  1.861802, val:  73.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2300%\n",
      "layer   2  Sparsity: 80.7504%\n",
      "layer   3  Sparsity: 82.2551%\n",
      "total_backward_count 1390180 real_backward_count 184782  13.292%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.700034/  1.870946, val:  67.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2352%\n",
      "layer   2  Sparsity: 80.7073%\n",
      "layer   3  Sparsity: 81.5955%\n",
      "total_backward_count 1399970 real_backward_count 185849  13.275%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.691966/  1.864529, val:  67.08%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2277%\n",
      "layer   2  Sparsity: 80.6378%\n",
      "layer   3  Sparsity: 81.7380%\n",
      "total_backward_count 1409760 real_backward_count 186946  13.261%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.691344/  1.848601, val:  66.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2523%\n",
      "layer   2  Sparsity: 80.8499%\n",
      "layer   3  Sparsity: 81.7005%\n",
      "total_backward_count 1419550 real_backward_count 187989  13.243%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.673440/  1.843037, val:  61.25%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2756%\n",
      "layer   2  Sparsity: 80.8852%\n",
      "layer   3  Sparsity: 82.1723%\n",
      "total_backward_count 1429340 real_backward_count 189052  13.227%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.680437/  1.860200, val:  67.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2632%\n",
      "layer   2  Sparsity: 80.7818%\n",
      "layer   3  Sparsity: 82.2275%\n",
      "total_backward_count 1439130 real_backward_count 190098  13.209%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.692808/  1.869033, val:  65.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2501%\n",
      "layer   2  Sparsity: 80.7888%\n",
      "layer   3  Sparsity: 82.5011%\n",
      "total_backward_count 1448920 real_backward_count 191107  13.190%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.692569/  1.863079, val:  68.33%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2479%\n",
      "layer   2  Sparsity: 80.8713%\n",
      "layer   3  Sparsity: 82.0827%\n",
      "total_backward_count 1458710 real_backward_count 192181  13.175%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.691210/  1.856155, val:  67.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2377%\n",
      "layer   2  Sparsity: 81.0364%\n",
      "layer   3  Sparsity: 82.6919%\n",
      "total_backward_count 1468500 real_backward_count 193238  13.159%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.694770/  1.880418, val:  67.08%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3077%\n",
      "layer   2  Sparsity: 81.0543%\n",
      "layer   3  Sparsity: 82.5290%\n",
      "total_backward_count 1478290 real_backward_count 194234  13.139%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.693394/  1.844125, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2606%\n",
      "layer   2  Sparsity: 80.6241%\n",
      "layer   3  Sparsity: 82.6038%\n",
      "total_backward_count 1488080 real_backward_count 195310  13.125%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.679285/  1.867876, val:  59.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2640%\n",
      "layer   2  Sparsity: 80.2062%\n",
      "layer   3  Sparsity: 82.2569%\n",
      "total_backward_count 1497870 real_backward_count 196312  13.106%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.694023/  1.845527, val:  75.83%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2133%\n",
      "layer   2  Sparsity: 80.9263%\n",
      "layer   3  Sparsity: 82.7270%\n",
      "total_backward_count 1507660 real_backward_count 197358  13.090%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.678838/  1.847540, val:  64.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2469%\n",
      "layer   2  Sparsity: 80.8379%\n",
      "layer   3  Sparsity: 82.7168%\n",
      "total_backward_count 1517450 real_backward_count 198365  13.072%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.666146/  1.842093, val:  59.58%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2762%\n",
      "layer   2  Sparsity: 80.6701%\n",
      "layer   3  Sparsity: 82.2107%\n",
      "total_backward_count 1527240 real_backward_count 199366  13.054%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.685806/  1.864184, val:  63.75%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2268%\n",
      "layer   2  Sparsity: 80.4194%\n",
      "layer   3  Sparsity: 82.3473%\n",
      "total_backward_count 1537030 real_backward_count 200378  13.037%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.678220/  1.845477, val:  65.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2405%\n",
      "layer   2  Sparsity: 80.7839%\n",
      "layer   3  Sparsity: 82.2780%\n",
      "total_backward_count 1546820 real_backward_count 201367  13.018%\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.657793/  1.854263, val:  68.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2725%\n",
      "layer   2  Sparsity: 80.9950%\n",
      "layer   3  Sparsity: 82.4457%\n",
      "total_backward_count 1556610 real_backward_count 202449  13.006%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.655316/  1.845036, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2812%\n",
      "layer   2  Sparsity: 80.9420%\n",
      "layer   3  Sparsity: 82.1071%\n",
      "total_backward_count 1566400 real_backward_count 203433  12.987%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.668924/  1.853828, val:  70.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2809%\n",
      "layer   2  Sparsity: 80.7056%\n",
      "layer   3  Sparsity: 81.8449%\n",
      "total_backward_count 1576190 real_backward_count 204421  12.969%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.663850/  1.856408, val:  65.83%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2489%\n",
      "layer   2  Sparsity: 80.5599%\n",
      "layer   3  Sparsity: 81.7779%\n",
      "total_backward_count 1585980 real_backward_count 205409  12.952%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.663243/  1.831414, val:  68.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2376%\n",
      "layer   2  Sparsity: 80.8525%\n",
      "layer   3  Sparsity: 81.6775%\n",
      "total_backward_count 1595770 real_backward_count 206442  12.937%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.653258/  1.816757, val:  69.17%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2404%\n",
      "layer   2  Sparsity: 80.7845%\n",
      "layer   3  Sparsity: 81.2775%\n",
      "total_backward_count 1605560 real_backward_count 207422  12.919%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.662757/  1.830228, val:  64.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2394%\n",
      "layer   2  Sparsity: 80.4422%\n",
      "layer   3  Sparsity: 82.3855%\n",
      "total_backward_count 1615350 real_backward_count 208425  12.903%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.664737/  1.824100, val:  75.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2578%\n",
      "layer   2  Sparsity: 80.9016%\n",
      "layer   3  Sparsity: 82.4388%\n",
      "total_backward_count 1625140 real_backward_count 209456  12.888%\n",
      "fc layer 3 self.abs_max_out: 872.0\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.658796/  1.865314, val:  68.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2784%\n",
      "layer   2  Sparsity: 81.1126%\n",
      "layer   3  Sparsity: 82.3900%\n",
      "total_backward_count 1634930 real_backward_count 210400  12.869%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.675962/  1.855543, val:  66.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2087%\n",
      "layer   2  Sparsity: 80.9817%\n",
      "layer   3  Sparsity: 82.0410%\n",
      "total_backward_count 1644720 real_backward_count 211349  12.850%\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.676798/  1.830265, val:  66.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2880%\n",
      "layer   2  Sparsity: 81.0326%\n",
      "layer   3  Sparsity: 82.3197%\n",
      "total_backward_count 1654510 real_backward_count 212327  12.833%\n",
      "lif layer 1 self.abs_max_v: 18074.5\n",
      "lif layer 1 self.abs_max_v: 18646.5\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.661261/  1.844091, val:  71.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2589%\n",
      "layer   2  Sparsity: 80.8801%\n",
      "layer   3  Sparsity: 82.4046%\n",
      "total_backward_count 1664300 real_backward_count 213291  12.816%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.658958/  1.849963, val:  62.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2554%\n",
      "layer   2  Sparsity: 80.7273%\n",
      "layer   3  Sparsity: 82.1440%\n",
      "total_backward_count 1674090 real_backward_count 214253  12.798%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.678142/  1.870650, val:  57.50%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2112%\n",
      "layer   2  Sparsity: 80.7527%\n",
      "layer   3  Sparsity: 81.8874%\n",
      "total_backward_count 1683880 real_backward_count 215258  12.783%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.685683/  1.844184, val:  70.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2500%\n",
      "layer   2  Sparsity: 80.8033%\n",
      "layer   3  Sparsity: 82.0416%\n",
      "total_backward_count 1693670 real_backward_count 216332  12.773%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.652573/  1.827490, val:  65.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2235%\n",
      "layer   2  Sparsity: 80.6656%\n",
      "layer   3  Sparsity: 81.7647%\n",
      "total_backward_count 1703460 real_backward_count 217288  12.756%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.667115/  1.854979, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2264%\n",
      "layer   2  Sparsity: 80.6258%\n",
      "layer   3  Sparsity: 81.6399%\n",
      "total_backward_count 1713250 real_backward_count 218290  12.741%\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.682123/  1.834846, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.2038%\n",
      "layer   2  Sparsity: 80.2625%\n",
      "layer   3  Sparsity: 81.4228%\n",
      "total_backward_count 1723040 real_backward_count 219307  12.728%\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.654939/  1.839558, val:  71.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2441%\n",
      "layer   2  Sparsity: 80.2792%\n",
      "layer   3  Sparsity: 81.1569%\n",
      "total_backward_count 1732830 real_backward_count 220295  12.713%\n",
      "fc layer 1 self.abs_max_out: 10817.0\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.645309/  1.814627, val:  67.92%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2531%\n",
      "layer   2  Sparsity: 80.4860%\n",
      "layer   3  Sparsity: 81.2059%\n",
      "total_backward_count 1742620 real_backward_count 221249  12.696%\n",
      "fc layer 1 self.abs_max_out: 10966.0\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.641459/  1.848560, val:  61.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2322%\n",
      "layer   2  Sparsity: 80.6647%\n",
      "layer   3  Sparsity: 81.4990%\n",
      "total_backward_count 1752410 real_backward_count 222214  12.680%\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.640387/  1.835055, val:  72.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2606%\n",
      "layer   2  Sparsity: 80.8096%\n",
      "layer   3  Sparsity: 81.9973%\n",
      "total_backward_count 1762200 real_backward_count 223195  12.666%\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.664915/  1.824684, val:  62.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2202%\n",
      "layer   2  Sparsity: 80.9246%\n",
      "layer   3  Sparsity: 82.3498%\n",
      "total_backward_count 1771990 real_backward_count 224163  12.650%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.649573/  1.836877, val:  67.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2464%\n",
      "layer   2  Sparsity: 80.9105%\n",
      "layer   3  Sparsity: 81.7609%\n",
      "total_backward_count 1781780 real_backward_count 225152  12.636%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.648004/  1.803205, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.1979%\n",
      "layer   2  Sparsity: 80.9889%\n",
      "layer   3  Sparsity: 81.6748%\n",
      "total_backward_count 1791570 real_backward_count 226148  12.623%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.637599/  1.834869, val:  60.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2394%\n",
      "layer   2  Sparsity: 81.2718%\n",
      "layer   3  Sparsity: 81.4725%\n",
      "total_backward_count 1801360 real_backward_count 227083  12.606%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.632747/  1.827324, val:  62.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2576%\n",
      "layer   2  Sparsity: 81.1361%\n",
      "layer   3  Sparsity: 81.4400%\n",
      "total_backward_count 1811150 real_backward_count 228057  12.592%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.629661/  1.846056, val:  61.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2230%\n",
      "layer   2  Sparsity: 80.9982%\n",
      "layer   3  Sparsity: 81.1018%\n",
      "total_backward_count 1820940 real_backward_count 229058  12.579%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.629333/  1.821788, val:  66.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2198%\n",
      "layer   2  Sparsity: 80.8345%\n",
      "layer   3  Sparsity: 81.1559%\n",
      "total_backward_count 1830730 real_backward_count 230076  12.567%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.628457/  1.829651, val:  62.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.2438%\n",
      "layer   2  Sparsity: 81.2617%\n",
      "layer   3  Sparsity: 81.5572%\n",
      "total_backward_count 1840520 real_backward_count 231100  12.556%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.641579/  1.844704, val:  69.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2322%\n",
      "layer   2  Sparsity: 81.1716%\n",
      "layer   3  Sparsity: 81.3906%\n",
      "total_backward_count 1850310 real_backward_count 232049  12.541%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.619717/  1.807230, val:  63.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2052%\n",
      "layer   2  Sparsity: 81.0515%\n",
      "layer   3  Sparsity: 81.0527%\n",
      "total_backward_count 1860100 real_backward_count 232955  12.524%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.628590/  1.789817, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2057%\n",
      "layer   2  Sparsity: 81.0470%\n",
      "layer   3  Sparsity: 81.4048%\n",
      "total_backward_count 1869890 real_backward_count 233871  12.507%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.624811/  1.834248, val:  62.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2449%\n",
      "layer   2  Sparsity: 81.2069%\n",
      "layer   3  Sparsity: 81.6997%\n",
      "total_backward_count 1879680 real_backward_count 234821  12.493%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.624688/  1.827754, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2418%\n",
      "layer   2  Sparsity: 81.0013%\n",
      "layer   3  Sparsity: 81.8740%\n",
      "total_backward_count 1889470 real_backward_count 235773  12.478%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.610845/  1.847428, val:  66.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2634%\n",
      "layer   2  Sparsity: 80.6664%\n",
      "layer   3  Sparsity: 81.8055%\n",
      "total_backward_count 1899260 real_backward_count 236704  12.463%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.653589/  1.816487, val:  63.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2497%\n",
      "layer   2  Sparsity: 80.4041%\n",
      "layer   3  Sparsity: 81.8666%\n",
      "total_backward_count 1909050 real_backward_count 237674  12.450%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.639629/  1.833967, val:  71.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.2015%\n",
      "layer   2  Sparsity: 80.5844%\n",
      "layer   3  Sparsity: 81.7607%\n",
      "total_backward_count 1918840 real_backward_count 238646  12.437%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.635666/  1.809438, val:  67.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2656%\n",
      "layer   2  Sparsity: 80.8630%\n",
      "layer   3  Sparsity: 81.3868%\n",
      "total_backward_count 1928630 real_backward_count 239634  12.425%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.628957/  1.814844, val:  63.33%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3299%\n",
      "layer   2  Sparsity: 80.5254%\n",
      "layer   3  Sparsity: 81.1784%\n",
      "total_backward_count 1938420 real_backward_count 240609  12.413%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.621593/  1.802925, val:  72.92%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.2320%\n",
      "layer   2  Sparsity: 80.7811%\n",
      "layer   3  Sparsity: 81.3322%\n",
      "total_backward_count 1948210 real_backward_count 241553  12.399%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.621724/  1.788940, val:  67.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.2550%\n",
      "layer   2  Sparsity: 80.6500%\n",
      "layer   3  Sparsity: 81.1757%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10291335a6e48f4aa0f2c9c1a4f7b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.62172</td></tr><tr><td>val_acc_best</td><td>0.7625</td></tr><tr><td>val_acc_now</td><td>0.67083</td></tr><tr><td>val_loss</td><td>1.78894</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-sweep-111</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0my51tv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0my51tv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_162613-j0my51tv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x115sum6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_204939-x115sum6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x115sum6' target=\"_blank\">floral-sweep-117</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x115sum6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x115sum6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251116_204948_443', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 125.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 193.0\n",
      "fc layer 1 self.abs_max_out: 214.0\n",
      "lif layer 1 self.abs_max_v: 284.0\n",
      "lif layer 1 self.abs_max_v: 310.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 1 self.abs_max_out: 217.0\n",
      "fc layer 2 self.abs_max_out: 88.0\n",
      "lif layer 2 self.abs_max_v: 97.0\n",
      "lif layer 1 self.abs_max_v: 310.5\n",
      "fc layer 1 self.abs_max_out: 229.0\n",
      "lif layer 1 self.abs_max_v: 380.5\n",
      "fc layer 1 self.abs_max_out: 299.0\n",
      "lif layer 1 self.abs_max_v: 413.5\n",
      "fc layer 2 self.abs_max_out: 90.0\n",
      "lif layer 2 self.abs_max_v: 115.5\n",
      "fc layer 1 self.abs_max_out: 327.0\n",
      "fc layer 2 self.abs_max_out: 158.0\n",
      "lif layer 2 self.abs_max_v: 152.0\n",
      "lif layer 1 self.abs_max_v: 452.0\n",
      "fc layer 2 self.abs_max_out: 214.0\n",
      "lif layer 2 self.abs_max_v: 216.5\n",
      "fc layer 1 self.abs_max_out: 528.0\n",
      "lif layer 1 self.abs_max_v: 528.0\n",
      "lif layer 2 self.abs_max_v: 266.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 1 self.abs_max_out: 581.0\n",
      "lif layer 1 self.abs_max_v: 581.0\n",
      "fc layer 1 self.abs_max_out: 637.0\n",
      "lif layer 1 self.abs_max_v: 637.0\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 647.0\n",
      "lif layer 1 self.abs_max_v: 651.0\n",
      "lif layer 2 self.abs_max_v: 282.5\n",
      "lif layer 2 self.abs_max_v: 289.0\n",
      "fc layer 2 self.abs_max_out: 268.0\n",
      "lif layer 2 self.abs_max_v: 313.5\n",
      "fc layer 3 self.abs_max_out: 25.0\n",
      "lif layer 2 self.abs_max_v: 328.0\n",
      "lif layer 2 self.abs_max_v: 335.0\n",
      "lif layer 2 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 392.5\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "fc layer 2 self.abs_max_out: 348.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "lif layer 2 self.abs_max_v: 414.5\n",
      "fc layer 3 self.abs_max_out: 41.0\n",
      "fc layer 1 self.abs_max_out: 668.0\n",
      "lif layer 1 self.abs_max_v: 668.0\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 509.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 1 self.abs_max_v: 710.0\n",
      "fc layer 1 self.abs_max_out: 783.0\n",
      "lif layer 1 self.abs_max_v: 783.0\n",
      "lif layer 2 self.abs_max_v: 525.5\n",
      "fc layer 1 self.abs_max_out: 1059.0\n",
      "lif layer 1 self.abs_max_v: 1059.0\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "lif layer 2 self.abs_max_v: 593.5\n",
      "fc layer 1 self.abs_max_out: 1161.0\n",
      "lif layer 1 self.abs_max_v: 1161.0\n",
      "fc layer 2 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "fc layer 2 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 526.0\n",
      "lif layer 2 self.abs_max_v: 636.5\n",
      "fc layer 1 self.abs_max_out: 1212.0\n",
      "lif layer 1 self.abs_max_v: 1212.0\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "lif layer 2 self.abs_max_v: 708.5\n",
      "lif layer 2 self.abs_max_v: 749.5\n",
      "lif layer 2 self.abs_max_v: 811.5\n",
      "fc layer 2 self.abs_max_out: 563.0\n",
      "fc layer 2 self.abs_max_out: 566.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 662.0\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "lif layer 2 self.abs_max_v: 833.5\n",
      "fc layer 1 self.abs_max_out: 1282.0\n",
      "lif layer 1 self.abs_max_v: 1282.0\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 1 self.abs_max_out: 1328.0\n",
      "lif layer 1 self.abs_max_v: 1328.0\n",
      "fc layer 2 self.abs_max_out: 687.0\n",
      "fc layer 2 self.abs_max_out: 794.0\n",
      "fc layer 1 self.abs_max_out: 1491.0\n",
      "lif layer 1 self.abs_max_v: 1491.0\n",
      "lif layer 2 self.abs_max_v: 870.0\n",
      "fc layer 1 self.abs_max_out: 1509.0\n",
      "lif layer 1 self.abs_max_v: 1509.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 1 self.abs_max_out: 1656.0\n",
      "lif layer 1 self.abs_max_v: 1656.0\n",
      "fc layer 1 self.abs_max_out: 1757.0\n",
      "lif layer 1 self.abs_max_v: 1757.0\n",
      "fc layer 3 self.abs_max_out: 132.0\n",
      "fc layer 2 self.abs_max_out: 798.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "lif layer 1 self.abs_max_v: 1796.0\n",
      "fc layer 1 self.abs_max_out: 1797.0\n",
      "lif layer 1 self.abs_max_v: 1797.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 1 self.abs_max_out: 1851.0\n",
      "lif layer 1 self.abs_max_v: 1851.0\n",
      "lif layer 2 self.abs_max_v: 894.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 931.0\n",
      "fc layer 1 self.abs_max_out: 1865.0\n",
      "lif layer 1 self.abs_max_v: 1865.0\n",
      "fc layer 2 self.abs_max_out: 1025.0\n",
      "lif layer 2 self.abs_max_v: 1025.0\n",
      "fc layer 1 self.abs_max_out: 1966.0\n",
      "lif layer 1 self.abs_max_v: 1966.0\n",
      "fc layer 1 self.abs_max_out: 2013.0\n",
      "lif layer 1 self.abs_max_v: 2013.0\n",
      "fc layer 1 self.abs_max_out: 2119.0\n",
      "lif layer 1 self.abs_max_v: 2119.0\n",
      "fc layer 1 self.abs_max_out: 2124.0\n",
      "lif layer 1 self.abs_max_v: 2124.0\n",
      "fc layer 2 self.abs_max_out: 1035.0\n",
      "lif layer 2 self.abs_max_v: 1035.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "lif layer 1 self.abs_max_v: 2167.0\n",
      "fc layer 1 self.abs_max_out: 2383.0\n",
      "lif layer 1 self.abs_max_v: 2383.0\n",
      "fc layer 2 self.abs_max_out: 1041.0\n",
      "lif layer 2 self.abs_max_v: 1041.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.180453/  2.211025, val:  29.58%, val_best:  29.58%, tr:  81.92%, tr_best:  81.92%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1305%\n",
      "layer   2  Sparsity: 89.5484%\n",
      "layer   3  Sparsity: 94.8507%\n",
      "total_backward_count 9790 real_backward_count 3888  39.714%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2397.0\n",
      "lif layer 1 self.abs_max_v: 2397.0\n",
      "fc layer 1 self.abs_max_out: 2697.0\n",
      "lif layer 1 self.abs_max_v: 2697.0\n",
      "fc layer 2 self.abs_max_out: 1107.0\n",
      "lif layer 2 self.abs_max_v: 1107.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "lif layer 2 self.abs_max_v: 1137.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "lif layer 2 self.abs_max_v: 1141.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.148132/  2.207830, val:  46.25%, val_best:  46.25%, tr:  96.63%, tr_best:  96.63%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1353%\n",
      "layer   2  Sparsity: 88.7458%\n",
      "layer   3  Sparsity: 93.5553%\n",
      "total_backward_count 19580 real_backward_count 6455  32.967%\n",
      "fc layer 1 self.abs_max_out: 2823.0\n",
      "lif layer 1 self.abs_max_v: 2823.0\n",
      "fc layer 2 self.abs_max_out: 1179.0\n",
      "lif layer 2 self.abs_max_v: 1179.0\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "lif layer 2 self.abs_max_v: 1245.0\n",
      "fc layer 2 self.abs_max_out: 1249.0\n",
      "lif layer 2 self.abs_max_v: 1249.0\n",
      "fc layer 1 self.abs_max_out: 2844.0\n",
      "lif layer 1 self.abs_max_v: 2844.0\n",
      "fc layer 1 self.abs_max_out: 2855.0\n",
      "lif layer 1 self.abs_max_v: 2855.0\n",
      "fc layer 2 self.abs_max_out: 1272.0\n",
      "lif layer 2 self.abs_max_v: 1272.0\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "lif layer 2 self.abs_max_v: 1311.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.140203/  2.208151, val:  42.08%, val_best:  46.25%, tr:  97.65%, tr_best:  97.65%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1318%\n",
      "layer   2  Sparsity: 88.4662%\n",
      "layer   3  Sparsity: 92.5932%\n",
      "total_backward_count 29370 real_backward_count 8695  29.605%\n",
      "fc layer 1 self.abs_max_out: 2884.0\n",
      "lif layer 1 self.abs_max_v: 2884.0\n",
      "fc layer 1 self.abs_max_out: 2988.0\n",
      "lif layer 1 self.abs_max_v: 2988.0\n",
      "fc layer 1 self.abs_max_out: 3118.0\n",
      "lif layer 1 self.abs_max_v: 3118.0\n",
      "fc layer 2 self.abs_max_out: 1346.0\n",
      "lif layer 2 self.abs_max_v: 1346.0\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "lif layer 2 self.abs_max_v: 1387.0\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "lif layer 2 self.abs_max_v: 1400.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.131130/  2.195309, val:  36.67%, val_best:  46.25%, tr:  98.67%, tr_best:  98.67%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1332%\n",
      "layer   2  Sparsity: 88.3806%\n",
      "layer   3  Sparsity: 92.0313%\n",
      "total_backward_count 39160 real_backward_count 10840  27.681%\n",
      "fc layer 1 self.abs_max_out: 3223.0\n",
      "lif layer 1 self.abs_max_v: 3223.0\n",
      "fc layer 2 self.abs_max_out: 1431.0\n",
      "lif layer 2 self.abs_max_v: 1431.0\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "fc layer 2 self.abs_max_out: 1460.0\n",
      "lif layer 2 self.abs_max_v: 1460.0\n",
      "fc layer 2 self.abs_max_out: 1477.0\n",
      "lif layer 2 self.abs_max_v: 1477.0\n",
      "fc layer 2 self.abs_max_out: 1583.0\n",
      "lif layer 2 self.abs_max_v: 1583.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.127151/  2.184652, val:  39.58%, val_best:  46.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1400%\n",
      "layer   2  Sparsity: 87.9848%\n",
      "layer   3  Sparsity: 91.6661%\n",
      "total_backward_count 48950 real_backward_count 12840  26.231%\n",
      "fc layer 1 self.abs_max_out: 3314.0\n",
      "lif layer 1 self.abs_max_v: 3314.0\n",
      "fc layer 1 self.abs_max_out: 3451.0\n",
      "lif layer 1 self.abs_max_v: 3451.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.117548/  2.190268, val:  48.33%, val_best:  48.33%, tr:  98.37%, tr_best:  99.28%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1227%\n",
      "layer   2  Sparsity: 87.7125%\n",
      "layer   3  Sparsity: 91.4781%\n",
      "total_backward_count 58740 real_backward_count 14798  25.192%\n",
      "fc layer 2 self.abs_max_out: 1614.0\n",
      "lif layer 2 self.abs_max_v: 1614.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.111373/  2.184079, val:  44.58%, val_best:  48.33%, tr:  98.77%, tr_best:  99.28%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1580%\n",
      "layer   2  Sparsity: 87.5243%\n",
      "layer   3  Sparsity: 91.2139%\n",
      "total_backward_count 68530 real_backward_count 16675  24.332%\n",
      "fc layer 1 self.abs_max_out: 3453.0\n",
      "lif layer 1 self.abs_max_v: 3453.0\n",
      "fc layer 3 self.abs_max_out: 155.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.102441/  2.165860, val:  48.33%, val_best:  48.33%, tr:  98.98%, tr_best:  99.28%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1247%\n",
      "layer   2  Sparsity: 87.4960%\n",
      "layer   3  Sparsity: 91.3263%\n",
      "total_backward_count 78320 real_backward_count 18428  23.529%\n",
      "fc layer 1 self.abs_max_out: 3498.0\n",
      "lif layer 1 self.abs_max_v: 3498.0\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 3714.0\n",
      "lif layer 1 self.abs_max_v: 3714.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.102093/  2.163096, val:  52.08%, val_best:  52.08%, tr:  98.67%, tr_best:  99.28%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1409%\n",
      "layer   2  Sparsity: 87.1984%\n",
      "layer   3  Sparsity: 91.3233%\n",
      "total_backward_count 88110 real_backward_count 20333  23.077%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.092099/  2.179270, val:  43.75%, val_best:  52.08%, tr:  98.67%, tr_best:  99.28%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1187%\n",
      "layer   2  Sparsity: 87.4007%\n",
      "layer   3  Sparsity: 91.2078%\n",
      "total_backward_count 97900 real_backward_count 22136  22.611%\n",
      "fc layer 1 self.abs_max_out: 3755.0\n",
      "lif layer 1 self.abs_max_v: 3755.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.086391/  2.167444, val:  49.58%, val_best:  52.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1249%\n",
      "layer   2  Sparsity: 87.2440%\n",
      "layer   3  Sparsity: 90.7788%\n",
      "total_backward_count 107690 real_backward_count 23955  22.244%\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "lif layer 2 self.abs_max_v: 1623.0\n",
      "fc layer 1 self.abs_max_out: 3775.0\n",
      "lif layer 1 self.abs_max_v: 3775.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.082339/  2.139796, val:  55.00%, val_best:  55.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1311%\n",
      "layer   2  Sparsity: 87.0481%\n",
      "layer   3  Sparsity: 90.5632%\n",
      "total_backward_count 117480 real_backward_count 25751  21.919%\n",
      "fc layer 1 self.abs_max_out: 3836.0\n",
      "lif layer 1 self.abs_max_v: 3836.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.071111/  2.158029, val:  46.67%, val_best:  55.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1459%\n",
      "layer   2  Sparsity: 86.9258%\n",
      "layer   3  Sparsity: 90.7021%\n",
      "total_backward_count 127270 real_backward_count 27484  21.595%\n",
      "fc layer 1 self.abs_max_out: 3837.0\n",
      "lif layer 1 self.abs_max_v: 3837.0\n",
      "fc layer 1 self.abs_max_out: 4017.0\n",
      "lif layer 1 self.abs_max_v: 4017.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.083054/  2.146477, val:  52.08%, val_best:  55.00%, tr:  99.28%, tr_best:  99.39%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1504%\n",
      "layer   2  Sparsity: 86.6628%\n",
      "layer   3  Sparsity: 91.0085%\n",
      "total_backward_count 137060 real_backward_count 29131  21.254%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.080554/  2.152985, val:  46.67%, val_best:  55.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1538%\n",
      "layer   2  Sparsity: 86.7366%\n",
      "layer   3  Sparsity: 90.9073%\n",
      "total_backward_count 146850 real_backward_count 30803  20.976%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.077218/  2.150738, val:  52.50%, val_best:  55.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1182%\n",
      "layer   2  Sparsity: 86.6637%\n",
      "layer   3  Sparsity: 90.9629%\n",
      "total_backward_count 156640 real_backward_count 32459  20.722%\n",
      "fc layer 1 self.abs_max_out: 4027.0\n",
      "lif layer 1 self.abs_max_v: 4027.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.065761/  2.134342, val:  64.17%, val_best:  64.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1305%\n",
      "layer   2  Sparsity: 86.2851%\n",
      "layer   3  Sparsity: 90.8574%\n",
      "total_backward_count 166430 real_backward_count 34078  20.476%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.064090/  2.130334, val:  68.75%, val_best:  68.75%, tr:  99.39%, tr_best:  99.59%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1410%\n",
      "layer   2  Sparsity: 86.2406%\n",
      "layer   3  Sparsity: 90.8556%\n",
      "total_backward_count 176220 real_backward_count 35733  20.277%\n",
      "fc layer 1 self.abs_max_out: 4126.0\n",
      "lif layer 1 self.abs_max_v: 4126.0\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "lif layer 2 self.abs_max_v: 1643.0\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.064052/  2.118673, val:  62.92%, val_best:  68.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1242%\n",
      "layer   2  Sparsity: 86.4354%\n",
      "layer   3  Sparsity: 90.8733%\n",
      "total_backward_count 186010 real_backward_count 37356  20.083%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.052361/  2.122759, val:  40.00%, val_best:  68.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1368%\n",
      "layer   2  Sparsity: 86.3564%\n",
      "layer   3  Sparsity: 90.7531%\n",
      "total_backward_count 195800 real_backward_count 38905  19.870%\n",
      "fc layer 1 self.abs_max_out: 4166.0\n",
      "lif layer 1 self.abs_max_v: 4166.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.043613/  2.132223, val:  44.17%, val_best:  68.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1257%\n",
      "layer   2  Sparsity: 85.9943%\n",
      "layer   3  Sparsity: 90.2510%\n",
      "total_backward_count 205590 real_backward_count 40412  19.657%\n",
      "lif layer 2 self.abs_max_v: 1676.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.039216/  2.114418, val:  60.83%, val_best:  68.75%, tr:  99.49%, tr_best:  99.69%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1088%\n",
      "layer   2  Sparsity: 86.1514%\n",
      "layer   3  Sparsity: 90.3929%\n",
      "total_backward_count 215380 real_backward_count 42021  19.510%\n",
      "lif layer 2 self.abs_max_v: 1698.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.048026/  2.113628, val:  58.75%, val_best:  68.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1170%\n",
      "layer   2  Sparsity: 86.2088%\n",
      "layer   3  Sparsity: 90.6931%\n",
      "total_backward_count 225170 real_backward_count 43606  19.366%\n",
      "lif layer 2 self.abs_max_v: 1721.5\n",
      "lif layer 2 self.abs_max_v: 1740.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.052010/  2.108653, val:  57.08%, val_best:  68.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1456%\n",
      "layer   2  Sparsity: 86.5182%\n",
      "layer   3  Sparsity: 90.4147%\n",
      "total_backward_count 234960 real_backward_count 45152  19.217%\n",
      "lif layer 2 self.abs_max_v: 1807.0\n",
      "fc layer 1 self.abs_max_out: 4235.0\n",
      "lif layer 1 self.abs_max_v: 4235.0\n",
      "lif layer 2 self.abs_max_v: 1840.0\n",
      "fc layer 3 self.abs_max_out: 195.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.037402/  2.100619, val:  73.33%, val_best:  73.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1165%\n",
      "layer   2  Sparsity: 86.4969%\n",
      "layer   3  Sparsity: 90.3338%\n",
      "total_backward_count 244750 real_backward_count 46702  19.082%\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 1 self.abs_max_out: 4330.0\n",
      "lif layer 1 self.abs_max_v: 4330.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.024928/  2.091532, val:  56.67%, val_best:  73.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1276%\n",
      "layer   2  Sparsity: 86.4902%\n",
      "layer   3  Sparsity: 90.0998%\n",
      "total_backward_count 254540 real_backward_count 48247  18.955%\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.023495/  2.095079, val:  60.42%, val_best:  73.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1181%\n",
      "layer   2  Sparsity: 86.4713%\n",
      "layer   3  Sparsity: 90.3160%\n",
      "total_backward_count 264330 real_backward_count 49724  18.811%\n",
      "lif layer 2 self.abs_max_v: 1888.0\n",
      "lif layer 2 self.abs_max_v: 1924.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.030882/  2.104904, val:  72.50%, val_best:  73.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1422%\n",
      "layer   2  Sparsity: 86.5425%\n",
      "layer   3  Sparsity: 90.4163%\n",
      "total_backward_count 274120 real_backward_count 51213  18.683%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.024897/  2.086443, val:  57.08%, val_best:  73.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1505%\n",
      "layer   2  Sparsity: 86.3659%\n",
      "layer   3  Sparsity: 90.2095%\n",
      "total_backward_count 283910 real_backward_count 52620  18.534%\n",
      "lif layer 2 self.abs_max_v: 1976.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.014535/  2.080018, val:  50.83%, val_best:  73.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1461%\n",
      "layer   2  Sparsity: 86.1488%\n",
      "layer   3  Sparsity: 90.0056%\n",
      "total_backward_count 293700 real_backward_count 54045  18.401%\n",
      "lif layer 2 self.abs_max_v: 2002.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 1 self.abs_max_out: 4392.0\n",
      "lif layer 1 self.abs_max_v: 4392.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.016627/  2.095143, val:  69.17%, val_best:  73.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1442%\n",
      "layer   2  Sparsity: 86.1831%\n",
      "layer   3  Sparsity: 90.2856%\n",
      "total_backward_count 303490 real_backward_count 55471  18.278%\n",
      "lif layer 2 self.abs_max_v: 2131.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.021235/  2.095459, val:  52.08%, val_best:  73.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1466%\n",
      "layer   2  Sparsity: 86.1436%\n",
      "layer   3  Sparsity: 90.3167%\n",
      "total_backward_count 313280 real_backward_count 56847  18.146%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.007686/  2.068402, val:  71.25%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1215%\n",
      "layer   2  Sparsity: 85.9049%\n",
      "layer   3  Sparsity: 89.7784%\n",
      "total_backward_count 323070 real_backward_count 58205  18.016%\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.994992/  2.071495, val:  67.92%, val_best:  73.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1417%\n",
      "layer   2  Sparsity: 85.7890%\n",
      "layer   3  Sparsity: 89.8442%\n",
      "total_backward_count 332860 real_backward_count 59573  17.897%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.987149/  2.067778, val:  70.42%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1490%\n",
      "layer   2  Sparsity: 85.8539%\n",
      "layer   3  Sparsity: 89.9292%\n",
      "total_backward_count 342650 real_backward_count 60944  17.786%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.990358/  2.059721, val:  72.92%, val_best:  73.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1515%\n",
      "layer   2  Sparsity: 85.9827%\n",
      "layer   3  Sparsity: 90.0490%\n",
      "total_backward_count 352440 real_backward_count 62281  17.671%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.978271/  2.058832, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1318%\n",
      "layer   2  Sparsity: 85.9689%\n",
      "layer   3  Sparsity: 89.8077%\n",
      "total_backward_count 362230 real_backward_count 63572  17.550%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.979649/  2.065906, val:  74.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1357%\n",
      "layer   2  Sparsity: 86.0949%\n",
      "layer   3  Sparsity: 90.0778%\n",
      "total_backward_count 372020 real_backward_count 64745  17.404%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.981114/  2.064557, val:  73.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.1453%\n",
      "layer   2  Sparsity: 85.8655%\n",
      "layer   3  Sparsity: 89.5274%\n",
      "total_backward_count 381810 real_backward_count 66092  17.310%\n",
      "lif layer 2 self.abs_max_v: 2137.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.974987/  2.061186, val:  68.75%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1502%\n",
      "layer   2  Sparsity: 85.8068%\n",
      "layer   3  Sparsity: 89.5318%\n",
      "total_backward_count 391600 real_backward_count 67384  17.207%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.987822/  2.051529, val:  70.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1176%\n",
      "layer   2  Sparsity: 85.8498%\n",
      "layer   3  Sparsity: 89.6467%\n",
      "total_backward_count 401390 real_backward_count 68627  17.097%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.994685/  2.060495, val:  65.83%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1386%\n",
      "layer   2  Sparsity: 86.0287%\n",
      "layer   3  Sparsity: 89.8710%\n",
      "total_backward_count 411180 real_backward_count 69866  16.992%\n",
      "fc layer 2 self.abs_max_out: 1658.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.982473/  2.069947, val:  71.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1378%\n",
      "layer   2  Sparsity: 86.0089%\n",
      "layer   3  Sparsity: 89.7139%\n",
      "total_backward_count 420970 real_backward_count 71089  16.887%\n",
      "lif layer 2 self.abs_max_v: 2141.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.985318/  2.068703, val:  78.33%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1418%\n",
      "layer   2  Sparsity: 85.9145%\n",
      "layer   3  Sparsity: 90.0411%\n",
      "total_backward_count 430760 real_backward_count 72323  16.790%\n",
      "fc layer 1 self.abs_max_out: 4410.0\n",
      "lif layer 1 self.abs_max_v: 4410.0\n",
      "lif layer 2 self.abs_max_v: 2212.0\n",
      "lif layer 1 self.abs_max_v: 4413.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.979891/  2.060019, val:  73.33%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1482%\n",
      "layer   2  Sparsity: 85.9325%\n",
      "layer   3  Sparsity: 90.0348%\n",
      "total_backward_count 440550 real_backward_count 73496  16.683%\n",
      "fc layer 1 self.abs_max_out: 4458.0\n",
      "lif layer 1 self.abs_max_v: 4458.0\n",
      "lif layer 1 self.abs_max_v: 4559.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.979661/  2.043637, val:  76.67%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1421%\n",
      "layer   2  Sparsity: 85.9479%\n",
      "layer   3  Sparsity: 90.0065%\n",
      "total_backward_count 450340 real_backward_count 74690  16.585%\n",
      "fc layer 1 self.abs_max_out: 4480.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.985082/  2.059571, val:  71.25%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1103%\n",
      "layer   2  Sparsity: 86.1541%\n",
      "layer   3  Sparsity: 90.2456%\n",
      "total_backward_count 460130 real_backward_count 75830  16.480%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.979869/  2.052584, val:  64.17%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1370%\n",
      "layer   2  Sparsity: 86.0700%\n",
      "layer   3  Sparsity: 90.1705%\n",
      "total_backward_count 469920 real_backward_count 76918  16.368%\n",
      "fc layer 1 self.abs_max_out: 4497.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.981910/  2.060717, val:  75.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1131%\n",
      "layer   2  Sparsity: 85.9899%\n",
      "layer   3  Sparsity: 90.2395%\n",
      "total_backward_count 479710 real_backward_count 78127  16.286%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.984645/  2.043202, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1315%\n",
      "layer   2  Sparsity: 86.0151%\n",
      "layer   3  Sparsity: 90.1379%\n",
      "total_backward_count 489500 real_backward_count 79298  16.200%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.973624/  2.059290, val:  74.17%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1300%\n",
      "layer   2  Sparsity: 86.0287%\n",
      "layer   3  Sparsity: 90.3194%\n",
      "total_backward_count 499290 real_backward_count 80438  16.110%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.979533/  2.058542, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1517%\n",
      "layer   2  Sparsity: 85.9557%\n",
      "layer   3  Sparsity: 90.5087%\n",
      "total_backward_count 509080 real_backward_count 81523  16.014%\n",
      "lif layer 2 self.abs_max_v: 2250.5\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.977395/  2.042001, val:  77.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1229%\n",
      "layer   2  Sparsity: 86.0309%\n",
      "layer   3  Sparsity: 90.2029%\n",
      "total_backward_count 518870 real_backward_count 82630  15.925%\n",
      "fc layer 1 self.abs_max_out: 4512.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.974605/  2.041718, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1271%\n",
      "layer   2  Sparsity: 85.7989%\n",
      "layer   3  Sparsity: 90.4114%\n",
      "total_backward_count 528660 real_backward_count 83693  15.831%\n",
      "lif layer 1 self.abs_max_v: 4563.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.973664/  2.045646, val:  80.42%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1241%\n",
      "layer   2  Sparsity: 85.6771%\n",
      "layer   3  Sparsity: 90.3279%\n",
      "total_backward_count 538450 real_backward_count 84756  15.741%\n",
      "lif layer 1 self.abs_max_v: 4736.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.980961/  2.050272, val:  83.75%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1267%\n",
      "layer   2  Sparsity: 85.7490%\n",
      "layer   3  Sparsity: 90.1292%\n",
      "total_backward_count 548240 real_backward_count 85856  15.660%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.977168/  2.058367, val:  81.25%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1309%\n",
      "layer   2  Sparsity: 85.6382%\n",
      "layer   3  Sparsity: 90.4534%\n",
      "total_backward_count 558030 real_backward_count 86953  15.582%\n",
      "fc layer 2 self.abs_max_out: 1777.0\n",
      "lif layer 2 self.abs_max_v: 2285.5\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.982443/  2.056010, val:  70.00%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1510%\n",
      "layer   2  Sparsity: 85.8203%\n",
      "layer   3  Sparsity: 90.5758%\n",
      "total_backward_count 567820 real_backward_count 88017  15.501%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.972760/  2.060964, val:  77.92%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1281%\n",
      "layer   2  Sparsity: 85.7981%\n",
      "layer   3  Sparsity: 90.4696%\n",
      "total_backward_count 577610 real_backward_count 89059  15.419%\n",
      "lif layer 2 self.abs_max_v: 2341.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.979789/  2.049777, val:  68.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1482%\n",
      "layer   2  Sparsity: 85.7077%\n",
      "layer   3  Sparsity: 90.1363%\n",
      "total_backward_count 587400 real_backward_count 90121  15.342%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.977688/  2.049455, val:  76.67%, val_best:  83.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1120%\n",
      "layer   2  Sparsity: 85.7492%\n",
      "layer   3  Sparsity: 90.1575%\n",
      "total_backward_count 597190 real_backward_count 91203  15.272%\n",
      "lif layer 1 self.abs_max_v: 4899.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.976428/  2.038833, val:  79.17%, val_best:  83.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1259%\n",
      "layer   2  Sparsity: 85.8099%\n",
      "layer   3  Sparsity: 90.3944%\n",
      "total_backward_count 606980 real_backward_count 92321  15.210%\n",
      "lif layer 1 self.abs_max_v: 4944.5\n",
      "lif layer 1 self.abs_max_v: 5140.5\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.959156/  2.051303, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1275%\n",
      "layer   2  Sparsity: 85.7334%\n",
      "layer   3  Sparsity: 90.2032%\n",
      "total_backward_count 616770 real_backward_count 93341  15.134%\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 1 self.abs_max_out: 4583.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.962136/  2.038298, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1220%\n",
      "layer   2  Sparsity: 85.8324%\n",
      "layer   3  Sparsity: 90.1744%\n",
      "total_backward_count 626560 real_backward_count 94361  15.060%\n",
      "lif layer 2 self.abs_max_v: 2369.5\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.966700/  2.035954, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1515%\n",
      "layer   2  Sparsity: 85.7442%\n",
      "layer   3  Sparsity: 90.2352%\n",
      "total_backward_count 636350 real_backward_count 95396  14.991%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.966833/  2.051870, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1350%\n",
      "layer   2  Sparsity: 85.7060%\n",
      "layer   3  Sparsity: 90.1144%\n",
      "total_backward_count 646140 real_backward_count 96464  14.929%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.965324/  2.042364, val:  74.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1323%\n",
      "layer   2  Sparsity: 85.8364%\n",
      "layer   3  Sparsity: 90.1492%\n",
      "total_backward_count 655930 real_backward_count 97450  14.857%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.961387/  2.036702, val:  77.50%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1244%\n",
      "layer   2  Sparsity: 85.8815%\n",
      "layer   3  Sparsity: 89.9950%\n",
      "total_backward_count 665720 real_backward_count 98429  14.785%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.962280/  2.036818, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1209%\n",
      "layer   2  Sparsity: 85.8411%\n",
      "layer   3  Sparsity: 90.0707%\n",
      "total_backward_count 675510 real_backward_count 99388  14.713%\n",
      "fc layer 1 self.abs_max_out: 4620.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.954824/  2.038455, val:  78.75%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1466%\n",
      "layer   2  Sparsity: 85.7687%\n",
      "layer   3  Sparsity: 90.0058%\n",
      "total_backward_count 685300 real_backward_count 100338  14.641%\n",
      "lif layer 1 self.abs_max_v: 5179.0\n",
      "lif layer 1 self.abs_max_v: 5247.5\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.958737/  2.037162, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1597%\n",
      "layer   2  Sparsity: 85.7576%\n",
      "layer   3  Sparsity: 90.0177%\n",
      "total_backward_count 695090 real_backward_count 101310  14.575%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.963416/  2.039805, val:  70.83%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1321%\n",
      "layer   2  Sparsity: 85.6274%\n",
      "layer   3  Sparsity: 90.0716%\n",
      "total_backward_count 704880 real_backward_count 102355  14.521%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.974341/  2.042790, val:  82.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1362%\n",
      "layer   2  Sparsity: 85.5556%\n",
      "layer   3  Sparsity: 90.0685%\n",
      "total_backward_count 714670 real_backward_count 103334  14.459%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.964626/  2.038222, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1375%\n",
      "layer   2  Sparsity: 85.5818%\n",
      "layer   3  Sparsity: 90.0868%\n",
      "total_backward_count 724460 real_backward_count 104247  14.390%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.963369/  2.044352, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1327%\n",
      "layer   2  Sparsity: 85.6901%\n",
      "layer   3  Sparsity: 90.0774%\n",
      "total_backward_count 734250 real_backward_count 105204  14.328%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.955601/  2.005806, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1133%\n",
      "layer   2  Sparsity: 85.7087%\n",
      "layer   3  Sparsity: 90.0193%\n",
      "total_backward_count 744040 real_backward_count 106180  14.271%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.939790/  2.019121, val:  85.42%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1527%\n",
      "layer   2  Sparsity: 85.6582%\n",
      "layer   3  Sparsity: 89.9393%\n",
      "total_backward_count 753830 real_backward_count 107131  14.212%\n",
      "fc layer 2 self.abs_max_out: 1823.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.946587/  2.036722, val:  65.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1175%\n",
      "layer   2  Sparsity: 85.6315%\n",
      "layer   3  Sparsity: 90.0369%\n",
      "total_backward_count 763620 real_backward_count 108056  14.150%\n",
      "fc layer 2 self.abs_max_out: 1885.0\n",
      "fc layer 1 self.abs_max_out: 4622.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.943154/  2.014493, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1105%\n",
      "layer   2  Sparsity: 85.3865%\n",
      "layer   3  Sparsity: 89.9396%\n",
      "total_backward_count 773410 real_backward_count 109014  14.095%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.932668/  2.015622, val:  80.00%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1309%\n",
      "layer   2  Sparsity: 85.4135%\n",
      "layer   3  Sparsity: 89.9828%\n",
      "total_backward_count 783200 real_backward_count 109936  14.037%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.934919/  2.007698, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1366%\n",
      "layer   2  Sparsity: 85.5176%\n",
      "layer   3  Sparsity: 90.1274%\n",
      "total_backward_count 792990 real_backward_count 110911  13.986%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.951425/  2.028583, val:  82.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1241%\n",
      "layer   2  Sparsity: 85.6417%\n",
      "layer   3  Sparsity: 90.0965%\n",
      "total_backward_count 802780 real_backward_count 111826  13.930%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.945055/  2.012169, val:  75.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1450%\n",
      "layer   2  Sparsity: 85.5876%\n",
      "layer   3  Sparsity: 89.7169%\n",
      "total_backward_count 812570 real_backward_count 112754  13.876%\n",
      "fc layer 1 self.abs_max_out: 4758.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.920734/  2.009092, val:  76.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1281%\n",
      "layer   2  Sparsity: 85.7583%\n",
      "layer   3  Sparsity: 89.7862%\n",
      "total_backward_count 822360 real_backward_count 113644  13.819%\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.921604/  1.998599, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1486%\n",
      "layer   2  Sparsity: 85.7616%\n",
      "layer   3  Sparsity: 89.8714%\n",
      "total_backward_count 832150 real_backward_count 114502  13.760%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.929617/  2.016159, val:  74.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1526%\n",
      "layer   2  Sparsity: 85.6502%\n",
      "layer   3  Sparsity: 90.0571%\n",
      "total_backward_count 841940 real_backward_count 115377  13.704%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.932242/  2.003436, val:  82.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1421%\n",
      "layer   2  Sparsity: 85.5359%\n",
      "layer   3  Sparsity: 89.9557%\n",
      "total_backward_count 851730 real_backward_count 116236  13.647%\n",
      "lif layer 2 self.abs_max_v: 2397.5\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.927971/  2.015167, val:  78.75%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1315%\n",
      "layer   2  Sparsity: 85.5124%\n",
      "layer   3  Sparsity: 89.9104%\n",
      "total_backward_count 861520 real_backward_count 117119  13.594%\n",
      "lif layer 1 self.abs_max_v: 5299.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.930014/  2.001101, val:  77.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1449%\n",
      "layer   2  Sparsity: 85.5156%\n",
      "layer   3  Sparsity: 89.7691%\n",
      "total_backward_count 871310 real_backward_count 118014  13.544%\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.932605/  1.994424, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1165%\n",
      "layer   2  Sparsity: 85.4877%\n",
      "layer   3  Sparsity: 89.6503%\n",
      "total_backward_count 881100 real_backward_count 118883  13.493%\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.920587/  2.001973, val:  77.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1135%\n",
      "layer   2  Sparsity: 85.5462%\n",
      "layer   3  Sparsity: 89.8310%\n",
      "total_backward_count 890890 real_backward_count 119720  13.438%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.915649/  1.992011, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1348%\n",
      "layer   2  Sparsity: 85.5904%\n",
      "layer   3  Sparsity: 89.9565%\n",
      "total_backward_count 900680 real_backward_count 120539  13.383%\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.934024/  2.011213, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1210%\n",
      "layer   2  Sparsity: 85.5428%\n",
      "layer   3  Sparsity: 89.8792%\n",
      "total_backward_count 910470 real_backward_count 121370  13.330%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.925142/  2.006138, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1505%\n",
      "layer   2  Sparsity: 85.4723%\n",
      "layer   3  Sparsity: 89.6305%\n",
      "total_backward_count 920260 real_backward_count 122191  13.278%\n",
      "fc layer 1 self.abs_max_out: 4791.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.920004/  1.996589, val:  81.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1391%\n",
      "layer   2  Sparsity: 85.5207%\n",
      "layer   3  Sparsity: 89.4850%\n",
      "total_backward_count 930050 real_backward_count 123082  13.234%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.906702/  1.986212, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1269%\n",
      "layer   2  Sparsity: 85.5208%\n",
      "layer   3  Sparsity: 89.6811%\n",
      "total_backward_count 939840 real_backward_count 123944  13.188%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.907747/  1.998431, val:  80.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1428%\n",
      "layer   2  Sparsity: 85.5877%\n",
      "layer   3  Sparsity: 89.9423%\n",
      "total_backward_count 949630 real_backward_count 124776  13.139%\n",
      "fc layer 2 self.abs_max_out: 1935.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.919753/  1.984701, val:  83.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1362%\n",
      "layer   2  Sparsity: 85.5112%\n",
      "layer   3  Sparsity: 90.0202%\n",
      "total_backward_count 959420 real_backward_count 125595  13.091%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.915081/  2.001142, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1267%\n",
      "layer   2  Sparsity: 85.6187%\n",
      "layer   3  Sparsity: 90.0808%\n",
      "total_backward_count 969210 real_backward_count 126381  13.040%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.915177/  1.984795, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1345%\n",
      "layer   2  Sparsity: 85.6340%\n",
      "layer   3  Sparsity: 90.0978%\n",
      "total_backward_count 979000 real_backward_count 127191  12.992%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.916755/  1.996846, val:  84.17%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1533%\n",
      "layer   2  Sparsity: 85.6137%\n",
      "layer   3  Sparsity: 90.0556%\n",
      "total_backward_count 988790 real_backward_count 128012  12.946%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.914635/  1.997625, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1361%\n",
      "layer   2  Sparsity: 85.6833%\n",
      "layer   3  Sparsity: 90.1531%\n",
      "total_backward_count 998580 real_backward_count 128799  12.898%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.919148/  2.001833, val:  85.83%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1363%\n",
      "layer   2  Sparsity: 85.6911%\n",
      "layer   3  Sparsity: 89.9197%\n",
      "total_backward_count 1008370 real_backward_count 129598  12.852%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.920617/  2.007169, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1224%\n",
      "layer   2  Sparsity: 85.5490%\n",
      "layer   3  Sparsity: 89.8152%\n",
      "total_backward_count 1018160 real_backward_count 130357  12.803%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.914354/  1.999133, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1346%\n",
      "layer   2  Sparsity: 85.6154%\n",
      "layer   3  Sparsity: 89.6375%\n",
      "total_backward_count 1027950 real_backward_count 131138  12.757%\n",
      "fc layer 1 self.abs_max_out: 4843.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.913746/  1.995484, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1299%\n",
      "layer   2  Sparsity: 85.6459%\n",
      "layer   3  Sparsity: 89.7749%\n",
      "total_backward_count 1037740 real_backward_count 131896  12.710%\n",
      "fc layer 2 self.abs_max_out: 1954.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.904927/  1.980908, val:  83.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1302%\n",
      "layer   2  Sparsity: 85.5255%\n",
      "layer   3  Sparsity: 89.6295%\n",
      "total_backward_count 1047530 real_backward_count 132687  12.667%\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "fc layer 1 self.abs_max_out: 4852.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.909336/  1.976901, val:  82.92%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1235%\n",
      "layer   2  Sparsity: 85.3909%\n",
      "layer   3  Sparsity: 89.8363%\n",
      "total_backward_count 1057320 real_backward_count 133518  12.628%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.903597/  1.992524, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1132%\n",
      "layer   2  Sparsity: 85.4885%\n",
      "layer   3  Sparsity: 89.8422%\n",
      "total_backward_count 1067110 real_backward_count 134236  12.579%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.908322/  1.989341, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1494%\n",
      "layer   2  Sparsity: 85.6135%\n",
      "layer   3  Sparsity: 89.7363%\n",
      "total_backward_count 1076900 real_backward_count 135004  12.536%\n",
      "fc layer 1 self.abs_max_out: 4855.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.900585/  1.977700, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1344%\n",
      "layer   2  Sparsity: 85.4699%\n",
      "layer   3  Sparsity: 89.6589%\n",
      "total_backward_count 1086690 real_backward_count 135766  12.494%\n",
      "fc layer 1 self.abs_max_out: 4856.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.901071/  1.984022, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1174%\n",
      "layer   2  Sparsity: 85.5532%\n",
      "layer   3  Sparsity: 90.0304%\n",
      "total_backward_count 1096480 real_backward_count 136542  12.453%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.914071/  1.991078, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1240%\n",
      "layer   2  Sparsity: 85.5321%\n",
      "layer   3  Sparsity: 90.0774%\n",
      "total_backward_count 1106270 real_backward_count 137262  12.408%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.909356/  1.978893, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1338%\n",
      "layer   2  Sparsity: 85.5565%\n",
      "layer   3  Sparsity: 90.1442%\n",
      "total_backward_count 1116060 real_backward_count 137985  12.364%\n",
      "lif layer 2 self.abs_max_v: 2409.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.895891/  1.969631, val:  82.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1538%\n",
      "layer   2  Sparsity: 85.5274%\n",
      "layer   3  Sparsity: 89.7426%\n",
      "total_backward_count 1125850 real_backward_count 138666  12.317%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.887364/  1.967221, val:  85.00%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1430%\n",
      "layer   2  Sparsity: 85.5883%\n",
      "layer   3  Sparsity: 89.5945%\n",
      "total_backward_count 1135640 real_backward_count 139403  12.275%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.881228/  1.967267, val:  84.17%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1548%\n",
      "layer   2  Sparsity: 85.5806%\n",
      "layer   3  Sparsity: 89.6978%\n",
      "total_backward_count 1145430 real_backward_count 140135  12.234%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.887184/  1.962595, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1324%\n",
      "layer   2  Sparsity: 85.5081%\n",
      "layer   3  Sparsity: 89.8230%\n",
      "total_backward_count 1155220 real_backward_count 140850  12.192%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.879480/  1.956222, val:  83.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1335%\n",
      "layer   2  Sparsity: 85.5043%\n",
      "layer   3  Sparsity: 89.7538%\n",
      "total_backward_count 1165010 real_backward_count 141592  12.154%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.879966/  1.973791, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1456%\n",
      "layer   2  Sparsity: 85.6236%\n",
      "layer   3  Sparsity: 90.0095%\n",
      "total_backward_count 1174800 real_backward_count 142268  12.110%\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.893762/  1.979985, val:  78.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1316%\n",
      "layer   2  Sparsity: 85.6154%\n",
      "layer   3  Sparsity: 89.9865%\n",
      "total_backward_count 1184590 real_backward_count 142999  12.072%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.891691/  1.975196, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1322%\n",
      "layer   2  Sparsity: 85.5676%\n",
      "layer   3  Sparsity: 89.9676%\n",
      "total_backward_count 1194380 real_backward_count 143747  12.035%\n",
      "fc layer 2 self.abs_max_out: 2058.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.894739/  1.983387, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1458%\n",
      "layer   2  Sparsity: 85.4601%\n",
      "layer   3  Sparsity: 90.0371%\n",
      "total_backward_count 1204170 real_backward_count 144447  11.996%\n",
      "fc layer 2 self.abs_max_out: 2069.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.892814/  1.962762, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1573%\n",
      "layer   2  Sparsity: 85.5233%\n",
      "layer   3  Sparsity: 90.0785%\n",
      "total_backward_count 1213960 real_backward_count 145151  11.957%\n",
      "lif layer 2 self.abs_max_v: 2428.5\n",
      "lif layer 2 self.abs_max_v: 2456.5\n",
      "fc layer 2 self.abs_max_out: 2080.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.886158/  1.957818, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1319%\n",
      "layer   2  Sparsity: 85.4765%\n",
      "layer   3  Sparsity: 90.0416%\n",
      "total_backward_count 1223750 real_backward_count 145833  11.917%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.890344/  1.964775, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1283%\n",
      "layer   2  Sparsity: 85.4991%\n",
      "layer   3  Sparsity: 90.0230%\n",
      "total_backward_count 1233540 real_backward_count 146470  11.874%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.879596/  1.966040, val:  81.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1353%\n",
      "layer   2  Sparsity: 85.4317%\n",
      "layer   3  Sparsity: 89.9163%\n",
      "total_backward_count 1243330 real_backward_count 147148  11.835%\n",
      "fc layer 2 self.abs_max_out: 2100.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.884317/  1.967445, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1198%\n",
      "layer   2  Sparsity: 85.5168%\n",
      "layer   3  Sparsity: 89.7458%\n",
      "total_backward_count 1253120 real_backward_count 147855  11.799%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.878697/  1.956478, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1318%\n",
      "layer   2  Sparsity: 85.5480%\n",
      "layer   3  Sparsity: 89.6970%\n",
      "total_backward_count 1262910 real_backward_count 148511  11.759%\n",
      "fc layer 1 self.abs_max_out: 4857.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.881019/  1.959011, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1476%\n",
      "layer   2  Sparsity: 85.5626%\n",
      "layer   3  Sparsity: 89.6367%\n",
      "total_backward_count 1272700 real_backward_count 149228  11.725%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.882409/  1.971143, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1451%\n",
      "layer   2  Sparsity: 85.5331%\n",
      "layer   3  Sparsity: 89.8647%\n",
      "total_backward_count 1282490 real_backward_count 149881  11.687%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.889100/  1.983268, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1503%\n",
      "layer   2  Sparsity: 85.4686%\n",
      "layer   3  Sparsity: 89.9130%\n",
      "total_backward_count 1292280 real_backward_count 150616  11.655%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.887346/  1.974622, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1549%\n",
      "layer   2  Sparsity: 85.5013%\n",
      "layer   3  Sparsity: 89.9359%\n",
      "total_backward_count 1302070 real_backward_count 151309  11.621%\n",
      "fc layer 1 self.abs_max_out: 4868.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.891982/  1.974386, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1268%\n",
      "layer   2  Sparsity: 85.4767%\n",
      "layer   3  Sparsity: 89.9026%\n",
      "total_backward_count 1311860 real_backward_count 152031  11.589%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.892180/  1.978126, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1496%\n",
      "layer   2  Sparsity: 85.6422%\n",
      "layer   3  Sparsity: 89.9269%\n",
      "total_backward_count 1321650 real_backward_count 152652  11.550%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.888771/  1.952170, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1227%\n",
      "layer   2  Sparsity: 85.5384%\n",
      "layer   3  Sparsity: 89.7480%\n",
      "total_backward_count 1331440 real_backward_count 153327  11.516%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.867881/  1.958582, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1436%\n",
      "layer   2  Sparsity: 85.5584%\n",
      "layer   3  Sparsity: 89.7583%\n",
      "total_backward_count 1341230 real_backward_count 153944  11.478%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.874697/  1.969375, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1132%\n",
      "layer   2  Sparsity: 85.4674%\n",
      "layer   3  Sparsity: 89.8653%\n",
      "total_backward_count 1351020 real_backward_count 154598  11.443%\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "lif layer 2 self.abs_max_v: 2466.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.875414/  1.969987, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1194%\n",
      "layer   2  Sparsity: 85.4271%\n",
      "layer   3  Sparsity: 89.8080%\n",
      "total_backward_count 1360810 real_backward_count 155300  11.412%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.885578/  1.969366, val:  88.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1497%\n",
      "layer   2  Sparsity: 85.4818%\n",
      "layer   3  Sparsity: 90.0015%\n",
      "total_backward_count 1370600 real_backward_count 155919  11.376%\n",
      "lif layer 1 self.abs_max_v: 5313.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.886398/  1.968895, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1502%\n",
      "layer   2  Sparsity: 85.3829%\n",
      "layer   3  Sparsity: 89.9689%\n",
      "total_backward_count 1380390 real_backward_count 156573  11.343%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.879582/  1.961413, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1178%\n",
      "layer   2  Sparsity: 85.4779%\n",
      "layer   3  Sparsity: 90.1664%\n",
      "total_backward_count 1390180 real_backward_count 157195  11.308%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.880074/  1.968146, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1469%\n",
      "layer   2  Sparsity: 85.6219%\n",
      "layer   3  Sparsity: 90.2393%\n",
      "total_backward_count 1399970 real_backward_count 157815  11.273%\n",
      "lif layer 1 self.abs_max_v: 5339.0\n",
      "lif layer 1 self.abs_max_v: 5344.5\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.882640/  1.960939, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1708%\n",
      "layer   2  Sparsity: 85.5849%\n",
      "layer   3  Sparsity: 90.1796%\n",
      "total_backward_count 1409760 real_backward_count 158435  11.238%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.879747/  1.955083, val:  83.33%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1518%\n",
      "layer   2  Sparsity: 85.5153%\n",
      "layer   3  Sparsity: 90.1264%\n",
      "total_backward_count 1419550 real_backward_count 159095  11.207%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.871909/  1.954745, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1215%\n",
      "layer   2  Sparsity: 85.5557%\n",
      "layer   3  Sparsity: 90.1416%\n",
      "total_backward_count 1429340 real_backward_count 159691  11.172%\n",
      "lif layer 1 self.abs_max_v: 5387.5\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.882267/  1.961086, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1412%\n",
      "layer   2  Sparsity: 85.6209%\n",
      "layer   3  Sparsity: 90.0648%\n",
      "total_backward_count 1439130 real_backward_count 160356  11.143%\n",
      "fc layer 1 self.abs_max_out: 4882.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.876939/  1.965889, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1506%\n",
      "layer   2  Sparsity: 85.4837%\n",
      "layer   3  Sparsity: 90.1564%\n",
      "total_backward_count 1448920 real_backward_count 160917  11.106%\n",
      "fc layer 2 self.abs_max_out: 2159.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.885828/  1.968153, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1618%\n",
      "layer   2  Sparsity: 85.6852%\n",
      "layer   3  Sparsity: 90.1106%\n",
      "total_backward_count 1458710 real_backward_count 161527  11.073%\n",
      "lif layer 2 self.abs_max_v: 2468.5\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.886084/  1.952789, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1425%\n",
      "layer   2  Sparsity: 85.6065%\n",
      "layer   3  Sparsity: 90.0641%\n",
      "total_backward_count 1468500 real_backward_count 162123  11.040%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.875941/  1.953103, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1332%\n",
      "layer   2  Sparsity: 85.6169%\n",
      "layer   3  Sparsity: 90.2354%\n",
      "total_backward_count 1478290 real_backward_count 162695  11.006%\n",
      "lif layer 2 self.abs_max_v: 2516.5\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.876872/  1.959479, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1117%\n",
      "layer   2  Sparsity: 85.7064%\n",
      "layer   3  Sparsity: 90.0657%\n",
      "total_backward_count 1488080 real_backward_count 163306  10.974%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.865172/  1.945894, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1365%\n",
      "layer   2  Sparsity: 85.5326%\n",
      "layer   3  Sparsity: 89.9229%\n",
      "total_backward_count 1497870 real_backward_count 163962  10.946%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.858137/  1.929270, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.0868%\n",
      "layer   2  Sparsity: 85.4736%\n",
      "layer   3  Sparsity: 89.9384%\n",
      "total_backward_count 1507660 real_backward_count 164588  10.917%\n",
      "fc layer 2 self.abs_max_out: 2167.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.847067/  1.941888, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1239%\n",
      "layer   2  Sparsity: 85.5216%\n",
      "layer   3  Sparsity: 89.9631%\n",
      "total_backward_count 1517450 real_backward_count 165175  10.885%\n",
      "fc layer 2 self.abs_max_out: 2208.0\n",
      "fc layer 1 self.abs_max_out: 4892.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.857375/  1.952154, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1293%\n",
      "layer   2  Sparsity: 85.7049%\n",
      "layer   3  Sparsity: 90.0234%\n",
      "total_backward_count 1527240 real_backward_count 165768  10.854%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.868381/  1.955885, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1156%\n",
      "layer   2  Sparsity: 85.6236%\n",
      "layer   3  Sparsity: 90.1610%\n",
      "total_backward_count 1537030 real_backward_count 166382  10.825%\n",
      "fc layer 1 self.abs_max_out: 4912.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.870189/  1.959251, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1433%\n",
      "layer   2  Sparsity: 85.6388%\n",
      "layer   3  Sparsity: 90.0851%\n",
      "total_backward_count 1546820 real_backward_count 166952  10.793%\n",
      "lif layer 2 self.abs_max_v: 2530.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.864941/  1.943974, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1260%\n",
      "layer   2  Sparsity: 85.6012%\n",
      "layer   3  Sparsity: 90.0682%\n",
      "total_backward_count 1556610 real_backward_count 167497  10.760%\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.853794/  1.953326, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1342%\n",
      "layer   2  Sparsity: 85.5909%\n",
      "layer   3  Sparsity: 90.0365%\n",
      "total_backward_count 1566400 real_backward_count 168050  10.728%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.869300/  1.950503, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1303%\n",
      "layer   2  Sparsity: 85.5599%\n",
      "layer   3  Sparsity: 90.1498%\n",
      "total_backward_count 1576190 real_backward_count 168638  10.699%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.857991/  1.928041, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1214%\n",
      "layer   2  Sparsity: 85.6390%\n",
      "layer   3  Sparsity: 89.8623%\n",
      "total_backward_count 1585980 real_backward_count 169231  10.670%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.853989/  1.949513, val:  80.00%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1206%\n",
      "layer   2  Sparsity: 85.6152%\n",
      "layer   3  Sparsity: 89.9106%\n",
      "total_backward_count 1595770 real_backward_count 169805  10.641%\n",
      "fc layer 1 self.abs_max_out: 4941.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.864118/  1.951479, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1101%\n",
      "layer   2  Sparsity: 85.6719%\n",
      "layer   3  Sparsity: 90.0252%\n",
      "total_backward_count 1605560 real_backward_count 170385  10.612%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.880533/  1.971163, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1179%\n",
      "layer   2  Sparsity: 85.4781%\n",
      "layer   3  Sparsity: 90.2216%\n",
      "total_backward_count 1615350 real_backward_count 170950  10.583%\n",
      "fc layer 1 self.abs_max_out: 4948.0\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.862563/  1.956253, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1287%\n",
      "layer   2  Sparsity: 85.4390%\n",
      "layer   3  Sparsity: 90.0115%\n",
      "total_backward_count 1625140 real_backward_count 171563  10.557%\n",
      "fc layer 1 self.abs_max_out: 4958.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.855197/  1.944341, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1318%\n",
      "layer   2  Sparsity: 85.5126%\n",
      "layer   3  Sparsity: 90.0094%\n",
      "total_backward_count 1634930 real_backward_count 172119  10.528%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.853925/  1.947394, val:  82.92%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1512%\n",
      "layer   2  Sparsity: 85.6034%\n",
      "layer   3  Sparsity: 89.8412%\n",
      "total_backward_count 1644720 real_backward_count 172691  10.500%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.859601/  1.955770, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1211%\n",
      "layer   2  Sparsity: 85.5244%\n",
      "layer   3  Sparsity: 89.9018%\n",
      "total_backward_count 1654510 real_backward_count 173247  10.471%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.861815/  1.950498, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1445%\n",
      "layer   2  Sparsity: 85.5974%\n",
      "layer   3  Sparsity: 89.9556%\n",
      "total_backward_count 1664300 real_backward_count 173789  10.442%\n",
      "fc layer 3 self.abs_max_out: 288.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.860144/  1.953236, val:  79.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1164%\n",
      "layer   2  Sparsity: 85.6064%\n",
      "layer   3  Sparsity: 90.0188%\n",
      "total_backward_count 1674090 real_backward_count 174321  10.413%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.863835/  1.951046, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.1139%\n",
      "layer   2  Sparsity: 85.6059%\n",
      "layer   3  Sparsity: 89.9726%\n",
      "total_backward_count 1683880 real_backward_count 174876  10.385%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.863071/  1.945166, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1467%\n",
      "layer   2  Sparsity: 85.6583%\n",
      "layer   3  Sparsity: 89.9335%\n",
      "total_backward_count 1693670 real_backward_count 175416  10.357%\n",
      "fc layer 1 self.abs_max_out: 4982.0\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.868587/  1.946477, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1470%\n",
      "layer   2  Sparsity: 85.5757%\n",
      "layer   3  Sparsity: 89.9746%\n",
      "total_backward_count 1703460 real_backward_count 175925  10.328%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.852773/  1.944155, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1494%\n",
      "layer   2  Sparsity: 85.5817%\n",
      "layer   3  Sparsity: 89.7945%\n",
      "total_backward_count 1713250 real_backward_count 176489  10.301%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.857983/  1.936302, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1024%\n",
      "layer   2  Sparsity: 85.5669%\n",
      "layer   3  Sparsity: 90.0188%\n",
      "total_backward_count 1723040 real_backward_count 177076  10.277%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.851043/  1.932880, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1423%\n",
      "layer   2  Sparsity: 85.6369%\n",
      "layer   3  Sparsity: 90.2311%\n",
      "total_backward_count 1732830 real_backward_count 177568  10.247%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.854689/  1.954546, val:  78.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1573%\n",
      "layer   2  Sparsity: 85.6014%\n",
      "layer   3  Sparsity: 89.9509%\n",
      "total_backward_count 1742620 real_backward_count 178088  10.220%\n",
      "lif layer 1 self.abs_max_v: 5440.0\n",
      "lif layer 1 self.abs_max_v: 5552.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.849059/  1.929388, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1375%\n",
      "layer   2  Sparsity: 85.5311%\n",
      "layer   3  Sparsity: 89.7856%\n",
      "total_backward_count 1752410 real_backward_count 178633  10.194%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.837561/  1.930098, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1292%\n",
      "layer   2  Sparsity: 85.5744%\n",
      "layer   3  Sparsity: 89.9503%\n",
      "total_backward_count 1762200 real_backward_count 179140  10.166%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.841984/  1.919863, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1483%\n",
      "layer   2  Sparsity: 85.6929%\n",
      "layer   3  Sparsity: 90.0166%\n",
      "total_backward_count 1771990 real_backward_count 179642  10.138%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.829249/  1.930651, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1154%\n",
      "layer   2  Sparsity: 85.7069%\n",
      "layer   3  Sparsity: 90.1514%\n",
      "total_backward_count 1781780 real_backward_count 180162  10.111%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.842894/  1.939668, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1317%\n",
      "layer   2  Sparsity: 85.5668%\n",
      "layer   3  Sparsity: 90.1795%\n",
      "total_backward_count 1791570 real_backward_count 180729  10.088%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.843008/  1.936486, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1189%\n",
      "layer   2  Sparsity: 85.6005%\n",
      "layer   3  Sparsity: 90.2267%\n",
      "total_backward_count 1801360 real_backward_count 181259  10.062%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.841173/  1.935435, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1413%\n",
      "layer   2  Sparsity: 85.6171%\n",
      "layer   3  Sparsity: 90.0445%\n",
      "total_backward_count 1811150 real_backward_count 181766  10.036%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.841212/  1.938874, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1210%\n",
      "layer   2  Sparsity: 85.5087%\n",
      "layer   3  Sparsity: 89.9938%\n",
      "total_backward_count 1820940 real_backward_count 182297  10.011%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.848079/  1.933173, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1357%\n",
      "layer   2  Sparsity: 85.5296%\n",
      "layer   3  Sparsity: 89.8611%\n",
      "total_backward_count 1830730 real_backward_count 182865   9.989%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.828809/  1.929109, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1169%\n",
      "layer   2  Sparsity: 85.6582%\n",
      "layer   3  Sparsity: 89.9623%\n",
      "total_backward_count 1840520 real_backward_count 183375   9.963%\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.836402/  1.930478, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1268%\n",
      "layer   2  Sparsity: 85.6847%\n",
      "layer   3  Sparsity: 90.0032%\n",
      "total_backward_count 1850310 real_backward_count 183861   9.937%\n",
      "fc layer 1 self.abs_max_out: 4991.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.834972/  1.922754, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1430%\n",
      "layer   2  Sparsity: 85.5832%\n",
      "layer   3  Sparsity: 89.9714%\n",
      "total_backward_count 1860100 real_backward_count 184378   9.912%\n",
      "fc layer 1 self.abs_max_out: 4999.0\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.835571/  1.937301, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1331%\n",
      "layer   2  Sparsity: 85.5130%\n",
      "layer   3  Sparsity: 89.9439%\n",
      "total_backward_count 1869890 real_backward_count 184913   9.889%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.846399/  1.928355, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1186%\n",
      "layer   2  Sparsity: 85.5525%\n",
      "layer   3  Sparsity: 89.9040%\n",
      "total_backward_count 1879680 real_backward_count 185428   9.865%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.833819/  1.923979, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1231%\n",
      "layer   2  Sparsity: 85.6653%\n",
      "layer   3  Sparsity: 89.8691%\n",
      "total_backward_count 1889470 real_backward_count 185946   9.841%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.830198/  1.931097, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1324%\n",
      "layer   2  Sparsity: 85.4882%\n",
      "layer   3  Sparsity: 90.0897%\n",
      "total_backward_count 1899260 real_backward_count 186432   9.816%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.823843/  1.908333, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1262%\n",
      "layer   2  Sparsity: 85.4409%\n",
      "layer   3  Sparsity: 90.0136%\n",
      "total_backward_count 1909050 real_backward_count 186910   9.791%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.825365/  1.922905, val:  82.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1547%\n",
      "layer   2  Sparsity: 85.5480%\n",
      "layer   3  Sparsity: 89.9705%\n",
      "total_backward_count 1918840 real_backward_count 187418   9.767%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.824025/  1.928192, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1410%\n",
      "layer   2  Sparsity: 85.5521%\n",
      "layer   3  Sparsity: 90.1178%\n",
      "total_backward_count 1928630 real_backward_count 187900   9.743%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.827414/  1.925255, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1122%\n",
      "layer   2  Sparsity: 85.5721%\n",
      "layer   3  Sparsity: 90.0788%\n",
      "total_backward_count 1938420 real_backward_count 188399   9.719%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.835576/  1.928816, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1466%\n",
      "layer   2  Sparsity: 85.5415%\n",
      "layer   3  Sparsity: 90.0644%\n",
      "total_backward_count 1948210 real_backward_count 188908   9.696%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.838691/  1.935194, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1347%\n",
      "layer   2  Sparsity: 85.5747%\n",
      "layer   3  Sparsity: 90.0337%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe0dd2e462b469288af2bd0e8ca857c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.83869</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>1.93519</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-sweep-117</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x115sum6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x115sum6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_204939-x115sum6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 91dgjwfs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_011254-91dgjwfs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/91dgjwfs' target=\"_blank\">pretty-sweep-122</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/91dgjwfs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/91dgjwfs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_011303_667', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 169.0\n",
      "lif layer 1 self.abs_max_v: 169.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 430.0\n",
      "lif layer 2 self.abs_max_v: 430.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 1 self.abs_max_out: 227.0\n",
      "lif layer 1 self.abs_max_v: 227.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 676.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 1 self.abs_max_v: 299.0\n",
      "fc layer 2 self.abs_max_out: 540.0\n",
      "lif layer 2 self.abs_max_v: 794.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "lif layer 1 self.abs_max_v: 313.5\n",
      "fc layer 1 self.abs_max_out: 458.0\n",
      "lif layer 1 self.abs_max_v: 458.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "lif layer 1 self.abs_max_v: 528.5\n",
      "fc layer 2 self.abs_max_out: 564.0\n",
      "lif layer 2 self.abs_max_v: 956.0\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "lif layer 1 self.abs_max_v: 618.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "lif layer 1 self.abs_max_v: 665.5\n",
      "lif layer 1 self.abs_max_v: 756.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 1 self.abs_max_out: 563.0\n",
      "lif layer 2 self.abs_max_v: 987.0\n",
      "fc layer 3 self.abs_max_out: 361.0\n",
      "fc layer 2 self.abs_max_out: 733.0\n",
      "fc layer 1 self.abs_max_out: 578.0\n",
      "fc layer 1 self.abs_max_out: 864.0\n",
      "lif layer 1 self.abs_max_v: 888.0\n",
      "fc layer 1 self.abs_max_out: 976.0\n",
      "lif layer 1 self.abs_max_v: 1228.0\n",
      "fc layer 1 self.abs_max_out: 986.0\n",
      "lif layer 1 self.abs_max_v: 1320.0\n",
      "lif layer 2 self.abs_max_v: 1036.0\n",
      "lif layer 1 self.abs_max_v: 1430.0\n",
      "lif layer 2 self.abs_max_v: 1176.0\n",
      "fc layer 1 self.abs_max_out: 1067.0\n",
      "fc layer 2 self.abs_max_out: 741.0\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 1 self.abs_max_out: 1136.0\n",
      "lif layer 1 self.abs_max_v: 1442.5\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "lif layer 1 self.abs_max_v: 1584.5\n",
      "fc layer 1 self.abs_max_out: 1198.0\n",
      "lif layer 1 self.abs_max_v: 1796.5\n",
      "lif layer 2 self.abs_max_v: 1198.0\n",
      "lif layer 2 self.abs_max_v: 1205.0\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "lif layer 2 self.abs_max_v: 1259.5\n",
      "lif layer 2 self.abs_max_v: 1269.0\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 1275.5\n",
      "lif layer 2 self.abs_max_v: 1362.0\n",
      "lif layer 2 self.abs_max_v: 1376.0\n",
      "lif layer 2 self.abs_max_v: 1518.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "lif layer 1 self.abs_max_v: 1835.0\n",
      "lif layer 1 self.abs_max_v: 1867.5\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "fc layer 1 self.abs_max_out: 1212.0\n",
      "fc layer 1 self.abs_max_out: 1366.0\n",
      "lif layer 1 self.abs_max_v: 1908.5\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "fc layer 2 self.abs_max_out: 985.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "lif layer 1 self.abs_max_v: 1972.5\n",
      "lif layer 1 self.abs_max_v: 2105.5\n",
      "lif layer 2 self.abs_max_v: 1574.0\n",
      "lif layer 1 self.abs_max_v: 2130.0\n",
      "lif layer 2 self.abs_max_v: 1575.5\n",
      "lif layer 1 self.abs_max_v: 2145.0\n",
      "lif layer 1 self.abs_max_v: 2317.5\n",
      "fc layer 2 self.abs_max_out: 1098.0\n",
      "lif layer 2 self.abs_max_v: 1648.0\n",
      "lif layer 2 self.abs_max_v: 1721.5\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "lif layer 2 self.abs_max_v: 1732.5\n",
      "fc layer 3 self.abs_max_out: 436.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "lif layer 2 self.abs_max_v: 1742.0\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1805.5\n",
      "lif layer 2 self.abs_max_v: 1816.5\n",
      "lif layer 1 self.abs_max_v: 2324.5\n",
      "lif layer 2 self.abs_max_v: 1899.0\n",
      "lif layer 2 self.abs_max_v: 1926.5\n",
      "fc layer 2 self.abs_max_out: 1144.0\n",
      "lif layer 2 self.abs_max_v: 2107.5\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "fc layer 1 self.abs_max_out: 1405.0\n",
      "lif layer 2 self.abs_max_v: 2118.0\n",
      "fc layer 1 self.abs_max_out: 1477.0\n",
      "fc layer 1 self.abs_max_out: 1512.0\n",
      "fc layer 1 self.abs_max_out: 1635.0\n",
      "lif layer 1 self.abs_max_v: 2410.0\n",
      "lif layer 1 self.abs_max_v: 2414.0\n",
      "lif layer 1 self.abs_max_v: 2448.0\n",
      "lif layer 1 self.abs_max_v: 2476.0\n",
      "fc layer 1 self.abs_max_out: 1775.0\n",
      "lif layer 1 self.abs_max_v: 2547.0\n",
      "fc layer 2 self.abs_max_out: 1194.0\n",
      "fc layer 2 self.abs_max_out: 1201.0\n",
      "fc layer 2 self.abs_max_out: 1205.0\n",
      "fc layer 2 self.abs_max_out: 1215.0\n",
      "lif layer 2 self.abs_max_v: 2231.0\n",
      "lif layer 2 self.abs_max_v: 2241.5\n",
      "fc layer 2 self.abs_max_out: 1216.0\n",
      "lif layer 2 self.abs_max_v: 2337.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "lif layer 2 self.abs_max_v: 2422.0\n",
      "fc layer 2 self.abs_max_out: 1293.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 2609.0\n",
      "lif layer 1 self.abs_max_v: 2669.0\n",
      "lif layer 1 self.abs_max_v: 2775.0\n",
      "lif layer 1 self.abs_max_v: 2939.5\n",
      "lif layer 1 self.abs_max_v: 3063.0\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 3 self.abs_max_out: 576.0\n",
      "fc layer 1 self.abs_max_out: 1777.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "fc layer 1 self.abs_max_out: 1805.0\n",
      "lif layer 1 self.abs_max_v: 3161.0\n",
      "lif layer 1 self.abs_max_v: 3296.5\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "lif layer 1 self.abs_max_v: 3320.5\n",
      "lif layer 1 self.abs_max_v: 3475.5\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "lif layer 1 self.abs_max_v: 3696.0\n",
      "lif layer 1 self.abs_max_v: 3727.0\n",
      "fc layer 1 self.abs_max_out: 1985.0\n",
      "fc layer 1 self.abs_max_out: 2077.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "fc layer 1 self.abs_max_out: 2198.0\n",
      "lif layer 1 self.abs_max_v: 3811.0\n",
      "fc layer 1 self.abs_max_out: 2302.0\n",
      "lif layer 1 self.abs_max_v: 4207.5\n",
      "fc layer 1 self.abs_max_out: 2448.0\n",
      "lif layer 1 self.abs_max_v: 4552.0\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "fc layer 1 self.abs_max_out: 2518.0\n",
      "lif layer 1 self.abs_max_v: 4579.5\n",
      "fc layer 2 self.abs_max_out: 1383.0\n",
      "fc layer 2 self.abs_max_out: 1472.0\n",
      "fc layer 1 self.abs_max_out: 2787.0\n",
      "lif layer 1 self.abs_max_v: 4753.5\n",
      "lif layer 2 self.abs_max_v: 2490.0\n",
      "lif layer 2 self.abs_max_v: 2495.0\n",
      "lif layer 2 self.abs_max_v: 2609.5\n",
      "lif layer 2 self.abs_max_v: 2677.0\n",
      "fc layer 2 self.abs_max_out: 1482.0\n",
      "lif layer 1 self.abs_max_v: 4772.5\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "lif layer 1 self.abs_max_v: 4855.0\n",
      "lif layer 1 self.abs_max_v: 5069.5\n",
      "fc layer 1 self.abs_max_out: 2827.0\n",
      "lif layer 1 self.abs_max_v: 5362.0\n",
      "fc layer 2 self.abs_max_out: 1509.0\n",
      "fc layer 1 self.abs_max_out: 2914.0\n",
      "fc layer 1 self.abs_max_out: 3015.0\n",
      "lif layer 1 self.abs_max_v: 5676.0\n",
      "lif layer 1 self.abs_max_v: 5827.0\n",
      "lif layer 1 self.abs_max_v: 5904.5\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.301236/  1.846969, val:  25.83%, val_best:  25.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 69.3799%\n",
      "layer   3  Sparsity: 52.9157%\n",
      "total_backward_count 9790 real_backward_count 1240  12.666%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "lif layer 2 self.abs_max_v: 2727.5\n",
      "lif layer 2 self.abs_max_v: 2816.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "fc layer 3 self.abs_max_out: 827.0\n",
      "fc layer 3 self.abs_max_out: 838.0\n",
      "fc layer 1 self.abs_max_out: 3040.0\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "lif layer 2 self.abs_max_v: 2882.5\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "fc layer 3 self.abs_max_out: 906.0\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "fc layer 2 self.abs_max_out: 1550.0\n",
      "fc layer 2 self.abs_max_out: 1552.0\n",
      "fc layer 2 self.abs_max_out: 1588.0\n",
      "lif layer 2 self.abs_max_v: 3027.0\n",
      "lif layer 2 self.abs_max_v: 3028.0\n",
      "lif layer 2 self.abs_max_v: 3052.5\n",
      "fc layer 1 self.abs_max_out: 3142.0\n",
      "fc layer 1 self.abs_max_out: 3202.0\n",
      "fc layer 1 self.abs_max_out: 3220.0\n",
      "lif layer 1 self.abs_max_v: 6090.5\n",
      "lif layer 1 self.abs_max_v: 6205.5\n",
      "lif layer 1 self.abs_max_v: 6248.0\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.133191/  1.646988, val:  44.58%, val_best:  44.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6763%\n",
      "layer   3  Sparsity: 58.2738%\n",
      "total_backward_count 19580 real_backward_count 2435  12.436%\n",
      "lif layer 2 self.abs_max_v: 3104.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 2 self.abs_max_out: 1680.0\n",
      "lif layer 2 self.abs_max_v: 3125.5\n",
      "fc layer 1 self.abs_max_out: 3274.0\n",
      "fc layer 1 self.abs_max_out: 3356.0\n",
      "fc layer 1 self.abs_max_out: 3406.0\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "fc layer 2 self.abs_max_out: 1690.0\n",
      "fc layer 2 self.abs_max_out: 1706.0\n",
      "fc layer 3 self.abs_max_out: 963.0\n",
      "fc layer 1 self.abs_max_out: 3589.0\n",
      "lif layer 1 self.abs_max_v: 6491.5\n",
      "lif layer 2 self.abs_max_v: 3185.0\n",
      "fc layer 3 self.abs_max_out: 989.0\n",
      "fc layer 3 self.abs_max_out: 996.0\n",
      "fc layer 1 self.abs_max_out: 3633.0\n",
      "lif layer 1 self.abs_max_v: 6620.5\n",
      "lif layer 1 self.abs_max_v: 6891.5\n",
      "fc layer 1 self.abs_max_out: 3659.0\n",
      "lif layer 1 self.abs_max_v: 7105.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "lif layer 2 self.abs_max_v: 3211.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.108314/  1.689224, val:  36.25%, val_best:  44.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5826%\n",
      "layer   3  Sparsity: 59.7933%\n",
      "total_backward_count 29370 real_backward_count 3631  12.363%\n",
      "fc layer 3 self.abs_max_out: 1007.0\n",
      "fc layer 3 self.abs_max_out: 1031.0\n",
      "lif layer 2 self.abs_max_v: 3298.5\n",
      "fc layer 1 self.abs_max_out: 3670.0\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "lif layer 2 self.abs_max_v: 3312.5\n",
      "lif layer 2 self.abs_max_v: 3357.5\n",
      "fc layer 1 self.abs_max_out: 3979.0\n",
      "fc layer 2 self.abs_max_out: 1827.0\n",
      "fc layer 2 self.abs_max_out: 1868.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 2 self.abs_max_out: 2054.0\n",
      "lif layer 1 self.abs_max_v: 7227.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.044884/  1.551379, val:  50.83%, val_best:  50.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.3054%\n",
      "layer   3  Sparsity: 60.6110%\n",
      "total_backward_count 39160 real_backward_count 4800  12.257%\n",
      "fc layer 1 self.abs_max_out: 4113.0\n",
      "lif layer 2 self.abs_max_v: 3377.5\n",
      "lif layer 2 self.abs_max_v: 3461.5\n",
      "fc layer 3 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 3493.0\n",
      "lif layer 2 self.abs_max_v: 3522.5\n",
      "lif layer 2 self.abs_max_v: 3530.5\n",
      "lif layer 2 self.abs_max_v: 3559.5\n",
      "lif layer 2 self.abs_max_v: 3696.0\n",
      "lif layer 2 self.abs_max_v: 3763.5\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "lif layer 1 self.abs_max_v: 7539.5\n",
      "lif layer 1 self.abs_max_v: 7623.0\n",
      "lif layer 1 self.abs_max_v: 7732.5\n",
      "fc layer 3 self.abs_max_out: 1122.0\n",
      "fc layer 3 self.abs_max_out: 1141.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  0.998552/  1.462889, val:  52.50%, val_best:  52.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.4393%\n",
      "layer   3  Sparsity: 59.6458%\n",
      "total_backward_count 48950 real_backward_count 5933  12.121%\n",
      "fc layer 3 self.abs_max_out: 1192.0\n",
      "fc layer 2 self.abs_max_out: 2066.0\n",
      "lif layer 1 self.abs_max_v: 7915.5\n",
      "lif layer 1 self.abs_max_v: 7944.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  0.947101/  1.623563, val:  37.08%, val_best:  52.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2121%\n",
      "layer   3  Sparsity: 58.5938%\n",
      "total_backward_count 58740 real_backward_count 7062  12.022%\n",
      "lif layer 2 self.abs_max_v: 3863.0\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "fc layer 3 self.abs_max_out: 1249.0\n",
      "fc layer 3 self.abs_max_out: 1274.0\n",
      "lif layer 2 self.abs_max_v: 3897.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  0.912853/  1.549063, val:  44.58%, val_best:  52.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4441%\n",
      "layer   3  Sparsity: 57.7320%\n",
      "total_backward_count 68530 real_backward_count 8151  11.894%\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "fc layer 2 self.abs_max_out: 2415.0\n",
      "lif layer 2 self.abs_max_v: 4287.0\n",
      "fc layer 1 self.abs_max_out: 4396.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  0.864007/  1.374348, val:  53.75%, val_best:  53.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7949%\n",
      "layer   3  Sparsity: 58.9062%\n",
      "total_backward_count 78320 real_backward_count 9255  11.817%\n",
      "fc layer 1 self.abs_max_out: 4489.0\n",
      "lif layer 2 self.abs_max_v: 4290.0\n",
      "lif layer 2 self.abs_max_v: 4333.0\n",
      "fc layer 2 self.abs_max_out: 2477.0\n",
      "lif layer 2 self.abs_max_v: 4546.5\n",
      "fc layer 1 self.abs_max_out: 4566.0\n",
      "fc layer 1 self.abs_max_out: 4626.0\n",
      "lif layer 1 self.abs_max_v: 8100.5\n",
      "lif layer 1 self.abs_max_v: 8194.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  0.865390/  1.430772, val:  51.25%, val_best:  53.75%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.4187%\n",
      "layer   3  Sparsity: 60.5665%\n",
      "total_backward_count 88110 real_backward_count 10347  11.743%\n",
      "lif layer 2 self.abs_max_v: 4605.5\n",
      "fc layer 2 self.abs_max_out: 2579.0\n",
      "lif layer 2 self.abs_max_v: 4691.5\n",
      "lif layer 2 self.abs_max_v: 4715.0\n",
      "lif layer 2 self.abs_max_v: 4819.5\n",
      "lif layer 2 self.abs_max_v: 4904.0\n",
      "fc layer 3 self.abs_max_out: 1354.0\n",
      "fc layer 2 self.abs_max_out: 2617.0\n",
      "lif layer 1 self.abs_max_v: 8231.0\n",
      "lif layer 1 self.abs_max_v: 8259.5\n",
      "fc layer 1 self.abs_max_out: 4893.0\n",
      "lif layer 1 self.abs_max_v: 8345.5\n",
      "lif layer 1 self.abs_max_v: 8688.5\n",
      "lif layer 1 self.abs_max_v: 8874.5\n",
      "lif layer 1 self.abs_max_v: 9028.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.834090/  1.403517, val:  54.17%, val_best:  54.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.9005%\n",
      "layer   3  Sparsity: 60.8677%\n",
      "total_backward_count 97900 real_backward_count 11398  11.642%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.790789/  1.373420, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.1327%\n",
      "layer   3  Sparsity: 60.3855%\n",
      "total_backward_count 107690 real_backward_count 12420  11.533%\n",
      "fc layer 1 self.abs_max_out: 4907.0\n",
      "fc layer 1 self.abs_max_out: 5014.0\n",
      "fc layer 1 self.abs_max_out: 5134.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.778036/  1.377163, val:  51.25%, val_best:  55.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.2580%\n",
      "layer   3  Sparsity: 60.0692%\n",
      "total_backward_count 117480 real_backward_count 13454  11.452%\n",
      "fc layer 3 self.abs_max_out: 1370.0\n",
      "fc layer 1 self.abs_max_out: 5272.0\n",
      "fc layer 1 self.abs_max_out: 5276.0\n",
      "fc layer 1 self.abs_max_out: 5367.0\n",
      "fc layer 1 self.abs_max_out: 5431.0\n",
      "fc layer 1 self.abs_max_out: 5497.0\n",
      "fc layer 1 self.abs_max_out: 5534.0\n",
      "fc layer 1 self.abs_max_out: 5746.0\n",
      "fc layer 1 self.abs_max_out: 5766.0\n",
      "fc layer 1 self.abs_max_out: 6008.0\n",
      "fc layer 1 self.abs_max_out: 6064.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.732791/  1.320128, val:  57.92%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.4781%\n",
      "layer   3  Sparsity: 60.2126%\n",
      "total_backward_count 127270 real_backward_count 14448  11.352%\n",
      "fc layer 1 self.abs_max_out: 6067.0\n",
      "fc layer 1 self.abs_max_out: 6091.0\n",
      "fc layer 1 self.abs_max_out: 6110.0\n",
      "fc layer 3 self.abs_max_out: 1373.0\n",
      "lif layer 2 self.abs_max_v: 4918.0\n",
      "lif layer 1 self.abs_max_v: 9135.5\n",
      "lif layer 1 self.abs_max_v: 9223.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.748058/  1.382440, val:  49.58%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4036%\n",
      "layer   3  Sparsity: 59.6970%\n",
      "total_backward_count 137060 real_backward_count 15410  11.243%\n",
      "fc layer 3 self.abs_max_out: 1386.0\n",
      "fc layer 1 self.abs_max_out: 6130.0\n",
      "fc layer 1 self.abs_max_out: 6177.0\n",
      "fc layer 3 self.abs_max_out: 1436.0\n",
      "fc layer 3 self.abs_max_out: 1470.0\n",
      "lif layer 1 self.abs_max_v: 9333.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.699725/  1.383279, val:  53.33%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6698%\n",
      "layer   3  Sparsity: 59.9767%\n",
      "total_backward_count 146850 real_backward_count 16380  11.154%\n",
      "lif layer 2 self.abs_max_v: 4945.5\n",
      "lif layer 1 self.abs_max_v: 9588.5\n",
      "lif layer 1 self.abs_max_v: 9765.5\n",
      "fc layer 3 self.abs_max_out: 1471.0\n",
      "fc layer 3 self.abs_max_out: 1500.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.698598/  1.301165, val:  57.08%, val_best:  57.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6104%\n",
      "layer   3  Sparsity: 60.4223%\n",
      "total_backward_count 156640 real_backward_count 17379  11.095%\n",
      "fc layer 3 self.abs_max_out: 1569.0\n",
      "fc layer 3 self.abs_max_out: 1628.0\n",
      "lif layer 1 self.abs_max_v: 10160.0\n",
      "lif layer 1 self.abs_max_v: 10360.0\n",
      "lif layer 1 self.abs_max_v: 10599.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.654454/  1.229546, val:  62.08%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2923%\n",
      "layer   3  Sparsity: 60.8488%\n",
      "total_backward_count 166430 real_backward_count 18317  11.006%\n",
      "fc layer 3 self.abs_max_out: 1656.0\n",
      "fc layer 3 self.abs_max_out: 1753.0\n",
      "fc layer 3 self.abs_max_out: 1780.0\n",
      "fc layer 1 self.abs_max_out: 6211.0\n",
      "fc layer 1 self.abs_max_out: 6369.0\n",
      "lif layer 1 self.abs_max_v: 10678.5\n",
      "lif layer 1 self.abs_max_v: 11064.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.654104/  1.198346, val:  60.83%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6071%\n",
      "layer   3  Sparsity: 61.3810%\n",
      "total_backward_count 176220 real_backward_count 19294  10.949%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.630299/  1.397837, val:  49.17%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.1249%\n",
      "layer   3  Sparsity: 61.0757%\n",
      "total_backward_count 186010 real_backward_count 20262  10.893%\n",
      "fc layer 1 self.abs_max_out: 6471.0\n",
      "fc layer 2 self.abs_max_out: 2633.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.608782/  1.358430, val:  49.17%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.1957%\n",
      "layer   3  Sparsity: 61.4139%\n",
      "total_backward_count 195800 real_backward_count 21171  10.813%\n",
      "fc layer 1 self.abs_max_out: 6582.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.573720/  1.222491, val:  57.92%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3672%\n",
      "layer   3  Sparsity: 61.7579%\n",
      "total_backward_count 205590 real_backward_count 22058  10.729%\n",
      "fc layer 1 self.abs_max_out: 6797.0\n",
      "fc layer 2 self.abs_max_out: 2711.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.572832/  1.161448, val:  60.83%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4422%\n",
      "layer   3  Sparsity: 60.1101%\n",
      "total_backward_count 215380 real_backward_count 22998  10.678%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.544947/  1.223284, val:  56.67%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6356%\n",
      "layer   3  Sparsity: 59.9822%\n",
      "total_backward_count 225170 real_backward_count 23896  10.612%\n",
      "fc layer 1 self.abs_max_out: 6914.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.519467/  1.156047, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.1682%\n",
      "layer   3  Sparsity: 60.0164%\n",
      "total_backward_count 234960 real_backward_count 24734  10.527%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.507063/  1.129934, val:  60.83%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6830%\n",
      "layer   3  Sparsity: 60.1340%\n",
      "total_backward_count 244750 real_backward_count 25516  10.425%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.502674/  1.087980, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5315%\n",
      "layer   3  Sparsity: 60.4910%\n",
      "total_backward_count 254540 real_backward_count 26311  10.337%\n",
      "fc layer 2 self.abs_max_out: 2836.0\n",
      "fc layer 2 self.abs_max_out: 2850.0\n",
      "lif layer 2 self.abs_max_v: 5202.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.488400/  1.159167, val:  62.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5301%\n",
      "layer   3  Sparsity: 60.2671%\n",
      "total_backward_count 264330 real_backward_count 27101  10.253%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.459643/  1.003074, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.3244%\n",
      "layer   3  Sparsity: 60.6094%\n",
      "total_backward_count 274120 real_backward_count 27864  10.165%\n",
      "fc layer 3 self.abs_max_out: 1814.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.456177/  1.156398, val:  68.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.8977%\n",
      "layer   3  Sparsity: 59.7231%\n",
      "total_backward_count 283910 real_backward_count 28540  10.052%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.425290/  1.108038, val:  60.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.7805%\n",
      "layer   3  Sparsity: 59.6913%\n",
      "total_backward_count 293700 real_backward_count 29222   9.950%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.416735/  0.996323, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.8662%\n",
      "layer   3  Sparsity: 59.6727%\n",
      "total_backward_count 303490 real_backward_count 29881   9.846%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.416314/  1.120242, val:  66.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.6598%\n",
      "layer   3  Sparsity: 59.1735%\n",
      "total_backward_count 313280 real_backward_count 30545   9.750%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.404030/  1.034973, val:  68.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.1849%\n",
      "layer   3  Sparsity: 59.0676%\n",
      "total_backward_count 323070 real_backward_count 31174   9.649%\n",
      "lif layer 1 self.abs_max_v: 11383.5\n",
      "lif layer 1 self.abs_max_v: 11941.0\n",
      "lif layer 1 self.abs_max_v: 12023.5\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.386682/  1.069140, val:  68.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.1344%\n",
      "layer   3  Sparsity: 60.0653%\n",
      "total_backward_count 332860 real_backward_count 31771   9.545%\n",
      "lif layer 1 self.abs_max_v: 12140.5\n",
      "lif layer 1 self.abs_max_v: 12355.0\n",
      "lif layer 1 self.abs_max_v: 12458.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.379504/  1.150924, val:  67.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.0361%\n",
      "layer   3  Sparsity: 60.6064%\n",
      "total_backward_count 342650 real_backward_count 32373   9.448%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.383119/  0.957560, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.8740%\n",
      "layer   3  Sparsity: 60.8109%\n",
      "total_backward_count 352440 real_backward_count 32995   9.362%\n",
      "fc layer 3 self.abs_max_out: 1843.0\n",
      "lif layer 1 self.abs_max_v: 12470.0\n",
      "lif layer 1 self.abs_max_v: 12559.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.375858/  0.954268, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.9178%\n",
      "layer   3  Sparsity: 60.2385%\n",
      "total_backward_count 362230 real_backward_count 33615   9.280%\n",
      "lif layer 1 self.abs_max_v: 12573.5\n",
      "fc layer 1 self.abs_max_out: 6987.0\n",
      "lif layer 1 self.abs_max_v: 12651.5\n",
      "lif layer 2 self.abs_max_v: 5210.5\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.350291/  1.148378, val:  65.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.2361%\n",
      "layer   3  Sparsity: 60.2499%\n",
      "total_backward_count 372020 real_backward_count 34156   9.181%\n",
      "fc layer 2 self.abs_max_out: 2992.0\n",
      "lif layer 2 self.abs_max_v: 5251.0\n",
      "lif layer 2 self.abs_max_v: 5325.5\n",
      "fc layer 1 self.abs_max_out: 7089.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.364816/  1.145677, val:  72.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4379%\n",
      "layer   3  Sparsity: 60.5560%\n",
      "total_backward_count 381810 real_backward_count 34730   9.096%\n",
      "lif layer 1 self.abs_max_v: 12750.0\n",
      "lif layer 2 self.abs_max_v: 5343.5\n",
      "lif layer 2 self.abs_max_v: 5382.5\n",
      "fc layer 2 self.abs_max_out: 3207.0\n",
      "lif layer 2 self.abs_max_v: 5386.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.357263/  1.187945, val:  65.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4055%\n",
      "layer   3  Sparsity: 60.0467%\n",
      "total_backward_count 391600 real_backward_count 35265   9.005%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.339328/  1.061325, val:  67.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4610%\n",
      "layer   3  Sparsity: 59.7637%\n",
      "total_backward_count 401390 real_backward_count 35821   8.924%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.333721/  1.012735, val:  71.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.1130%\n",
      "layer   3  Sparsity: 60.7936%\n",
      "total_backward_count 411180 real_backward_count 36375   8.846%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.323461/  1.014761, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.7486%\n",
      "layer   3  Sparsity: 61.3761%\n",
      "total_backward_count 420970 real_backward_count 36926   8.772%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.303386/  0.943599, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.9648%\n",
      "layer   3  Sparsity: 60.6331%\n",
      "total_backward_count 430760 real_backward_count 37390   8.680%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.301255/  0.986441, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.0431%\n",
      "layer   3  Sparsity: 61.4601%\n",
      "total_backward_count 440550 real_backward_count 37857   8.593%\n",
      "fc layer 3 self.abs_max_out: 1968.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.288503/  1.023847, val:  69.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.0195%\n",
      "layer   3  Sparsity: 61.7842%\n",
      "total_backward_count 450340 real_backward_count 38279   8.500%\n",
      "fc layer 3 self.abs_max_out: 1969.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.285158/  0.970505, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.0518%\n",
      "layer   3  Sparsity: 61.6000%\n",
      "total_backward_count 460130 real_backward_count 38731   8.417%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.281898/  0.994724, val:  70.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 73.9393%\n",
      "layer   3  Sparsity: 62.5692%\n",
      "total_backward_count 469920 real_backward_count 39158   8.333%\n",
      "lif layer 1 self.abs_max_v: 12813.5\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.271402/  0.969951, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.1786%\n",
      "layer   3  Sparsity: 62.3357%\n",
      "total_backward_count 479710 real_backward_count 39529   8.240%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.271362/  0.943683, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6766%\n",
      "layer   3  Sparsity: 61.7709%\n",
      "total_backward_count 489500 real_backward_count 39901   8.151%\n",
      "lif layer 1 self.abs_max_v: 12858.0\n",
      "fc layer 1 self.abs_max_out: 7181.0\n",
      "lif layer 1 self.abs_max_v: 13610.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.277608/  0.971895, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5906%\n",
      "layer   3  Sparsity: 61.5455%\n",
      "total_backward_count 499290 real_backward_count 40294   8.070%\n",
      "fc layer 1 self.abs_max_out: 7237.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.271937/  1.018358, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6543%\n",
      "layer   3  Sparsity: 61.6300%\n",
      "total_backward_count 509080 real_backward_count 40669   7.989%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.276854/  0.911917, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5847%\n",
      "layer   3  Sparsity: 62.7511%\n",
      "total_backward_count 518870 real_backward_count 41036   7.909%\n",
      "lif layer 2 self.abs_max_v: 5469.0\n",
      "lif layer 2 self.abs_max_v: 5530.5\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.277683/  0.975277, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4935%\n",
      "layer   3  Sparsity: 62.6794%\n",
      "total_backward_count 528660 real_backward_count 41399   7.831%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.274959/  0.891139, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.3064%\n",
      "layer   3  Sparsity: 62.3577%\n",
      "total_backward_count 538450 real_backward_count 41775   7.758%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.260377/  0.913592, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.2984%\n",
      "layer   3  Sparsity: 62.7443%\n",
      "total_backward_count 548240 real_backward_count 42125   7.684%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.246880/  0.902885, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.2957%\n",
      "layer   3  Sparsity: 62.7143%\n",
      "total_backward_count 558030 real_backward_count 42411   7.600%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.236021/  0.924729, val:  77.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6230%\n",
      "layer   3  Sparsity: 62.7676%\n",
      "total_backward_count 567820 real_backward_count 42673   7.515%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.244096/  0.899406, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.0579%\n",
      "layer   3  Sparsity: 61.8798%\n",
      "total_backward_count 577610 real_backward_count 42978   7.441%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.247968/  0.944384, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.0757%\n",
      "layer   3  Sparsity: 62.0469%\n",
      "total_backward_count 587400 real_backward_count 43272   7.367%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.231932/  0.991571, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8929%\n",
      "layer   3  Sparsity: 62.8502%\n",
      "total_backward_count 597190 real_backward_count 43544   7.291%\n",
      "lif layer 2 self.abs_max_v: 5831.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.232677/  0.877705, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8210%\n",
      "layer   3  Sparsity: 62.8565%\n",
      "total_backward_count 606980 real_backward_count 43820   7.219%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.221073/  0.853812, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.7917%\n",
      "layer   3  Sparsity: 63.3804%\n",
      "total_backward_count 616770 real_backward_count 44074   7.146%\n",
      "fc layer 3 self.abs_max_out: 1977.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.211344/  0.924952, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6351%\n",
      "layer   3  Sparsity: 63.5149%\n",
      "total_backward_count 626560 real_backward_count 44277   7.067%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.210732/  0.937473, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4849%\n",
      "layer   3  Sparsity: 63.6066%\n",
      "total_backward_count 636350 real_backward_count 44482   6.990%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.223233/  1.010663, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.3995%\n",
      "layer   3  Sparsity: 62.7163%\n",
      "total_backward_count 646140 real_backward_count 44754   6.926%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.227654/  1.009741, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5338%\n",
      "layer   3  Sparsity: 62.4812%\n",
      "total_backward_count 655930 real_backward_count 45015   6.863%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.234374/  0.925239, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4537%\n",
      "layer   3  Sparsity: 62.2399%\n",
      "total_backward_count 665720 real_backward_count 45264   6.799%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.232150/  0.909072, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.4099%\n",
      "layer   3  Sparsity: 61.8705%\n",
      "total_backward_count 675510 real_backward_count 45512   6.737%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.219563/  1.060745, val:  67.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.7115%\n",
      "layer   3  Sparsity: 62.2539%\n",
      "total_backward_count 685300 real_backward_count 45724   6.672%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.202107/  0.885024, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5719%\n",
      "layer   3  Sparsity: 62.8940%\n",
      "total_backward_count 695090 real_backward_count 45903   6.604%\n",
      "fc layer 3 self.abs_max_out: 1995.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.193367/  0.900594, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6966%\n",
      "layer   3  Sparsity: 63.3303%\n",
      "total_backward_count 704880 real_backward_count 46079   6.537%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.199659/  0.945958, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6153%\n",
      "layer   3  Sparsity: 62.7777%\n",
      "total_backward_count 714670 real_backward_count 46248   6.471%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.189322/  0.916566, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.6265%\n",
      "layer   3  Sparsity: 62.8890%\n",
      "total_backward_count 724460 real_backward_count 46414   6.407%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.202276/  0.947199, val:  83.33%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8663%\n",
      "layer   3  Sparsity: 63.1944%\n",
      "total_backward_count 734250 real_backward_count 46601   6.347%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.200387/  0.906109, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.7822%\n",
      "layer   3  Sparsity: 63.3669%\n",
      "total_backward_count 744040 real_backward_count 46757   6.284%\n",
      "fc layer 3 self.abs_max_out: 2004.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.192755/  0.918860, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.3328%\n",
      "layer   3  Sparsity: 63.0202%\n",
      "total_backward_count 753830 real_backward_count 46915   6.224%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.183412/  0.943849, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.2503%\n",
      "layer   3  Sparsity: 62.5807%\n",
      "total_backward_count 763620 real_backward_count 47067   6.164%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.187264/  0.914808, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5122%\n",
      "layer   3  Sparsity: 62.5304%\n",
      "total_backward_count 773410 real_backward_count 47225   6.106%\n",
      "fc layer 3 self.abs_max_out: 2015.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.182535/  0.873900, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.5875%\n",
      "layer   3  Sparsity: 62.8532%\n",
      "total_backward_count 783200 real_backward_count 47364   6.047%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.177954/  0.866788, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.9065%\n",
      "layer   3  Sparsity: 62.5335%\n",
      "total_backward_count 792990 real_backward_count 47489   5.989%\n",
      "fc layer 3 self.abs_max_out: 2038.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.174898/  0.902121, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8160%\n",
      "layer   3  Sparsity: 63.0265%\n",
      "total_backward_count 802780 real_backward_count 47643   5.935%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.176143/  0.914593, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.7491%\n",
      "layer   3  Sparsity: 62.8582%\n",
      "total_backward_count 812570 real_backward_count 47768   5.879%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.177210/  0.897053, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.9094%\n",
      "layer   3  Sparsity: 62.5666%\n",
      "total_backward_count 822360 real_backward_count 47889   5.823%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.181306/  0.947430, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8217%\n",
      "layer   3  Sparsity: 62.3538%\n",
      "total_backward_count 832150 real_backward_count 48037   5.773%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.171203/  0.905325, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.0236%\n",
      "layer   3  Sparsity: 62.8500%\n",
      "total_backward_count 841940 real_backward_count 48170   5.721%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.162315/  0.888310, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.1814%\n",
      "layer   3  Sparsity: 63.0226%\n",
      "total_backward_count 851730 real_backward_count 48289   5.670%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.159932/  0.986272, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2258%\n",
      "layer   3  Sparsity: 62.9450%\n",
      "total_backward_count 861520 real_backward_count 48379   5.616%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.162531/  0.894930, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3031%\n",
      "layer   3  Sparsity: 62.9150%\n",
      "total_backward_count 871310 real_backward_count 48468   5.563%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.164673/  0.881421, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.0958%\n",
      "layer   3  Sparsity: 62.1244%\n",
      "total_backward_count 881100 real_backward_count 48577   5.513%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.157071/  0.917054, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.1280%\n",
      "layer   3  Sparsity: 61.9983%\n",
      "total_backward_count 890890 real_backward_count 48664   5.462%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.167137/  0.873942, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8871%\n",
      "layer   3  Sparsity: 62.0419%\n",
      "total_backward_count 900680 real_backward_count 48767   5.414%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.165024/  0.889974, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2458%\n",
      "layer   3  Sparsity: 62.4954%\n",
      "total_backward_count 910470 real_backward_count 48873   5.368%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.155846/  0.848397, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2769%\n",
      "layer   3  Sparsity: 62.4617%\n",
      "total_backward_count 920260 real_backward_count 48953   5.319%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.161581/  0.874928, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 74.8889%\n",
      "layer   3  Sparsity: 62.4105%\n",
      "total_backward_count 930050 real_backward_count 49058   5.275%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.153188/  0.887695, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.0265%\n",
      "layer   3  Sparsity: 62.7485%\n",
      "total_backward_count 939840 real_backward_count 49149   5.230%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.152799/  0.947937, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3397%\n",
      "layer   3  Sparsity: 62.5361%\n",
      "total_backward_count 949630 real_backward_count 49250   5.186%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.147204/  0.889939, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4453%\n",
      "layer   3  Sparsity: 62.6711%\n",
      "total_backward_count 959420 real_backward_count 49346   5.143%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.142117/  0.895133, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5060%\n",
      "layer   3  Sparsity: 62.8568%\n",
      "total_backward_count 969210 real_backward_count 49406   5.098%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.141691/  0.880344, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5976%\n",
      "layer   3  Sparsity: 62.7856%\n",
      "total_backward_count 979000 real_backward_count 49487   5.055%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.140887/  0.898678, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5945%\n",
      "layer   3  Sparsity: 62.8155%\n",
      "total_backward_count 988790 real_backward_count 49552   5.011%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.135059/  0.879532, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5044%\n",
      "layer   3  Sparsity: 63.5779%\n",
      "total_backward_count 998580 real_backward_count 49632   4.970%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.130670/  0.910378, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4959%\n",
      "layer   3  Sparsity: 63.6216%\n",
      "total_backward_count 1008370 real_backward_count 49685   4.927%\n",
      "fc layer 3 self.abs_max_out: 2067.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.135309/  0.886742, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4325%\n",
      "layer   3  Sparsity: 63.8956%\n",
      "total_backward_count 1018160 real_backward_count 49757   4.887%\n",
      "fc layer 3 self.abs_max_out: 2121.0\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.136833/  0.880735, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4970%\n",
      "layer   3  Sparsity: 63.9117%\n",
      "total_backward_count 1027950 real_backward_count 49836   4.848%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.129379/  0.915047, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4406%\n",
      "layer   3  Sparsity: 63.6813%\n",
      "total_backward_count 1037740 real_backward_count 49884   4.807%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.136440/  0.902116, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2999%\n",
      "layer   3  Sparsity: 63.6577%\n",
      "total_backward_count 1047530 real_backward_count 49972   4.770%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.137420/  0.906893, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2420%\n",
      "layer   3  Sparsity: 63.8661%\n",
      "total_backward_count 1057320 real_backward_count 50037   4.732%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.125452/  0.898314, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4124%\n",
      "layer   3  Sparsity: 63.9171%\n",
      "total_backward_count 1067110 real_backward_count 50096   4.695%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.128443/  0.934654, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4510%\n",
      "layer   3  Sparsity: 64.1380%\n",
      "total_backward_count 1076900 real_backward_count 50170   4.659%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.126662/  0.949027, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5791%\n",
      "layer   3  Sparsity: 64.4329%\n",
      "total_backward_count 1086690 real_backward_count 50234   4.623%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.126119/  0.911966, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6763%\n",
      "layer   3  Sparsity: 64.1870%\n",
      "total_backward_count 1096480 real_backward_count 50281   4.586%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.129915/  0.859163, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4726%\n",
      "layer   3  Sparsity: 64.2516%\n",
      "total_backward_count 1106270 real_backward_count 50344   4.551%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.122122/  0.907520, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3919%\n",
      "layer   3  Sparsity: 64.4785%\n",
      "total_backward_count 1116060 real_backward_count 50407   4.517%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.129869/  0.870157, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4045%\n",
      "layer   3  Sparsity: 64.0971%\n",
      "total_backward_count 1125850 real_backward_count 50465   4.482%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.128392/  0.840122, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2682%\n",
      "layer   3  Sparsity: 64.1116%\n",
      "total_backward_count 1135640 real_backward_count 50533   4.450%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.125728/  0.865109, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4265%\n",
      "layer   3  Sparsity: 64.1249%\n",
      "total_backward_count 1145430 real_backward_count 50595   4.417%\n",
      "lif layer 2 self.abs_max_v: 5862.5\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.121338/  0.879351, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5334%\n",
      "layer   3  Sparsity: 63.9927%\n",
      "total_backward_count 1155220 real_backward_count 50646   4.384%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.117507/  0.883112, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5324%\n",
      "layer   3  Sparsity: 64.1594%\n",
      "total_backward_count 1165010 real_backward_count 50680   4.350%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.117772/  0.860303, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4848%\n",
      "layer   3  Sparsity: 63.7206%\n",
      "total_backward_count 1174800 real_backward_count 50738   4.319%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.113208/  0.839912, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3151%\n",
      "layer   3  Sparsity: 63.8914%\n",
      "total_backward_count 1184590 real_backward_count 50794   4.288%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.112371/  0.859048, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4172%\n",
      "layer   3  Sparsity: 63.8330%\n",
      "total_backward_count 1194380 real_backward_count 50847   4.257%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.105751/  0.875160, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2446%\n",
      "layer   3  Sparsity: 64.3706%\n",
      "total_backward_count 1204170 real_backward_count 50895   4.227%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.103860/  0.939460, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.2530%\n",
      "layer   3  Sparsity: 64.2588%\n",
      "total_backward_count 1213960 real_backward_count 50931   4.195%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.112953/  0.858251, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3763%\n",
      "layer   3  Sparsity: 64.4117%\n",
      "total_backward_count 1223750 real_backward_count 50982   4.166%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.108378/  0.867219, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4506%\n",
      "layer   3  Sparsity: 64.1503%\n",
      "total_backward_count 1233540 real_backward_count 51026   4.137%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.105874/  0.884070, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4917%\n",
      "layer   3  Sparsity: 64.2385%\n",
      "total_backward_count 1243330 real_backward_count 51067   4.107%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.103644/  0.872636, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4523%\n",
      "layer   3  Sparsity: 64.1409%\n",
      "total_backward_count 1253120 real_backward_count 51103   4.078%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.099554/  0.898748, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6201%\n",
      "layer   3  Sparsity: 64.5050%\n",
      "total_backward_count 1262910 real_backward_count 51141   4.049%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.098304/  0.886689, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7674%\n",
      "layer   3  Sparsity: 64.3939%\n",
      "total_backward_count 1272700 real_backward_count 51172   4.021%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.095334/  0.904122, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4554%\n",
      "layer   3  Sparsity: 64.7866%\n",
      "total_backward_count 1282490 real_backward_count 51198   3.992%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.102892/  0.895598, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4853%\n",
      "layer   3  Sparsity: 64.3437%\n",
      "total_backward_count 1292280 real_backward_count 51246   3.966%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.101463/  0.883858, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7811%\n",
      "layer   3  Sparsity: 63.9182%\n",
      "total_backward_count 1302070 real_backward_count 51284   3.939%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.095281/  0.898200, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8120%\n",
      "layer   3  Sparsity: 63.8626%\n",
      "total_backward_count 1311860 real_backward_count 51293   3.910%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.092756/  0.953539, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8091%\n",
      "layer   3  Sparsity: 63.8546%\n",
      "total_backward_count 1321650 real_backward_count 51306   3.882%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.104329/  0.898121, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7671%\n",
      "layer   3  Sparsity: 63.9317%\n",
      "total_backward_count 1331440 real_backward_count 51339   3.856%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.104945/  0.870053, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6200%\n",
      "layer   3  Sparsity: 64.0731%\n",
      "total_backward_count 1341230 real_backward_count 51381   3.831%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.109507/  0.896724, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5100%\n",
      "layer   3  Sparsity: 63.8234%\n",
      "total_backward_count 1351020 real_backward_count 51427   3.807%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.109783/  0.865248, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6283%\n",
      "layer   3  Sparsity: 64.2002%\n",
      "total_backward_count 1360810 real_backward_count 51471   3.782%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.101056/  0.894039, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6261%\n",
      "layer   3  Sparsity: 64.5296%\n",
      "total_backward_count 1370600 real_backward_count 51510   3.758%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.104355/  0.878679, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8114%\n",
      "layer   3  Sparsity: 64.5306%\n",
      "total_backward_count 1380390 real_backward_count 51549   3.734%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.094047/  0.895651, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8270%\n",
      "layer   3  Sparsity: 64.4452%\n",
      "total_backward_count 1390180 real_backward_count 51562   3.709%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.091546/  0.887732, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8033%\n",
      "layer   3  Sparsity: 64.4018%\n",
      "total_backward_count 1399970 real_backward_count 51581   3.684%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.098849/  0.892325, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7818%\n",
      "layer   3  Sparsity: 64.5920%\n",
      "total_backward_count 1409760 real_backward_count 51609   3.661%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.101144/  0.934714, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7256%\n",
      "layer   3  Sparsity: 64.7237%\n",
      "total_backward_count 1419550 real_backward_count 51640   3.638%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.097864/  0.898666, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6312%\n",
      "layer   3  Sparsity: 64.5210%\n",
      "total_backward_count 1429340 real_backward_count 51663   3.614%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.100631/  0.912145, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5961%\n",
      "layer   3  Sparsity: 64.5886%\n",
      "total_backward_count 1439130 real_backward_count 51707   3.593%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.093011/  0.871353, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5201%\n",
      "layer   3  Sparsity: 64.7351%\n",
      "total_backward_count 1448920 real_backward_count 51725   3.570%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.093511/  0.882368, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5911%\n",
      "layer   3  Sparsity: 64.6183%\n",
      "total_backward_count 1458710 real_backward_count 51741   3.547%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.093910/  0.949474, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5990%\n",
      "layer   3  Sparsity: 64.2164%\n",
      "total_backward_count 1468500 real_backward_count 51759   3.525%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.096075/  0.899601, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6574%\n",
      "layer   3  Sparsity: 64.3228%\n",
      "total_backward_count 1478290 real_backward_count 51776   3.502%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.095889/  0.907219, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6626%\n",
      "layer   3  Sparsity: 64.5543%\n",
      "total_backward_count 1488080 real_backward_count 51798   3.481%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.092438/  0.915187, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8160%\n",
      "layer   3  Sparsity: 64.5052%\n",
      "total_backward_count 1497870 real_backward_count 51826   3.460%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.093509/  0.870694, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8086%\n",
      "layer   3  Sparsity: 64.7408%\n",
      "total_backward_count 1507660 real_backward_count 51845   3.439%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.099256/  0.907702, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7767%\n",
      "layer   3  Sparsity: 64.3431%\n",
      "total_backward_count 1517450 real_backward_count 51874   3.418%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.105459/  0.870818, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7902%\n",
      "layer   3  Sparsity: 64.1080%\n",
      "total_backward_count 1527240 real_backward_count 51913   3.399%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.096292/  0.879718, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7893%\n",
      "layer   3  Sparsity: 64.5784%\n",
      "total_backward_count 1537030 real_backward_count 51936   3.379%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.100149/  0.899735, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.9394%\n",
      "layer   3  Sparsity: 64.4946%\n",
      "total_backward_count 1546820 real_backward_count 51969   3.360%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.095447/  0.927195, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8362%\n",
      "layer   3  Sparsity: 64.2060%\n",
      "total_backward_count 1556610 real_backward_count 51989   3.340%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.100182/  0.881670, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.9195%\n",
      "layer   3  Sparsity: 63.7472%\n",
      "total_backward_count 1566400 real_backward_count 52019   3.321%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.097828/  0.951499, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8986%\n",
      "layer   3  Sparsity: 63.7835%\n",
      "total_backward_count 1576190 real_backward_count 52053   3.302%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.097754/  0.889679, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8025%\n",
      "layer   3  Sparsity: 64.1458%\n",
      "total_backward_count 1585980 real_backward_count 52076   3.284%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.089322/  0.868556, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7117%\n",
      "layer   3  Sparsity: 64.4440%\n",
      "total_backward_count 1595770 real_backward_count 52092   3.264%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.092612/  0.842234, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5690%\n",
      "layer   3  Sparsity: 64.0907%\n",
      "total_backward_count 1605560 real_backward_count 52118   3.246%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.091421/  0.869217, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4878%\n",
      "layer   3  Sparsity: 64.0779%\n",
      "total_backward_count 1615350 real_backward_count 52137   3.228%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.097348/  0.961012, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4027%\n",
      "layer   3  Sparsity: 64.2180%\n",
      "total_backward_count 1625140 real_backward_count 52161   3.210%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.096461/  0.895031, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4772%\n",
      "layer   3  Sparsity: 64.6018%\n",
      "total_backward_count 1634930 real_backward_count 52189   3.192%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.091278/  0.874550, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5468%\n",
      "layer   3  Sparsity: 64.4467%\n",
      "total_backward_count 1644720 real_backward_count 52202   3.174%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.092831/  0.920319, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5466%\n",
      "layer   3  Sparsity: 64.4542%\n",
      "total_backward_count 1654510 real_backward_count 52230   3.157%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.094192/  0.860491, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3545%\n",
      "layer   3  Sparsity: 64.4045%\n",
      "total_backward_count 1664300 real_backward_count 52250   3.139%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.090585/  0.857577, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.3199%\n",
      "layer   3  Sparsity: 64.4103%\n",
      "total_backward_count 1674090 real_backward_count 52276   3.123%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.097564/  0.937677, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4158%\n",
      "layer   3  Sparsity: 64.0496%\n",
      "total_backward_count 1683880 real_backward_count 52301   3.106%\n",
      "fc layer 3 self.abs_max_out: 2152.0\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.098489/  0.901467, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6201%\n",
      "layer   3  Sparsity: 64.3041%\n",
      "total_backward_count 1693670 real_backward_count 52325   3.089%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.097697/  0.925632, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8024%\n",
      "layer   3  Sparsity: 64.3195%\n",
      "total_backward_count 1703460 real_backward_count 52365   3.074%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.093766/  0.865779, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8030%\n",
      "layer   3  Sparsity: 64.5559%\n",
      "total_backward_count 1713250 real_backward_count 52392   3.058%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.097731/  0.928023, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5963%\n",
      "layer   3  Sparsity: 64.7543%\n",
      "total_backward_count 1723040 real_backward_count 52429   3.043%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.096403/  0.879255, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5543%\n",
      "layer   3  Sparsity: 64.8626%\n",
      "total_backward_count 1732830 real_backward_count 52457   3.027%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.096236/  0.952181, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6712%\n",
      "layer   3  Sparsity: 65.0463%\n",
      "total_backward_count 1742620 real_backward_count 52483   3.012%\n",
      "lif layer 1 self.abs_max_v: 13747.0\n",
      "lif layer 1 self.abs_max_v: 13783.5\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.097382/  0.919374, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6793%\n",
      "layer   3  Sparsity: 65.1532%\n",
      "total_backward_count 1752410 real_backward_count 52527   2.997%\n",
      "fc layer 1 self.abs_max_out: 7340.0\n",
      "lif layer 1 self.abs_max_v: 13866.5\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.088473/  0.868525, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5530%\n",
      "layer   3  Sparsity: 64.8613%\n",
      "total_backward_count 1762200 real_backward_count 52541   2.982%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.091832/  0.907407, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6476%\n",
      "layer   3  Sparsity: 64.7700%\n",
      "total_backward_count 1771990 real_backward_count 52555   2.966%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.092677/  0.896995, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7613%\n",
      "layer   3  Sparsity: 64.7810%\n",
      "total_backward_count 1781780 real_backward_count 52592   2.952%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.088408/  0.948072, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7634%\n",
      "layer   3  Sparsity: 64.7855%\n",
      "total_backward_count 1791570 real_backward_count 52617   2.937%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.092861/  0.912332, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7561%\n",
      "layer   3  Sparsity: 64.6188%\n",
      "total_backward_count 1801360 real_backward_count 52630   2.922%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.101132/  0.947507, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.8681%\n",
      "layer   3  Sparsity: 64.4213%\n",
      "total_backward_count 1811150 real_backward_count 52669   2.908%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.106504/  0.993067, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7780%\n",
      "layer   3  Sparsity: 64.7035%\n",
      "total_backward_count 1820940 real_backward_count 52716   2.895%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.105309/  0.924983, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7069%\n",
      "layer   3  Sparsity: 64.9652%\n",
      "total_backward_count 1830730 real_backward_count 52763   2.882%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.101006/  0.868807, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5294%\n",
      "layer   3  Sparsity: 64.8972%\n",
      "total_backward_count 1840520 real_backward_count 52787   2.868%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.092675/  0.912628, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6174%\n",
      "layer   3  Sparsity: 65.1588%\n",
      "total_backward_count 1850310 real_backward_count 52798   2.853%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.094956/  0.930194, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5532%\n",
      "layer   3  Sparsity: 64.9935%\n",
      "total_backward_count 1860100 real_backward_count 52816   2.839%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.094695/  0.973387, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4554%\n",
      "layer   3  Sparsity: 64.9706%\n",
      "total_backward_count 1869890 real_backward_count 52844   2.826%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.099164/  0.908456, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5112%\n",
      "layer   3  Sparsity: 64.7701%\n",
      "total_backward_count 1879680 real_backward_count 52867   2.813%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.095735/  0.911390, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5654%\n",
      "layer   3  Sparsity: 64.7317%\n",
      "total_backward_count 1889470 real_backward_count 52888   2.799%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.090212/  0.918183, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6188%\n",
      "layer   3  Sparsity: 64.7084%\n",
      "total_backward_count 1899260 real_backward_count 52913   2.786%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.086779/  0.948703, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.5931%\n",
      "layer   3  Sparsity: 64.9259%\n",
      "total_backward_count 1909050 real_backward_count 52931   2.773%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.085467/  0.935507, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6790%\n",
      "layer   3  Sparsity: 65.0398%\n",
      "total_backward_count 1918840 real_backward_count 52942   2.759%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.086906/  0.939869, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7575%\n",
      "layer   3  Sparsity: 64.9851%\n",
      "total_backward_count 1928630 real_backward_count 52961   2.746%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.087331/  0.937811, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.7393%\n",
      "layer   3  Sparsity: 65.0001%\n",
      "total_backward_count 1938420 real_backward_count 52987   2.734%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.090393/  0.948789, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.6094%\n",
      "layer   3  Sparsity: 65.1191%\n",
      "total_backward_count 1948210 real_backward_count 53019   2.721%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.086334/  0.945249, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 75.4466%\n",
      "layer   3  Sparsity: 64.9846%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54da9aa2909448ab78831fceb2de14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.08633</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.82917</td></tr><tr><td>val_loss</td><td>0.94525</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-sweep-122</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/91dgjwfs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/91dgjwfs</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_011254-91dgjwfs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8lk9qfnj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_053633-8lk9qfnj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8lk9qfnj' target=\"_blank\">bright-sweep-128</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8lk9qfnj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8lk9qfnj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_053642_089', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 304.0\n",
      "lif layer 1 self.abs_max_v: 304.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 333.0\n",
      "lif layer 1 self.abs_max_v: 422.5\n",
      "fc layer 2 self.abs_max_out: 66.0\n",
      "lif layer 2 self.abs_max_v: 81.0\n",
      "fc layer 1 self.abs_max_out: 346.0\n",
      "lif layer 1 self.abs_max_v: 467.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 136.5\n",
      "fc layer 1 self.abs_max_out: 488.0\n",
      "lif layer 1 self.abs_max_v: 530.0\n",
      "fc layer 2 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 254.0\n",
      "fc layer 1 self.abs_max_out: 906.0\n",
      "lif layer 1 self.abs_max_v: 906.0\n",
      "fc layer 2 self.abs_max_out: 367.0\n",
      "lif layer 2 self.abs_max_v: 380.0\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "fc layer 1 self.abs_max_out: 1082.0\n",
      "lif layer 1 self.abs_max_v: 1134.5\n",
      "lif layer 2 self.abs_max_v: 449.0\n",
      "fc layer 1 self.abs_max_out: 1494.0\n",
      "lif layer 1 self.abs_max_v: 1671.5\n",
      "fc layer 2 self.abs_max_out: 456.0\n",
      "lif layer 2 self.abs_max_v: 680.5\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "lif layer 1 self.abs_max_v: 1773.0\n",
      "fc layer 1 self.abs_max_out: 1807.0\n",
      "lif layer 1 self.abs_max_v: 2019.5\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 2 self.abs_max_out: 459.0\n",
      "fc layer 2 self.abs_max_out: 521.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 758.0\n",
      "fc layer 3 self.abs_max_out: 115.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 1 self.abs_max_out: 1944.0\n",
      "lif layer 1 self.abs_max_v: 2057.5\n",
      "fc layer 1 self.abs_max_out: 1967.0\n",
      "fc layer 1 self.abs_max_out: 1997.0\n",
      "lif layer 1 self.abs_max_v: 2119.5\n",
      "fc layer 2 self.abs_max_out: 603.0\n",
      "lif layer 2 self.abs_max_v: 834.5\n",
      "fc layer 2 self.abs_max_out: 727.0\n",
      "lif layer 2 self.abs_max_v: 856.5\n",
      "fc layer 1 self.abs_max_out: 2267.0\n",
      "lif layer 1 self.abs_max_v: 2267.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "lif layer 2 self.abs_max_v: 872.0\n",
      "fc layer 1 self.abs_max_out: 2529.0\n",
      "lif layer 1 self.abs_max_v: 2529.0\n",
      "fc layer 2 self.abs_max_out: 918.0\n",
      "lif layer 2 self.abs_max_v: 964.5\n",
      "fc layer 2 self.abs_max_out: 935.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 2 self.abs_max_out: 984.0\n",
      "lif layer 2 self.abs_max_v: 1002.5\n",
      "lif layer 2 self.abs_max_v: 1033.5\n",
      "lif layer 2 self.abs_max_v: 1054.5\n",
      "lif layer 2 self.abs_max_v: 1140.5\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "fc layer 3 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 1179.5\n",
      "lif layer 2 self.abs_max_v: 1191.0\n",
      "fc layer 1 self.abs_max_out: 2577.0\n",
      "lif layer 1 self.abs_max_v: 2577.0\n",
      "lif layer 2 self.abs_max_v: 1250.5\n",
      "lif layer 2 self.abs_max_v: 1291.5\n",
      "lif layer 2 self.abs_max_v: 1308.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "fc layer 2 self.abs_max_out: 1035.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 2 self.abs_max_out: 1187.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "fc layer 2 self.abs_max_out: 1283.0\n",
      "lif layer 2 self.abs_max_v: 1341.5\n",
      "lif layer 2 self.abs_max_v: 1358.0\n",
      "lif layer 2 self.abs_max_v: 1358.5\n",
      "lif layer 2 self.abs_max_v: 1393.5\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 1451.0\n",
      "fc layer 1 self.abs_max_out: 2814.0\n",
      "lif layer 1 self.abs_max_v: 2814.0\n",
      "lif layer 2 self.abs_max_v: 1464.5\n",
      "lif layer 2 self.abs_max_v: 1532.5\n",
      "lif layer 1 self.abs_max_v: 2924.5\n",
      "lif layer 1 self.abs_max_v: 3064.5\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1326.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 2 self.abs_max_out: 1462.0\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "lif layer 2 self.abs_max_v: 1579.5\n",
      "fc layer 2 self.abs_max_out: 1472.0\n",
      "fc layer 2 self.abs_max_out: 1491.0\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "lif layer 2 self.abs_max_v: 1622.0\n",
      "lif layer 2 self.abs_max_v: 1645.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 1747.0\n",
      "lif layer 2 self.abs_max_v: 1778.5\n",
      "lif layer 2 self.abs_max_v: 1792.5\n",
      "lif layer 2 self.abs_max_v: 1856.5\n",
      "fc layer 2 self.abs_max_out: 1573.0\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "lif layer 2 self.abs_max_v: 1882.5\n",
      "lif layer 2 self.abs_max_v: 1916.5\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "lif layer 2 self.abs_max_v: 1955.0\n",
      "lif layer 2 self.abs_max_v: 1958.5\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 3 self.abs_max_out: 363.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 2 self.abs_max_out: 1767.0\n",
      "lif layer 2 self.abs_max_v: 1970.0\n",
      "lif layer 2 self.abs_max_v: 1990.0\n",
      "lif layer 2 self.abs_max_v: 2159.0\n",
      "lif layer 2 self.abs_max_v: 2171.5\n",
      "lif layer 2 self.abs_max_v: 2280.0\n",
      "lif layer 2 self.abs_max_v: 2345.0\n",
      "lif layer 2 self.abs_max_v: 2351.5\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "fc layer 2 self.abs_max_out: 1779.0\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 2359.5\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "fc layer 2 self.abs_max_out: 1896.0\n",
      "lif layer 2 self.abs_max_v: 2417.0\n",
      "lif layer 1 self.abs_max_v: 3226.5\n",
      "lif layer 1 self.abs_max_v: 3402.0\n",
      "lif layer 1 self.abs_max_v: 3769.0\n",
      "lif layer 1 self.abs_max_v: 3904.5\n",
      "fc layer 1 self.abs_max_out: 2981.0\n",
      "lif layer 2 self.abs_max_v: 2473.5\n",
      "lif layer 1 self.abs_max_v: 3977.5\n",
      "lif layer 1 self.abs_max_v: 4186.0\n",
      "lif layer 1 self.abs_max_v: 4197.0\n",
      "fc layer 1 self.abs_max_out: 3097.0\n",
      "fc layer 2 self.abs_max_out: 2045.0\n",
      "fc layer 2 self.abs_max_out: 2057.0\n",
      "lif layer 1 self.abs_max_v: 4225.5\n",
      "lif layer 2 self.abs_max_v: 2505.0\n",
      "lif layer 2 self.abs_max_v: 2748.5\n",
      "lif layer 2 self.abs_max_v: 2840.5\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "lif layer 1 self.abs_max_v: 4412.0\n",
      "lif layer 1 self.abs_max_v: 4451.0\n",
      "fc layer 1 self.abs_max_out: 3405.0\n",
      "fc layer 2 self.abs_max_out: 2063.0\n",
      "lif layer 1 self.abs_max_v: 5096.5\n",
      "fc layer 2 self.abs_max_out: 2100.0\n",
      "fc layer 2 self.abs_max_out: 2176.0\n",
      "lif layer 1 self.abs_max_v: 5309.5\n",
      "lif layer 1 self.abs_max_v: 5538.0\n",
      "lif layer 1 self.abs_max_v: 5623.0\n",
      "lif layer 1 self.abs_max_v: 6064.5\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "lif layer 1 self.abs_max_v: 6131.5\n",
      "lif layer 1 self.abs_max_v: 6310.5\n",
      "fc layer 1 self.abs_max_out: 3412.0\n",
      "fc layer 1 self.abs_max_out: 3604.0\n",
      "lif layer 2 self.abs_max_v: 2971.0\n",
      "fc layer 1 self.abs_max_out: 3638.0\n",
      "lif layer 1 self.abs_max_v: 6435.5\n",
      "fc layer 1 self.abs_max_out: 3826.0\n",
      "lif layer 1 self.abs_max_v: 7040.5\n",
      "lif layer 1 self.abs_max_v: 7111.0\n",
      "fc layer 1 self.abs_max_out: 3953.0\n",
      "fc layer 1 self.abs_max_out: 4065.0\n",
      "fc layer 1 self.abs_max_out: 4170.0\n",
      "fc layer 1 self.abs_max_out: 4317.0\n",
      "fc layer 2 self.abs_max_out: 2186.0\n",
      "lif layer 1 self.abs_max_v: 7397.5\n",
      "fc layer 1 self.abs_max_out: 4336.0\n",
      "fc layer 1 self.abs_max_out: 4378.0\n",
      "lif layer 1 self.abs_max_v: 7943.0\n",
      "lif layer 1 self.abs_max_v: 8157.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 2213.0\n",
      "fc layer 2 self.abs_max_out: 2254.0\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "fc layer 2 self.abs_max_out: 2259.0\n",
      "fc layer 2 self.abs_max_out: 2263.0\n",
      "fc layer 2 self.abs_max_out: 2320.0\n",
      "fc layer 1 self.abs_max_out: 4413.0\n",
      "fc layer 1 self.abs_max_out: 4688.0\n",
      "fc layer 1 self.abs_max_out: 4702.0\n",
      "lif layer 1 self.abs_max_v: 8650.0\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "lif layer 1 self.abs_max_v: 8871.5\n",
      "lif layer 1 self.abs_max_v: 8876.0\n",
      "fc layer 1 self.abs_max_out: 5246.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.626427/  1.899535, val:  39.58%, val_best:  39.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5631%\n",
      "layer   2  Sparsity: 71.7573%\n",
      "layer   3  Sparsity: 75.1485%\n",
      "total_backward_count 9790 real_backward_count 1604  16.384%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2373.0\n",
      "fc layer 2 self.abs_max_out: 2456.0\n",
      "fc layer 2 self.abs_max_out: 2647.0\n",
      "fc layer 2 self.abs_max_out: 2654.0\n",
      "fc layer 2 self.abs_max_out: 2701.0\n",
      "fc layer 3 self.abs_max_out: 522.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "lif layer 1 self.abs_max_v: 9082.0\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "lif layer 1 self.abs_max_v: 9250.0\n",
      "lif layer 2 self.abs_max_v: 2981.5\n",
      "lif layer 2 self.abs_max_v: 3106.0\n",
      "lif layer 2 self.abs_max_v: 3187.0\n",
      "fc layer 2 self.abs_max_out: 2717.0\n",
      "fc layer 1 self.abs_max_out: 5296.0\n",
      "fc layer 1 self.abs_max_out: 5498.0\n",
      "lif layer 1 self.abs_max_v: 9359.0\n",
      "fc layer 1 self.abs_max_out: 5582.0\n",
      "lif layer 1 self.abs_max_v: 10190.0\n",
      "lif layer 1 self.abs_max_v: 10207.0\n",
      "fc layer 1 self.abs_max_out: 5593.0\n",
      "fc layer 1 self.abs_max_out: 5835.0\n",
      "lif layer 1 self.abs_max_v: 10416.0\n",
      "lif layer 1 self.abs_max_v: 10553.0\n",
      "lif layer 1 self.abs_max_v: 10616.5\n",
      "lif layer 1 self.abs_max_v: 10765.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.505575/  1.835831, val:  41.25%, val_best:  41.25%, tr:  99.28%, tr_best:  99.49%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5907%\n",
      "layer   2  Sparsity: 79.0292%\n",
      "layer   3  Sparsity: 77.2286%\n",
      "total_backward_count 19580 real_backward_count 3078  15.720%\n",
      "lif layer 2 self.abs_max_v: 3215.0\n",
      "lif layer 2 self.abs_max_v: 3272.5\n",
      "fc layer 1 self.abs_max_out: 5842.0\n",
      "lif layer 2 self.abs_max_v: 3306.0\n",
      "lif layer 2 self.abs_max_v: 3487.0\n",
      "lif layer 2 self.abs_max_v: 3583.5\n",
      "fc layer 2 self.abs_max_out: 2736.0\n",
      "fc layer 1 self.abs_max_out: 6104.0\n",
      "lif layer 1 self.abs_max_v: 10891.5\n",
      "fc layer 1 self.abs_max_out: 6117.0\n",
      "lif layer 1 self.abs_max_v: 11464.0\n",
      "fc layer 1 self.abs_max_out: 6763.0\n",
      "lif layer 2 self.abs_max_v: 3601.0\n",
      "lif layer 1 self.abs_max_v: 11494.5\n",
      "lif layer 2 self.abs_max_v: 3638.5\n",
      "lif layer 2 self.abs_max_v: 3658.5\n",
      "lif layer 2 self.abs_max_v: 4020.5\n",
      "lif layer 1 self.abs_max_v: 11870.5\n",
      "lif layer 1 self.abs_max_v: 12014.5\n",
      "lif layer 1 self.abs_max_v: 12125.0\n",
      "lif layer 1 self.abs_max_v: 12344.5\n",
      "fc layer 1 self.abs_max_out: 6992.0\n",
      "lif layer 1 self.abs_max_v: 12830.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.506260/  1.839750, val:  48.33%, val_best:  48.33%, tr:  99.08%, tr_best:  99.49%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5273%\n",
      "layer   2  Sparsity: 77.4448%\n",
      "layer   3  Sparsity: 77.0452%\n",
      "total_backward_count 29370 real_backward_count 4514  15.369%\n",
      "fc layer 2 self.abs_max_out: 2885.0\n",
      "fc layer 2 self.abs_max_out: 2898.0\n",
      "lif layer 2 self.abs_max_v: 4101.5\n",
      "fc layer 1 self.abs_max_out: 7227.0\n",
      "fc layer 2 self.abs_max_out: 3031.0\n",
      "fc layer 2 self.abs_max_out: 3053.0\n",
      "fc layer 1 self.abs_max_out: 7442.0\n",
      "fc layer 1 self.abs_max_out: 8221.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.501659/  1.878698, val:  38.33%, val_best:  48.33%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5675%\n",
      "layer   2  Sparsity: 78.5962%\n",
      "layer   3  Sparsity: 78.1874%\n",
      "total_backward_count 39160 real_backward_count 5905  15.079%\n",
      "lif layer 2 self.abs_max_v: 4241.5\n",
      "fc layer 2 self.abs_max_out: 3092.0\n",
      "lif layer 1 self.abs_max_v: 12928.5\n",
      "lif layer 1 self.abs_max_v: 13224.5\n",
      "lif layer 1 self.abs_max_v: 13427.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.506753/  1.836597, val:  43.33%, val_best:  48.33%, tr:  99.49%, tr_best:  99.59%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5762%\n",
      "layer   2  Sparsity: 78.3678%\n",
      "layer   3  Sparsity: 78.7311%\n",
      "total_backward_count 48950 real_backward_count 7261  14.834%\n",
      "lif layer 2 self.abs_max_v: 4368.5\n",
      "lif layer 1 self.abs_max_v: 13728.0\n",
      "lif layer 1 self.abs_max_v: 13870.0\n",
      "lif layer 1 self.abs_max_v: 14028.0\n",
      "lif layer 1 self.abs_max_v: 14744.0\n",
      "lif layer 1 self.abs_max_v: 15271.0\n",
      "fc layer 1 self.abs_max_out: 8721.0\n",
      "lif layer 1 self.abs_max_v: 15919.0\n",
      "lif layer 1 self.abs_max_v: 16056.0\n",
      "fc layer 2 self.abs_max_out: 3353.0\n",
      "lif layer 2 self.abs_max_v: 4716.5\n",
      "fc layer 2 self.abs_max_out: 3395.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.477116/  1.866441, val:  51.67%, val_best:  51.67%, tr:  99.39%, tr_best:  99.59%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5384%\n",
      "layer   2  Sparsity: 78.6505%\n",
      "layer   3  Sparsity: 78.6567%\n",
      "total_backward_count 58740 real_backward_count 8619  14.673%\n",
      "fc layer 1 self.abs_max_out: 8919.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.513819/  1.787598, val:  57.08%, val_best:  57.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5968%\n",
      "layer   2  Sparsity: 78.3482%\n",
      "layer   3  Sparsity: 79.6580%\n",
      "total_backward_count 68530 real_backward_count 9937  14.500%\n",
      "fc layer 2 self.abs_max_out: 3480.0\n",
      "fc layer 2 self.abs_max_out: 3632.0\n",
      "lif layer 2 self.abs_max_v: 4717.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.485937/  1.777706, val:  45.00%, val_best:  57.08%, tr:  99.28%, tr_best:  99.59%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5349%\n",
      "layer   2  Sparsity: 77.4305%\n",
      "layer   3  Sparsity: 78.8220%\n",
      "total_backward_count 78320 real_backward_count 11140  14.224%\n",
      "lif layer 2 self.abs_max_v: 4863.0\n",
      "lif layer 2 self.abs_max_v: 4966.0\n",
      "lif layer 2 self.abs_max_v: 5078.0\n",
      "lif layer 2 self.abs_max_v: 5090.5\n",
      "lif layer 2 self.abs_max_v: 5351.5\n",
      "lif layer 2 self.abs_max_v: 5433.5\n",
      "lif layer 2 self.abs_max_v: 5697.5\n",
      "lif layer 1 self.abs_max_v: 16649.0\n",
      "lif layer 1 self.abs_max_v: 16952.5\n",
      "fc layer 1 self.abs_max_out: 9041.0\n",
      "lif layer 1 self.abs_max_v: 17073.5\n",
      "fc layer 1 self.abs_max_out: 9281.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.519032/  1.800539, val:  53.33%, val_best:  57.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5692%\n",
      "layer   2  Sparsity: 79.2496%\n",
      "layer   3  Sparsity: 79.6539%\n",
      "total_backward_count 88110 real_backward_count 12410  14.085%\n",
      "fc layer 1 self.abs_max_out: 9440.0\n",
      "lif layer 1 self.abs_max_v: 17563.0\n",
      "fc layer 2 self.abs_max_out: 3647.0\n",
      "fc layer 2 self.abs_max_out: 3776.0\n",
      "fc layer 2 self.abs_max_out: 3792.0\n",
      "lif layer 1 self.abs_max_v: 17647.0\n",
      "fc layer 1 self.abs_max_out: 9561.0\n",
      "fc layer 1 self.abs_max_out: 9781.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.467371/  1.814295, val:  45.83%, val_best:  57.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5343%\n",
      "layer   2  Sparsity: 80.4678%\n",
      "layer   3  Sparsity: 79.0376%\n",
      "total_backward_count 97900 real_backward_count 13671  13.964%\n",
      "fc layer 1 self.abs_max_out: 9852.0\n",
      "lif layer 1 self.abs_max_v: 18629.0\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "lif layer 1 self.abs_max_v: 18780.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.444554/  1.765093, val:  44.17%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5582%\n",
      "layer   2  Sparsity: 80.0103%\n",
      "layer   3  Sparsity: 79.4921%\n",
      "total_backward_count 107690 real_backward_count 14899  13.835%\n",
      "fc layer 1 self.abs_max_out: 10520.0\n",
      "lif layer 1 self.abs_max_v: 18918.0\n",
      "lif layer 1 self.abs_max_v: 18926.0\n",
      "lif layer 1 self.abs_max_v: 19432.5\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "fc layer 3 self.abs_max_out: 694.0\n",
      "lif layer 1 self.abs_max_v: 20119.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.469468/  1.761312, val:  48.75%, val_best:  57.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5701%\n",
      "layer   2  Sparsity: 78.9579%\n",
      "layer   3  Sparsity: 79.0231%\n",
      "total_backward_count 117480 real_backward_count 16155  13.751%\n",
      "fc layer 1 self.abs_max_out: 10750.0\n",
      "lif layer 1 self.abs_max_v: 20178.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.430600/  1.747019, val:  55.00%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5813%\n",
      "layer   2  Sparsity: 77.9261%\n",
      "layer   3  Sparsity: 79.1763%\n",
      "total_backward_count 127270 real_backward_count 17321  13.610%\n",
      "fc layer 1 self.abs_max_out: 10897.0\n",
      "lif layer 2 self.abs_max_v: 5738.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.408035/  1.744968, val:  50.00%, val_best:  57.08%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5750%\n",
      "layer   2  Sparsity: 77.4607%\n",
      "layer   3  Sparsity: 79.1121%\n",
      "total_backward_count 137060 real_backward_count 18518  13.511%\n",
      "fc layer 2 self.abs_max_out: 3831.0\n",
      "lif layer 2 self.abs_max_v: 5879.0\n",
      "fc layer 2 self.abs_max_out: 3975.0\n",
      "lif layer 2 self.abs_max_v: 6013.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.368348/  1.696434, val:  53.75%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5894%\n",
      "layer   2  Sparsity: 77.0900%\n",
      "layer   3  Sparsity: 77.9698%\n",
      "total_backward_count 146850 real_backward_count 19616  13.358%\n",
      "lif layer 2 self.abs_max_v: 6082.0\n",
      "fc layer 2 self.abs_max_out: 4067.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.345207/  1.761977, val:  49.58%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5233%\n",
      "layer   2  Sparsity: 77.1311%\n",
      "layer   3  Sparsity: 78.2215%\n",
      "total_backward_count 156640 real_backward_count 20769  13.259%\n",
      "fc layer 2 self.abs_max_out: 4129.0\n",
      "lif layer 2 self.abs_max_v: 6623.5\n",
      "lif layer 2 self.abs_max_v: 6722.5\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.366261/  1.735316, val:  56.25%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5557%\n",
      "layer   2  Sparsity: 77.4001%\n",
      "layer   3  Sparsity: 78.3931%\n",
      "total_backward_count 166430 real_backward_count 21913  13.166%\n",
      "fc layer 1 self.abs_max_out: 11458.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.381050/  1.697369, val:  52.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5782%\n",
      "layer   2  Sparsity: 77.4865%\n",
      "layer   3  Sparsity: 78.3066%\n",
      "total_backward_count 176220 real_backward_count 23063  13.088%\n",
      "fc layer 1 self.abs_max_out: 11559.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.365201/  1.631142, val:  59.58%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5395%\n",
      "layer   2  Sparsity: 76.5227%\n",
      "layer   3  Sparsity: 78.7750%\n",
      "total_backward_count 186010 real_backward_count 24243  13.033%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.338837/  1.713861, val:  46.67%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5599%\n",
      "layer   2  Sparsity: 76.2436%\n",
      "layer   3  Sparsity: 78.7440%\n",
      "total_backward_count 195800 real_backward_count 25333  12.938%\n",
      "fc layer 1 self.abs_max_out: 11602.0\n",
      "lif layer 1 self.abs_max_v: 20577.5\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.318802/  1.728212, val:  45.00%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5460%\n",
      "layer   2  Sparsity: 75.9596%\n",
      "layer   3  Sparsity: 78.3037%\n",
      "total_backward_count 205590 real_backward_count 26437  12.859%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.363601/  1.721004, val:  53.33%, val_best:  59.58%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5023%\n",
      "layer   2  Sparsity: 74.8276%\n",
      "layer   3  Sparsity: 77.8552%\n",
      "total_backward_count 215380 real_backward_count 27608  12.818%\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "fc layer 1 self.abs_max_out: 11633.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.316229/  1.572389, val:  62.08%, val_best:  62.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5503%\n",
      "layer   2  Sparsity: 75.9425%\n",
      "layer   3  Sparsity: 77.6042%\n",
      "total_backward_count 225170 real_backward_count 28751  12.769%\n",
      "fc layer 1 self.abs_max_out: 11963.0\n",
      "lif layer 1 self.abs_max_v: 21541.0\n",
      "lif layer 1 self.abs_max_v: 21576.5\n",
      "fc layer 3 self.abs_max_out: 741.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.296295/  1.631379, val:  62.92%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5700%\n",
      "layer   2  Sparsity: 75.5626%\n",
      "layer   3  Sparsity: 76.6532%\n",
      "total_backward_count 234960 real_backward_count 29889  12.721%\n",
      "fc layer 3 self.abs_max_out: 754.0\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "fc layer 3 self.abs_max_out: 771.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.286388/  1.604824, val:  60.42%, val_best:  62.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5325%\n",
      "layer   2  Sparsity: 76.0045%\n",
      "layer   3  Sparsity: 76.9643%\n",
      "total_backward_count 244750 real_backward_count 31009  12.670%\n",
      "fc layer 2 self.abs_max_out: 4177.0\n",
      "lif layer 2 self.abs_max_v: 6863.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.234198/  1.589151, val:  60.42%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5548%\n",
      "layer   2  Sparsity: 75.8030%\n",
      "layer   3  Sparsity: 76.0343%\n",
      "total_backward_count 254540 real_backward_count 32146  12.629%\n",
      "fc layer 2 self.abs_max_out: 4240.0\n",
      "fc layer 2 self.abs_max_out: 4246.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.210456/  1.579750, val:  59.17%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5765%\n",
      "layer   2  Sparsity: 75.4032%\n",
      "layer   3  Sparsity: 74.8400%\n",
      "total_backward_count 264330 real_backward_count 33218  12.567%\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.179582/  1.493655, val:  60.00%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5865%\n",
      "layer   2  Sparsity: 75.0283%\n",
      "layer   3  Sparsity: 75.2759%\n",
      "total_backward_count 274120 real_backward_count 34298  12.512%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.189290/  1.666281, val:  52.92%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6066%\n",
      "layer   2  Sparsity: 74.9518%\n",
      "layer   3  Sparsity: 75.9902%\n",
      "total_backward_count 283910 real_backward_count 35339  12.447%\n",
      "fc layer 1 self.abs_max_out: 12904.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.213079/  1.596032, val:  50.42%, val_best:  62.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6116%\n",
      "layer   2  Sparsity: 74.2478%\n",
      "layer   3  Sparsity: 76.1443%\n",
      "total_backward_count 293700 real_backward_count 36394  12.392%\n",
      "lif layer 1 self.abs_max_v: 22382.0\n",
      "lif layer 2 self.abs_max_v: 7007.0\n",
      "lif layer 2 self.abs_max_v: 7235.5\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.204000/  1.619824, val:  55.42%, val_best:  62.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5791%\n",
      "layer   2  Sparsity: 74.4877%\n",
      "layer   3  Sparsity: 74.4811%\n",
      "total_backward_count 303490 real_backward_count 37496  12.355%\n",
      "lif layer 1 self.abs_max_v: 22700.5\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.182947/  1.656569, val:  50.83%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.6141%\n",
      "layer   2  Sparsity: 74.4359%\n",
      "layer   3  Sparsity: 76.4877%\n",
      "total_backward_count 313280 real_backward_count 38587  12.317%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.219212/  1.602596, val:  56.25%, val_best:  62.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5343%\n",
      "layer   2  Sparsity: 75.1570%\n",
      "layer   3  Sparsity: 76.8950%\n",
      "total_backward_count 323070 real_backward_count 39613  12.261%\n",
      "lif layer 1 self.abs_max_v: 22787.0\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 858.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.168021/  1.598430, val:  56.25%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5792%\n",
      "layer   2  Sparsity: 74.4703%\n",
      "layer   3  Sparsity: 74.8105%\n",
      "total_backward_count 332860 real_backward_count 40622  12.204%\n",
      "lif layer 2 self.abs_max_v: 7405.0\n",
      "lif layer 2 self.abs_max_v: 7492.5\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.145451/  1.533933, val:  55.42%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5920%\n",
      "layer   2  Sparsity: 73.8203%\n",
      "layer   3  Sparsity: 75.9158%\n",
      "total_backward_count 342650 real_backward_count 41674  12.162%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.162896/  1.542010, val:  60.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5855%\n",
      "layer   2  Sparsity: 74.0874%\n",
      "layer   3  Sparsity: 75.5432%\n",
      "total_backward_count 352440 real_backward_count 42705  12.117%\n",
      "fc layer 1 self.abs_max_out: 13125.0\n",
      "lif layer 1 self.abs_max_v: 23306.5\n",
      "fc layer 1 self.abs_max_out: 13659.0\n",
      "lif layer 1 self.abs_max_v: 23841.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.123570/  1.514767, val:  59.17%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5302%\n",
      "layer   2  Sparsity: 73.8450%\n",
      "layer   3  Sparsity: 74.8221%\n",
      "total_backward_count 362230 real_backward_count 43743  12.076%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.090220/  1.516767, val:  55.00%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5538%\n",
      "layer   2  Sparsity: 73.9100%\n",
      "layer   3  Sparsity: 74.4288%\n",
      "total_backward_count 372020 real_backward_count 44699  12.015%\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.113560/  1.554785, val:  56.67%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6041%\n",
      "layer   2  Sparsity: 73.6288%\n",
      "layer   3  Sparsity: 75.5773%\n",
      "total_backward_count 381810 real_backward_count 45762  11.986%\n",
      "lif layer 1 self.abs_max_v: 24228.5\n",
      "fc layer 1 self.abs_max_out: 14216.0\n",
      "fc layer 1 self.abs_max_out: 14789.0\n",
      "lif layer 1 self.abs_max_v: 26043.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.142256/  1.544072, val:  59.17%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5979%\n",
      "layer   2  Sparsity: 73.4820%\n",
      "layer   3  Sparsity: 75.9603%\n",
      "total_backward_count 391600 real_backward_count 46705  11.927%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.091642/  1.507016, val:  61.25%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5334%\n",
      "layer   2  Sparsity: 73.6770%\n",
      "layer   3  Sparsity: 75.4636%\n",
      "total_backward_count 401390 real_backward_count 47701  11.884%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.082213/  1.456580, val:  66.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.5698%\n",
      "layer   2  Sparsity: 74.0284%\n",
      "layer   3  Sparsity: 74.9878%\n",
      "total_backward_count 411180 real_backward_count 48689  11.841%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.113100/  1.552017, val:  56.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5963%\n",
      "layer   2  Sparsity: 73.3437%\n",
      "layer   3  Sparsity: 76.3659%\n",
      "total_backward_count 420970 real_backward_count 49643  11.793%\n",
      "lif layer 2 self.abs_max_v: 7657.0\n",
      "lif layer 2 self.abs_max_v: 7671.5\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.124946/  1.464648, val:  62.08%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5582%\n",
      "layer   2  Sparsity: 72.5525%\n",
      "layer   3  Sparsity: 76.1750%\n",
      "total_backward_count 430760 real_backward_count 50656  11.760%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.162541/  1.526799, val:  58.75%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5604%\n",
      "layer   2  Sparsity: 73.1079%\n",
      "layer   3  Sparsity: 76.3127%\n",
      "total_backward_count 440550 real_backward_count 51677  11.730%\n",
      "fc layer 2 self.abs_max_out: 4508.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.068804/  1.499895, val:  56.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5965%\n",
      "layer   2  Sparsity: 73.4137%\n",
      "layer   3  Sparsity: 74.0894%\n",
      "total_backward_count 450340 real_backward_count 52672  11.696%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.066807/  1.470814, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5153%\n",
      "layer   2  Sparsity: 73.6463%\n",
      "layer   3  Sparsity: 73.5141%\n",
      "total_backward_count 460130 real_backward_count 53643  11.658%\n",
      "lif layer 2 self.abs_max_v: 7743.5\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.033640/  1.556724, val:  51.67%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5794%\n",
      "layer   2  Sparsity: 73.7211%\n",
      "layer   3  Sparsity: 73.3343%\n",
      "total_backward_count 469920 real_backward_count 54620  11.623%\n",
      "lif layer 2 self.abs_max_v: 7854.5\n",
      "lif layer 2 self.abs_max_v: 8009.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.049212/  1.565272, val:  52.92%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5134%\n",
      "layer   2  Sparsity: 72.6845%\n",
      "layer   3  Sparsity: 73.9381%\n",
      "total_backward_count 479710 real_backward_count 55559  11.582%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.026159/  1.387204, val:  59.17%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5633%\n",
      "layer   2  Sparsity: 72.8334%\n",
      "layer   3  Sparsity: 73.5465%\n",
      "total_backward_count 489500 real_backward_count 56483  11.539%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.992756/  1.430113, val:  58.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5554%\n",
      "layer   2  Sparsity: 73.2359%\n",
      "layer   3  Sparsity: 74.3274%\n",
      "total_backward_count 499290 real_backward_count 57433  11.503%\n",
      "lif layer 2 self.abs_max_v: 8018.5\n",
      "fc layer 2 self.abs_max_out: 4713.0\n",
      "lif layer 2 self.abs_max_v: 8667.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.981465/  1.509828, val:  49.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5823%\n",
      "layer   2  Sparsity: 73.0538%\n",
      "layer   3  Sparsity: 74.3597%\n",
      "total_backward_count 509080 real_backward_count 58367  11.465%\n",
      "fc layer 2 self.abs_max_out: 4775.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.017918/  1.458968, val:  64.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5322%\n",
      "layer   2  Sparsity: 73.2821%\n",
      "layer   3  Sparsity: 74.1737%\n",
      "total_backward_count 518870 real_backward_count 59333  11.435%\n",
      "lif layer 2 self.abs_max_v: 8707.5\n",
      "fc layer 2 self.abs_max_out: 4944.0\n",
      "lif layer 2 self.abs_max_v: 9212.5\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.038681/  1.465985, val:  59.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5375%\n",
      "layer   2  Sparsity: 72.9386%\n",
      "layer   3  Sparsity: 74.8671%\n",
      "total_backward_count 528660 real_backward_count 60263  11.399%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.028990/  1.472091, val:  57.50%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5482%\n",
      "layer   2  Sparsity: 72.5004%\n",
      "layer   3  Sparsity: 74.3226%\n",
      "total_backward_count 538450 real_backward_count 61189  11.364%\n",
      "lif layer 2 self.abs_max_v: 9497.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.064498/  1.417430, val:  61.25%, val_best:  66.25%, tr:  99.18%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5588%\n",
      "layer   2  Sparsity: 72.9060%\n",
      "layer   3  Sparsity: 74.6470%\n",
      "total_backward_count 548240 real_backward_count 62150  11.336%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.027355/  1.489560, val:  51.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5520%\n",
      "layer   2  Sparsity: 73.4309%\n",
      "layer   3  Sparsity: 75.0907%\n",
      "total_backward_count 558030 real_backward_count 63092  11.306%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.998135/  1.488375, val:  50.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6000%\n",
      "layer   2  Sparsity: 72.7495%\n",
      "layer   3  Sparsity: 74.5840%\n",
      "total_backward_count 567820 real_backward_count 64027  11.276%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.970670/  1.369867, val:  62.92%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5463%\n",
      "layer   2  Sparsity: 72.6370%\n",
      "layer   3  Sparsity: 72.8877%\n",
      "total_backward_count 577610 real_backward_count 64924  11.240%\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.959215/  1.416095, val:  60.42%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5738%\n",
      "layer   2  Sparsity: 72.2387%\n",
      "layer   3  Sparsity: 72.6012%\n",
      "total_backward_count 587400 real_backward_count 65841  11.209%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.982664/  1.485208, val:  55.00%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5044%\n",
      "layer   2  Sparsity: 72.9058%\n",
      "layer   3  Sparsity: 72.7522%\n",
      "total_backward_count 597190 real_backward_count 66782  11.183%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.053839/  1.406726, val:  67.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5341%\n",
      "layer   2  Sparsity: 72.8856%\n",
      "layer   3  Sparsity: 75.4884%\n",
      "total_backward_count 606980 real_backward_count 67782  11.167%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.997146/  1.428672, val:  57.92%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5718%\n",
      "layer   2  Sparsity: 73.6717%\n",
      "layer   3  Sparsity: 74.0162%\n",
      "total_backward_count 616770 real_backward_count 68735  11.144%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.009937/  1.475194, val:  58.75%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5594%\n",
      "layer   2  Sparsity: 72.4370%\n",
      "layer   3  Sparsity: 74.8614%\n",
      "total_backward_count 626560 real_backward_count 69636  11.114%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.958632/  1.499410, val:  51.25%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6028%\n",
      "layer   2  Sparsity: 72.8169%\n",
      "layer   3  Sparsity: 74.8909%\n",
      "total_backward_count 636350 real_backward_count 70555  11.087%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.959726/  1.431168, val:  58.75%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5606%\n",
      "layer   2  Sparsity: 72.8140%\n",
      "layer   3  Sparsity: 73.8418%\n",
      "total_backward_count 646140 real_backward_count 71526  11.070%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.961293/  1.413557, val:  60.00%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5941%\n",
      "layer   2  Sparsity: 73.0637%\n",
      "layer   3  Sparsity: 74.2075%\n",
      "total_backward_count 655930 real_backward_count 72469  11.048%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.948216/  1.332533, val:  61.67%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5583%\n",
      "layer   2  Sparsity: 72.6957%\n",
      "layer   3  Sparsity: 73.5996%\n",
      "total_backward_count 665720 real_backward_count 73378  11.022%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.956767/  1.348553, val:  62.50%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5355%\n",
      "layer   2  Sparsity: 73.0771%\n",
      "layer   3  Sparsity: 74.1303%\n",
      "total_backward_count 675510 real_backward_count 74308  11.000%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.919860/  1.387665, val:  59.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5834%\n",
      "layer   2  Sparsity: 72.8600%\n",
      "layer   3  Sparsity: 73.1620%\n",
      "total_backward_count 685300 real_backward_count 75201  10.973%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.910091/  1.411827, val:  58.75%, val_best:  67.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5783%\n",
      "layer   2  Sparsity: 72.9069%\n",
      "layer   3  Sparsity: 73.5946%\n",
      "total_backward_count 695090 real_backward_count 76097  10.948%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.942224/  1.429505, val:  60.00%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5521%\n",
      "layer   2  Sparsity: 73.0039%\n",
      "layer   3  Sparsity: 73.7126%\n",
      "total_backward_count 704880 real_backward_count 76970  10.920%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.973490/  1.400519, val:  59.58%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5655%\n",
      "layer   2  Sparsity: 73.3875%\n",
      "layer   3  Sparsity: 73.8955%\n",
      "total_backward_count 714670 real_backward_count 77865  10.895%\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.896882/  1.474867, val:  58.75%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5775%\n",
      "layer   2  Sparsity: 73.6737%\n",
      "layer   3  Sparsity: 72.4730%\n",
      "total_backward_count 724460 real_backward_count 78705  10.864%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.912942/  1.431247, val:  61.67%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5761%\n",
      "layer   2  Sparsity: 74.1762%\n",
      "layer   3  Sparsity: 73.1670%\n",
      "total_backward_count 734250 real_backward_count 79624  10.844%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.925302/  1.322751, val:  65.83%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5658%\n",
      "layer   2  Sparsity: 73.8210%\n",
      "layer   3  Sparsity: 73.3921%\n",
      "total_backward_count 744040 real_backward_count 80536  10.824%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.914237/  1.372936, val:  60.00%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6141%\n",
      "layer   2  Sparsity: 73.3470%\n",
      "layer   3  Sparsity: 72.7857%\n",
      "total_backward_count 753830 real_backward_count 81468  10.807%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.906556/  1.435148, val:  57.50%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5241%\n",
      "layer   2  Sparsity: 73.6468%\n",
      "layer   3  Sparsity: 72.9514%\n",
      "total_backward_count 763620 real_backward_count 82329  10.781%\n",
      "fc layer 3 self.abs_max_out: 1017.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.882763/  1.338968, val:  66.25%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5267%\n",
      "layer   2  Sparsity: 73.4185%\n",
      "layer   3  Sparsity: 73.5651%\n",
      "total_backward_count 773410 real_backward_count 83164  10.753%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.912423/  1.408222, val:  60.42%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5324%\n",
      "layer   2  Sparsity: 73.1368%\n",
      "layer   3  Sparsity: 73.9486%\n",
      "total_backward_count 783200 real_backward_count 84060  10.733%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.888634/  1.357574, val:  60.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5599%\n",
      "layer   2  Sparsity: 72.6422%\n",
      "layer   3  Sparsity: 72.9282%\n",
      "total_backward_count 792990 real_backward_count 84916  10.708%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.886790/  1.415760, val:  54.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5415%\n",
      "layer   2  Sparsity: 72.8825%\n",
      "layer   3  Sparsity: 74.1248%\n",
      "total_backward_count 802780 real_backward_count 85793  10.687%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.889441/  1.365491, val:  60.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5803%\n",
      "layer   2  Sparsity: 73.1005%\n",
      "layer   3  Sparsity: 73.5068%\n",
      "total_backward_count 812570 real_backward_count 86658  10.665%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.887535/  1.434442, val:  55.00%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5694%\n",
      "layer   2  Sparsity: 72.5838%\n",
      "layer   3  Sparsity: 73.3920%\n",
      "total_backward_count 822360 real_backward_count 87561  10.648%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.856904/  1.330188, val:  61.67%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5865%\n",
      "layer   2  Sparsity: 73.0480%\n",
      "layer   3  Sparsity: 72.9523%\n",
      "total_backward_count 832150 real_backward_count 88430  10.627%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.844607/  1.355613, val:  57.08%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6007%\n",
      "layer   2  Sparsity: 72.9007%\n",
      "layer   3  Sparsity: 73.9449%\n",
      "total_backward_count 841940 real_backward_count 89304  10.607%\n",
      "fc layer 3 self.abs_max_out: 1033.0\n",
      "fc layer 3 self.abs_max_out: 1089.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.857876/  1.298872, val:  68.33%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5796%\n",
      "layer   2  Sparsity: 73.1640%\n",
      "layer   3  Sparsity: 72.8668%\n",
      "total_backward_count 851730 real_backward_count 90155  10.585%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.862999/  1.329920, val:  60.83%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5671%\n",
      "layer   2  Sparsity: 73.0055%\n",
      "layer   3  Sparsity: 72.3071%\n",
      "total_backward_count 861520 real_backward_count 91010  10.564%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.858156/  1.330632, val:  60.00%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5959%\n",
      "layer   2  Sparsity: 73.2576%\n",
      "layer   3  Sparsity: 73.1263%\n",
      "total_backward_count 871310 real_backward_count 91874  10.544%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.875418/  1.284987, val:  61.25%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5609%\n",
      "layer   2  Sparsity: 71.9753%\n",
      "layer   3  Sparsity: 73.2082%\n",
      "total_backward_count 881100 real_backward_count 92776  10.530%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.840928/  1.316641, val:  59.58%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5480%\n",
      "layer   2  Sparsity: 72.2398%\n",
      "layer   3  Sparsity: 73.2063%\n",
      "total_backward_count 890890 real_backward_count 93664  10.514%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.827090/  1.288084, val:  67.08%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5637%\n",
      "layer   2  Sparsity: 72.4596%\n",
      "layer   3  Sparsity: 73.1979%\n",
      "total_backward_count 900680 real_backward_count 94508  10.493%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.835236/  1.336102, val:  60.42%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5188%\n",
      "layer   2  Sparsity: 71.7772%\n",
      "layer   3  Sparsity: 72.0929%\n",
      "total_backward_count 910470 real_backward_count 95391  10.477%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.825820/  1.307224, val:  63.33%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5733%\n",
      "layer   2  Sparsity: 72.4409%\n",
      "layer   3  Sparsity: 72.8287%\n",
      "total_backward_count 920260 real_backward_count 96271  10.461%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.815561/  1.364511, val:  57.50%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6259%\n",
      "layer   2  Sparsity: 73.0425%\n",
      "layer   3  Sparsity: 72.6289%\n",
      "total_backward_count 930050 real_backward_count 97119  10.442%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.828635/  1.289056, val:  64.17%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5412%\n",
      "layer   2  Sparsity: 72.5545%\n",
      "layer   3  Sparsity: 74.1418%\n",
      "total_backward_count 939840 real_backward_count 97976  10.425%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.823481/  1.330463, val:  60.00%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5456%\n",
      "layer   2  Sparsity: 72.3736%\n",
      "layer   3  Sparsity: 74.1624%\n",
      "total_backward_count 949630 real_backward_count 98834  10.408%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.820369/  1.269414, val:  58.75%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5552%\n",
      "layer   2  Sparsity: 72.7072%\n",
      "layer   3  Sparsity: 73.0233%\n",
      "total_backward_count 959420 real_backward_count 99669  10.388%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.792631/  1.339705, val:  57.92%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5354%\n",
      "layer   2  Sparsity: 73.2692%\n",
      "layer   3  Sparsity: 72.2779%\n",
      "total_backward_count 969210 real_backward_count 100509  10.370%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.805228/  1.260188, val:  67.92%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5917%\n",
      "layer   2  Sparsity: 72.8499%\n",
      "layer   3  Sparsity: 72.0554%\n",
      "total_backward_count 979000 real_backward_count 101302  10.347%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.805377/  1.351984, val:  62.08%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5660%\n",
      "layer   2  Sparsity: 73.4568%\n",
      "layer   3  Sparsity: 72.1951%\n",
      "total_backward_count 988790 real_backward_count 102154  10.331%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.852691/  1.328207, val:  66.25%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5817%\n",
      "layer   2  Sparsity: 73.1795%\n",
      "layer   3  Sparsity: 73.9267%\n",
      "total_backward_count 998580 real_backward_count 103042  10.319%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.861624/  1.286264, val:  62.92%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5490%\n",
      "layer   2  Sparsity: 72.7673%\n",
      "layer   3  Sparsity: 74.0500%\n",
      "total_backward_count 1008370 real_backward_count 103860  10.300%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.799245/  1.290472, val:  61.67%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5536%\n",
      "layer   2  Sparsity: 72.3708%\n",
      "layer   3  Sparsity: 72.5155%\n",
      "total_backward_count 1018160 real_backward_count 104645  10.278%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.809915/  1.273203, val:  62.50%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5713%\n",
      "layer   2  Sparsity: 71.4847%\n",
      "layer   3  Sparsity: 71.8071%\n",
      "total_backward_count 1027950 real_backward_count 105488  10.262%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.778303/  1.273515, val:  65.83%, val_best:  68.33%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5592%\n",
      "layer   2  Sparsity: 71.3041%\n",
      "layer   3  Sparsity: 71.5685%\n",
      "total_backward_count 1037740 real_backward_count 106330  10.246%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.797477/  1.301834, val:  63.33%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5454%\n",
      "layer   2  Sparsity: 71.1984%\n",
      "layer   3  Sparsity: 73.0178%\n",
      "total_backward_count 1047530 real_backward_count 107152  10.229%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.797142/  1.325147, val:  53.75%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5560%\n",
      "layer   2  Sparsity: 71.1248%\n",
      "layer   3  Sparsity: 71.4579%\n",
      "total_backward_count 1057320 real_backward_count 108065  10.221%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.800694/  1.308928, val:  63.75%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5693%\n",
      "layer   2  Sparsity: 71.1295%\n",
      "layer   3  Sparsity: 72.2144%\n",
      "total_backward_count 1067110 real_backward_count 108917  10.207%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.810848/  1.292509, val:  61.67%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5869%\n",
      "layer   2  Sparsity: 70.9618%\n",
      "layer   3  Sparsity: 73.2899%\n",
      "total_backward_count 1076900 real_backward_count 109786  10.195%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.771462/  1.257861, val:  65.42%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5801%\n",
      "layer   2  Sparsity: 71.2342%\n",
      "layer   3  Sparsity: 72.2096%\n",
      "total_backward_count 1086690 real_backward_count 110627  10.180%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.759276/  1.271261, val:  66.67%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5054%\n",
      "layer   2  Sparsity: 71.6966%\n",
      "layer   3  Sparsity: 72.1888%\n",
      "total_backward_count 1096480 real_backward_count 111461  10.165%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.757506/  1.196684, val:  64.58%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5623%\n",
      "layer   2  Sparsity: 72.0844%\n",
      "layer   3  Sparsity: 73.2091%\n",
      "total_backward_count 1106270 real_backward_count 112299  10.151%\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.723095/  1.206562, val:  62.50%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5679%\n",
      "layer   2  Sparsity: 71.5492%\n",
      "layer   3  Sparsity: 71.0339%\n",
      "total_backward_count 1116060 real_backward_count 113076  10.132%\n",
      "fc layer 3 self.abs_max_out: 1150.0\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.707824/  1.282605, val:  57.92%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6225%\n",
      "layer   2  Sparsity: 71.3760%\n",
      "layer   3  Sparsity: 71.4726%\n",
      "total_backward_count 1125850 real_backward_count 113865  10.114%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.740884/  1.291222, val:  62.50%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5505%\n",
      "layer   2  Sparsity: 70.8083%\n",
      "layer   3  Sparsity: 72.2652%\n",
      "total_backward_count 1135640 real_backward_count 114708  10.101%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.750345/  1.257148, val:  64.58%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6130%\n",
      "layer   2  Sparsity: 70.8517%\n",
      "layer   3  Sparsity: 71.3930%\n",
      "total_backward_count 1145430 real_backward_count 115536  10.087%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.739399/  1.180724, val:  68.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5725%\n",
      "layer   2  Sparsity: 70.9592%\n",
      "layer   3  Sparsity: 73.0444%\n",
      "total_backward_count 1155220 real_backward_count 116343  10.071%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.756861/  1.225935, val:  63.33%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5571%\n",
      "layer   2  Sparsity: 71.1930%\n",
      "layer   3  Sparsity: 73.3591%\n",
      "total_backward_count 1165010 real_backward_count 117101  10.052%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.757252/  1.192596, val:  66.67%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6106%\n",
      "layer   2  Sparsity: 71.1274%\n",
      "layer   3  Sparsity: 73.2776%\n",
      "total_backward_count 1174800 real_backward_count 117937  10.039%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.753889/  1.209055, val:  67.08%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5335%\n",
      "layer   2  Sparsity: 71.1202%\n",
      "layer   3  Sparsity: 73.0896%\n",
      "total_backward_count 1184590 real_backward_count 118728  10.023%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.740048/  1.256549, val:  61.67%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5700%\n",
      "layer   2  Sparsity: 71.2354%\n",
      "layer   3  Sparsity: 73.3521%\n",
      "total_backward_count 1194380 real_backward_count 119490  10.004%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.736674/  1.248533, val:  63.75%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5876%\n",
      "layer   2  Sparsity: 71.2341%\n",
      "layer   3  Sparsity: 72.3292%\n",
      "total_backward_count 1204170 real_backward_count 120353   9.995%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.722300/  1.301149, val:  60.83%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6109%\n",
      "layer   2  Sparsity: 71.5439%\n",
      "layer   3  Sparsity: 72.1843%\n",
      "total_backward_count 1213960 real_backward_count 121192   9.983%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.714850/  1.253564, val:  60.42%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5556%\n",
      "layer   2  Sparsity: 71.6405%\n",
      "layer   3  Sparsity: 72.1751%\n",
      "total_backward_count 1223750 real_backward_count 121990   9.969%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.721883/  1.260128, val:  62.08%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5509%\n",
      "layer   2  Sparsity: 71.1533%\n",
      "layer   3  Sparsity: 72.7573%\n",
      "total_backward_count 1233540 real_backward_count 122781   9.954%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.734132/  1.287943, val:  62.50%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.6011%\n",
      "layer   2  Sparsity: 71.3541%\n",
      "layer   3  Sparsity: 72.7002%\n",
      "total_backward_count 1243330 real_backward_count 123628   9.943%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.694891/  1.238005, val:  65.83%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5586%\n",
      "layer   2  Sparsity: 71.1651%\n",
      "layer   3  Sparsity: 71.9618%\n",
      "total_backward_count 1253120 real_backward_count 124416   9.928%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.685900/  1.226121, val:  66.25%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5460%\n",
      "layer   2  Sparsity: 70.8291%\n",
      "layer   3  Sparsity: 71.1303%\n",
      "total_backward_count 1262910 real_backward_count 125202   9.914%\n",
      "fc layer 3 self.abs_max_out: 1161.0\n",
      "fc layer 3 self.abs_max_out: 1196.0\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.692453/  1.139943, val:  66.25%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5948%\n",
      "layer   2  Sparsity: 70.8765%\n",
      "layer   3  Sparsity: 70.9752%\n",
      "total_backward_count 1272700 real_backward_count 126005   9.901%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.678627/  1.305670, val:  57.08%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6073%\n",
      "layer   2  Sparsity: 71.1014%\n",
      "layer   3  Sparsity: 71.6288%\n",
      "total_backward_count 1282490 real_backward_count 126806   9.887%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.678525/  1.265509, val:  59.58%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5939%\n",
      "layer   2  Sparsity: 71.5920%\n",
      "layer   3  Sparsity: 71.3984%\n",
      "total_backward_count 1292280 real_backward_count 127655   9.878%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.671584/  1.274650, val:  59.17%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6140%\n",
      "layer   2  Sparsity: 71.4047%\n",
      "layer   3  Sparsity: 70.0799%\n",
      "total_backward_count 1302070 real_backward_count 128417   9.863%\n",
      "fc layer 3 self.abs_max_out: 1215.0\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.664825/  1.252447, val:  58.33%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5414%\n",
      "layer   2  Sparsity: 71.5306%\n",
      "layer   3  Sparsity: 70.6459%\n",
      "total_backward_count 1311860 real_backward_count 129233   9.851%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.657818/  1.234349, val:  61.67%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5909%\n",
      "layer   2  Sparsity: 70.9675%\n",
      "layer   3  Sparsity: 72.1583%\n",
      "total_backward_count 1321650 real_backward_count 130059   9.841%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.669056/  1.126365, val:  65.00%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5654%\n",
      "layer   2  Sparsity: 71.0559%\n",
      "layer   3  Sparsity: 72.7461%\n",
      "total_backward_count 1331440 real_backward_count 130916   9.833%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.665135/  1.161086, val:  66.25%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5831%\n",
      "layer   2  Sparsity: 70.8768%\n",
      "layer   3  Sparsity: 72.9986%\n",
      "total_backward_count 1341230 real_backward_count 131663   9.817%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.679311/  1.229098, val:  62.92%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5106%\n",
      "layer   2  Sparsity: 70.5386%\n",
      "layer   3  Sparsity: 72.5989%\n",
      "total_backward_count 1351020 real_backward_count 132461   9.805%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.687670/  1.159515, val:  64.58%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5422%\n",
      "layer   2  Sparsity: 70.3820%\n",
      "layer   3  Sparsity: 72.2192%\n",
      "total_backward_count 1360810 real_backward_count 133294   9.795%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.682801/  1.212086, val:  62.50%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6007%\n",
      "layer   2  Sparsity: 70.7393%\n",
      "layer   3  Sparsity: 71.9788%\n",
      "total_backward_count 1370600 real_backward_count 134131   9.786%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.684226/  1.274870, val:  57.50%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.6066%\n",
      "layer   2  Sparsity: 70.4219%\n",
      "layer   3  Sparsity: 72.2347%\n",
      "total_backward_count 1380390 real_backward_count 134963   9.777%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.684498/  1.138206, val:  73.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5158%\n",
      "layer   2  Sparsity: 71.6012%\n",
      "layer   3  Sparsity: 70.7327%\n",
      "total_backward_count 1390180 real_backward_count 135789   9.768%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.697233/  1.229083, val:  62.08%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.6049%\n",
      "layer   2  Sparsity: 71.1743%\n",
      "layer   3  Sparsity: 71.3484%\n",
      "total_backward_count 1399970 real_backward_count 136577   9.756%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.652613/  1.329191, val:  56.25%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.6291%\n",
      "layer   2  Sparsity: 71.0963%\n",
      "layer   3  Sparsity: 72.7145%\n",
      "total_backward_count 1409760 real_backward_count 137366   9.744%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.692621/  1.170385, val:  62.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5981%\n",
      "layer   2  Sparsity: 71.1979%\n",
      "layer   3  Sparsity: 71.5520%\n",
      "total_backward_count 1419550 real_backward_count 138193   9.735%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.649421/  1.169742, val:  61.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5058%\n",
      "layer   2  Sparsity: 71.4361%\n",
      "layer   3  Sparsity: 70.5468%\n",
      "total_backward_count 1429340 real_backward_count 138936   9.720%\n",
      "fc layer 3 self.abs_max_out: 1273.0\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.652134/  1.175511, val:  62.92%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5927%\n",
      "layer   2  Sparsity: 71.4132%\n",
      "layer   3  Sparsity: 70.8438%\n",
      "total_backward_count 1439130 real_backward_count 139765   9.712%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.647526/  1.209577, val:  64.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5808%\n",
      "layer   2  Sparsity: 71.8044%\n",
      "layer   3  Sparsity: 71.1428%\n",
      "total_backward_count 1448920 real_backward_count 140484   9.696%\n",
      "fc layer 3 self.abs_max_out: 1281.0\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.648673/  1.298285, val:  57.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.6345%\n",
      "layer   2  Sparsity: 70.8492%\n",
      "layer   3  Sparsity: 70.6685%\n",
      "total_backward_count 1458710 real_backward_count 141286   9.686%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.658117/  1.211198, val:  61.25%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5626%\n",
      "layer   2  Sparsity: 71.0352%\n",
      "layer   3  Sparsity: 71.4880%\n",
      "total_backward_count 1468500 real_backward_count 142069   9.674%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.642324/  1.244554, val:  59.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5341%\n",
      "layer   2  Sparsity: 71.4028%\n",
      "layer   3  Sparsity: 72.6209%\n",
      "total_backward_count 1478290 real_backward_count 142822   9.661%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.656016/  1.232835, val:  58.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5418%\n",
      "layer   2  Sparsity: 71.3663%\n",
      "layer   3  Sparsity: 72.2364%\n",
      "total_backward_count 1488080 real_backward_count 143591   9.649%\n",
      "fc layer 3 self.abs_max_out: 1298.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.670651/  1.274611, val:  56.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5778%\n",
      "layer   2  Sparsity: 71.6459%\n",
      "layer   3  Sparsity: 72.3550%\n",
      "total_backward_count 1497870 real_backward_count 144349   9.637%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.669594/  1.175671, val:  64.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.4895%\n",
      "layer   2  Sparsity: 71.5354%\n",
      "layer   3  Sparsity: 71.4203%\n",
      "total_backward_count 1507660 real_backward_count 145182   9.630%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.642790/  1.177795, val:  62.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5496%\n",
      "layer   2  Sparsity: 71.8227%\n",
      "layer   3  Sparsity: 71.6207%\n",
      "total_backward_count 1517450 real_backward_count 145941   9.618%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.666690/  1.243200, val:  57.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5145%\n",
      "layer   2  Sparsity: 72.0694%\n",
      "layer   3  Sparsity: 72.0954%\n",
      "total_backward_count 1527240 real_backward_count 146684   9.605%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.647490/  1.237343, val:  60.42%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5369%\n",
      "layer   2  Sparsity: 71.6392%\n",
      "layer   3  Sparsity: 72.0861%\n",
      "total_backward_count 1537030 real_backward_count 147420   9.591%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.671959/  1.246031, val:  59.58%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5557%\n",
      "layer   2  Sparsity: 71.1616%\n",
      "layer   3  Sparsity: 71.5662%\n",
      "total_backward_count 1546820 real_backward_count 148216   9.582%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.654413/  1.180436, val:  64.17%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5549%\n",
      "layer   2  Sparsity: 71.8515%\n",
      "layer   3  Sparsity: 70.7103%\n",
      "total_backward_count 1556610 real_backward_count 149023   9.574%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.669600/  1.250227, val:  59.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5908%\n",
      "layer   2  Sparsity: 72.1475%\n",
      "layer   3  Sparsity: 71.7105%\n",
      "total_backward_count 1566400 real_backward_count 149817   9.564%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.671790/  1.218188, val:  60.83%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5700%\n",
      "layer   2  Sparsity: 71.8306%\n",
      "layer   3  Sparsity: 71.7529%\n",
      "total_backward_count 1576190 real_backward_count 150603   9.555%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.690210/  1.236082, val:  62.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5363%\n",
      "layer   2  Sparsity: 71.9483%\n",
      "layer   3  Sparsity: 71.7104%\n",
      "total_backward_count 1585980 real_backward_count 151419   9.547%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.662138/  1.297049, val:  55.00%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5172%\n",
      "layer   2  Sparsity: 72.0689%\n",
      "layer   3  Sparsity: 71.1016%\n",
      "total_backward_count 1595770 real_backward_count 152215   9.539%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.655052/  1.224271, val:  62.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5104%\n",
      "layer   2  Sparsity: 72.1624%\n",
      "layer   3  Sparsity: 71.3280%\n",
      "total_backward_count 1605560 real_backward_count 152964   9.527%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.664301/  1.233611, val:  65.83%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5412%\n",
      "layer   2  Sparsity: 72.2671%\n",
      "layer   3  Sparsity: 72.2783%\n",
      "total_backward_count 1615350 real_backward_count 153761   9.519%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.677099/  1.192310, val:  63.33%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5432%\n",
      "layer   2  Sparsity: 72.4955%\n",
      "layer   3  Sparsity: 70.6993%\n",
      "total_backward_count 1625140 real_backward_count 154527   9.509%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.662874/  1.192663, val:  65.00%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.5641%\n",
      "layer   2  Sparsity: 72.9299%\n",
      "layer   3  Sparsity: 71.1800%\n",
      "total_backward_count 1634930 real_backward_count 155244   9.495%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.641136/  1.308505, val:  58.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5611%\n",
      "layer   2  Sparsity: 72.4337%\n",
      "layer   3  Sparsity: 71.4015%\n",
      "total_backward_count 1644720 real_backward_count 155934   9.481%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.648220/  1.215436, val:  64.58%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5733%\n",
      "layer   2  Sparsity: 71.9217%\n",
      "layer   3  Sparsity: 71.9479%\n",
      "total_backward_count 1654510 real_backward_count 156716   9.472%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.638857/  1.252867, val:  57.92%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5497%\n",
      "layer   2  Sparsity: 72.6816%\n",
      "layer   3  Sparsity: 70.6639%\n",
      "total_backward_count 1664300 real_backward_count 157459   9.461%\n",
      "fc layer 1 self.abs_max_out: 15348.0\n",
      "lif layer 1 self.abs_max_v: 27349.0\n",
      "lif layer 1 self.abs_max_v: 27424.0\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.630847/  1.180302, val:  65.83%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5248%\n",
      "layer   2  Sparsity: 72.2429%\n",
      "layer   3  Sparsity: 71.0892%\n",
      "total_backward_count 1674090 real_backward_count 158254   9.453%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.647697/  1.227451, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5247%\n",
      "layer   2  Sparsity: 71.9927%\n",
      "layer   3  Sparsity: 71.6608%\n",
      "total_backward_count 1683880 real_backward_count 159004   9.443%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.670054/  1.178667, val:  63.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5981%\n",
      "layer   2  Sparsity: 71.6105%\n",
      "layer   3  Sparsity: 72.7753%\n",
      "total_backward_count 1693670 real_backward_count 159810   9.436%\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.614602/  1.212696, val:  63.33%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5828%\n",
      "layer   2  Sparsity: 71.8140%\n",
      "layer   3  Sparsity: 72.1059%\n",
      "total_backward_count 1703460 real_backward_count 160561   9.426%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.610578/  1.158265, val:  64.17%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5770%\n",
      "layer   2  Sparsity: 71.8309%\n",
      "layer   3  Sparsity: 73.0395%\n",
      "total_backward_count 1713250 real_backward_count 161267   9.413%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.619855/  1.235254, val:  61.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5102%\n",
      "layer   2  Sparsity: 72.0557%\n",
      "layer   3  Sparsity: 72.3499%\n",
      "total_backward_count 1723040 real_backward_count 162038   9.404%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.618642/  1.159298, val:  67.50%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5576%\n",
      "layer   2  Sparsity: 71.8881%\n",
      "layer   3  Sparsity: 72.7411%\n",
      "total_backward_count 1732830 real_backward_count 162769   9.393%\n",
      "fc layer 3 self.abs_max_out: 1328.0\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.617434/  1.158507, val:  67.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5944%\n",
      "layer   2  Sparsity: 71.5795%\n",
      "layer   3  Sparsity: 71.5942%\n",
      "total_backward_count 1742620 real_backward_count 163477   9.381%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.614605/  1.190133, val:  66.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5960%\n",
      "layer   2  Sparsity: 71.4941%\n",
      "layer   3  Sparsity: 71.0805%\n",
      "total_backward_count 1752410 real_backward_count 164183   9.369%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.621400/  1.177251, val:  62.08%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5769%\n",
      "layer   2  Sparsity: 72.0105%\n",
      "layer   3  Sparsity: 72.5984%\n",
      "total_backward_count 1762200 real_backward_count 164932   9.359%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.648612/  1.216457, val:  62.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5897%\n",
      "layer   2  Sparsity: 72.0472%\n",
      "layer   3  Sparsity: 71.6167%\n",
      "total_backward_count 1771990 real_backward_count 165667   9.349%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.636145/  1.258643, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5289%\n",
      "layer   2  Sparsity: 72.0071%\n",
      "layer   3  Sparsity: 71.5310%\n",
      "total_backward_count 1781780 real_backward_count 166414   9.340%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.623292/  1.123824, val:  68.33%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5746%\n",
      "layer   2  Sparsity: 71.6099%\n",
      "layer   3  Sparsity: 70.8712%\n",
      "total_backward_count 1791570 real_backward_count 167139   9.329%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.635592/  1.189712, val:  64.17%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5075%\n",
      "layer   2  Sparsity: 71.6890%\n",
      "layer   3  Sparsity: 72.1537%\n",
      "total_backward_count 1801360 real_backward_count 167817   9.316%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.645778/  1.222878, val:  59.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5464%\n",
      "layer   2  Sparsity: 72.4256%\n",
      "layer   3  Sparsity: 72.7041%\n",
      "total_backward_count 1811150 real_backward_count 168579   9.308%\n",
      "fc layer 3 self.abs_max_out: 1391.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.654863/  1.202221, val:  64.58%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5226%\n",
      "layer   2  Sparsity: 71.6658%\n",
      "layer   3  Sparsity: 72.8053%\n",
      "total_backward_count 1820940 real_backward_count 169346   9.300%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.631460/  1.133762, val:  65.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5765%\n",
      "layer   2  Sparsity: 71.6261%\n",
      "layer   3  Sparsity: 72.4895%\n",
      "total_backward_count 1830730 real_backward_count 170073   9.290%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.641295/  1.244086, val:  62.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5851%\n",
      "layer   2  Sparsity: 71.7314%\n",
      "layer   3  Sparsity: 72.9096%\n",
      "total_backward_count 1840520 real_backward_count 170825   9.281%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.648026/  1.226498, val:  61.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5688%\n",
      "layer   2  Sparsity: 71.0940%\n",
      "layer   3  Sparsity: 72.1456%\n",
      "total_backward_count 1850310 real_backward_count 171561   9.272%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.637428/  1.151088, val:  64.17%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5556%\n",
      "layer   2  Sparsity: 70.4780%\n",
      "layer   3  Sparsity: 71.5510%\n",
      "total_backward_count 1860100 real_backward_count 172256   9.261%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.624797/  1.179756, val:  61.67%, val_best:  73.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5918%\n",
      "layer   2  Sparsity: 71.1940%\n",
      "layer   3  Sparsity: 72.1146%\n",
      "total_backward_count 1869890 real_backward_count 172967   9.250%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.632165/  1.262212, val:  55.83%, val_best:  73.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5434%\n",
      "layer   2  Sparsity: 71.1720%\n",
      "layer   3  Sparsity: 71.8829%\n",
      "total_backward_count 1879680 real_backward_count 173710   9.241%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.597614/  1.195571, val:  62.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5754%\n",
      "layer   2  Sparsity: 71.4267%\n",
      "layer   3  Sparsity: 71.1187%\n",
      "total_backward_count 1889470 real_backward_count 174441   9.232%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.606362/  1.237902, val:  57.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5725%\n",
      "layer   2  Sparsity: 71.2881%\n",
      "layer   3  Sparsity: 72.0385%\n",
      "total_backward_count 1899260 real_backward_count 175182   9.224%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.617355/  1.154886, val:  63.33%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 85.5494%\n",
      "layer   2  Sparsity: 71.2330%\n",
      "layer   3  Sparsity: 72.5309%\n",
      "total_backward_count 1909050 real_backward_count 175867   9.212%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.632936/  1.153887, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5806%\n",
      "layer   2  Sparsity: 71.2505%\n",
      "layer   3  Sparsity: 72.0398%\n",
      "total_backward_count 1918840 real_backward_count 176620   9.205%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.607530/  1.114620, val:  67.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.5747%\n",
      "layer   2  Sparsity: 71.4355%\n",
      "layer   3  Sparsity: 71.5724%\n",
      "total_backward_count 1928630 real_backward_count 177323   9.194%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.586543/  1.187553, val:  62.08%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5549%\n",
      "layer   2  Sparsity: 70.9530%\n",
      "layer   3  Sparsity: 70.1219%\n",
      "total_backward_count 1938420 real_backward_count 178097   9.188%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.580557/  1.206708, val:  57.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5695%\n",
      "layer   2  Sparsity: 70.5450%\n",
      "layer   3  Sparsity: 70.5029%\n",
      "total_backward_count 1948210 real_backward_count 178842   9.180%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.613813/  1.190465, val:  62.08%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 85.5489%\n",
      "layer   2  Sparsity: 70.6886%\n",
      "layer   3  Sparsity: 70.2843%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9892583daf2496d9ed554b9e97bb8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÑ‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÑ‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>0.61381</td></tr><tr><td>val_acc_best</td><td>0.7375</td></tr><tr><td>val_acc_now</td><td>0.62083</td></tr><tr><td>val_loss</td><td>1.19046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-sweep-128</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8lk9qfnj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8lk9qfnj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_053633-8lk9qfnj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h5wnrcpa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_100034-h5wnrcpa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h5wnrcpa' target=\"_blank\">ruby-sweep-133</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h5wnrcpa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h5wnrcpa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_100043_178', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 355.0\n",
      "lif layer 1 self.abs_max_v: 355.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 354.0\n",
      "lif layer 2 self.abs_max_v: 354.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 27.0\n",
      "fc layer 1 self.abs_max_out: 430.0\n",
      "lif layer 1 self.abs_max_v: 543.5\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "lif layer 2 self.abs_max_v: 542.5\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 1 self.abs_max_out: 445.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "lif layer 2 self.abs_max_v: 586.5\n",
      "lif layer 1 self.abs_max_v: 588.5\n",
      "fc layer 2 self.abs_max_out: 512.0\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 1 self.abs_max_out: 470.0\n",
      "lif layer 1 self.abs_max_v: 703.5\n",
      "lif layer 2 self.abs_max_v: 776.0\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "fc layer 1 self.abs_max_out: 606.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "lif layer 1 self.abs_max_v: 719.0\n",
      "fc layer 2 self.abs_max_out: 651.0\n",
      "fc layer 2 self.abs_max_out: 735.0\n",
      "lif layer 2 self.abs_max_v: 879.5\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "lif layer 2 self.abs_max_v: 1024.0\n",
      "fc layer 1 self.abs_max_out: 802.0\n",
      "lif layer 1 self.abs_max_v: 1041.5\n",
      "lif layer 2 self.abs_max_v: 1097.5\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "lif layer 1 self.abs_max_v: 1219.5\n",
      "fc layer 2 self.abs_max_out: 859.0\n",
      "lif layer 2 self.abs_max_v: 1216.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "fc layer 1 self.abs_max_out: 1001.0\n",
      "lif layer 2 self.abs_max_v: 1223.0\n",
      "fc layer 1 self.abs_max_out: 1121.0\n",
      "lif layer 2 self.abs_max_v: 1277.5\n",
      "fc layer 1 self.abs_max_out: 1252.0\n",
      "lif layer 1 self.abs_max_v: 1252.0\n",
      "fc layer 2 self.abs_max_out: 876.0\n",
      "fc layer 3 self.abs_max_out: 413.0\n",
      "lif layer 1 self.abs_max_v: 1279.5\n",
      "lif layer 1 self.abs_max_v: 1582.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 1279.5\n",
      "lif layer 2 self.abs_max_v: 1421.5\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 2 self.abs_max_out: 1089.0\n",
      "lif layer 2 self.abs_max_v: 1549.5\n",
      "lif layer 2 self.abs_max_v: 1703.5\n",
      "lif layer 2 self.abs_max_v: 1711.0\n",
      "lif layer 2 self.abs_max_v: 1745.5\n",
      "lif layer 1 self.abs_max_v: 1584.5\n",
      "lif layer 2 self.abs_max_v: 1778.0\n",
      "lif layer 1 self.abs_max_v: 1630.5\n",
      "fc layer 2 self.abs_max_out: 1149.0\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "fc layer 1 self.abs_max_out: 1266.0\n",
      "lif layer 1 self.abs_max_v: 1663.5\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "lif layer 2 self.abs_max_v: 1781.5\n",
      "fc layer 1 self.abs_max_out: 1431.0\n",
      "lif layer 1 self.abs_max_v: 1681.0\n",
      "lif layer 2 self.abs_max_v: 1883.0\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "lif layer 1 self.abs_max_v: 2102.0\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "fc layer 1 self.abs_max_out: 1779.0\n",
      "fc layer 1 self.abs_max_out: 1806.0\n",
      "lif layer 2 self.abs_max_v: 1994.5\n",
      "lif layer 2 self.abs_max_v: 2231.5\n",
      "lif layer 1 self.abs_max_v: 2188.5\n",
      "fc layer 2 self.abs_max_out: 1478.0\n",
      "lif layer 2 self.abs_max_v: 2416.5\n",
      "fc layer 1 self.abs_max_out: 1932.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "lif layer 1 self.abs_max_v: 2287.5\n",
      "fc layer 2 self.abs_max_out: 1494.0\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 1 self.abs_max_out: 2067.0\n",
      "lif layer 2 self.abs_max_v: 2568.0\n",
      "lif layer 1 self.abs_max_v: 2290.5\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "lif layer 1 self.abs_max_v: 2563.0\n",
      "fc layer 2 self.abs_max_out: 1549.0\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "lif layer 1 self.abs_max_v: 2627.5\n",
      "lif layer 1 self.abs_max_v: 2665.0\n",
      "lif layer 1 self.abs_max_v: 2781.5\n",
      "lif layer 2 self.abs_max_v: 2702.5\n",
      "fc layer 1 self.abs_max_out: 2143.0\n",
      "fc layer 2 self.abs_max_out: 1835.0\n",
      "fc layer 1 self.abs_max_out: 2308.0\n",
      "fc layer 1 self.abs_max_out: 2319.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 1 self.abs_max_out: 2422.0\n",
      "lif layer 2 self.abs_max_v: 2760.0\n",
      "fc layer 1 self.abs_max_out: 2560.0\n",
      "lif layer 1 self.abs_max_v: 2867.5\n",
      "lif layer 1 self.abs_max_v: 3214.0\n",
      "lif layer 2 self.abs_max_v: 2763.0\n",
      "lif layer 2 self.abs_max_v: 3050.5\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "lif layer 1 self.abs_max_v: 3646.5\n",
      "lif layer 2 self.abs_max_v: 3069.0\n",
      "fc layer 2 self.abs_max_out: 1850.0\n",
      "fc layer 2 self.abs_max_out: 1857.0\n",
      "fc layer 2 self.abs_max_out: 1889.0\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "fc layer 2 self.abs_max_out: 1964.0\n",
      "fc layer 2 self.abs_max_out: 2051.0\n",
      "fc layer 2 self.abs_max_out: 2094.0\n",
      "fc layer 2 self.abs_max_out: 2156.0\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "fc layer 1 self.abs_max_out: 2612.0\n",
      "lif layer 1 self.abs_max_v: 4181.0\n",
      "fc layer 1 self.abs_max_out: 2940.0\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "lif layer 2 self.abs_max_v: 3203.5\n",
      "lif layer 2 self.abs_max_v: 3316.5\n",
      "fc layer 2 self.abs_max_out: 2333.0\n",
      "fc layer 2 self.abs_max_out: 2339.0\n",
      "fc layer 2 self.abs_max_out: 2479.0\n",
      "fc layer 2 self.abs_max_out: 2548.0\n",
      "fc layer 2 self.abs_max_out: 2587.0\n",
      "lif layer 2 self.abs_max_v: 3380.5\n",
      "lif layer 2 self.abs_max_v: 3456.0\n",
      "lif layer 2 self.abs_max_v: 3640.5\n",
      "fc layer 1 self.abs_max_out: 3373.0\n",
      "fc layer 2 self.abs_max_out: 2627.0\n",
      "lif layer 1 self.abs_max_v: 4360.0\n",
      "lif layer 1 self.abs_max_v: 4378.0\n",
      "lif layer 2 self.abs_max_v: 3660.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.925864/  2.053691, val:  35.00%, val_best:  35.00%, tr:  88.87%, tr_best:  88.87%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2932%\n",
      "layer   2  Sparsity: 71.9743%\n",
      "layer   3  Sparsity: 71.9715%\n",
      "total_backward_count 9790 real_backward_count 3168  32.360%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 4436.0\n",
      "fc layer 1 self.abs_max_out: 3650.0\n",
      "fc layer 2 self.abs_max_out: 2645.0\n",
      "fc layer 2 self.abs_max_out: 2745.0\n",
      "fc layer 2 self.abs_max_out: 2766.0\n",
      "fc layer 2 self.abs_max_out: 2994.0\n",
      "lif layer 2 self.abs_max_v: 3671.5\n",
      "lif layer 2 self.abs_max_v: 3728.5\n",
      "lif layer 2 self.abs_max_v: 3811.5\n",
      "lif layer 2 self.abs_max_v: 3824.0\n",
      "lif layer 2 self.abs_max_v: 3872.5\n",
      "lif layer 2 self.abs_max_v: 3897.5\n",
      "lif layer 2 self.abs_max_v: 3993.0\n",
      "lif layer 1 self.abs_max_v: 4491.0\n",
      "lif layer 1 self.abs_max_v: 4592.5\n",
      "lif layer 1 self.abs_max_v: 5129.5\n",
      "lif layer 2 self.abs_max_v: 4103.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.864450/  2.034163, val:  40.83%, val_best:  40.83%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3054%\n",
      "layer   2  Sparsity: 72.2586%\n",
      "layer   3  Sparsity: 67.9215%\n",
      "total_backward_count 19580 real_backward_count 5092  26.006%\n",
      "fc layer 2 self.abs_max_out: 3042.0\n",
      "fc layer 2 self.abs_max_out: 3046.0\n",
      "lif layer 2 self.abs_max_v: 4218.0\n",
      "fc layer 2 self.abs_max_out: 3121.0\n",
      "lif layer 2 self.abs_max_v: 4366.0\n",
      "fc layer 2 self.abs_max_out: 3159.0\n",
      "lif layer 1 self.abs_max_v: 5197.0\n",
      "lif layer 1 self.abs_max_v: 5938.5\n",
      "lif layer 1 self.abs_max_v: 5999.5\n",
      "fc layer 2 self.abs_max_out: 3191.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.885484/  2.045085, val:  37.92%, val_best:  40.83%, tr:  98.98%, tr_best:  98.98%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3216%\n",
      "layer   2  Sparsity: 73.8200%\n",
      "layer   3  Sparsity: 67.6436%\n",
      "total_backward_count 29370 real_backward_count 6844  23.303%\n",
      "fc layer 2 self.abs_max_out: 3197.0\n",
      "fc layer 2 self.abs_max_out: 3201.0\n",
      "fc layer 2 self.abs_max_out: 3244.0\n",
      "lif layer 1 self.abs_max_v: 6207.5\n",
      "lif layer 1 self.abs_max_v: 6223.5\n",
      "lif layer 1 self.abs_max_v: 6258.0\n",
      "fc layer 2 self.abs_max_out: 3310.0\n",
      "fc layer 2 self.abs_max_out: 3333.0\n",
      "fc layer 2 self.abs_max_out: 3412.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.895757/  2.067767, val:  37.50%, val_best:  40.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2962%\n",
      "layer   2  Sparsity: 74.4638%\n",
      "layer   3  Sparsity: 67.6513%\n",
      "total_backward_count 39160 real_backward_count 8469  21.627%\n",
      "fc layer 2 self.abs_max_out: 3523.0\n",
      "fc layer 2 self.abs_max_out: 3651.0\n",
      "lif layer 1 self.abs_max_v: 6309.5\n",
      "lif layer 1 self.abs_max_v: 6370.0\n",
      "fc layer 1 self.abs_max_out: 3673.0\n",
      "fc layer 2 self.abs_max_out: 3724.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.887524/  2.025301, val:  47.50%, val_best:  47.50%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2674%\n",
      "layer   2  Sparsity: 74.3728%\n",
      "layer   3  Sparsity: 66.7027%\n",
      "total_backward_count 48950 real_backward_count 9975  20.378%\n",
      "fc layer 1 self.abs_max_out: 3812.0\n",
      "fc layer 1 self.abs_max_out: 3979.0\n",
      "lif layer 1 self.abs_max_v: 6957.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.875139/  2.045163, val:  49.17%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2954%\n",
      "layer   2  Sparsity: 75.3509%\n",
      "layer   3  Sparsity: 66.9944%\n",
      "total_backward_count 58740 real_backward_count 11469  19.525%\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 3 self.abs_max_out: 696.0\n",
      "fc layer 2 self.abs_max_out: 3743.0\n",
      "fc layer 1 self.abs_max_out: 4053.0\n",
      "fc layer 1 self.abs_max_out: 4184.0\n",
      "lif layer 1 self.abs_max_v: 6974.0\n",
      "fc layer 2 self.abs_max_out: 3788.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.886551/  2.041822, val:  45.42%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3036%\n",
      "layer   2  Sparsity: 76.5180%\n",
      "layer   3  Sparsity: 67.4895%\n",
      "total_backward_count 68530 real_backward_count 13000  18.970%\n",
      "fc layer 2 self.abs_max_out: 3796.0\n",
      "fc layer 1 self.abs_max_out: 4310.0\n",
      "lif layer 1 self.abs_max_v: 7196.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.889635/  2.026800, val:  50.83%, val_best:  50.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2992%\n",
      "layer   2  Sparsity: 75.8675%\n",
      "layer   3  Sparsity: 67.6075%\n",
      "total_backward_count 78320 real_backward_count 14409  18.398%\n",
      "lif layer 2 self.abs_max_v: 4505.0\n",
      "fc layer 2 self.abs_max_out: 3828.0\n",
      "fc layer 1 self.abs_max_out: 4382.0\n",
      "lif layer 1 self.abs_max_v: 7460.5\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.897762/  2.053967, val:  53.75%, val_best:  53.75%, tr:  99.28%, tr_best:  99.69%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3070%\n",
      "layer   2  Sparsity: 75.3611%\n",
      "layer   3  Sparsity: 67.7308%\n",
      "total_backward_count 88110 real_backward_count 15891  18.035%\n",
      "fc layer 2 self.abs_max_out: 3858.0\n",
      "fc layer 2 self.abs_max_out: 3912.0\n",
      "fc layer 1 self.abs_max_out: 4527.0\n",
      "lif layer 1 self.abs_max_v: 7655.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.897833/  2.052261, val:  47.08%, val_best:  53.75%, tr:  99.18%, tr_best:  99.69%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2938%\n",
      "layer   2  Sparsity: 75.5612%\n",
      "layer   3  Sparsity: 67.4693%\n",
      "total_backward_count 97900 real_backward_count 17338  17.710%\n",
      "lif layer 2 self.abs_max_v: 4695.5\n",
      "lif layer 2 self.abs_max_v: 4878.0\n",
      "fc layer 2 self.abs_max_out: 4183.0\n",
      "fc layer 1 self.abs_max_out: 4848.0\n",
      "lif layer 1 self.abs_max_v: 8108.5\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.898112/  2.048891, val:  47.50%, val_best:  53.75%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2754%\n",
      "layer   2  Sparsity: 75.1974%\n",
      "layer   3  Sparsity: 67.3230%\n",
      "total_backward_count 107690 real_backward_count 18711  17.375%\n",
      "fc layer 2 self.abs_max_out: 4267.0\n",
      "fc layer 1 self.abs_max_out: 4959.0\n",
      "lif layer 1 self.abs_max_v: 8335.5\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.913219/  2.027854, val:  50.83%, val_best:  53.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2607%\n",
      "layer   2  Sparsity: 75.2929%\n",
      "layer   3  Sparsity: 68.1390%\n",
      "total_backward_count 117480 real_backward_count 20149  17.151%\n",
      "fc layer 2 self.abs_max_out: 4291.0\n",
      "fc layer 2 self.abs_max_out: 4508.0\n",
      "fc layer 1 self.abs_max_out: 5210.0\n",
      "lif layer 1 self.abs_max_v: 8737.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.903174/  2.031769, val:  50.83%, val_best:  53.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2824%\n",
      "layer   2  Sparsity: 74.7257%\n",
      "layer   3  Sparsity: 68.2031%\n",
      "total_backward_count 127270 real_backward_count 21546  16.929%\n",
      "fc layer 1 self.abs_max_out: 5298.0\n",
      "lif layer 1 self.abs_max_v: 8869.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.915974/  2.056420, val:  45.00%, val_best:  53.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2560%\n",
      "layer   2  Sparsity: 75.8847%\n",
      "layer   3  Sparsity: 69.0439%\n",
      "total_backward_count 137060 real_backward_count 22949  16.744%\n",
      "fc layer 1 self.abs_max_out: 5333.0\n",
      "fc layer 1 self.abs_max_out: 5570.0\n",
      "lif layer 1 self.abs_max_v: 9366.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.920729/  2.053290, val:  37.50%, val_best:  53.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2587%\n",
      "layer   2  Sparsity: 75.4732%\n",
      "layer   3  Sparsity: 69.1833%\n",
      "total_backward_count 146850 real_backward_count 24326  16.565%\n",
      "fc layer 1 self.abs_max_out: 5612.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.923827/  2.055660, val:  52.92%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2769%\n",
      "layer   2  Sparsity: 75.3195%\n",
      "layer   3  Sparsity: 69.8534%\n",
      "total_backward_count 156640 real_backward_count 25731  16.427%\n",
      "lif layer 1 self.abs_max_v: 9426.5\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.923097/  2.058364, val:  60.42%, val_best:  60.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2887%\n",
      "layer   2  Sparsity: 75.5667%\n",
      "layer   3  Sparsity: 70.9989%\n",
      "total_backward_count 166430 real_backward_count 27091  16.278%\n",
      "fc layer 1 self.abs_max_out: 5696.0\n",
      "lif layer 1 self.abs_max_v: 9705.5\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.943107/  2.048837, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3137%\n",
      "layer   2  Sparsity: 75.8038%\n",
      "layer   3  Sparsity: 72.1222%\n",
      "total_backward_count 176220 real_backward_count 28487  16.166%\n",
      "lif layer 2 self.abs_max_v: 4955.0\n",
      "lif layer 2 self.abs_max_v: 5070.5\n",
      "fc layer 1 self.abs_max_out: 5793.0\n",
      "lif layer 1 self.abs_max_v: 9770.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.938653/  2.045190, val:  52.92%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2865%\n",
      "layer   2  Sparsity: 75.0532%\n",
      "layer   3  Sparsity: 72.0304%\n",
      "total_backward_count 186010 real_backward_count 29883  16.065%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.941424/  2.067180, val:  41.67%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2786%\n",
      "layer   2  Sparsity: 75.0066%\n",
      "layer   3  Sparsity: 72.8089%\n",
      "total_backward_count 195800 real_backward_count 31178  15.923%\n",
      "fc layer 1 self.abs_max_out: 5867.0\n",
      "lif layer 1 self.abs_max_v: 9924.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.944593/  2.065959, val:  46.25%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3048%\n",
      "layer   2  Sparsity: 75.0220%\n",
      "layer   3  Sparsity: 73.2942%\n",
      "total_backward_count 205590 real_backward_count 32509  15.813%\n",
      "fc layer 1 self.abs_max_out: 6061.0\n",
      "lif layer 1 self.abs_max_v: 10387.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.955006/  2.077883, val:  49.58%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2946%\n",
      "layer   2  Sparsity: 75.0082%\n",
      "layer   3  Sparsity: 73.6135%\n",
      "total_backward_count 215380 real_backward_count 33928  15.753%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.950815/  2.036678, val:  61.67%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2767%\n",
      "layer   2  Sparsity: 75.2351%\n",
      "layer   3  Sparsity: 73.9155%\n",
      "total_backward_count 225170 real_backward_count 35304  15.679%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.942414/  2.042916, val:  61.25%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2869%\n",
      "layer   2  Sparsity: 75.5374%\n",
      "layer   3  Sparsity: 74.0273%\n",
      "total_backward_count 234960 real_backward_count 36661  15.603%\n",
      "lif layer 2 self.abs_max_v: 5166.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.949174/  2.053399, val:  54.58%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3001%\n",
      "layer   2  Sparsity: 75.2269%\n",
      "layer   3  Sparsity: 74.4359%\n",
      "total_backward_count 244750 real_backward_count 37934  15.499%\n",
      "lif layer 1 self.abs_max_v: 10491.5\n",
      "lif layer 1 self.abs_max_v: 10812.0\n",
      "fc layer 1 self.abs_max_out: 6442.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.949811/  2.049470, val:  62.08%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2725%\n",
      "layer   2  Sparsity: 74.7935%\n",
      "layer   3  Sparsity: 74.8842%\n",
      "total_backward_count 254540 real_backward_count 39320  15.447%\n",
      "lif layer 2 self.abs_max_v: 5204.5\n",
      "lif layer 2 self.abs_max_v: 5325.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.952859/  2.047930, val:  60.83%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3039%\n",
      "layer   2  Sparsity: 75.1183%\n",
      "layer   3  Sparsity: 75.1976%\n",
      "total_backward_count 264330 real_backward_count 40664  15.384%\n",
      "lif layer 2 self.abs_max_v: 5399.5\n",
      "lif layer 1 self.abs_max_v: 10815.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.963059/  2.082281, val:  62.92%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2799%\n",
      "layer   2  Sparsity: 74.5968%\n",
      "layer   3  Sparsity: 75.8815%\n",
      "total_backward_count 274120 real_backward_count 41981  15.315%\n",
      "fc layer 1 self.abs_max_out: 6619.0\n",
      "lif layer 1 self.abs_max_v: 10916.5\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.968155/  2.085396, val:  54.17%, val_best:  62.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2890%\n",
      "layer   2  Sparsity: 74.9089%\n",
      "layer   3  Sparsity: 75.7023%\n",
      "total_backward_count 283910 real_backward_count 43240  15.230%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.966358/  2.061281, val:  51.67%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2792%\n",
      "layer   2  Sparsity: 74.7797%\n",
      "layer   3  Sparsity: 75.9001%\n",
      "total_backward_count 293700 real_backward_count 44521  15.159%\n",
      "fc layer 1 self.abs_max_out: 6727.0\n",
      "lif layer 1 self.abs_max_v: 11384.5\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.956243/  2.064529, val:  60.83%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2656%\n",
      "layer   2  Sparsity: 74.1182%\n",
      "layer   3  Sparsity: 75.7980%\n",
      "total_backward_count 303490 real_backward_count 45789  15.087%\n",
      "fc layer 1 self.abs_max_out: 6738.0\n",
      "fc layer 2 self.abs_max_out: 4540.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.965286/  2.049750, val:  52.08%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2663%\n",
      "layer   2  Sparsity: 73.6603%\n",
      "layer   3  Sparsity: 75.8745%\n",
      "total_backward_count 313280 real_backward_count 47066  15.024%\n",
      "fc layer 1 self.abs_max_out: 6806.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.958601/  2.072751, val:  52.92%, val_best:  62.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2804%\n",
      "layer   2  Sparsity: 74.3875%\n",
      "layer   3  Sparsity: 76.7405%\n",
      "total_backward_count 323070 real_backward_count 48312  14.954%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.962219/  2.061674, val:  57.92%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2960%\n",
      "layer   2  Sparsity: 73.7808%\n",
      "layer   3  Sparsity: 77.2734%\n",
      "total_backward_count 332860 real_backward_count 49574  14.893%\n",
      "lif layer 1 self.abs_max_v: 11416.0\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.965738/  2.084326, val:  49.58%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2594%\n",
      "layer   2  Sparsity: 74.2845%\n",
      "layer   3  Sparsity: 77.6797%\n",
      "total_backward_count 342650 real_backward_count 50826  14.833%\n",
      "fc layer 1 self.abs_max_out: 6844.0\n",
      "lif layer 1 self.abs_max_v: 11715.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.976390/  2.062341, val:  65.42%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2826%\n",
      "layer   2  Sparsity: 74.6868%\n",
      "layer   3  Sparsity: 77.7701%\n",
      "total_backward_count 352440 real_backward_count 52091  14.780%\n",
      "lif layer 1 self.abs_max_v: 11748.5\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.977111/  2.063829, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2833%\n",
      "layer   2  Sparsity: 74.7112%\n",
      "layer   3  Sparsity: 78.0410%\n",
      "total_backward_count 362230 real_backward_count 53323  14.721%\n",
      "lif layer 1 self.abs_max_v: 11754.5\n",
      "lif layer 1 self.abs_max_v: 11817.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.972947/  2.069182, val:  54.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2948%\n",
      "layer   2  Sparsity: 74.4133%\n",
      "layer   3  Sparsity: 77.8609%\n",
      "total_backward_count 372020 real_backward_count 54474  14.643%\n",
      "lif layer 1 self.abs_max_v: 12135.5\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.962297/  2.074059, val:  52.08%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3305%\n",
      "layer   2  Sparsity: 74.1668%\n",
      "layer   3  Sparsity: 77.9738%\n",
      "total_backward_count 381810 real_backward_count 55710  14.591%\n",
      "fc layer 1 self.abs_max_out: 7044.0\n",
      "fc layer 2 self.abs_max_out: 4616.0\n",
      "lif layer 1 self.abs_max_v: 12157.5\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.967407/  2.059998, val:  71.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2903%\n",
      "layer   2  Sparsity: 74.1618%\n",
      "layer   3  Sparsity: 78.2009%\n",
      "total_backward_count 391600 real_backward_count 56921  14.535%\n",
      "lif layer 1 self.abs_max_v: 12182.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.975441/  2.059735, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3026%\n",
      "layer   2  Sparsity: 74.5150%\n",
      "layer   3  Sparsity: 78.5083%\n",
      "total_backward_count 401390 real_backward_count 58083  14.470%\n",
      "fc layer 1 self.abs_max_out: 7177.0\n",
      "lif layer 1 self.abs_max_v: 12321.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.966150/  2.047407, val:  61.67%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2750%\n",
      "layer   2  Sparsity: 74.1124%\n",
      "layer   3  Sparsity: 78.7165%\n",
      "total_backward_count 411180 real_backward_count 59272  14.415%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.974174/  2.073046, val:  62.08%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.2768%\n",
      "layer   2  Sparsity: 74.1674%\n",
      "layer   3  Sparsity: 79.0270%\n",
      "total_backward_count 420970 real_backward_count 60460  14.362%\n",
      "fc layer 1 self.abs_max_out: 7221.0\n",
      "lif layer 1 self.abs_max_v: 12551.5\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.983147/  2.063544, val:  70.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3190%\n",
      "layer   2  Sparsity: 73.8958%\n",
      "layer   3  Sparsity: 79.0315%\n",
      "total_backward_count 430760 real_backward_count 61615  14.304%\n",
      "lif layer 2 self.abs_max_v: 5510.5\n",
      "fc layer 1 self.abs_max_out: 7222.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.977109/  2.057956, val:  64.58%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3104%\n",
      "layer   2  Sparsity: 74.0676%\n",
      "layer   3  Sparsity: 79.1603%\n",
      "total_backward_count 440550 real_backward_count 62749  14.243%\n",
      "fc layer 1 self.abs_max_out: 7291.0\n",
      "lif layer 2 self.abs_max_v: 5522.0\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.982723/  2.061541, val:  71.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3070%\n",
      "layer   2  Sparsity: 73.6947%\n",
      "layer   3  Sparsity: 79.7957%\n",
      "total_backward_count 450340 real_backward_count 63904  14.190%\n",
      "fc layer 1 self.abs_max_out: 7440.0\n",
      "lif layer 1 self.abs_max_v: 12877.5\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.988951/  2.071341, val:  67.92%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3035%\n",
      "layer   2  Sparsity: 73.5774%\n",
      "layer   3  Sparsity: 79.5827%\n",
      "total_backward_count 460130 real_backward_count 65002  14.127%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.971293/  2.066251, val:  62.92%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2883%\n",
      "layer   2  Sparsity: 73.7347%\n",
      "layer   3  Sparsity: 79.3678%\n",
      "total_backward_count 469920 real_backward_count 66093  14.065%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.975373/  2.063449, val:  71.25%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3061%\n",
      "layer   2  Sparsity: 73.3549%\n",
      "layer   3  Sparsity: 79.9295%\n",
      "total_backward_count 479710 real_backward_count 67171  14.002%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.975337/  2.054548, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2755%\n",
      "layer   2  Sparsity: 73.2263%\n",
      "layer   3  Sparsity: 80.0696%\n",
      "total_backward_count 489500 real_backward_count 68253  13.943%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.975792/  2.064752, val:  65.00%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2926%\n",
      "layer   2  Sparsity: 73.7420%\n",
      "layer   3  Sparsity: 80.4907%\n",
      "total_backward_count 499290 real_backward_count 69381  13.896%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.970955/  2.057243, val:  69.58%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2656%\n",
      "layer   2  Sparsity: 73.6153%\n",
      "layer   3  Sparsity: 80.2021%\n",
      "total_backward_count 509080 real_backward_count 70453  13.839%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.968752/  2.057856, val:  75.42%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2925%\n",
      "layer   2  Sparsity: 73.4168%\n",
      "layer   3  Sparsity: 80.2772%\n",
      "total_backward_count 518870 real_backward_count 71545  13.789%\n",
      "fc layer 1 self.abs_max_out: 7626.0\n",
      "lif layer 1 self.abs_max_v: 12944.5\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.971242/  2.038477, val:  62.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2917%\n",
      "layer   2  Sparsity: 73.1610%\n",
      "layer   3  Sparsity: 80.3488%\n",
      "total_backward_count 528660 real_backward_count 72571  13.727%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.963057/  2.054640, val:  71.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2953%\n",
      "layer   2  Sparsity: 72.6665%\n",
      "layer   3  Sparsity: 80.3289%\n",
      "total_backward_count 538450 real_backward_count 73601  13.669%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.967829/  2.044517, val:  74.58%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3021%\n",
      "layer   2  Sparsity: 73.0966%\n",
      "layer   3  Sparsity: 80.5104%\n",
      "total_backward_count 548240 real_backward_count 74672  13.620%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.957574/  2.037912, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3155%\n",
      "layer   2  Sparsity: 73.0797%\n",
      "layer   3  Sparsity: 80.8806%\n",
      "total_backward_count 558030 real_backward_count 75666  13.559%\n",
      "fc layer 1 self.abs_max_out: 7800.0\n",
      "lif layer 1 self.abs_max_v: 13322.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.958076/  2.042123, val:  75.00%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2866%\n",
      "layer   2  Sparsity: 72.8678%\n",
      "layer   3  Sparsity: 80.9877%\n",
      "total_backward_count 567820 real_backward_count 76672  13.503%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.961485/  2.048251, val:  76.25%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2767%\n",
      "layer   2  Sparsity: 73.1695%\n",
      "layer   3  Sparsity: 80.7688%\n",
      "total_backward_count 577610 real_backward_count 77639  13.441%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.972029/  2.054698, val:  67.08%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2744%\n",
      "layer   2  Sparsity: 73.0361%\n",
      "layer   3  Sparsity: 80.7769%\n",
      "total_backward_count 587400 real_backward_count 78536  13.370%\n",
      "fc layer 1 self.abs_max_out: 7892.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.978179/  2.055622, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3121%\n",
      "layer   2  Sparsity: 73.0042%\n",
      "layer   3  Sparsity: 81.0253%\n",
      "total_backward_count 597190 real_backward_count 79547  13.320%\n",
      "lif layer 2 self.abs_max_v: 5728.5\n",
      "fc layer 1 self.abs_max_out: 7930.0\n",
      "lif layer 1 self.abs_max_v: 13441.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.973324/  2.038035, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2775%\n",
      "layer   2  Sparsity: 72.7378%\n",
      "layer   3  Sparsity: 80.8748%\n",
      "total_backward_count 606980 real_backward_count 80538  13.269%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.973246/  2.059949, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2571%\n",
      "layer   2  Sparsity: 72.4803%\n",
      "layer   3  Sparsity: 80.9234%\n",
      "total_backward_count 616770 real_backward_count 81497  13.214%\n",
      "fc layer 1 self.abs_max_out: 8331.0\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.971999/  2.049098, val:  65.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.2889%\n",
      "layer   2  Sparsity: 72.8057%\n",
      "layer   3  Sparsity: 80.4726%\n",
      "total_backward_count 626560 real_backward_count 82433  13.156%\n",
      "lif layer 1 self.abs_max_v: 13618.0\n",
      "fc layer 1 self.abs_max_out: 8395.0\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.965159/  2.040826, val:  73.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2666%\n",
      "layer   2  Sparsity: 72.6060%\n",
      "layer   3  Sparsity: 80.1228%\n",
      "total_backward_count 636350 real_backward_count 83427  13.110%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.963652/  2.047078, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2715%\n",
      "layer   2  Sparsity: 73.0514%\n",
      "layer   3  Sparsity: 80.4418%\n",
      "total_backward_count 646140 real_backward_count 84322  13.050%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.965171/  2.059854, val:  78.75%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2874%\n",
      "layer   2  Sparsity: 72.5846%\n",
      "layer   3  Sparsity: 80.7466%\n",
      "total_backward_count 655930 real_backward_count 85263  12.999%\n",
      "fc layer 1 self.abs_max_out: 8479.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.972911/  2.037846, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2879%\n",
      "layer   2  Sparsity: 72.3742%\n",
      "layer   3  Sparsity: 81.0771%\n",
      "total_backward_count 665720 real_backward_count 86169  12.944%\n",
      "fc layer 1 self.abs_max_out: 8612.0\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.985346/  2.063813, val:  75.00%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2705%\n",
      "layer   2  Sparsity: 72.3064%\n",
      "layer   3  Sparsity: 81.4166%\n",
      "total_backward_count 675510 real_backward_count 87110  12.895%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.970833/  2.042249, val:  75.83%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3200%\n",
      "layer   2  Sparsity: 72.0525%\n",
      "layer   3  Sparsity: 81.4557%\n",
      "total_backward_count 685300 real_backward_count 87997  12.841%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.971221/  2.049582, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2986%\n",
      "layer   2  Sparsity: 72.0420%\n",
      "layer   3  Sparsity: 81.1299%\n",
      "total_backward_count 695090 real_backward_count 88914  12.792%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.968482/  2.052251, val:  76.67%, val_best:  79.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2816%\n",
      "layer   2  Sparsity: 72.1027%\n",
      "layer   3  Sparsity: 81.7478%\n",
      "total_backward_count 704880 real_backward_count 89817  12.742%\n",
      "lif layer 2 self.abs_max_v: 5730.0\n",
      "fc layer 1 self.abs_max_out: 8893.0\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.975109/  2.038418, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2902%\n",
      "layer   2  Sparsity: 72.3983%\n",
      "layer   3  Sparsity: 82.2576%\n",
      "total_backward_count 714670 real_backward_count 90734  12.696%\n",
      "lif layer 2 self.abs_max_v: 5731.5\n",
      "lif layer 2 self.abs_max_v: 5907.0\n",
      "lif layer 2 self.abs_max_v: 5988.0\n",
      "lif layer 2 self.abs_max_v: 6221.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.977364/  2.053309, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3021%\n",
      "layer   2  Sparsity: 72.0815%\n",
      "layer   3  Sparsity: 82.0689%\n",
      "total_backward_count 724460 real_backward_count 91597  12.643%\n",
      "lif layer 1 self.abs_max_v: 13895.0\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.969612/  2.062259, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.2568%\n",
      "layer   2  Sparsity: 71.9742%\n",
      "layer   3  Sparsity: 81.8919%\n",
      "total_backward_count 734250 real_backward_count 92492  12.597%\n",
      "lif layer 2 self.abs_max_v: 6382.5\n",
      "lif layer 2 self.abs_max_v: 6546.5\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.976255/  2.039774, val:  82.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.2839%\n",
      "layer   2  Sparsity: 72.1267%\n",
      "layer   3  Sparsity: 81.8865%\n",
      "total_backward_count 744040 real_backward_count 93399  12.553%\n",
      "lif layer 2 self.abs_max_v: 6612.5\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.971583/  2.058173, val:  81.25%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2813%\n",
      "layer   2  Sparsity: 72.1549%\n",
      "layer   3  Sparsity: 81.8641%\n",
      "total_backward_count 753830 real_backward_count 94272  12.506%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.984542/  2.064972, val:  64.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3154%\n",
      "layer   2  Sparsity: 72.4256%\n",
      "layer   3  Sparsity: 82.0966%\n",
      "total_backward_count 763620 real_backward_count 95096  12.453%\n",
      "fc layer 1 self.abs_max_out: 8979.0\n",
      "lif layer 1 self.abs_max_v: 14581.5\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.973798/  2.051915, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2640%\n",
      "layer   2  Sparsity: 71.8043%\n",
      "layer   3  Sparsity: 81.8110%\n",
      "total_backward_count 773410 real_backward_count 95945  12.405%\n",
      "fc layer 1 self.abs_max_out: 9005.0\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.975590/  2.052728, val:  80.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2950%\n",
      "layer   2  Sparsity: 72.3397%\n",
      "layer   3  Sparsity: 82.0229%\n",
      "total_backward_count 783200 real_backward_count 96736  12.351%\n",
      "fc layer 1 self.abs_max_out: 9100.0\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.982253/  2.063099, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2904%\n",
      "layer   2  Sparsity: 72.2757%\n",
      "layer   3  Sparsity: 82.4577%\n",
      "total_backward_count 792990 real_backward_count 97571  12.304%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.983365/  2.060938, val:  74.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2999%\n",
      "layer   2  Sparsity: 72.2315%\n",
      "layer   3  Sparsity: 82.1146%\n",
      "total_backward_count 802780 real_backward_count 98396  12.257%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.986275/  2.057568, val:  75.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2857%\n",
      "layer   2  Sparsity: 72.1370%\n",
      "layer   3  Sparsity: 82.1945%\n",
      "total_backward_count 812570 real_backward_count 99197  12.208%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.983053/  2.053166, val:  77.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.3047%\n",
      "layer   2  Sparsity: 71.5581%\n",
      "layer   3  Sparsity: 82.1709%\n",
      "total_backward_count 822360 real_backward_count 100018  12.162%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.993712/  2.057910, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2822%\n",
      "layer   2  Sparsity: 71.9174%\n",
      "layer   3  Sparsity: 82.3837%\n",
      "total_backward_count 832150 real_backward_count 100825  12.116%\n",
      "lif layer 2 self.abs_max_v: 6683.5\n",
      "lif layer 2 self.abs_max_v: 6735.0\n",
      "lif layer 2 self.abs_max_v: 6888.5\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.988947/  2.068280, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3115%\n",
      "layer   2  Sparsity: 72.4430%\n",
      "layer   3  Sparsity: 82.5310%\n",
      "total_backward_count 841940 real_backward_count 101665  12.075%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.983876/  2.056904, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3177%\n",
      "layer   2  Sparsity: 71.8685%\n",
      "layer   3  Sparsity: 82.5778%\n",
      "total_backward_count 851730 real_backward_count 102440  12.027%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.991178/  2.062073, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2882%\n",
      "layer   2  Sparsity: 72.2095%\n",
      "layer   3  Sparsity: 82.6940%\n",
      "total_backward_count 861520 real_backward_count 103225  11.982%\n",
      "lif layer 1 self.abs_max_v: 14688.0\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.994881/  2.054374, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2884%\n",
      "layer   2  Sparsity: 71.9346%\n",
      "layer   3  Sparsity: 82.5217%\n",
      "total_backward_count 871310 real_backward_count 104005  11.937%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.988664/  2.058071, val:  80.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2899%\n",
      "layer   2  Sparsity: 72.1319%\n",
      "layer   3  Sparsity: 82.5836%\n",
      "total_backward_count 881100 real_backward_count 104818  11.896%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.983426/  2.062337, val:  72.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3238%\n",
      "layer   2  Sparsity: 72.0970%\n",
      "layer   3  Sparsity: 82.8091%\n",
      "total_backward_count 890890 real_backward_count 105619  11.855%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.976362/  2.036925, val:  80.42%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2869%\n",
      "layer   2  Sparsity: 71.7961%\n",
      "layer   3  Sparsity: 82.6094%\n",
      "total_backward_count 900680 real_backward_count 106385  11.812%\n",
      "lif layer 2 self.abs_max_v: 6956.0\n",
      "lif layer 2 self.abs_max_v: 7120.5\n",
      "lif layer 2 self.abs_max_v: 7426.5\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.973916/  2.044381, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3016%\n",
      "layer   2  Sparsity: 71.8949%\n",
      "layer   3  Sparsity: 82.5657%\n",
      "total_backward_count 910470 real_backward_count 107170  11.771%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.979340/  2.060956, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2856%\n",
      "layer   2  Sparsity: 71.8562%\n",
      "layer   3  Sparsity: 82.6539%\n",
      "total_backward_count 920260 real_backward_count 107952  11.731%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.988412/  2.067389, val:  79.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2751%\n",
      "layer   2  Sparsity: 71.7948%\n",
      "layer   3  Sparsity: 82.7027%\n",
      "total_backward_count 930050 real_backward_count 108678  11.685%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.987310/  2.054154, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2839%\n",
      "layer   2  Sparsity: 71.4756%\n",
      "layer   3  Sparsity: 82.6427%\n",
      "total_backward_count 939840 real_backward_count 109438  11.644%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.981969/  2.044221, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3031%\n",
      "layer   2  Sparsity: 71.3925%\n",
      "layer   3  Sparsity: 82.3830%\n",
      "total_backward_count 949630 real_backward_count 110208  11.605%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.967461/  2.047265, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2678%\n",
      "layer   2  Sparsity: 71.3338%\n",
      "layer   3  Sparsity: 82.9128%\n",
      "total_backward_count 959420 real_backward_count 110934  11.563%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.977710/  2.053663, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2650%\n",
      "layer   2  Sparsity: 71.1370%\n",
      "layer   3  Sparsity: 82.7032%\n",
      "total_backward_count 969210 real_backward_count 111657  11.520%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.976188/  2.044651, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2713%\n",
      "layer   2  Sparsity: 71.3413%\n",
      "layer   3  Sparsity: 82.5732%\n",
      "total_backward_count 979000 real_backward_count 112416  11.483%\n",
      "fc layer 1 self.abs_max_out: 9207.0\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.971069/  2.024796, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2983%\n",
      "layer   2  Sparsity: 71.3630%\n",
      "layer   3  Sparsity: 82.2935%\n",
      "total_backward_count 988790 real_backward_count 113153  11.444%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.966193/  2.041520, val:  83.75%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3153%\n",
      "layer   2  Sparsity: 71.7354%\n",
      "layer   3  Sparsity: 82.7882%\n",
      "total_backward_count 998580 real_backward_count 113912  11.407%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.981278/  2.050350, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3044%\n",
      "layer   2  Sparsity: 71.4531%\n",
      "layer   3  Sparsity: 83.2002%\n",
      "total_backward_count 1008370 real_backward_count 114639  11.369%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.980760/  2.049731, val:  76.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2515%\n",
      "layer   2  Sparsity: 71.3788%\n",
      "layer   3  Sparsity: 83.1326%\n",
      "total_backward_count 1018160 real_backward_count 115355  11.330%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.972323/  2.029104, val:  82.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2790%\n",
      "layer   2  Sparsity: 71.4802%\n",
      "layer   3  Sparsity: 83.2750%\n",
      "total_backward_count 1027950 real_backward_count 116071  11.292%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.980283/  2.042433, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2665%\n",
      "layer   2  Sparsity: 71.5853%\n",
      "layer   3  Sparsity: 83.3573%\n",
      "total_backward_count 1037740 real_backward_count 116794  11.255%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.974477/  2.039100, val:  75.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3206%\n",
      "layer   2  Sparsity: 71.3777%\n",
      "layer   3  Sparsity: 82.9128%\n",
      "total_backward_count 1047530 real_backward_count 117474  11.214%\n",
      "fc layer 1 self.abs_max_out: 9393.0\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.975783/  2.053282, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2678%\n",
      "layer   2  Sparsity: 71.7218%\n",
      "layer   3  Sparsity: 83.3761%\n",
      "total_backward_count 1057320 real_backward_count 118183  11.178%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.977393/  2.042443, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2967%\n",
      "layer   2  Sparsity: 71.8275%\n",
      "layer   3  Sparsity: 83.3872%\n",
      "total_backward_count 1067110 real_backward_count 118877  11.140%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.978409/  2.047846, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2930%\n",
      "layer   2  Sparsity: 71.2076%\n",
      "layer   3  Sparsity: 83.2888%\n",
      "total_backward_count 1076900 real_backward_count 119608  11.107%\n",
      "fc layer 1 self.abs_max_out: 9592.0\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.979113/  2.050786, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2810%\n",
      "layer   2  Sparsity: 71.0300%\n",
      "layer   3  Sparsity: 83.4267%\n",
      "total_backward_count 1086690 real_backward_count 120302  11.070%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.984622/  2.049495, val:  82.50%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2790%\n",
      "layer   2  Sparsity: 70.9642%\n",
      "layer   3  Sparsity: 83.6412%\n",
      "total_backward_count 1096480 real_backward_count 120954  11.031%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.987503/  2.049102, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2745%\n",
      "layer   2  Sparsity: 71.3176%\n",
      "layer   3  Sparsity: 83.6747%\n",
      "total_backward_count 1106270 real_backward_count 121658  10.997%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.981628/  2.044456, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2872%\n",
      "layer   2  Sparsity: 71.2682%\n",
      "layer   3  Sparsity: 83.8000%\n",
      "total_backward_count 1116060 real_backward_count 122303  10.958%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.977737/  2.055690, val:  66.67%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2877%\n",
      "layer   2  Sparsity: 71.1768%\n",
      "layer   3  Sparsity: 83.5252%\n",
      "total_backward_count 1125850 real_backward_count 123005  10.926%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.976704/  2.035340, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2968%\n",
      "layer   2  Sparsity: 71.0461%\n",
      "layer   3  Sparsity: 83.5200%\n",
      "total_backward_count 1135640 real_backward_count 123702  10.893%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.966892/  2.034974, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2863%\n",
      "layer   2  Sparsity: 71.0394%\n",
      "layer   3  Sparsity: 83.6743%\n",
      "total_backward_count 1145430 real_backward_count 124378  10.859%\n",
      "fc layer 1 self.abs_max_out: 9706.0\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.975125/  2.052346, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3001%\n",
      "layer   2  Sparsity: 71.0995%\n",
      "layer   3  Sparsity: 84.0606%\n",
      "total_backward_count 1155220 real_backward_count 125018  10.822%\n",
      "fc layer 1 self.abs_max_out: 9732.0\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.977134/  2.051844, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2898%\n",
      "layer   2  Sparsity: 71.2736%\n",
      "layer   3  Sparsity: 83.9739%\n",
      "total_backward_count 1165010 real_backward_count 125648  10.785%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.979714/  2.058232, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2737%\n",
      "layer   2  Sparsity: 71.5890%\n",
      "layer   3  Sparsity: 84.1733%\n",
      "total_backward_count 1174800 real_backward_count 126299  10.751%\n",
      "lif layer 2 self.abs_max_v: 7458.5\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.983902/  2.047572, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3124%\n",
      "layer   2  Sparsity: 71.3448%\n",
      "layer   3  Sparsity: 84.0058%\n",
      "total_backward_count 1184590 real_backward_count 126947  10.717%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.971034/  2.040446, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2908%\n",
      "layer   2  Sparsity: 71.2751%\n",
      "layer   3  Sparsity: 83.8832%\n",
      "total_backward_count 1194380 real_backward_count 127640  10.687%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.976206/  2.049533, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3055%\n",
      "layer   2  Sparsity: 71.1015%\n",
      "layer   3  Sparsity: 84.1139%\n",
      "total_backward_count 1204170 real_backward_count 128267  10.652%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.986465/  2.052866, val:  81.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3109%\n",
      "layer   2  Sparsity: 70.9242%\n",
      "layer   3  Sparsity: 84.0711%\n",
      "total_backward_count 1213960 real_backward_count 128898  10.618%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.981357/  2.028943, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3172%\n",
      "layer   2  Sparsity: 71.0017%\n",
      "layer   3  Sparsity: 83.7675%\n",
      "total_backward_count 1223750 real_backward_count 129520  10.584%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.972315/  2.052514, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3052%\n",
      "layer   2  Sparsity: 71.1943%\n",
      "layer   3  Sparsity: 83.8917%\n",
      "total_backward_count 1233540 real_backward_count 130172  10.553%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.970288/  2.041476, val:  82.50%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3077%\n",
      "layer   2  Sparsity: 71.3681%\n",
      "layer   3  Sparsity: 83.9226%\n",
      "total_backward_count 1243330 real_backward_count 130752  10.516%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.967809/  2.028348, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2587%\n",
      "layer   2  Sparsity: 71.0561%\n",
      "layer   3  Sparsity: 83.8370%\n",
      "total_backward_count 1253120 real_backward_count 131376  10.484%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.967290/  2.048370, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2946%\n",
      "layer   2  Sparsity: 70.7564%\n",
      "layer   3  Sparsity: 84.1230%\n",
      "total_backward_count 1262910 real_backward_count 131984  10.451%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.963306/  2.035664, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2968%\n",
      "layer   2  Sparsity: 70.8595%\n",
      "layer   3  Sparsity: 84.0653%\n",
      "total_backward_count 1272700 real_backward_count 132589  10.418%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.976427/  2.042992, val:  79.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3114%\n",
      "layer   2  Sparsity: 70.9333%\n",
      "layer   3  Sparsity: 84.0041%\n",
      "total_backward_count 1282490 real_backward_count 133244  10.389%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.972157/  2.045780, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2773%\n",
      "layer   2  Sparsity: 70.9918%\n",
      "layer   3  Sparsity: 84.1898%\n",
      "total_backward_count 1292280 real_backward_count 133882  10.360%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.973345/  2.045755, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2963%\n",
      "layer   2  Sparsity: 70.9676%\n",
      "layer   3  Sparsity: 84.2000%\n",
      "total_backward_count 1302070 real_backward_count 134483  10.328%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.976580/  2.043291, val:  80.42%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3014%\n",
      "layer   2  Sparsity: 70.8039%\n",
      "layer   3  Sparsity: 84.0906%\n",
      "total_backward_count 1311860 real_backward_count 135101  10.298%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.969062/  2.027348, val:  81.67%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2985%\n",
      "layer   2  Sparsity: 70.6649%\n",
      "layer   3  Sparsity: 84.1705%\n",
      "total_backward_count 1321650 real_backward_count 135708  10.268%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.974546/  2.025742, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2804%\n",
      "layer   2  Sparsity: 70.6818%\n",
      "layer   3  Sparsity: 84.2187%\n",
      "total_backward_count 1331440 real_backward_count 136361  10.242%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.961701/  2.047477, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.2494%\n",
      "layer   2  Sparsity: 70.9149%\n",
      "layer   3  Sparsity: 84.1797%\n",
      "total_backward_count 1341230 real_backward_count 136944  10.210%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.963598/  2.034187, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2619%\n",
      "layer   2  Sparsity: 71.2086%\n",
      "layer   3  Sparsity: 84.2515%\n",
      "total_backward_count 1351020 real_backward_count 137518  10.179%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.974497/  2.044281, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2931%\n",
      "layer   2  Sparsity: 71.0895%\n",
      "layer   3  Sparsity: 84.4801%\n",
      "total_backward_count 1360810 real_backward_count 138119  10.150%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.974639/  2.033991, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2772%\n",
      "layer   2  Sparsity: 71.0438%\n",
      "layer   3  Sparsity: 84.6108%\n",
      "total_backward_count 1370600 real_backward_count 138728  10.122%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.975294/  2.039161, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2823%\n",
      "layer   2  Sparsity: 71.1554%\n",
      "layer   3  Sparsity: 84.5967%\n",
      "total_backward_count 1380390 real_backward_count 139277  10.090%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.973702/  2.048662, val:  81.67%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2945%\n",
      "layer   2  Sparsity: 70.8230%\n",
      "layer   3  Sparsity: 84.6978%\n",
      "total_backward_count 1390180 real_backward_count 139807  10.057%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.972495/  2.039055, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3074%\n",
      "layer   2  Sparsity: 70.4737%\n",
      "layer   3  Sparsity: 84.4161%\n",
      "total_backward_count 1399970 real_backward_count 140396  10.029%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.967039/  2.022288, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2952%\n",
      "layer   2  Sparsity: 70.5965%\n",
      "layer   3  Sparsity: 84.4367%\n",
      "total_backward_count 1409760 real_backward_count 140908   9.995%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.966658/  2.030012, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2515%\n",
      "layer   2  Sparsity: 70.6997%\n",
      "layer   3  Sparsity: 84.8030%\n",
      "total_backward_count 1419550 real_backward_count 141497   9.968%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.963351/  2.033099, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2969%\n",
      "layer   2  Sparsity: 70.9168%\n",
      "layer   3  Sparsity: 84.8650%\n",
      "total_backward_count 1429340 real_backward_count 142031   9.937%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.971786/  2.042904, val:  79.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2695%\n",
      "layer   2  Sparsity: 70.6281%\n",
      "layer   3  Sparsity: 84.6725%\n",
      "total_backward_count 1439130 real_backward_count 142613   9.910%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.956248/  2.016915, val:  82.50%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2607%\n",
      "layer   2  Sparsity: 71.0204%\n",
      "layer   3  Sparsity: 84.5160%\n",
      "total_backward_count 1448920 real_backward_count 143133   9.879%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.955828/  2.011662, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3161%\n",
      "layer   2  Sparsity: 70.7871%\n",
      "layer   3  Sparsity: 84.3462%\n",
      "total_backward_count 1458710 real_backward_count 143688   9.850%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.952799/  2.030812, val:  71.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2977%\n",
      "layer   2  Sparsity: 70.7276%\n",
      "layer   3  Sparsity: 84.3005%\n",
      "total_backward_count 1468500 real_backward_count 144231   9.822%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.952810/  2.026455, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2930%\n",
      "layer   2  Sparsity: 70.5694%\n",
      "layer   3  Sparsity: 84.3377%\n",
      "total_backward_count 1478290 real_backward_count 144795   9.795%\n",
      "lif layer 1 self.abs_max_v: 14716.5\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.956787/  2.034966, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2844%\n",
      "layer   2  Sparsity: 70.7604%\n",
      "layer   3  Sparsity: 84.3733%\n",
      "total_backward_count 1488080 real_backward_count 145364   9.769%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.950976/  2.035554, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2802%\n",
      "layer   2  Sparsity: 70.8804%\n",
      "layer   3  Sparsity: 84.5302%\n",
      "total_backward_count 1497870 real_backward_count 145872   9.739%\n",
      "fc layer 1 self.abs_max_out: 9737.0\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.961062/  2.028008, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2715%\n",
      "layer   2  Sparsity: 70.5712%\n",
      "layer   3  Sparsity: 84.7721%\n",
      "total_backward_count 1507660 real_backward_count 146412   9.711%\n",
      "fc layer 1 self.abs_max_out: 9758.0\n",
      "lif layer 1 self.abs_max_v: 15030.0\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.955822/  2.017365, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2831%\n",
      "layer   2  Sparsity: 70.7811%\n",
      "layer   3  Sparsity: 84.5832%\n",
      "total_backward_count 1517450 real_backward_count 146927   9.682%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.957057/  2.037705, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3085%\n",
      "layer   2  Sparsity: 70.9575%\n",
      "layer   3  Sparsity: 84.8140%\n",
      "total_backward_count 1527240 real_backward_count 147466   9.656%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.959871/  2.021070, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2403%\n",
      "layer   2  Sparsity: 71.0516%\n",
      "layer   3  Sparsity: 84.7763%\n",
      "total_backward_count 1537030 real_backward_count 147989   9.628%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.967898/  2.042725, val:  82.50%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3315%\n",
      "layer   2  Sparsity: 70.9531%\n",
      "layer   3  Sparsity: 84.4706%\n",
      "total_backward_count 1546820 real_backward_count 148537   9.603%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.965606/  2.038493, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2899%\n",
      "layer   2  Sparsity: 70.6274%\n",
      "layer   3  Sparsity: 84.6333%\n",
      "total_backward_count 1556610 real_backward_count 149062   9.576%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.966056/  2.046124, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3029%\n",
      "layer   2  Sparsity: 70.4331%\n",
      "layer   3  Sparsity: 84.4634%\n",
      "total_backward_count 1566400 real_backward_count 149592   9.550%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.966373/  2.035892, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2961%\n",
      "layer   2  Sparsity: 70.6856%\n",
      "layer   3  Sparsity: 84.5413%\n",
      "total_backward_count 1576190 real_backward_count 150089   9.522%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.953923/  2.028090, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2837%\n",
      "layer   2  Sparsity: 70.7695%\n",
      "layer   3  Sparsity: 84.6171%\n",
      "total_backward_count 1585980 real_backward_count 150629   9.498%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.955150/  2.016086, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2675%\n",
      "layer   2  Sparsity: 70.6315%\n",
      "layer   3  Sparsity: 84.7230%\n",
      "total_backward_count 1595770 real_backward_count 151114   9.470%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.949227/  2.022181, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.2780%\n",
      "layer   2  Sparsity: 70.8728%\n",
      "layer   3  Sparsity: 84.5421%\n",
      "total_backward_count 1605560 real_backward_count 151618   9.443%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.944610/  2.011761, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3270%\n",
      "layer   2  Sparsity: 70.6663%\n",
      "layer   3  Sparsity: 84.8833%\n",
      "total_backward_count 1615350 real_backward_count 152116   9.417%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.951556/  2.032890, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2931%\n",
      "layer   2  Sparsity: 70.7210%\n",
      "layer   3  Sparsity: 85.2224%\n",
      "total_backward_count 1625140 real_backward_count 152614   9.391%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.954836/  2.026721, val:  76.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2749%\n",
      "layer   2  Sparsity: 70.5829%\n",
      "layer   3  Sparsity: 84.7619%\n",
      "total_backward_count 1634930 real_backward_count 153114   9.365%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.957556/  2.029033, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2777%\n",
      "layer   2  Sparsity: 70.4988%\n",
      "layer   3  Sparsity: 84.5858%\n",
      "total_backward_count 1644720 real_backward_count 153632   9.341%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.953664/  2.017792, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2813%\n",
      "layer   2  Sparsity: 70.6072%\n",
      "layer   3  Sparsity: 84.5492%\n",
      "total_backward_count 1654510 real_backward_count 154101   9.314%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.952268/  2.025417, val:  84.58%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3180%\n",
      "layer   2  Sparsity: 70.2624%\n",
      "layer   3  Sparsity: 84.5386%\n",
      "total_backward_count 1664300 real_backward_count 154627   9.291%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.956969/  2.030485, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2735%\n",
      "layer   2  Sparsity: 70.4039%\n",
      "layer   3  Sparsity: 84.8233%\n",
      "total_backward_count 1674090 real_backward_count 155137   9.267%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.954730/  2.039684, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2763%\n",
      "layer   2  Sparsity: 70.3929%\n",
      "layer   3  Sparsity: 85.0314%\n",
      "total_backward_count 1683880 real_backward_count 155624   9.242%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.966779/  2.030697, val:  83.33%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2782%\n",
      "layer   2  Sparsity: 70.4679%\n",
      "layer   3  Sparsity: 84.9310%\n",
      "total_backward_count 1693670 real_backward_count 156078   9.215%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.964329/  2.024481, val:  82.50%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2876%\n",
      "layer   2  Sparsity: 70.3356%\n",
      "layer   3  Sparsity: 84.5986%\n",
      "total_backward_count 1703460 real_backward_count 156586   9.192%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.951451/  2.028213, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2717%\n",
      "layer   2  Sparsity: 70.4881%\n",
      "layer   3  Sparsity: 84.8167%\n",
      "total_backward_count 1713250 real_backward_count 157063   9.168%\n",
      "fc layer 1 self.abs_max_out: 9772.0\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.947885/  2.024432, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2974%\n",
      "layer   2  Sparsity: 70.6498%\n",
      "layer   3  Sparsity: 84.9661%\n",
      "total_backward_count 1723040 real_backward_count 157499   9.141%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.953289/  2.028850, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2876%\n",
      "layer   2  Sparsity: 70.8086%\n",
      "layer   3  Sparsity: 85.0538%\n",
      "total_backward_count 1732830 real_backward_count 157976   9.117%\n",
      "fc layer 1 self.abs_max_out: 9775.0\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.954400/  2.021735, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2826%\n",
      "layer   2  Sparsity: 70.8365%\n",
      "layer   3  Sparsity: 85.0098%\n",
      "total_backward_count 1742620 real_backward_count 158453   9.093%\n",
      "fc layer 1 self.abs_max_out: 9835.0\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.950090/  2.020405, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2819%\n",
      "layer   2  Sparsity: 70.8013%\n",
      "layer   3  Sparsity: 85.2503%\n",
      "total_backward_count 1752410 real_backward_count 158910   9.068%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.949994/  2.034480, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2684%\n",
      "layer   2  Sparsity: 70.7653%\n",
      "layer   3  Sparsity: 85.1539%\n",
      "total_backward_count 1762200 real_backward_count 159346   9.042%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.949379/  2.008471, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2849%\n",
      "layer   2  Sparsity: 70.5357%\n",
      "layer   3  Sparsity: 84.9174%\n",
      "total_backward_count 1771990 real_backward_count 159819   9.019%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.948125/  2.028004, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2628%\n",
      "layer   2  Sparsity: 70.5776%\n",
      "layer   3  Sparsity: 85.0539%\n",
      "total_backward_count 1781780 real_backward_count 160302   8.997%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.947631/  2.023254, val:  78.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2928%\n",
      "layer   2  Sparsity: 70.6272%\n",
      "layer   3  Sparsity: 85.1856%\n",
      "total_backward_count 1791570 real_backward_count 160773   8.974%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.939456/  2.017803, val:  84.58%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2601%\n",
      "layer   2  Sparsity: 70.4894%\n",
      "layer   3  Sparsity: 85.2420%\n",
      "total_backward_count 1801360 real_backward_count 161242   8.951%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.947718/  2.021262, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.2901%\n",
      "layer   2  Sparsity: 70.3201%\n",
      "layer   3  Sparsity: 85.2436%\n",
      "total_backward_count 1811150 real_backward_count 161717   8.929%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.939746/  2.011594, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2951%\n",
      "layer   2  Sparsity: 70.5363%\n",
      "layer   3  Sparsity: 85.2409%\n",
      "total_backward_count 1820940 real_backward_count 162162   8.905%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.949787/  2.023110, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2842%\n",
      "layer   2  Sparsity: 70.4868%\n",
      "layer   3  Sparsity: 85.3185%\n",
      "total_backward_count 1830730 real_backward_count 162634   8.884%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.948489/  2.019875, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2952%\n",
      "layer   2  Sparsity: 70.4221%\n",
      "layer   3  Sparsity: 85.4444%\n",
      "total_backward_count 1840520 real_backward_count 163077   8.860%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.945668/  2.013794, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2894%\n",
      "layer   2  Sparsity: 70.5401%\n",
      "layer   3  Sparsity: 85.7091%\n",
      "total_backward_count 1850310 real_backward_count 163514   8.837%\n",
      "fc layer 1 self.abs_max_out: 9843.0\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.948048/  2.005796, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2707%\n",
      "layer   2  Sparsity: 70.4254%\n",
      "layer   3  Sparsity: 85.2951%\n",
      "total_backward_count 1860100 real_backward_count 163977   8.815%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.950169/  2.021238, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3064%\n",
      "layer   2  Sparsity: 70.4295%\n",
      "layer   3  Sparsity: 85.4377%\n",
      "total_backward_count 1869890 real_backward_count 164364   8.790%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.945995/  2.023562, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2940%\n",
      "layer   2  Sparsity: 70.4815%\n",
      "layer   3  Sparsity: 85.4198%\n",
      "total_backward_count 1879680 real_backward_count 164800   8.767%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.945272/  2.015477, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.3294%\n",
      "layer   2  Sparsity: 70.3985%\n",
      "layer   3  Sparsity: 85.1426%\n",
      "total_backward_count 1889470 real_backward_count 165278   8.747%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.936302/  2.016601, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.2881%\n",
      "layer   2  Sparsity: 70.4591%\n",
      "layer   3  Sparsity: 85.3185%\n",
      "total_backward_count 1899260 real_backward_count 165726   8.726%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.951432/  2.032955, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2835%\n",
      "layer   2  Sparsity: 70.4076%\n",
      "layer   3  Sparsity: 85.5092%\n",
      "total_backward_count 1909050 real_backward_count 166193   8.706%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.949325/  2.024761, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3058%\n",
      "layer   2  Sparsity: 70.3253%\n",
      "layer   3  Sparsity: 85.5032%\n",
      "total_backward_count 1918840 real_backward_count 166628   8.684%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.950457/  2.025514, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3036%\n",
      "layer   2  Sparsity: 70.4655%\n",
      "layer   3  Sparsity: 85.4053%\n",
      "total_backward_count 1928630 real_backward_count 167037   8.661%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.959380/  2.046138, val:  80.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2690%\n",
      "layer   2  Sparsity: 70.5681%\n",
      "layer   3  Sparsity: 85.4351%\n",
      "total_backward_count 1938420 real_backward_count 167453   8.639%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.967964/  2.040616, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.2825%\n",
      "layer   2  Sparsity: 70.5273%\n",
      "layer   3  Sparsity: 85.4649%\n",
      "total_backward_count 1948210 real_backward_count 167876   8.617%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.964237/  2.032365, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.2661%\n",
      "layer   2  Sparsity: 70.2596%\n",
      "layer   3  Sparsity: 85.6510%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bceb24d3564797a1a59e59b65de499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.96424</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>2.03237</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-133</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h5wnrcpa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h5wnrcpa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_100034-h5wnrcpa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ajpevd3k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_142252-ajpevd3k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ajpevd3k' target=\"_blank\">prime-sweep-141</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ajpevd3k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ajpevd3k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_142259_718', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 571.0\n",
      "lif layer 1 self.abs_max_v: 571.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "lif layer 2 self.abs_max_v: 762.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "lif layer 1 self.abs_max_v: 828.5\n",
      "fc layer 2 self.abs_max_out: 1686.0\n",
      "lif layer 2 self.abs_max_v: 2040.5\n",
      "fc layer 1 self.abs_max_out: 811.0\n",
      "lif layer 1 self.abs_max_v: 1050.5\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "lif layer 2 self.abs_max_v: 2249.5\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "fc layer 1 self.abs_max_out: 885.0\n",
      "lif layer 1 self.abs_max_v: 1157.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "fc layer 1 self.abs_max_out: 944.0\n",
      "lif layer 1 self.abs_max_v: 1219.5\n",
      "fc layer 1 self.abs_max_out: 1107.0\n",
      "lif layer 1 self.abs_max_v: 1348.0\n",
      "lif layer 2 self.abs_max_v: 2321.5\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 1 self.abs_max_out: 1374.0\n",
      "lif layer 1 self.abs_max_v: 1374.0\n",
      "lif layer 2 self.abs_max_v: 2706.0\n",
      "fc layer 3 self.abs_max_out: 996.0\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "lif layer 1 self.abs_max_v: 1633.5\n",
      "lif layer 1 self.abs_max_v: 1707.0\n",
      "fc layer 1 self.abs_max_out: 1771.0\n",
      "lif layer 1 self.abs_max_v: 2057.0\n",
      "fc layer 1 self.abs_max_out: 1783.0\n",
      "lif layer 1 self.abs_max_v: 2186.5\n",
      "fc layer 2 self.abs_max_out: 2069.0\n",
      "lif layer 2 self.abs_max_v: 2739.5\n",
      "fc layer 1 self.abs_max_out: 2042.0\n",
      "lif layer 1 self.abs_max_v: 2198.5\n",
      "fc layer 1 self.abs_max_out: 2412.0\n",
      "lif layer 1 self.abs_max_v: 2631.5\n",
      "fc layer 2 self.abs_max_out: 2376.0\n",
      "lif layer 2 self.abs_max_v: 2821.0\n",
      "lif layer 2 self.abs_max_v: 3491.0\n",
      "lif layer 2 self.abs_max_v: 3771.5\n",
      "lif layer 1 self.abs_max_v: 2755.5\n",
      "fc layer 3 self.abs_max_out: 1035.0\n",
      "lif layer 1 self.abs_max_v: 2868.5\n",
      "fc layer 3 self.abs_max_out: 1334.0\n",
      "fc layer 1 self.abs_max_out: 2531.0\n",
      "lif layer 2 self.abs_max_v: 3984.0\n",
      "fc layer 1 self.abs_max_out: 2628.0\n",
      "fc layer 2 self.abs_max_out: 2477.0\n",
      "lif layer 1 self.abs_max_v: 2966.5\n",
      "fc layer 1 self.abs_max_out: 2659.0\n",
      "lif layer 1 self.abs_max_v: 3116.0\n",
      "lif layer 1 self.abs_max_v: 3650.5\n",
      "fc layer 1 self.abs_max_out: 3237.0\n",
      "lif layer 1 self.abs_max_v: 4074.0\n",
      "fc layer 1 self.abs_max_out: 3327.0\n",
      "lif layer 2 self.abs_max_v: 4025.5\n",
      "lif layer 2 self.abs_max_v: 4087.0\n",
      "fc layer 2 self.abs_max_out: 2657.0\n",
      "lif layer 1 self.abs_max_v: 4856.5\n",
      "lif layer 2 self.abs_max_v: 4153.5\n",
      "fc layer 1 self.abs_max_out: 3585.0\n",
      "fc layer 2 self.abs_max_out: 2689.0\n",
      "lif layer 2 self.abs_max_v: 4436.0\n",
      "fc layer 3 self.abs_max_out: 1336.0\n",
      "fc layer 3 self.abs_max_out: 1450.0\n",
      "fc layer 2 self.abs_max_out: 2931.0\n",
      "lif layer 2 self.abs_max_v: 4469.5\n",
      "lif layer 2 self.abs_max_v: 4571.0\n",
      "fc layer 2 self.abs_max_out: 3137.0\n",
      "lif layer 2 self.abs_max_v: 4712.0\n",
      "lif layer 2 self.abs_max_v: 5087.0\n",
      "lif layer 2 self.abs_max_v: 5193.0\n",
      "lif layer 2 self.abs_max_v: 5286.5\n",
      "fc layer 1 self.abs_max_out: 3650.0\n",
      "lif layer 1 self.abs_max_v: 4947.5\n",
      "lif layer 1 self.abs_max_v: 5094.0\n",
      "lif layer 1 self.abs_max_v: 5321.5\n",
      "fc layer 2 self.abs_max_out: 3255.0\n",
      "lif layer 2 self.abs_max_v: 5754.0\n",
      "lif layer 2 self.abs_max_v: 5903.5\n",
      "fc layer 1 self.abs_max_out: 5127.0\n",
      "lif layer 1 self.abs_max_v: 5759.5\n",
      "lif layer 1 self.abs_max_v: 7143.5\n",
      "lif layer 1 self.abs_max_v: 7424.0\n",
      "lif layer 1 self.abs_max_v: 7427.0\n",
      "fc layer 3 self.abs_max_out: 1522.0\n",
      "fc layer 3 self.abs_max_out: 1532.0\n",
      "fc layer 3 self.abs_max_out: 1571.0\n",
      "fc layer 2 self.abs_max_out: 3259.0\n",
      "fc layer 2 self.abs_max_out: 3321.0\n",
      "lif layer 2 self.abs_max_v: 6086.0\n",
      "fc layer 2 self.abs_max_out: 3341.0\n",
      "fc layer 2 self.abs_max_out: 3349.0\n",
      "fc layer 2 self.abs_max_out: 3677.0\n",
      "lif layer 2 self.abs_max_v: 6143.0\n",
      "lif layer 2 self.abs_max_v: 6182.5\n",
      "fc layer 3 self.abs_max_out: 1685.0\n",
      "lif layer 1 self.abs_max_v: 7476.0\n",
      "lif layer 2 self.abs_max_v: 6210.5\n",
      "fc layer 1 self.abs_max_out: 5776.0\n",
      "fc layer 2 self.abs_max_out: 3689.0\n",
      "lif layer 2 self.abs_max_v: 6253.0\n",
      "fc layer 2 self.abs_max_out: 3735.0\n",
      "lif layer 2 self.abs_max_v: 6522.0\n",
      "fc layer 2 self.abs_max_out: 3948.0\n",
      "lif layer 1 self.abs_max_v: 8269.0\n",
      "lif layer 2 self.abs_max_v: 6783.0\n",
      "lif layer 2 self.abs_max_v: 7080.5\n",
      "lif layer 2 self.abs_max_v: 7120.5\n",
      "lif layer 2 self.abs_max_v: 7398.5\n",
      "fc layer 2 self.abs_max_out: 4044.0\n",
      "fc layer 2 self.abs_max_out: 4134.0\n",
      "fc layer 2 self.abs_max_out: 4179.0\n",
      "fc layer 2 self.abs_max_out: 4203.0\n",
      "lif layer 1 self.abs_max_v: 8770.0\n",
      "fc layer 3 self.abs_max_out: 1755.0\n",
      "fc layer 2 self.abs_max_out: 4444.0\n",
      "lif layer 1 self.abs_max_v: 8779.5\n",
      "fc layer 1 self.abs_max_out: 5983.0\n",
      "fc layer 1 self.abs_max_out: 6126.0\n",
      "lif layer 1 self.abs_max_v: 10029.5\n",
      "lif layer 1 self.abs_max_v: 10365.0\n",
      "lif layer 1 self.abs_max_v: 11073.5\n",
      "lif layer 1 self.abs_max_v: 11547.0\n",
      "fc layer 1 self.abs_max_out: 6521.0\n",
      "lif layer 1 self.abs_max_v: 12294.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.720939/  1.996914, val:  31.67%, val_best:  31.67%, tr:  97.85%, tr_best:  97.85%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2365%\n",
      "layer   2  Sparsity: 74.3748%\n",
      "layer   3  Sparsity: 66.8189%\n",
      "total_backward_count 9790 real_backward_count 1906  19.469%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 4461.0\n",
      "fc layer 2 self.abs_max_out: 4580.0\n",
      "lif layer 2 self.abs_max_v: 7584.0\n",
      "lif layer 2 self.abs_max_v: 7730.0\n",
      "fc layer 1 self.abs_max_out: 6582.0\n",
      "fc layer 1 self.abs_max_out: 6827.0\n",
      "fc layer 3 self.abs_max_out: 1795.0\n",
      "fc layer 2 self.abs_max_out: 4652.0\n",
      "fc layer 3 self.abs_max_out: 1829.0\n",
      "fc layer 3 self.abs_max_out: 1880.0\n",
      "fc layer 2 self.abs_max_out: 4778.0\n",
      "fc layer 2 self.abs_max_out: 4799.0\n",
      "lif layer 2 self.abs_max_v: 7835.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.638863/  1.896272, val:  47.08%, val_best:  47.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2548%\n",
      "layer   2  Sparsity: 75.0174%\n",
      "layer   3  Sparsity: 66.0665%\n",
      "total_backward_count 19580 real_backward_count 3410  17.416%\n",
      "fc layer 3 self.abs_max_out: 1892.0\n",
      "fc layer 3 self.abs_max_out: 2010.0\n",
      "fc layer 2 self.abs_max_out: 4966.0\n",
      "fc layer 1 self.abs_max_out: 6913.0\n",
      "fc layer 2 self.abs_max_out: 5071.0\n",
      "lif layer 2 self.abs_max_v: 8133.0\n",
      "lif layer 2 self.abs_max_v: 8428.0\n",
      "lif layer 2 self.abs_max_v: 8582.5\n",
      "fc layer 1 self.abs_max_out: 7266.0\n",
      "fc layer 1 self.abs_max_out: 7374.0\n",
      "lif layer 1 self.abs_max_v: 12569.5\n",
      "fc layer 1 self.abs_max_out: 7607.0\n",
      "lif layer 1 self.abs_max_v: 12675.0\n",
      "lif layer 1 self.abs_max_v: 12692.5\n",
      "fc layer 1 self.abs_max_out: 7884.0\n",
      "lif layer 1 self.abs_max_v: 14228.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.630271/  2.009833, val:  30.42%, val_best:  47.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2599%\n",
      "layer   2  Sparsity: 73.9699%\n",
      "layer   3  Sparsity: 65.8756%\n",
      "total_backward_count 29370 real_backward_count 4799  16.340%\n",
      "fc layer 3 self.abs_max_out: 2027.0\n",
      "fc layer 3 self.abs_max_out: 2108.0\n",
      "lif layer 1 self.abs_max_v: 14648.0\n",
      "fc layer 1 self.abs_max_out: 9204.0\n",
      "lif layer 1 self.abs_max_v: 16528.0\n",
      "fc layer 2 self.abs_max_out: 5155.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.629175/  1.956519, val:  43.33%, val_best:  47.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2412%\n",
      "layer   2  Sparsity: 74.1162%\n",
      "layer   3  Sparsity: 66.9063%\n",
      "total_backward_count 39160 real_backward_count 6153  15.712%\n",
      "fc layer 2 self.abs_max_out: 5387.0\n",
      "fc layer 2 self.abs_max_out: 5426.0\n",
      "fc layer 1 self.abs_max_out: 9601.0\n",
      "lif layer 1 self.abs_max_v: 17235.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.601415/  1.912883, val:  36.67%, val_best:  47.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2430%\n",
      "layer   2  Sparsity: 73.9302%\n",
      "layer   3  Sparsity: 66.8832%\n",
      "total_backward_count 48950 real_backward_count 7505  15.332%\n",
      "fc layer 3 self.abs_max_out: 2136.0\n",
      "fc layer 2 self.abs_max_out: 5464.0\n",
      "fc layer 2 self.abs_max_out: 5518.0\n",
      "fc layer 2 self.abs_max_out: 5576.0\n",
      "fc layer 2 self.abs_max_out: 5888.0\n",
      "fc layer 1 self.abs_max_out: 10090.0\n",
      "lif layer 1 self.abs_max_v: 18208.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.586276/  1.878033, val:  47.92%, val_best:  47.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2413%\n",
      "layer   2  Sparsity: 74.4967%\n",
      "layer   3  Sparsity: 66.4112%\n",
      "total_backward_count 58740 real_backward_count 8831  15.034%\n",
      "fc layer 3 self.abs_max_out: 2222.0\n",
      "fc layer 1 self.abs_max_out: 10129.0\n",
      "lif layer 1 self.abs_max_v: 18258.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.592700/  1.890546, val:  45.00%, val_best:  47.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2314%\n",
      "layer   2  Sparsity: 74.7096%\n",
      "layer   3  Sparsity: 66.4370%\n",
      "total_backward_count 68530 real_backward_count 10112  14.756%\n",
      "fc layer 2 self.abs_max_out: 6221.0\n",
      "lif layer 2 self.abs_max_v: 8880.0\n",
      "fc layer 1 self.abs_max_out: 10249.0\n",
      "lif layer 2 self.abs_max_v: 8983.5\n",
      "lif layer 2 self.abs_max_v: 9249.0\n",
      "lif layer 2 self.abs_max_v: 9454.5\n",
      "fc layer 3 self.abs_max_out: 2340.0\n",
      "fc layer 1 self.abs_max_out: 10845.0\n",
      "lif layer 1 self.abs_max_v: 19683.5\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.567845/  1.827232, val:  49.17%, val_best:  49.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2529%\n",
      "layer   2  Sparsity: 73.9466%\n",
      "layer   3  Sparsity: 66.9253%\n",
      "total_backward_count 78320 real_backward_count 11369  14.516%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.575005/  1.865855, val:  45.42%, val_best:  49.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2576%\n",
      "layer   2  Sparsity: 74.1620%\n",
      "layer   3  Sparsity: 67.5117%\n",
      "total_backward_count 88110 real_backward_count 12691  14.404%\n",
      "lif layer 2 self.abs_max_v: 9467.5\n",
      "fc layer 1 self.abs_max_out: 11042.0\n",
      "lif layer 1 self.abs_max_v: 20197.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.571274/  1.931560, val:  46.25%, val_best:  49.17%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2503%\n",
      "layer   2  Sparsity: 74.5173%\n",
      "layer   3  Sparsity: 68.5704%\n",
      "total_backward_count 97900 real_backward_count 13963  14.263%\n",
      "lif layer 2 self.abs_max_v: 9625.5\n",
      "lif layer 2 self.abs_max_v: 9850.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.590994/  1.895316, val:  39.17%, val_best:  49.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2543%\n",
      "layer   2  Sparsity: 73.9149%\n",
      "layer   3  Sparsity: 69.1588%\n",
      "total_backward_count 107690 real_backward_count 15223  14.136%\n",
      "lif layer 2 self.abs_max_v: 10045.5\n",
      "fc layer 1 self.abs_max_out: 11582.0\n",
      "lif layer 1 self.abs_max_v: 20824.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.604125/  1.881269, val:  47.92%, val_best:  49.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2248%\n",
      "layer   2  Sparsity: 74.2561%\n",
      "layer   3  Sparsity: 69.0491%\n",
      "total_backward_count 117480 real_backward_count 16493  14.039%\n",
      "fc layer 1 self.abs_max_out: 11992.0\n",
      "lif layer 1 self.abs_max_v: 21566.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.592713/  1.884267, val:  42.92%, val_best:  49.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2509%\n",
      "layer   2  Sparsity: 74.6923%\n",
      "layer   3  Sparsity: 69.6805%\n",
      "total_backward_count 127270 real_backward_count 17721  13.924%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.590756/  1.872198, val:  37.08%, val_best:  49.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2554%\n",
      "layer   2  Sparsity: 73.5169%\n",
      "layer   3  Sparsity: 69.3863%\n",
      "total_backward_count 137060 real_backward_count 19003  13.865%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.584621/  1.869204, val:  49.58%, val_best:  49.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2345%\n",
      "layer   2  Sparsity: 73.7198%\n",
      "layer   3  Sparsity: 69.7594%\n",
      "total_backward_count 146850 real_backward_count 20251  13.790%\n",
      "fc layer 2 self.abs_max_out: 6287.0\n",
      "fc layer 1 self.abs_max_out: 12455.0\n",
      "lif layer 1 self.abs_max_v: 22724.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.612774/  1.880540, val:  47.08%, val_best:  49.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2301%\n",
      "layer   2  Sparsity: 73.4744%\n",
      "layer   3  Sparsity: 70.6992%\n",
      "total_backward_count 156640 real_backward_count 21541  13.752%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.630504/  1.877694, val:  55.83%, val_best:  55.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2453%\n",
      "layer   2  Sparsity: 73.5587%\n",
      "layer   3  Sparsity: 72.1212%\n",
      "total_backward_count 166430 real_backward_count 22790  13.693%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.627576/  1.882198, val:  63.33%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2169%\n",
      "layer   2  Sparsity: 73.5752%\n",
      "layer   3  Sparsity: 72.9421%\n",
      "total_backward_count 176220 real_backward_count 24054  13.650%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.606315/  1.824661, val:  48.33%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2310%\n",
      "layer   2  Sparsity: 73.4812%\n",
      "layer   3  Sparsity: 72.4037%\n",
      "total_backward_count 186010 real_backward_count 25341  13.623%\n",
      "lif layer 2 self.abs_max_v: 10137.0\n",
      "lif layer 2 self.abs_max_v: 10172.5\n",
      "fc layer 1 self.abs_max_out: 13650.0\n",
      "lif layer 1 self.abs_max_v: 24660.5\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.604143/  1.888925, val:  43.33%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2290%\n",
      "layer   2  Sparsity: 73.6658%\n",
      "layer   3  Sparsity: 73.8039%\n",
      "total_backward_count 195800 real_backward_count 26620  13.596%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.626630/  1.904819, val:  44.58%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2349%\n",
      "layer   2  Sparsity: 73.4516%\n",
      "layer   3  Sparsity: 73.9852%\n",
      "total_backward_count 205590 real_backward_count 27909  13.575%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.640667/  1.899669, val:  46.25%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2491%\n",
      "layer   2  Sparsity: 72.8678%\n",
      "layer   3  Sparsity: 73.4171%\n",
      "total_backward_count 215380 real_backward_count 29260  13.585%\n",
      "lif layer 2 self.abs_max_v: 10183.5\n",
      "lif layer 2 self.abs_max_v: 10194.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.615585/  1.843887, val:  60.42%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2308%\n",
      "layer   2  Sparsity: 72.8532%\n",
      "layer   3  Sparsity: 71.9000%\n",
      "total_backward_count 225170 real_backward_count 30558  13.571%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.638276/  1.851226, val:  55.83%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2138%\n",
      "layer   2  Sparsity: 72.7834%\n",
      "layer   3  Sparsity: 73.4833%\n",
      "total_backward_count 234960 real_backward_count 31855  13.558%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.654262/  1.871814, val:  60.83%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2451%\n",
      "layer   2  Sparsity: 72.6596%\n",
      "layer   3  Sparsity: 74.5677%\n",
      "total_backward_count 244750 real_backward_count 33162  13.549%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.677291/  1.874637, val:  57.08%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2387%\n",
      "layer   2  Sparsity: 73.3396%\n",
      "layer   3  Sparsity: 75.4180%\n",
      "total_backward_count 254540 real_backward_count 34446  13.533%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.679428/  1.892389, val:  67.08%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2419%\n",
      "layer   2  Sparsity: 73.3982%\n",
      "layer   3  Sparsity: 75.7432%\n",
      "total_backward_count 264330 real_backward_count 35729  13.517%\n",
      "lif layer 2 self.abs_max_v: 10338.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.687519/  1.929964, val:  58.75%, val_best:  67.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2136%\n",
      "layer   2  Sparsity: 72.6194%\n",
      "layer   3  Sparsity: 75.9714%\n",
      "total_backward_count 274120 real_backward_count 37054  13.517%\n",
      "lif layer 2 self.abs_max_v: 10366.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.709584/  1.977489, val:  42.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2294%\n",
      "layer   2  Sparsity: 72.8664%\n",
      "layer   3  Sparsity: 77.1224%\n",
      "total_backward_count 283910 real_backward_count 38292  13.487%\n",
      "lif layer 2 self.abs_max_v: 10456.5\n",
      "fc layer 1 self.abs_max_out: 14158.0\n",
      "lif layer 1 self.abs_max_v: 25618.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.736700/  1.964875, val:  47.92%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2412%\n",
      "layer   2  Sparsity: 72.6063%\n",
      "layer   3  Sparsity: 78.2159%\n",
      "total_backward_count 293700 real_backward_count 39528  13.459%\n",
      "lif layer 2 self.abs_max_v: 10612.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.734094/  1.914065, val:  53.33%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2539%\n",
      "layer   2  Sparsity: 72.3258%\n",
      "layer   3  Sparsity: 78.1397%\n",
      "total_backward_count 303490 real_backward_count 40836  13.455%\n",
      "fc layer 1 self.abs_max_out: 14379.0\n",
      "lif layer 1 self.abs_max_v: 25899.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.713802/  1.909636, val:  53.33%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2203%\n",
      "layer   2  Sparsity: 72.5924%\n",
      "layer   3  Sparsity: 77.3343%\n",
      "total_backward_count 313280 real_backward_count 42105  13.440%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.715868/  1.957113, val:  58.33%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2332%\n",
      "layer   2  Sparsity: 72.8845%\n",
      "layer   3  Sparsity: 76.3156%\n",
      "total_backward_count 323070 real_backward_count 43341  13.415%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.704802/  1.932286, val:  54.17%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2610%\n",
      "layer   2  Sparsity: 72.9055%\n",
      "layer   3  Sparsity: 76.7021%\n",
      "total_backward_count 332860 real_backward_count 44567  13.389%\n",
      "fc layer 1 self.abs_max_out: 14522.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.729021/  1.934057, val:  59.58%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2417%\n",
      "layer   2  Sparsity: 73.1397%\n",
      "layer   3  Sparsity: 78.2813%\n",
      "total_backward_count 342650 real_backward_count 45847  13.380%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.687943/  1.855617, val:  60.42%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2290%\n",
      "layer   2  Sparsity: 72.7582%\n",
      "layer   3  Sparsity: 76.0399%\n",
      "total_backward_count 352440 real_backward_count 47097  13.363%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.695874/  1.889190, val:  60.83%, val_best:  67.08%, tr:  99.18%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2595%\n",
      "layer   2  Sparsity: 72.7031%\n",
      "layer   3  Sparsity: 77.5572%\n",
      "total_backward_count 362230 real_backward_count 48321  13.340%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.709461/  1.904577, val:  46.67%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2559%\n",
      "layer   2  Sparsity: 72.6533%\n",
      "layer   3  Sparsity: 77.8699%\n",
      "total_backward_count 372020 real_backward_count 49558  13.321%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.701624/  1.917553, val:  64.17%, val_best:  67.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2179%\n",
      "layer   2  Sparsity: 72.3655%\n",
      "layer   3  Sparsity: 77.5947%\n",
      "total_backward_count 381810 real_backward_count 50813  13.308%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.686522/  1.901451, val:  65.42%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2418%\n",
      "layer   2  Sparsity: 72.9783%\n",
      "layer   3  Sparsity: 77.1236%\n",
      "total_backward_count 391600 real_backward_count 51971  13.271%\n",
      "lif layer 2 self.abs_max_v: 10798.5\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.670816/  1.863943, val:  59.17%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2340%\n",
      "layer   2  Sparsity: 72.5139%\n",
      "layer   3  Sparsity: 75.7160%\n",
      "total_backward_count 401390 real_backward_count 53153  13.242%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.712514/  1.878356, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2414%\n",
      "layer   2  Sparsity: 72.6029%\n",
      "layer   3  Sparsity: 77.6127%\n",
      "total_backward_count 411180 real_backward_count 54307  13.208%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.707513/  1.869429, val:  57.50%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2543%\n",
      "layer   2  Sparsity: 72.6055%\n",
      "layer   3  Sparsity: 78.0119%\n",
      "total_backward_count 420970 real_backward_count 55497  13.183%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.728822/  1.926311, val:  67.92%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2625%\n",
      "layer   2  Sparsity: 72.5144%\n",
      "layer   3  Sparsity: 77.9579%\n",
      "total_backward_count 430760 real_backward_count 56690  13.160%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.740096/  1.873293, val:  58.75%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2757%\n",
      "layer   2  Sparsity: 72.3128%\n",
      "layer   3  Sparsity: 77.1947%\n",
      "total_backward_count 440550 real_backward_count 57865  13.135%\n",
      "lif layer 2 self.abs_max_v: 10819.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.720413/  1.909875, val:  61.67%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2310%\n",
      "layer   2  Sparsity: 71.8187%\n",
      "layer   3  Sparsity: 77.7622%\n",
      "total_backward_count 450340 real_backward_count 59032  13.108%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.718499/  1.901746, val:  67.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2276%\n",
      "layer   2  Sparsity: 72.0557%\n",
      "layer   3  Sparsity: 77.7423%\n",
      "total_backward_count 460130 real_backward_count 60263  13.097%\n",
      "lif layer 2 self.abs_max_v: 10864.5\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.724183/  1.952043, val:  45.42%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2525%\n",
      "layer   2  Sparsity: 72.5820%\n",
      "layer   3  Sparsity: 78.9237%\n",
      "total_backward_count 469920 real_backward_count 61400  13.066%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.752393/  1.932084, val:  65.42%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2207%\n",
      "layer   2  Sparsity: 72.3459%\n",
      "layer   3  Sparsity: 78.5728%\n",
      "total_backward_count 479710 real_backward_count 62549  13.039%\n",
      "lif layer 2 self.abs_max_v: 10865.5\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.713087/  1.899812, val:  64.17%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2263%\n",
      "layer   2  Sparsity: 71.9605%\n",
      "layer   3  Sparsity: 77.8057%\n",
      "total_backward_count 489500 real_backward_count 63723  13.018%\n",
      "lif layer 2 self.abs_max_v: 10961.5\n",
      "lif layer 2 self.abs_max_v: 11204.0\n",
      "fc layer 1 self.abs_max_out: 15434.0\n",
      "lif layer 1 self.abs_max_v: 27309.5\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.690354/  1.939074, val:  58.75%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2456%\n",
      "layer   2  Sparsity: 72.4604%\n",
      "layer   3  Sparsity: 76.2765%\n",
      "total_backward_count 499290 real_backward_count 64796  12.978%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.703184/  1.901667, val:  55.00%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2150%\n",
      "layer   2  Sparsity: 72.2326%\n",
      "layer   3  Sparsity: 76.8428%\n",
      "total_backward_count 509080 real_backward_count 65940  12.953%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.690937/  1.904992, val:  63.75%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2357%\n",
      "layer   2  Sparsity: 72.4007%\n",
      "layer   3  Sparsity: 77.8685%\n",
      "total_backward_count 518870 real_backward_count 67085  12.929%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.694584/  1.885526, val:  60.83%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2254%\n",
      "layer   2  Sparsity: 71.7401%\n",
      "layer   3  Sparsity: 76.5687%\n",
      "total_backward_count 528660 real_backward_count 68214  12.903%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.710107/  1.878652, val:  66.67%, val_best:  70.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2377%\n",
      "layer   2  Sparsity: 71.3070%\n",
      "layer   3  Sparsity: 77.4799%\n",
      "total_backward_count 538450 real_backward_count 69408  12.890%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.685844/  1.901044, val:  60.83%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2438%\n",
      "layer   2  Sparsity: 71.9240%\n",
      "layer   3  Sparsity: 76.7151%\n",
      "total_backward_count 548240 real_backward_count 70558  12.870%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.693751/  1.980122, val:  41.25%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2170%\n",
      "layer   2  Sparsity: 71.7823%\n",
      "layer   3  Sparsity: 76.9202%\n",
      "total_backward_count 558030 real_backward_count 71696  12.848%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.707027/  1.894666, val:  57.50%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2455%\n",
      "layer   2  Sparsity: 71.9476%\n",
      "layer   3  Sparsity: 75.7975%\n",
      "total_backward_count 567820 real_backward_count 72916  12.841%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.702515/  1.891056, val:  60.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2394%\n",
      "layer   2  Sparsity: 71.8886%\n",
      "layer   3  Sparsity: 77.5813%\n",
      "total_backward_count 577610 real_backward_count 74026  12.816%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.696631/  1.900464, val:  56.25%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 71.54 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 93.2486%\n",
      "layer   2  Sparsity: 71.7067%\n",
      "layer   3  Sparsity: 77.7332%\n",
      "total_backward_count 587400 real_backward_count 75149  12.793%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.704745/  1.901585, val:  59.17%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 70.69 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 93.2463%\n",
      "layer   2  Sparsity: 72.1694%\n",
      "layer   3  Sparsity: 77.6510%\n",
      "total_backward_count 597190 real_backward_count 76307  12.778%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.703493/  1.833130, val:  69.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 93.2209%\n",
      "layer   2  Sparsity: 72.0498%\n",
      "layer   3  Sparsity: 76.6758%\n",
      "total_backward_count 606980 real_backward_count 77477  12.764%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.699934/  1.915492, val:  65.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 93.2344%\n",
      "layer   2  Sparsity: 72.5093%\n",
      "layer   3  Sparsity: 77.6587%\n",
      "total_backward_count 616770 real_backward_count 78612  12.746%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.702139/  1.929550, val:  59.58%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2579%\n",
      "layer   2  Sparsity: 72.4187%\n",
      "layer   3  Sparsity: 77.0111%\n",
      "total_backward_count 626560 real_backward_count 79712  12.722%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.696566/  1.939135, val:  56.67%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2625%\n",
      "layer   2  Sparsity: 72.1909%\n",
      "layer   3  Sparsity: 77.4507%\n",
      "total_backward_count 636350 real_backward_count 80829  12.702%\n",
      "lif layer 2 self.abs_max_v: 11239.5\n",
      "lif layer 2 self.abs_max_v: 11254.0\n",
      "lif layer 2 self.abs_max_v: 11425.0\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.693447/  1.884811, val:  62.08%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2334%\n",
      "layer   2  Sparsity: 72.0901%\n",
      "layer   3  Sparsity: 76.8180%\n",
      "total_backward_count 646140 real_backward_count 81962  12.685%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.694360/  1.900832, val:  63.33%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2299%\n",
      "layer   2  Sparsity: 72.2461%\n",
      "layer   3  Sparsity: 76.4932%\n",
      "total_backward_count 655930 real_backward_count 83106  12.670%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.708667/  1.954491, val:  59.58%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2235%\n",
      "layer   2  Sparsity: 71.9813%\n",
      "layer   3  Sparsity: 78.4591%\n",
      "total_backward_count 665720 real_backward_count 84238  12.654%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.733437/  1.891871, val:  61.67%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2282%\n",
      "layer   2  Sparsity: 72.0135%\n",
      "layer   3  Sparsity: 78.6956%\n",
      "total_backward_count 675510 real_backward_count 85371  12.638%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.694251/  1.893933, val:  65.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2349%\n",
      "layer   2  Sparsity: 71.9684%\n",
      "layer   3  Sparsity: 77.4349%\n",
      "total_backward_count 685300 real_backward_count 86525  12.626%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.695326/  1.893168, val:  56.25%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2446%\n",
      "layer   2  Sparsity: 72.2421%\n",
      "layer   3  Sparsity: 77.8382%\n",
      "total_backward_count 695090 real_backward_count 87638  12.608%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.710214/  1.895398, val:  62.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2275%\n",
      "layer   2  Sparsity: 72.2795%\n",
      "layer   3  Sparsity: 78.4245%\n",
      "total_backward_count 704880 real_backward_count 88744  12.590%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.710534/  1.901056, val:  65.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2389%\n",
      "layer   2  Sparsity: 71.9979%\n",
      "layer   3  Sparsity: 78.7933%\n",
      "total_backward_count 714670 real_backward_count 89844  12.571%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.680233/  1.908348, val:  57.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2371%\n",
      "layer   2  Sparsity: 72.0962%\n",
      "layer   3  Sparsity: 78.6506%\n",
      "total_backward_count 724460 real_backward_count 90856  12.541%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.687595/  1.926948, val:  65.83%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2493%\n",
      "layer   2  Sparsity: 71.9266%\n",
      "layer   3  Sparsity: 78.6219%\n",
      "total_backward_count 734250 real_backward_count 91993  12.529%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.698645/  1.862987, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2288%\n",
      "layer   2  Sparsity: 72.2209%\n",
      "layer   3  Sparsity: 77.4605%\n",
      "total_backward_count 744040 real_backward_count 93053  12.506%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.682470/  1.880815, val:  71.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2505%\n",
      "layer   2  Sparsity: 72.3635%\n",
      "layer   3  Sparsity: 76.7309%\n",
      "total_backward_count 753830 real_backward_count 94154  12.490%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.679661/  1.938466, val:  47.92%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2481%\n",
      "layer   2  Sparsity: 72.0625%\n",
      "layer   3  Sparsity: 77.3736%\n",
      "total_backward_count 763620 real_backward_count 95247  12.473%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.718055/  1.886213, val:  60.83%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2545%\n",
      "layer   2  Sparsity: 72.5185%\n",
      "layer   3  Sparsity: 78.9001%\n",
      "total_backward_count 773410 real_backward_count 96337  12.456%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.698610/  1.903467, val:  63.75%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2368%\n",
      "layer   2  Sparsity: 72.5204%\n",
      "layer   3  Sparsity: 77.8452%\n",
      "total_backward_count 783200 real_backward_count 97413  12.438%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.692094/  1.898267, val:  67.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2238%\n",
      "layer   2  Sparsity: 72.2403%\n",
      "layer   3  Sparsity: 77.3169%\n",
      "total_backward_count 792990 real_backward_count 98540  12.426%\n",
      "fc layer 1 self.abs_max_out: 15775.0\n",
      "lif layer 1 self.abs_max_v: 28055.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.703555/  1.937699, val:  52.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2500%\n",
      "layer   2  Sparsity: 72.3220%\n",
      "layer   3  Sparsity: 78.5794%\n",
      "total_backward_count 802780 real_backward_count 99667  12.415%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.719953/  1.861895, val:  57.08%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2372%\n",
      "layer   2  Sparsity: 72.3366%\n",
      "layer   3  Sparsity: 77.6331%\n",
      "total_backward_count 812570 real_backward_count 100818  12.407%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.674748/  1.880805, val:  60.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2236%\n",
      "layer   2  Sparsity: 72.6358%\n",
      "layer   3  Sparsity: 77.4633%\n",
      "total_backward_count 822360 real_backward_count 101918  12.393%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.705108/  1.891217, val:  65.42%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.2398%\n",
      "layer   2  Sparsity: 71.6987%\n",
      "layer   3  Sparsity: 77.6481%\n",
      "total_backward_count 832150 real_backward_count 103000  12.378%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.719576/  1.896051, val:  69.17%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2415%\n",
      "layer   2  Sparsity: 72.1803%\n",
      "layer   3  Sparsity: 78.5343%\n",
      "total_backward_count 841940 real_backward_count 104041  12.357%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.729379/  1.935543, val:  49.58%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2240%\n",
      "layer   2  Sparsity: 71.9087%\n",
      "layer   3  Sparsity: 79.2667%\n",
      "total_backward_count 851730 real_backward_count 105151  12.346%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.720992/  1.893584, val:  66.25%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2086%\n",
      "layer   2  Sparsity: 72.1756%\n",
      "layer   3  Sparsity: 77.9347%\n",
      "total_backward_count 861520 real_backward_count 106226  12.330%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.713198/  1.870976, val:  64.58%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2209%\n",
      "layer   2  Sparsity: 71.9650%\n",
      "layer   3  Sparsity: 77.7873%\n",
      "total_backward_count 871310 real_backward_count 107326  12.318%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.704153/  1.862796, val:  69.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2446%\n",
      "layer   2  Sparsity: 72.2660%\n",
      "layer   3  Sparsity: 77.7774%\n",
      "total_backward_count 881100 real_backward_count 108411  12.304%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.683648/  1.922555, val:  61.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2435%\n",
      "layer   2  Sparsity: 72.0178%\n",
      "layer   3  Sparsity: 76.7373%\n",
      "total_backward_count 890890 real_backward_count 109502  12.291%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.686876/  1.877051, val:  64.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2168%\n",
      "layer   2  Sparsity: 72.4093%\n",
      "layer   3  Sparsity: 77.3936%\n",
      "total_backward_count 900680 real_backward_count 110556  12.275%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.681028/  1.858646, val:  58.75%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2561%\n",
      "layer   2  Sparsity: 72.2510%\n",
      "layer   3  Sparsity: 78.1103%\n",
      "total_backward_count 910470 real_backward_count 111590  12.256%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.671574/  1.913943, val:  56.25%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2545%\n",
      "layer   2  Sparsity: 71.8380%\n",
      "layer   3  Sparsity: 76.8371%\n",
      "total_backward_count 920260 real_backward_count 112650  12.241%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.664305/  1.851048, val:  59.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2565%\n",
      "layer   2  Sparsity: 72.0958%\n",
      "layer   3  Sparsity: 77.1802%\n",
      "total_backward_count 930050 real_backward_count 113706  12.226%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.686827/  1.923022, val:  61.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2537%\n",
      "layer   2  Sparsity: 71.9697%\n",
      "layer   3  Sparsity: 78.7666%\n",
      "total_backward_count 939840 real_backward_count 114765  12.211%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.716795/  1.917684, val:  54.58%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2145%\n",
      "layer   2  Sparsity: 71.9383%\n",
      "layer   3  Sparsity: 78.9355%\n",
      "total_backward_count 949630 real_backward_count 115847  12.199%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.731556/  1.884619, val:  71.67%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2495%\n",
      "layer   2  Sparsity: 72.2137%\n",
      "layer   3  Sparsity: 79.4477%\n",
      "total_backward_count 959420 real_backward_count 116872  12.182%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.724961/  1.923518, val:  49.58%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2498%\n",
      "layer   2  Sparsity: 71.8990%\n",
      "layer   3  Sparsity: 79.3324%\n",
      "total_backward_count 969210 real_backward_count 117993  12.174%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.716643/  1.891044, val:  65.00%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2507%\n",
      "layer   2  Sparsity: 72.2420%\n",
      "layer   3  Sparsity: 78.2156%\n",
      "total_backward_count 979000 real_backward_count 119034  12.159%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.719938/  1.938886, val:  57.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2358%\n",
      "layer   2  Sparsity: 72.5437%\n",
      "layer   3  Sparsity: 80.2713%\n",
      "total_backward_count 988790 real_backward_count 120125  12.149%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.730032/  1.871760, val:  67.08%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2347%\n",
      "layer   2  Sparsity: 71.8882%\n",
      "layer   3  Sparsity: 78.4163%\n",
      "total_backward_count 998580 real_backward_count 121223  12.140%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.734258/  1.914302, val:  70.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2728%\n",
      "layer   2  Sparsity: 72.4445%\n",
      "layer   3  Sparsity: 79.9527%\n",
      "total_backward_count 1008370 real_backward_count 122283  12.127%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.721538/  1.888006, val:  70.42%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2317%\n",
      "layer   2  Sparsity: 73.0242%\n",
      "layer   3  Sparsity: 79.0999%\n",
      "total_backward_count 1018160 real_backward_count 123311  12.111%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.691121/  1.862195, val:  74.17%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2576%\n",
      "layer   2  Sparsity: 72.3802%\n",
      "layer   3  Sparsity: 78.6509%\n",
      "total_backward_count 1027950 real_backward_count 124351  12.097%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.727640/  1.906939, val:  73.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2330%\n",
      "layer   2  Sparsity: 71.7751%\n",
      "layer   3  Sparsity: 79.9359%\n",
      "total_backward_count 1037740 real_backward_count 125469  12.091%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.728707/  1.982465, val:  60.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2464%\n",
      "layer   2  Sparsity: 72.0872%\n",
      "layer   3  Sparsity: 79.9142%\n",
      "total_backward_count 1047530 real_backward_count 126484  12.074%\n",
      "fc layer 2 self.abs_max_out: 6430.0\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.723694/  1.908773, val:  65.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2341%\n",
      "layer   2  Sparsity: 71.9710%\n",
      "layer   3  Sparsity: 77.8611%\n",
      "total_backward_count 1057320 real_backward_count 127544  12.063%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.688971/  1.849336, val:  69.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2352%\n",
      "layer   2  Sparsity: 71.7183%\n",
      "layer   3  Sparsity: 77.2571%\n",
      "total_backward_count 1067110 real_backward_count 128633  12.054%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.660912/  1.868017, val:  67.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2180%\n",
      "layer   2  Sparsity: 72.2084%\n",
      "layer   3  Sparsity: 77.4122%\n",
      "total_backward_count 1076900 real_backward_count 129634  12.038%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.657588/  1.832025, val:  70.00%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2503%\n",
      "layer   2  Sparsity: 72.0107%\n",
      "layer   3  Sparsity: 76.9719%\n",
      "total_backward_count 1086690 real_backward_count 130688  12.026%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.673043/  1.859236, val:  64.58%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2490%\n",
      "layer   2  Sparsity: 71.8298%\n",
      "layer   3  Sparsity: 78.4354%\n",
      "total_backward_count 1096480 real_backward_count 131637  12.005%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.714272/  1.877877, val:  67.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2553%\n",
      "layer   2  Sparsity: 71.8612%\n",
      "layer   3  Sparsity: 78.9682%\n",
      "total_backward_count 1106270 real_backward_count 132732  11.998%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.661074/  1.880402, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2108%\n",
      "layer   2  Sparsity: 72.2916%\n",
      "layer   3  Sparsity: 77.3836%\n",
      "total_backward_count 1116060 real_backward_count 133741  11.983%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.648927/  1.868651, val:  61.67%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2528%\n",
      "layer   2  Sparsity: 72.0947%\n",
      "layer   3  Sparsity: 75.3150%\n",
      "total_backward_count 1125850 real_backward_count 134756  11.969%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.643371/  1.898166, val:  54.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2423%\n",
      "layer   2  Sparsity: 72.1790%\n",
      "layer   3  Sparsity: 76.1797%\n",
      "total_backward_count 1135640 real_backward_count 135867  11.964%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.622582/  1.825589, val:  68.33%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2369%\n",
      "layer   2  Sparsity: 71.9188%\n",
      "layer   3  Sparsity: 76.3160%\n",
      "total_backward_count 1145430 real_backward_count 136896  11.951%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.660452/  1.868422, val:  68.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2477%\n",
      "layer   2  Sparsity: 72.5512%\n",
      "layer   3  Sparsity: 77.4952%\n",
      "total_backward_count 1155220 real_backward_count 137860  11.934%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.657727/  1.852427, val:  60.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2206%\n",
      "layer   2  Sparsity: 72.2274%\n",
      "layer   3  Sparsity: 77.1635%\n",
      "total_backward_count 1165010 real_backward_count 138871  11.920%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.652361/  1.847118, val:  71.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2316%\n",
      "layer   2  Sparsity: 72.2703%\n",
      "layer   3  Sparsity: 77.1009%\n",
      "total_backward_count 1174800 real_backward_count 139883  11.907%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.705233/  1.848806, val:  68.33%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2237%\n",
      "layer   2  Sparsity: 72.2429%\n",
      "layer   3  Sparsity: 78.0800%\n",
      "total_backward_count 1184590 real_backward_count 140951  11.899%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.653877/  1.903890, val:  65.00%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2432%\n",
      "layer   2  Sparsity: 72.1741%\n",
      "layer   3  Sparsity: 77.0030%\n",
      "total_backward_count 1194380 real_backward_count 141990  11.888%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.661859/  1.849063, val:  74.17%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2282%\n",
      "layer   2  Sparsity: 71.8117%\n",
      "layer   3  Sparsity: 76.9889%\n",
      "total_backward_count 1204170 real_backward_count 143071  11.881%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.650318/  1.801102, val:  65.00%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2395%\n",
      "layer   2  Sparsity: 71.6087%\n",
      "layer   3  Sparsity: 76.3760%\n",
      "total_backward_count 1213960 real_backward_count 144123  11.872%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.627258/  1.756486, val:  82.92%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2524%\n",
      "layer   2  Sparsity: 72.1701%\n",
      "layer   3  Sparsity: 76.6828%\n",
      "total_backward_count 1223750 real_backward_count 145163  11.862%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.623748/  1.860146, val:  54.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2150%\n",
      "layer   2  Sparsity: 72.2037%\n",
      "layer   3  Sparsity: 76.0103%\n",
      "total_backward_count 1233540 real_backward_count 146228  11.854%\n",
      "fc layer 1 self.abs_max_out: 16103.0\n",
      "lif layer 1 self.abs_max_v: 28476.0\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.649053/  1.889123, val:  55.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2612%\n",
      "layer   2  Sparsity: 72.1715%\n",
      "layer   3  Sparsity: 76.6330%\n",
      "total_backward_count 1243330 real_backward_count 147242  11.843%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.630012/  1.843438, val:  64.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2510%\n",
      "layer   2  Sparsity: 71.9845%\n",
      "layer   3  Sparsity: 75.5321%\n",
      "total_backward_count 1253120 real_backward_count 148258  11.831%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.600304/  1.878411, val:  65.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2203%\n",
      "layer   2  Sparsity: 72.3276%\n",
      "layer   3  Sparsity: 75.8486%\n",
      "total_backward_count 1262910 real_backward_count 149222  11.816%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.636509/  1.896140, val:  65.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2407%\n",
      "layer   2  Sparsity: 72.4467%\n",
      "layer   3  Sparsity: 77.5273%\n",
      "total_backward_count 1272700 real_backward_count 150217  11.803%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.649387/  1.865316, val:  49.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2230%\n",
      "layer   2  Sparsity: 72.4456%\n",
      "layer   3  Sparsity: 77.3291%\n",
      "total_backward_count 1282490 real_backward_count 151271  11.795%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.643366/  1.840337, val:  65.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2345%\n",
      "layer   2  Sparsity: 72.7508%\n",
      "layer   3  Sparsity: 77.5972%\n",
      "total_backward_count 1292280 real_backward_count 152321  11.787%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.655086/  1.844257, val:  58.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2453%\n",
      "layer   2  Sparsity: 72.5913%\n",
      "layer   3  Sparsity: 78.1107%\n",
      "total_backward_count 1302070 real_backward_count 153338  11.776%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.644071/  1.871445, val:  60.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2433%\n",
      "layer   2  Sparsity: 71.9997%\n",
      "layer   3  Sparsity: 76.7351%\n",
      "total_backward_count 1311860 real_backward_count 154362  11.767%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.632671/  1.798889, val:  70.42%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2251%\n",
      "layer   2  Sparsity: 72.4979%\n",
      "layer   3  Sparsity: 76.2511%\n",
      "total_backward_count 1321650 real_backward_count 155363  11.755%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.624056/  1.788749, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2204%\n",
      "layer   2  Sparsity: 72.5119%\n",
      "layer   3  Sparsity: 76.2240%\n",
      "total_backward_count 1331440 real_backward_count 156402  11.747%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.639558/  1.853389, val:  64.58%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2433%\n",
      "layer   2  Sparsity: 72.2345%\n",
      "layer   3  Sparsity: 77.2588%\n",
      "total_backward_count 1341230 real_backward_count 157376  11.734%\n",
      "fc layer 1 self.abs_max_out: 16193.0\n",
      "lif layer 1 self.abs_max_v: 28749.0\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.649485/  1.865956, val:  53.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2554%\n",
      "layer   2  Sparsity: 71.9032%\n",
      "layer   3  Sparsity: 76.9218%\n",
      "total_backward_count 1351020 real_backward_count 158386  11.723%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.636231/  1.816830, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2282%\n",
      "layer   2  Sparsity: 71.9432%\n",
      "layer   3  Sparsity: 75.7788%\n",
      "total_backward_count 1360810 real_backward_count 159381  11.712%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.634791/  1.850159, val:  65.00%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2405%\n",
      "layer   2  Sparsity: 71.8735%\n",
      "layer   3  Sparsity: 75.8550%\n",
      "total_backward_count 1370600 real_backward_count 160416  11.704%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.636782/  1.834724, val:  67.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2166%\n",
      "layer   2  Sparsity: 71.9267%\n",
      "layer   3  Sparsity: 74.8632%\n",
      "total_backward_count 1380390 real_backward_count 161345  11.688%\n",
      "fc layer 1 self.abs_max_out: 16415.0\n",
      "lif layer 1 self.abs_max_v: 29163.0\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.629799/  1.865611, val:  61.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2468%\n",
      "layer   2  Sparsity: 72.1212%\n",
      "layer   3  Sparsity: 75.2559%\n",
      "total_backward_count 1390180 real_backward_count 162311  11.676%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.664224/  1.882786, val:  66.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2233%\n",
      "layer   2  Sparsity: 72.2787%\n",
      "layer   3  Sparsity: 76.1362%\n",
      "total_backward_count 1399970 real_backward_count 163324  11.666%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.713225/  1.890701, val:  72.92%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2181%\n",
      "layer   2  Sparsity: 72.0648%\n",
      "layer   3  Sparsity: 79.9141%\n",
      "total_backward_count 1409760 real_backward_count 164393  11.661%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.674890/  1.834486, val:  72.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2518%\n",
      "layer   2  Sparsity: 72.1484%\n",
      "layer   3  Sparsity: 77.2680%\n",
      "total_backward_count 1419550 real_backward_count 165433  11.654%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.677457/  1.843055, val:  65.83%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2519%\n",
      "layer   2  Sparsity: 72.3446%\n",
      "layer   3  Sparsity: 77.5964%\n",
      "total_backward_count 1429340 real_backward_count 166468  11.646%\n",
      "lif layer 2 self.abs_max_v: 11447.5\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.668354/  1.873859, val:  70.83%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2464%\n",
      "layer   2  Sparsity: 72.5798%\n",
      "layer   3  Sparsity: 77.8543%\n",
      "total_backward_count 1439130 real_backward_count 167500  11.639%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.658764/  1.862836, val:  60.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2360%\n",
      "layer   2  Sparsity: 72.0968%\n",
      "layer   3  Sparsity: 77.4923%\n",
      "total_backward_count 1448920 real_backward_count 168431  11.625%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.661660/  1.864367, val:  70.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2456%\n",
      "layer   2  Sparsity: 71.8434%\n",
      "layer   3  Sparsity: 77.3944%\n",
      "total_backward_count 1458710 real_backward_count 169462  11.617%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.649907/  1.840408, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2397%\n",
      "layer   2  Sparsity: 72.0207%\n",
      "layer   3  Sparsity: 76.6161%\n",
      "total_backward_count 1468500 real_backward_count 170430  11.606%\n",
      "lif layer 2 self.abs_max_v: 11521.5\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.640183/  1.848828, val:  55.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2691%\n",
      "layer   2  Sparsity: 72.1261%\n",
      "layer   3  Sparsity: 75.9196%\n",
      "total_backward_count 1478290 real_backward_count 171415  11.595%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.639019/  1.818858, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2455%\n",
      "layer   2  Sparsity: 72.1358%\n",
      "layer   3  Sparsity: 77.5342%\n",
      "total_backward_count 1488080 real_backward_count 172403  11.586%\n",
      "fc layer 1 self.abs_max_out: 16858.0\n",
      "lif layer 1 self.abs_max_v: 30027.5\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.657872/  1.844257, val:  75.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2455%\n",
      "layer   2  Sparsity: 72.0626%\n",
      "layer   3  Sparsity: 78.3998%\n",
      "total_backward_count 1497870 real_backward_count 173453  11.580%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.665706/  1.788514, val:  74.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2376%\n",
      "layer   2  Sparsity: 72.2672%\n",
      "layer   3  Sparsity: 78.9589%\n",
      "total_backward_count 1507660 real_backward_count 174514  11.575%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.613774/  1.815401, val:  70.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2299%\n",
      "layer   2  Sparsity: 72.3004%\n",
      "layer   3  Sparsity: 77.4783%\n",
      "total_backward_count 1517450 real_backward_count 175508  11.566%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.596010/  1.819738, val:  75.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2486%\n",
      "layer   2  Sparsity: 72.0275%\n",
      "layer   3  Sparsity: 76.0150%\n",
      "total_backward_count 1527240 real_backward_count 176528  11.559%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.625247/  1.868087, val:  60.42%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2424%\n",
      "layer   2  Sparsity: 71.8667%\n",
      "layer   3  Sparsity: 77.4125%\n",
      "total_backward_count 1537030 real_backward_count 177547  11.551%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.654328/  1.868801, val:  74.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2268%\n",
      "layer   2  Sparsity: 71.9277%\n",
      "layer   3  Sparsity: 78.4011%\n",
      "total_backward_count 1546820 real_backward_count 178541  11.542%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.662818/  1.856355, val:  59.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2578%\n",
      "layer   2  Sparsity: 71.8242%\n",
      "layer   3  Sparsity: 76.9484%\n",
      "total_backward_count 1556610 real_backward_count 179564  11.536%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.623617/  1.816178, val:  69.58%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2457%\n",
      "layer   2  Sparsity: 72.0735%\n",
      "layer   3  Sparsity: 76.7057%\n",
      "total_backward_count 1566400 real_backward_count 180555  11.527%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.648369/  1.861876, val:  71.67%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2590%\n",
      "layer   2  Sparsity: 71.7630%\n",
      "layer   3  Sparsity: 77.1749%\n",
      "total_backward_count 1576190 real_backward_count 181572  11.520%\n",
      "fc layer 2 self.abs_max_out: 6498.0\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.651939/  1.857884, val:  62.08%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2394%\n",
      "layer   2  Sparsity: 72.3385%\n",
      "layer   3  Sparsity: 77.3817%\n",
      "total_backward_count 1585980 real_backward_count 182539  11.510%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.643910/  1.898361, val:  58.33%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2328%\n",
      "layer   2  Sparsity: 72.1797%\n",
      "layer   3  Sparsity: 77.4909%\n",
      "total_backward_count 1595770 real_backward_count 183528  11.501%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.672172/  1.910297, val:  61.25%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2414%\n",
      "layer   2  Sparsity: 72.1422%\n",
      "layer   3  Sparsity: 78.1216%\n",
      "total_backward_count 1605560 real_backward_count 184519  11.493%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.656499/  1.871854, val:  42.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2296%\n",
      "layer   2  Sparsity: 72.1387%\n",
      "layer   3  Sparsity: 76.6563%\n",
      "total_backward_count 1615350 real_backward_count 185565  11.488%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.656588/  1.851547, val:  70.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2428%\n",
      "layer   2  Sparsity: 71.9533%\n",
      "layer   3  Sparsity: 76.9818%\n",
      "total_backward_count 1625140 real_backward_count 186598  11.482%\n",
      "lif layer 2 self.abs_max_v: 11664.5\n",
      "fc layer 2 self.abs_max_out: 6595.0\n",
      "lif layer 2 self.abs_max_v: 12172.5\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.646756/  1.860188, val:  70.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2464%\n",
      "layer   2  Sparsity: 71.7669%\n",
      "layer   3  Sparsity: 76.9872%\n",
      "total_backward_count 1634930 real_backward_count 187551  11.472%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.657752/  1.849956, val:  58.33%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2269%\n",
      "layer   2  Sparsity: 72.0572%\n",
      "layer   3  Sparsity: 76.9521%\n",
      "total_backward_count 1644720 real_backward_count 188581  11.466%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.669545/  1.846626, val:  74.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2603%\n",
      "layer   2  Sparsity: 71.8512%\n",
      "layer   3  Sparsity: 78.1710%\n",
      "total_backward_count 1654510 real_backward_count 189623  11.461%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.666207/  1.844106, val:  79.17%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2410%\n",
      "layer   2  Sparsity: 72.1425%\n",
      "layer   3  Sparsity: 78.3302%\n",
      "total_backward_count 1664300 real_backward_count 190590  11.452%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.666893/  1.869693, val:  64.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2398%\n",
      "layer   2  Sparsity: 72.1039%\n",
      "layer   3  Sparsity: 78.2485%\n",
      "total_backward_count 1674090 real_backward_count 191593  11.445%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.675951/  1.912276, val:  55.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2149%\n",
      "layer   2  Sparsity: 71.9418%\n",
      "layer   3  Sparsity: 78.4807%\n",
      "total_backward_count 1683880 real_backward_count 192636  11.440%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.672384/  1.868994, val:  67.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2340%\n",
      "layer   2  Sparsity: 71.5824%\n",
      "layer   3  Sparsity: 77.9866%\n",
      "total_backward_count 1693670 real_backward_count 193707  11.437%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.670416/  1.852165, val:  70.00%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2287%\n",
      "layer   2  Sparsity: 71.6296%\n",
      "layer   3  Sparsity: 77.2815%\n",
      "total_backward_count 1703460 real_backward_count 194701  11.430%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.644527/  1.869133, val:  62.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2292%\n",
      "layer   2  Sparsity: 71.7705%\n",
      "layer   3  Sparsity: 75.3204%\n",
      "total_backward_count 1713250 real_backward_count 195760  11.426%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.646427/  1.848802, val:  60.00%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2104%\n",
      "layer   2  Sparsity: 71.9858%\n",
      "layer   3  Sparsity: 76.8867%\n",
      "total_backward_count 1723040 real_backward_count 196792  11.421%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.657121/  1.885535, val:  72.92%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2397%\n",
      "layer   2  Sparsity: 72.5760%\n",
      "layer   3  Sparsity: 78.0077%\n",
      "total_backward_count 1732830 real_backward_count 197783  11.414%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.663896/  1.848573, val:  68.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2371%\n",
      "layer   2  Sparsity: 71.9753%\n",
      "layer   3  Sparsity: 77.4557%\n",
      "total_backward_count 1742620 real_backward_count 198819  11.409%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.651115/  1.940126, val:  47.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2311%\n",
      "layer   2  Sparsity: 72.1309%\n",
      "layer   3  Sparsity: 77.8837%\n",
      "total_backward_count 1752410 real_backward_count 199850  11.404%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.657184/  1.821558, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2493%\n",
      "layer   2  Sparsity: 71.9155%\n",
      "layer   3  Sparsity: 77.1646%\n",
      "total_backward_count 1762200 real_backward_count 200956  11.404%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.652213/  1.845250, val:  77.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2227%\n",
      "layer   2  Sparsity: 71.9449%\n",
      "layer   3  Sparsity: 76.6929%\n",
      "total_backward_count 1771990 real_backward_count 201976  11.398%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.629598/  1.832293, val:  70.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2485%\n",
      "layer   2  Sparsity: 72.1909%\n",
      "layer   3  Sparsity: 75.9129%\n",
      "total_backward_count 1781780 real_backward_count 202944  11.390%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.652885/  1.871479, val:  65.83%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2152%\n",
      "layer   2  Sparsity: 72.0704%\n",
      "layer   3  Sparsity: 78.0902%\n",
      "total_backward_count 1791570 real_backward_count 203909  11.382%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.660252/  1.894327, val:  48.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2364%\n",
      "layer   2  Sparsity: 71.7670%\n",
      "layer   3  Sparsity: 77.8286%\n",
      "total_backward_count 1801360 real_backward_count 204914  11.376%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.644529/  1.872449, val:  59.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2379%\n",
      "layer   2  Sparsity: 71.9227%\n",
      "layer   3  Sparsity: 77.2726%\n",
      "total_backward_count 1811150 real_backward_count 205902  11.369%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.677652/  1.881768, val:  65.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2142%\n",
      "layer   2  Sparsity: 72.0185%\n",
      "layer   3  Sparsity: 78.1705%\n",
      "total_backward_count 1820940 real_backward_count 206902  11.362%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.656407/  1.868143, val:  63.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2294%\n",
      "layer   2  Sparsity: 72.1000%\n",
      "layer   3  Sparsity: 78.5892%\n",
      "total_backward_count 1830730 real_backward_count 207949  11.359%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.639868/  1.887944, val:  50.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2381%\n",
      "layer   2  Sparsity: 71.7061%\n",
      "layer   3  Sparsity: 77.9829%\n",
      "total_backward_count 1840520 real_backward_count 208991  11.355%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.670817/  1.907302, val:  65.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2306%\n",
      "layer   2  Sparsity: 71.3445%\n",
      "layer   3  Sparsity: 79.0502%\n",
      "total_backward_count 1850310 real_backward_count 210022  11.351%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.668267/  1.848195, val:  63.75%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.2149%\n",
      "layer   2  Sparsity: 71.6349%\n",
      "layer   3  Sparsity: 78.2913%\n",
      "total_backward_count 1860100 real_backward_count 211008  11.344%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.689595/  1.861217, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2240%\n",
      "layer   2  Sparsity: 71.6646%\n",
      "layer   3  Sparsity: 79.6228%\n",
      "total_backward_count 1869890 real_backward_count 212014  11.338%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.672535/  1.896366, val:  55.42%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2357%\n",
      "layer   2  Sparsity: 71.7216%\n",
      "layer   3  Sparsity: 78.3272%\n",
      "total_backward_count 1879680 real_backward_count 213086  11.336%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.650697/  1.896508, val:  72.92%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2245%\n",
      "layer   2  Sparsity: 71.9784%\n",
      "layer   3  Sparsity: 77.5852%\n",
      "total_backward_count 1889470 real_backward_count 214106  11.332%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.637808/  1.873510, val:  72.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2520%\n",
      "layer   2  Sparsity: 72.1138%\n",
      "layer   3  Sparsity: 77.3344%\n",
      "total_backward_count 1899260 real_backward_count 215030  11.322%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.665765/  1.845399, val:  68.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2419%\n",
      "layer   2  Sparsity: 71.8950%\n",
      "layer   3  Sparsity: 77.9886%\n",
      "total_backward_count 1909050 real_backward_count 216004  11.315%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.651207/  1.838571, val:  78.33%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2074%\n",
      "layer   2  Sparsity: 71.8632%\n",
      "layer   3  Sparsity: 78.6490%\n",
      "total_backward_count 1918840 real_backward_count 217042  11.311%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.651102/  1.860524, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2492%\n",
      "layer   2  Sparsity: 72.0002%\n",
      "layer   3  Sparsity: 79.0392%\n",
      "total_backward_count 1928630 real_backward_count 218019  11.304%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.649927/  1.817986, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.2782%\n",
      "layer   2  Sparsity: 71.7055%\n",
      "layer   3  Sparsity: 77.8605%\n",
      "total_backward_count 1938420 real_backward_count 219026  11.299%\n",
      "fc layer 1 self.abs_max_out: 17068.0\n",
      "lif layer 1 self.abs_max_v: 30557.5\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.650164/  1.869570, val:  67.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.2345%\n",
      "layer   2  Sparsity: 71.6242%\n",
      "layer   3  Sparsity: 78.4249%\n",
      "total_backward_count 1948210 real_backward_count 220017  11.293%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.646777/  1.830472, val:  68.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2445%\n",
      "layer   2  Sparsity: 71.4043%\n",
      "layer   3  Sparsity: 77.6711%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff600a7a42648958f38136f773001dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñà‚ñà‚ñÖ‚ñÑ‚ñá‚ñá‚ñÇ‚ñÇ‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñà‚ñà‚ñÖ‚ñÑ‚ñá‚ñá‚ñÇ‚ñÇ‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.64678</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.68333</td></tr><tr><td>val_loss</td><td>1.83047</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-sweep-141</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ajpevd3k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ajpevd3k</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_142252-ajpevd3k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cr7decnd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_184602-cr7decnd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cr7decnd' target=\"_blank\">graceful-sweep-146</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cr7decnd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cr7decnd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_184611_702', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 597.0\n",
      "lif layer 1 self.abs_max_v: 597.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "lif layer 2 self.abs_max_v: 1515.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 540.0\n",
      "fc layer 1 self.abs_max_out: 760.0\n",
      "lif layer 1 self.abs_max_v: 816.0\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "lif layer 2 self.abs_max_v: 2886.0\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "fc layer 1 self.abs_max_out: 783.0\n",
      "lif layer 1 self.abs_max_v: 1107.0\n",
      "lif layer 1 self.abs_max_v: 1115.5\n",
      "lif layer 2 self.abs_max_v: 2894.5\n",
      "fc layer 1 self.abs_max_out: 880.0\n",
      "lif layer 1 self.abs_max_v: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1377.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 1 self.abs_max_out: 1002.0\n",
      "lif layer 1 self.abs_max_v: 1557.5\n",
      "lif layer 2 self.abs_max_v: 3229.0\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "lif layer 2 self.abs_max_v: 3486.5\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "fc layer 1 self.abs_max_out: 1237.0\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "fc layer 1 self.abs_max_out: 1370.0\n",
      "lif layer 1 self.abs_max_v: 1896.5\n",
      "lif layer 1 self.abs_max_v: 2033.5\n",
      "fc layer 1 self.abs_max_out: 1519.0\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "fc layer 3 self.abs_max_out: 927.0\n",
      "fc layer 2 self.abs_max_out: 2312.0\n",
      "fc layer 1 self.abs_max_out: 1650.0\n",
      "fc layer 1 self.abs_max_out: 1658.0\n",
      "fc layer 3 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 2070.0\n",
      "fc layer 2 self.abs_max_out: 2491.0\n",
      "lif layer 2 self.abs_max_v: 3919.0\n",
      "fc layer 1 self.abs_max_out: 1752.0\n",
      "lif layer 1 self.abs_max_v: 2579.5\n",
      "lif layer 2 self.abs_max_v: 3946.0\n",
      "lif layer 1 self.abs_max_v: 2623.0\n",
      "fc layer 1 self.abs_max_out: 1760.0\n",
      "lif layer 1 self.abs_max_v: 2642.5\n",
      "lif layer 1 self.abs_max_v: 2658.0\n",
      "lif layer 1 self.abs_max_v: 2708.0\n",
      "fc layer 1 self.abs_max_out: 1919.0\n",
      "fc layer 1 self.abs_max_out: 2084.0\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "lif layer 2 self.abs_max_v: 3984.5\n",
      "lif layer 2 self.abs_max_v: 4479.5\n",
      "lif layer 1 self.abs_max_v: 2727.5\n",
      "fc layer 2 self.abs_max_out: 2785.0\n",
      "fc layer 1 self.abs_max_out: 2323.0\n",
      "lif layer 1 self.abs_max_v: 3455.0\n",
      "lif layer 2 self.abs_max_v: 4718.5\n",
      "lif layer 2 self.abs_max_v: 4747.5\n",
      "lif layer 1 self.abs_max_v: 3758.5\n",
      "fc layer 2 self.abs_max_out: 2828.0\n",
      "lif layer 2 self.abs_max_v: 5061.5\n",
      "fc layer 1 self.abs_max_out: 2471.0\n",
      "fc layer 2 self.abs_max_out: 3074.0\n",
      "lif layer 2 self.abs_max_v: 5605.0\n",
      "lif layer 1 self.abs_max_v: 3769.0\n",
      "fc layer 1 self.abs_max_out: 2736.0\n",
      "lif layer 1 self.abs_max_v: 4152.5\n",
      "lif layer 1 self.abs_max_v: 4328.0\n",
      "fc layer 2 self.abs_max_out: 3128.0\n",
      "fc layer 2 self.abs_max_out: 3135.0\n",
      "fc layer 1 self.abs_max_out: 2740.0\n",
      "fc layer 2 self.abs_max_out: 3212.0\n",
      "fc layer 1 self.abs_max_out: 2923.0\n",
      "fc layer 1 self.abs_max_out: 2988.0\n",
      "lif layer 1 self.abs_max_v: 4892.0\n",
      "fc layer 1 self.abs_max_out: 3163.0\n",
      "fc layer 3 self.abs_max_out: 1095.0\n",
      "fc layer 3 self.abs_max_out: 1257.0\n",
      "fc layer 3 self.abs_max_out: 1380.0\n",
      "fc layer 1 self.abs_max_out: 3328.0\n",
      "fc layer 1 self.abs_max_out: 3340.0\n",
      "lif layer 1 self.abs_max_v: 5025.0\n",
      "fc layer 1 self.abs_max_out: 3442.0\n",
      "lif layer 1 self.abs_max_v: 5348.0\n",
      "lif layer 1 self.abs_max_v: 5548.0\n",
      "fc layer 2 self.abs_max_out: 3286.0\n",
      "fc layer 1 self.abs_max_out: 3561.0\n",
      "lif layer 1 self.abs_max_v: 6005.0\n",
      "fc layer 2 self.abs_max_out: 3298.0\n",
      "fc layer 2 self.abs_max_out: 3400.0\n",
      "fc layer 1 self.abs_max_out: 3725.0\n",
      "lif layer 2 self.abs_max_v: 5765.0\n",
      "fc layer 1 self.abs_max_out: 3832.0\n",
      "lif layer 1 self.abs_max_v: 6269.5\n",
      "fc layer 1 self.abs_max_out: 3983.0\n",
      "lif layer 1 self.abs_max_v: 7028.5\n",
      "fc layer 2 self.abs_max_out: 3456.0\n",
      "lif layer 2 self.abs_max_v: 5866.0\n",
      "lif layer 2 self.abs_max_v: 5966.0\n",
      "fc layer 1 self.abs_max_out: 4076.0\n",
      "fc layer 2 self.abs_max_out: 3516.0\n",
      "lif layer 2 self.abs_max_v: 6023.5\n",
      "lif layer 2 self.abs_max_v: 6122.0\n",
      "lif layer 2 self.abs_max_v: 6408.0\n",
      "fc layer 1 self.abs_max_out: 4252.0\n",
      "fc layer 1 self.abs_max_out: 4534.0\n",
      "lif layer 1 self.abs_max_v: 7494.0\n",
      "lif layer 1 self.abs_max_v: 7845.0\n",
      "lif layer 1 self.abs_max_v: 7855.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.925972/  2.049130, val:  40.00%, val_best:  40.00%, tr:  85.39%, tr_best:  85.39%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7882%\n",
      "layer   2  Sparsity: 65.8994%\n",
      "layer   3  Sparsity: 62.4454%\n",
      "total_backward_count 9790 real_backward_count 3253  33.228%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 4741.0\n",
      "fc layer 2 self.abs_max_out: 3723.0\n",
      "lif layer 2 self.abs_max_v: 6490.5\n",
      "fc layer 2 self.abs_max_out: 3815.0\n",
      "lif layer 2 self.abs_max_v: 7060.5\n",
      "lif layer 1 self.abs_max_v: 8224.5\n",
      "lif layer 1 self.abs_max_v: 8486.0\n",
      "lif layer 1 self.abs_max_v: 8811.0\n",
      "fc layer 2 self.abs_max_out: 3869.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.888423/  2.032799, val:  45.00%, val_best:  45.00%, tr:  94.59%, tr_best:  94.59%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8091%\n",
      "layer   2  Sparsity: 67.3086%\n",
      "layer   3  Sparsity: 61.8068%\n",
      "total_backward_count 19580 real_backward_count 5415  27.656%\n",
      "fc layer 1 self.abs_max_out: 4919.0\n",
      "fc layer 1 self.abs_max_out: 5072.0\n",
      "lif layer 1 self.abs_max_v: 9206.0\n",
      "lif layer 1 self.abs_max_v: 9348.0\n",
      "lif layer 1 self.abs_max_v: 9412.0\n",
      "lif layer 1 self.abs_max_v: 9583.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.895491/  2.033384, val:  47.50%, val_best:  47.50%, tr:  97.75%, tr_best:  97.75%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8103%\n",
      "layer   2  Sparsity: 68.6477%\n",
      "layer   3  Sparsity: 61.2196%\n",
      "total_backward_count 29370 real_backward_count 7265  24.736%\n",
      "fc layer 1 self.abs_max_out: 5090.0\n",
      "fc layer 1 self.abs_max_out: 5205.0\n",
      "fc layer 2 self.abs_max_out: 3945.0\n",
      "fc layer 1 self.abs_max_out: 5287.0\n",
      "fc layer 1 self.abs_max_out: 5449.0\n",
      "lif layer 1 self.abs_max_v: 9771.0\n",
      "lif layer 1 self.abs_max_v: 9995.5\n",
      "lif layer 1 self.abs_max_v: 10121.0\n",
      "lif layer 1 self.abs_max_v: 10407.5\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.885310/  2.029496, val:  44.58%, val_best:  47.50%, tr:  98.06%, tr_best:  98.06%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7876%\n",
      "layer   2  Sparsity: 69.3612%\n",
      "layer   3  Sparsity: 60.2378%\n",
      "total_backward_count 39160 real_backward_count 8945  22.842%\n",
      "fc layer 1 self.abs_max_out: 5536.0\n",
      "fc layer 1 self.abs_max_out: 5657.0\n",
      "fc layer 1 self.abs_max_out: 5729.0\n",
      "lif layer 1 self.abs_max_v: 10550.0\n",
      "lif layer 1 self.abs_max_v: 10625.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.906869/  2.028578, val:  52.08%, val_best:  52.08%, tr:  99.28%, tr_best:  99.28%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7934%\n",
      "layer   2  Sparsity: 68.6389%\n",
      "layer   3  Sparsity: 59.6298%\n",
      "total_backward_count 48950 real_backward_count 10452  21.352%\n",
      "fc layer 1 self.abs_max_out: 5851.0\n",
      "lif layer 1 self.abs_max_v: 10640.5\n",
      "lif layer 1 self.abs_max_v: 10665.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.904740/  2.031285, val:  55.00%, val_best:  55.00%, tr:  99.18%, tr_best:  99.28%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7850%\n",
      "layer   2  Sparsity: 67.3588%\n",
      "layer   3  Sparsity: 59.2362%\n",
      "total_backward_count 58740 real_backward_count 11849  20.172%\n",
      "fc layer 2 self.abs_max_out: 3978.0\n",
      "fc layer 1 self.abs_max_out: 5938.0\n",
      "lif layer 1 self.abs_max_v: 10701.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.914582/  2.022152, val:  56.25%, val_best:  56.25%, tr:  99.69%, tr_best:  99.69%, epoch time: 80.72 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.7773%\n",
      "layer   2  Sparsity: 68.1607%\n",
      "layer   3  Sparsity: 59.0479%\n",
      "total_backward_count 68530 real_backward_count 13238  19.317%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.914338/  2.022772, val:  59.58%, val_best:  59.58%, tr:  99.28%, tr_best:  99.69%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7975%\n",
      "layer   2  Sparsity: 68.3290%\n",
      "layer   3  Sparsity: 59.8132%\n",
      "total_backward_count 78320 real_backward_count 14509  18.525%\n",
      "fc layer 1 self.abs_max_out: 6171.0\n",
      "fc layer 2 self.abs_max_out: 4026.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.914623/  2.023999, val:  57.92%, val_best:  59.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 80.67 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.8113%\n",
      "layer   2  Sparsity: 68.0466%\n",
      "layer   3  Sparsity: 59.1535%\n",
      "total_backward_count 88110 real_backward_count 15784  17.914%\n",
      "lif layer 1 self.abs_max_v: 10922.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.911703/  2.042547, val:  50.83%, val_best:  59.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.8002%\n",
      "layer   2  Sparsity: 68.1045%\n",
      "layer   3  Sparsity: 59.6184%\n",
      "total_backward_count 97900 real_backward_count 17014  17.379%\n",
      "fc layer 1 self.abs_max_out: 6297.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.923934/  2.029090, val:  51.67%, val_best:  59.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.8054%\n",
      "layer   2  Sparsity: 68.0534%\n",
      "layer   3  Sparsity: 59.4777%\n",
      "total_backward_count 107690 real_backward_count 18212  16.912%\n",
      "lif layer 1 self.abs_max_v: 11068.5\n",
      "fc layer 1 self.abs_max_out: 6363.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.921190/  2.035192, val:  61.25%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7661%\n",
      "layer   2  Sparsity: 68.1891%\n",
      "layer   3  Sparsity: 60.4493%\n",
      "total_backward_count 117480 real_backward_count 19433  16.542%\n",
      "fc layer 1 self.abs_max_out: 6422.0\n",
      "lif layer 1 self.abs_max_v: 11640.5\n",
      "fc layer 1 self.abs_max_out: 6725.0\n",
      "lif layer 1 self.abs_max_v: 12545.5\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.926341/  2.033283, val:  42.50%, val_best:  61.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7954%\n",
      "layer   2  Sparsity: 67.1095%\n",
      "layer   3  Sparsity: 59.8404%\n",
      "total_backward_count 127270 real_backward_count 20541  16.140%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.926231/  2.044493, val:  49.17%, val_best:  61.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8045%\n",
      "layer   2  Sparsity: 66.9496%\n",
      "layer   3  Sparsity: 59.6958%\n",
      "total_backward_count 137060 real_backward_count 21730  15.854%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.925098/  2.014934, val:  57.92%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7771%\n",
      "layer   2  Sparsity: 66.6918%\n",
      "layer   3  Sparsity: 59.5040%\n",
      "total_backward_count 146850 real_backward_count 22839  15.553%\n",
      "lif layer 2 self.abs_max_v: 7162.5\n",
      "fc layer 1 self.abs_max_out: 6894.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.924071/  2.028580, val:  55.00%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7788%\n",
      "layer   2  Sparsity: 66.6073%\n",
      "layer   3  Sparsity: 59.1163%\n",
      "total_backward_count 156640 real_backward_count 23988  15.314%\n",
      "fc layer 2 self.abs_max_out: 4052.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.914957/  2.022312, val:  58.33%, val_best:  61.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7963%\n",
      "layer   2  Sparsity: 66.6342%\n",
      "layer   3  Sparsity: 59.4644%\n",
      "total_backward_count 166430 real_backward_count 25099  15.081%\n",
      "fc layer 2 self.abs_max_out: 4320.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.924739/  2.023727, val:  58.75%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7611%\n",
      "layer   2  Sparsity: 67.1925%\n",
      "layer   3  Sparsity: 60.9496%\n",
      "total_backward_count 176220 real_backward_count 26225  14.882%\n",
      "lif layer 2 self.abs_max_v: 7251.5\n",
      "fc layer 1 self.abs_max_out: 7022.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.916160/  2.014907, val:  57.08%, val_best:  61.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 80.52 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7780%\n",
      "layer   2  Sparsity: 67.3197%\n",
      "layer   3  Sparsity: 62.0451%\n",
      "total_backward_count 186010 real_backward_count 27398  14.729%\n",
      "fc layer 1 self.abs_max_out: 7258.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.923382/  2.030971, val:  36.25%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7751%\n",
      "layer   2  Sparsity: 66.4433%\n",
      "layer   3  Sparsity: 62.2966%\n",
      "total_backward_count 195800 real_backward_count 28466  14.538%\n",
      "fc layer 2 self.abs_max_out: 4477.0\n",
      "lif layer 2 self.abs_max_v: 7784.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.924785/  2.032382, val:  46.67%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7826%\n",
      "layer   2  Sparsity: 66.0278%\n",
      "layer   3  Sparsity: 61.2814%\n",
      "total_backward_count 205590 real_backward_count 29500  14.349%\n",
      "fc layer 1 self.abs_max_out: 7312.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.917559/  2.036359, val:  56.25%, val_best:  61.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7954%\n",
      "layer   2  Sparsity: 66.5801%\n",
      "layer   3  Sparsity: 62.2762%\n",
      "total_backward_count 215380 real_backward_count 30566  14.192%\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.926609/  2.015871, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7777%\n",
      "layer   2  Sparsity: 65.9729%\n",
      "layer   3  Sparsity: 62.4504%\n",
      "total_backward_count 225170 real_backward_count 31620  14.043%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.931133/  2.027985, val:  63.33%, val_best:  65.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7522%\n",
      "layer   2  Sparsity: 65.8314%\n",
      "layer   3  Sparsity: 62.8894%\n",
      "total_backward_count 234960 real_backward_count 32693  13.914%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.929915/  2.010085, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7983%\n",
      "layer   2  Sparsity: 65.7157%\n",
      "layer   3  Sparsity: 63.6411%\n",
      "total_backward_count 244750 real_backward_count 33720  13.777%\n",
      "fc layer 1 self.abs_max_out: 7445.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.921958/  2.010922, val:  59.17%, val_best:  69.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7805%\n",
      "layer   2  Sparsity: 65.4977%\n",
      "layer   3  Sparsity: 63.2879%\n",
      "total_backward_count 254540 real_backward_count 34806  13.674%\n",
      "fc layer 1 self.abs_max_out: 7527.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.915059/  2.018490, val:  57.08%, val_best:  69.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7845%\n",
      "layer   2  Sparsity: 65.3454%\n",
      "layer   3  Sparsity: 63.4541%\n",
      "total_backward_count 264330 real_backward_count 35796  13.542%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.922273/  2.019071, val:  67.92%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.7523%\n",
      "layer   2  Sparsity: 64.9834%\n",
      "layer   3  Sparsity: 63.5913%\n",
      "total_backward_count 274120 real_backward_count 36807  13.427%\n",
      "lif layer 1 self.abs_max_v: 12576.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.923911/  2.034141, val:  59.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.7765%\n",
      "layer   2  Sparsity: 65.4530%\n",
      "layer   3  Sparsity: 63.6056%\n",
      "total_backward_count 283910 real_backward_count 37763  13.301%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.932503/  2.016450, val:  68.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7828%\n",
      "layer   2  Sparsity: 65.2571%\n",
      "layer   3  Sparsity: 63.0390%\n",
      "total_backward_count 293700 real_backward_count 38725  13.185%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.923477/  2.021203, val:  60.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8075%\n",
      "layer   2  Sparsity: 65.6304%\n",
      "layer   3  Sparsity: 63.5515%\n",
      "total_backward_count 303490 real_backward_count 39646  13.063%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.925655/  2.017863, val:  64.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7629%\n",
      "layer   2  Sparsity: 65.8899%\n",
      "layer   3  Sparsity: 64.2811%\n",
      "total_backward_count 313280 real_backward_count 40587  12.956%\n",
      "fc layer 1 self.abs_max_out: 7750.0\n",
      "lif layer 1 self.abs_max_v: 12687.5\n",
      "lif layer 1 self.abs_max_v: 12716.0\n",
      "lif layer 1 self.abs_max_v: 13058.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.926898/  2.020909, val:  67.50%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7797%\n",
      "layer   2  Sparsity: 65.7381%\n",
      "layer   3  Sparsity: 63.6471%\n",
      "total_backward_count 323070 real_backward_count 41452  12.831%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.922277/  2.016733, val:  68.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8107%\n",
      "layer   2  Sparsity: 65.8715%\n",
      "layer   3  Sparsity: 62.6664%\n",
      "total_backward_count 332860 real_backward_count 42395  12.737%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.923894/  2.004884, val:  54.17%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7842%\n",
      "layer   2  Sparsity: 65.8205%\n",
      "layer   3  Sparsity: 63.3069%\n",
      "total_backward_count 342650 real_backward_count 43259  12.625%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.915601/  2.009965, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7729%\n",
      "layer   2  Sparsity: 65.9470%\n",
      "layer   3  Sparsity: 63.8290%\n",
      "total_backward_count 352440 real_backward_count 44085  12.509%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.916071/  2.004848, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8143%\n",
      "layer   2  Sparsity: 65.7591%\n",
      "layer   3  Sparsity: 63.9832%\n",
      "total_backward_count 362230 real_backward_count 44917  12.400%\n",
      "fc layer 1 self.abs_max_out: 7888.0\n",
      "lif layer 1 self.abs_max_v: 13226.5\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.918985/  2.005403, val:  67.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.8032%\n",
      "layer   2  Sparsity: 65.6171%\n",
      "layer   3  Sparsity: 64.7193%\n",
      "total_backward_count 372020 real_backward_count 45674  12.277%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.920322/  2.015200, val:  65.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7529%\n",
      "layer   2  Sparsity: 65.6683%\n",
      "layer   3  Sparsity: 64.4098%\n",
      "total_backward_count 381810 real_backward_count 46507  12.181%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.920602/  2.025005, val:  69.58%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7959%\n",
      "layer   2  Sparsity: 65.4662%\n",
      "layer   3  Sparsity: 64.6429%\n",
      "total_backward_count 391600 real_backward_count 47321  12.084%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.919564/  2.004795, val:  71.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7745%\n",
      "layer   2  Sparsity: 65.0379%\n",
      "layer   3  Sparsity: 64.9021%\n",
      "total_backward_count 401390 real_backward_count 48156  11.997%\n",
      "fc layer 1 self.abs_max_out: 8022.0\n",
      "lif layer 1 self.abs_max_v: 13256.0\n",
      "lif layer 1 self.abs_max_v: 13828.0\n",
      "lif layer 1 self.abs_max_v: 14116.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.913043/  1.994597, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7860%\n",
      "layer   2  Sparsity: 65.6011%\n",
      "layer   3  Sparsity: 64.9770%\n",
      "total_backward_count 411180 real_backward_count 48973  11.910%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.904941/  1.999594, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.8115%\n",
      "layer   2  Sparsity: 65.6687%\n",
      "layer   3  Sparsity: 64.7747%\n",
      "total_backward_count 420970 real_backward_count 49813  11.833%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.913871/  1.998574, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.8196%\n",
      "layer   2  Sparsity: 65.4581%\n",
      "layer   3  Sparsity: 64.7601%\n",
      "total_backward_count 430760 real_backward_count 50600  11.747%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.910567/  2.015337, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8325%\n",
      "layer   2  Sparsity: 65.7400%\n",
      "layer   3  Sparsity: 64.8228%\n",
      "total_backward_count 440550 real_backward_count 51402  11.668%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.915304/  1.992681, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7668%\n",
      "layer   2  Sparsity: 65.3845%\n",
      "layer   3  Sparsity: 64.2239%\n",
      "total_backward_count 450340 real_backward_count 52184  11.588%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.913958/  2.019132, val:  61.25%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7686%\n",
      "layer   2  Sparsity: 65.4462%\n",
      "layer   3  Sparsity: 64.5765%\n",
      "total_backward_count 460130 real_backward_count 52946  11.507%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.904021/  2.000400, val:  52.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8104%\n",
      "layer   2  Sparsity: 65.2766%\n",
      "layer   3  Sparsity: 64.9415%\n",
      "total_backward_count 469920 real_backward_count 53661  11.419%\n",
      "fc layer 2 self.abs_max_out: 4484.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.905319/  2.004713, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7575%\n",
      "layer   2  Sparsity: 65.6599%\n",
      "layer   3  Sparsity: 65.8406%\n",
      "total_backward_count 479710 real_backward_count 54387  11.337%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.911866/  1.994685, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7739%\n",
      "layer   2  Sparsity: 65.0611%\n",
      "layer   3  Sparsity: 65.2021%\n",
      "total_backward_count 489500 real_backward_count 55111  11.259%\n",
      "fc layer 1 self.abs_max_out: 8039.0\n",
      "lif layer 1 self.abs_max_v: 14293.5\n",
      "lif layer 1 self.abs_max_v: 14448.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.911512/  1.995773, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7952%\n",
      "layer   2  Sparsity: 64.7790%\n",
      "layer   3  Sparsity: 65.4409%\n",
      "total_backward_count 499290 real_backward_count 55859  11.188%\n",
      "lif layer 2 self.abs_max_v: 7911.5\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.904659/  1.992677, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7603%\n",
      "layer   2  Sparsity: 64.9661%\n",
      "layer   3  Sparsity: 64.7597%\n",
      "total_backward_count 509080 real_backward_count 56604  11.119%\n",
      "fc layer 2 self.abs_max_out: 4528.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.902209/  1.993763, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7834%\n",
      "layer   2  Sparsity: 65.3749%\n",
      "layer   3  Sparsity: 65.2610%\n",
      "total_backward_count 518870 real_backward_count 57316  11.046%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.900310/  1.983334, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.65 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7726%\n",
      "layer   2  Sparsity: 65.1572%\n",
      "layer   3  Sparsity: 64.9959%\n",
      "total_backward_count 528660 real_backward_count 58023  10.975%\n",
      "fc layer 1 self.abs_max_out: 8163.0\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.899968/  1.988371, val:  75.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7866%\n",
      "layer   2  Sparsity: 64.9726%\n",
      "layer   3  Sparsity: 64.8952%\n",
      "total_backward_count 538450 real_backward_count 58731  10.907%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.903418/  1.994485, val:  75.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.7980%\n",
      "layer   2  Sparsity: 65.1530%\n",
      "layer   3  Sparsity: 64.5548%\n",
      "total_backward_count 548240 real_backward_count 59427  10.840%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.901214/  1.993305, val:  70.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7657%\n",
      "layer   2  Sparsity: 65.0001%\n",
      "layer   3  Sparsity: 63.8863%\n",
      "total_backward_count 558030 real_backward_count 60138  10.777%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.904873/  1.996359, val:  74.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.8011%\n",
      "layer   2  Sparsity: 65.0019%\n",
      "layer   3  Sparsity: 65.0961%\n",
      "total_backward_count 567820 real_backward_count 60813  10.710%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.901198/  1.989536, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7902%\n",
      "layer   2  Sparsity: 65.0198%\n",
      "layer   3  Sparsity: 65.1929%\n",
      "total_backward_count 577610 real_backward_count 61485  10.645%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.898339/  2.001304, val:  58.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8006%\n",
      "layer   2  Sparsity: 64.8032%\n",
      "layer   3  Sparsity: 65.8285%\n",
      "total_backward_count 587400 real_backward_count 62170  10.584%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.903947/  1.999198, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7924%\n",
      "layer   2  Sparsity: 65.1032%\n",
      "layer   3  Sparsity: 66.0986%\n",
      "total_backward_count 597190 real_backward_count 62849  10.524%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.902694/  1.982680, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7659%\n",
      "layer   2  Sparsity: 65.3545%\n",
      "layer   3  Sparsity: 66.2402%\n",
      "total_backward_count 606980 real_backward_count 63530  10.467%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.905578/  1.994867, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7859%\n",
      "layer   2  Sparsity: 65.0614%\n",
      "layer   3  Sparsity: 66.2663%\n",
      "total_backward_count 616770 real_backward_count 64192  10.408%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.915271/  2.007427, val:  70.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.8105%\n",
      "layer   2  Sparsity: 64.9474%\n",
      "layer   3  Sparsity: 65.5406%\n",
      "total_backward_count 626560 real_backward_count 64810  10.344%\n",
      "fc layer 1 self.abs_max_out: 8323.0\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.904846/  1.998876, val:  79.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.8107%\n",
      "layer   2  Sparsity: 64.9269%\n",
      "layer   3  Sparsity: 64.7961%\n",
      "total_backward_count 636350 real_backward_count 65521  10.296%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.905697/  1.998728, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.53 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7843%\n",
      "layer   2  Sparsity: 64.9070%\n",
      "layer   3  Sparsity: 65.5034%\n",
      "total_backward_count 646140 real_backward_count 66167  10.240%\n",
      "fc layer 2 self.abs_max_out: 4678.0\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.906113/  1.998735, val:  68.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7791%\n",
      "layer   2  Sparsity: 65.0343%\n",
      "layer   3  Sparsity: 66.0669%\n",
      "total_backward_count 655930 real_backward_count 66777  10.181%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.901413/  1.989343, val:  82.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.7646%\n",
      "layer   2  Sparsity: 65.3265%\n",
      "layer   3  Sparsity: 65.9854%\n",
      "total_backward_count 665720 real_backward_count 67404  10.125%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.903425/  1.975890, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7718%\n",
      "layer   2  Sparsity: 65.4390%\n",
      "layer   3  Sparsity: 65.8395%\n",
      "total_backward_count 675510 real_backward_count 67952  10.059%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.893002/  2.003124, val:  57.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7813%\n",
      "layer   2  Sparsity: 65.3110%\n",
      "layer   3  Sparsity: 66.2007%\n",
      "total_backward_count 685300 real_backward_count 68560  10.004%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.898777/  1.995241, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7991%\n",
      "layer   2  Sparsity: 65.3829%\n",
      "layer   3  Sparsity: 66.2756%\n",
      "total_backward_count 695090 real_backward_count 69222   9.959%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.907165/  1.999037, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7770%\n",
      "layer   2  Sparsity: 64.8792%\n",
      "layer   3  Sparsity: 65.8252%\n",
      "total_backward_count 704880 real_backward_count 69811   9.904%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.899897/  1.971661, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.80 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.7871%\n",
      "layer   2  Sparsity: 64.6623%\n",
      "layer   3  Sparsity: 65.6240%\n",
      "total_backward_count 714670 real_backward_count 70426   9.854%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.888214/  1.981781, val:  74.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7847%\n",
      "layer   2  Sparsity: 64.6988%\n",
      "layer   3  Sparsity: 65.8466%\n",
      "total_backward_count 724460 real_backward_count 71030   9.805%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.887931/  1.973920, val:  82.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7954%\n",
      "layer   2  Sparsity: 64.9155%\n",
      "layer   3  Sparsity: 65.9971%\n",
      "total_backward_count 734250 real_backward_count 71671   9.761%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.881519/  1.962438, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7797%\n",
      "layer   2  Sparsity: 64.8377%\n",
      "layer   3  Sparsity: 65.9218%\n",
      "total_backward_count 744040 real_backward_count 72293   9.716%\n",
      "lif layer 2 self.abs_max_v: 7915.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.882807/  1.970008, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7966%\n",
      "layer   2  Sparsity: 64.8170%\n",
      "layer   3  Sparsity: 66.2065%\n",
      "total_backward_count 753830 real_backward_count 72889   9.669%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.887606/  1.980641, val:  65.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7985%\n",
      "layer   2  Sparsity: 64.6538%\n",
      "layer   3  Sparsity: 66.3687%\n",
      "total_backward_count 763620 real_backward_count 73440   9.617%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.893398/  1.987145, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.8097%\n",
      "layer   2  Sparsity: 64.3025%\n",
      "layer   3  Sparsity: 66.2263%\n",
      "total_backward_count 773410 real_backward_count 74000   9.568%\n",
      "lif layer 2 self.abs_max_v: 7996.5\n",
      "lif layer 2 self.abs_max_v: 8297.5\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.895431/  1.986866, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7826%\n",
      "layer   2  Sparsity: 64.7379%\n",
      "layer   3  Sparsity: 66.2645%\n",
      "total_backward_count 783200 real_backward_count 74581   9.523%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.894887/  1.982692, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7671%\n",
      "layer   2  Sparsity: 64.7989%\n",
      "layer   3  Sparsity: 66.6056%\n",
      "total_backward_count 792990 real_backward_count 75202   9.483%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.894260/  1.989373, val:  73.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7929%\n",
      "layer   2  Sparsity: 64.4900%\n",
      "layer   3  Sparsity: 65.9274%\n",
      "total_backward_count 802780 real_backward_count 75794   9.441%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.893997/  1.979583, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7850%\n",
      "layer   2  Sparsity: 64.8571%\n",
      "layer   3  Sparsity: 66.6650%\n",
      "total_backward_count 812570 real_backward_count 76335   9.394%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.883976/  1.978301, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7642%\n",
      "layer   2  Sparsity: 64.9169%\n",
      "layer   3  Sparsity: 66.8085%\n",
      "total_backward_count 822360 real_backward_count 76878   9.348%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.887126/  1.966447, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7835%\n",
      "layer   2  Sparsity: 64.6845%\n",
      "layer   3  Sparsity: 65.6791%\n",
      "total_backward_count 832150 real_backward_count 77459   9.308%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.878271/  1.968578, val:  78.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7909%\n",
      "layer   2  Sparsity: 64.6387%\n",
      "layer   3  Sparsity: 65.0510%\n",
      "total_backward_count 841940 real_backward_count 78031   9.268%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.874548/  1.973228, val:  74.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.7740%\n",
      "layer   2  Sparsity: 64.7844%\n",
      "layer   3  Sparsity: 65.8803%\n",
      "total_backward_count 851730 real_backward_count 78591   9.227%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.875229/  1.964865, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.7442%\n",
      "layer   2  Sparsity: 64.5464%\n",
      "layer   3  Sparsity: 66.5247%\n",
      "total_backward_count 861520 real_backward_count 79126   9.184%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.876066/  1.956043, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.7630%\n",
      "layer   2  Sparsity: 64.5252%\n",
      "layer   3  Sparsity: 66.1723%\n",
      "total_backward_count 871310 real_backward_count 79703   9.147%\n",
      "fc layer 2 self.abs_max_out: 4817.0\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.875676/  1.967796, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7957%\n",
      "layer   2  Sparsity: 64.5137%\n",
      "layer   3  Sparsity: 65.6790%\n",
      "total_backward_count 881100 real_backward_count 80264   9.110%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.882008/  1.975409, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.75 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 91.7900%\n",
      "layer   2  Sparsity: 64.4656%\n",
      "layer   3  Sparsity: 66.1263%\n",
      "total_backward_count 890890 real_backward_count 80871   9.078%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.877532/  1.976627, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.31 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7593%\n",
      "layer   2  Sparsity: 64.3084%\n",
      "layer   3  Sparsity: 66.4368%\n",
      "total_backward_count 900680 real_backward_count 81390   9.037%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.877279/  1.967094, val:  78.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.21 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8035%\n",
      "layer   2  Sparsity: 64.3808%\n",
      "layer   3  Sparsity: 66.5381%\n",
      "total_backward_count 910470 real_backward_count 81892   8.994%\n",
      "fc layer 1 self.abs_max_out: 8343.0\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.883563/  1.974025, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.36 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8003%\n",
      "layer   2  Sparsity: 64.2854%\n",
      "layer   3  Sparsity: 66.3839%\n",
      "total_backward_count 920260 real_backward_count 82403   8.954%\n",
      "fc layer 1 self.abs_max_out: 8399.0\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.873424/  1.959643, val:  76.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.20 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8077%\n",
      "layer   2  Sparsity: 64.4568%\n",
      "layer   3  Sparsity: 65.6736%\n",
      "total_backward_count 930050 real_backward_count 82947   8.919%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.866965/  1.962961, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 89.20 seconds, 1.49 minutes\n",
      "layer   1  Sparsity: 91.8058%\n",
      "layer   2  Sparsity: 64.6157%\n",
      "layer   3  Sparsity: 65.7793%\n",
      "total_backward_count 939840 real_backward_count 83497   8.884%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.871549/  1.974403, val:  69.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.78 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7576%\n",
      "layer   2  Sparsity: 64.6286%\n",
      "layer   3  Sparsity: 66.1142%\n",
      "total_backward_count 949630 real_backward_count 84048   8.851%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.875231/  1.960055, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.03 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7934%\n",
      "layer   2  Sparsity: 64.3412%\n",
      "layer   3  Sparsity: 65.2947%\n",
      "total_backward_count 959420 real_backward_count 84591   8.817%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.872007/  1.966855, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7919%\n",
      "layer   2  Sparsity: 64.2581%\n",
      "layer   3  Sparsity: 65.5095%\n",
      "total_backward_count 969210 real_backward_count 85094   8.780%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.876666/  1.964630, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7912%\n",
      "layer   2  Sparsity: 64.3512%\n",
      "layer   3  Sparsity: 65.8587%\n",
      "total_backward_count 979000 real_backward_count 85603   8.744%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.873032/  1.965029, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.93 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7827%\n",
      "layer   2  Sparsity: 64.3162%\n",
      "layer   3  Sparsity: 65.8790%\n",
      "total_backward_count 988790 real_backward_count 86115   8.709%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.867184/  1.965270, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.14 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7790%\n",
      "layer   2  Sparsity: 64.9169%\n",
      "layer   3  Sparsity: 66.2571%\n",
      "total_backward_count 998580 real_backward_count 86651   8.677%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.870591/  1.957029, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8299%\n",
      "layer   2  Sparsity: 64.6028%\n",
      "layer   3  Sparsity: 65.8869%\n",
      "total_backward_count 1008370 real_backward_count 87130   8.641%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.868732/  1.957970, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7809%\n",
      "layer   2  Sparsity: 64.3628%\n",
      "layer   3  Sparsity: 66.3315%\n",
      "total_backward_count 1018160 real_backward_count 87653   8.609%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.870384/  1.965646, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.06 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8122%\n",
      "layer   2  Sparsity: 64.2236%\n",
      "layer   3  Sparsity: 66.8535%\n",
      "total_backward_count 1027950 real_backward_count 88123   8.573%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.876980/  1.967415, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.35 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7834%\n",
      "layer   2  Sparsity: 63.9238%\n",
      "layer   3  Sparsity: 66.8485%\n",
      "total_backward_count 1037740 real_backward_count 88641   8.542%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.869487/  1.962972, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.10 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7942%\n",
      "layer   2  Sparsity: 64.2973%\n",
      "layer   3  Sparsity: 66.5169%\n",
      "total_backward_count 1047530 real_backward_count 89155   8.511%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.868378/  1.954737, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7886%\n",
      "layer   2  Sparsity: 64.3739%\n",
      "layer   3  Sparsity: 66.8257%\n",
      "total_backward_count 1057320 real_backward_count 89657   8.480%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.869708/  1.959898, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.11 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7789%\n",
      "layer   2  Sparsity: 64.1591%\n",
      "layer   3  Sparsity: 66.6273%\n",
      "total_backward_count 1067110 real_backward_count 90153   8.448%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.870940/  1.962675, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.61 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 91.7520%\n",
      "layer   2  Sparsity: 64.2439%\n",
      "layer   3  Sparsity: 66.7329%\n",
      "total_backward_count 1076900 real_backward_count 90645   8.417%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.866006/  1.960019, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.22 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8044%\n",
      "layer   2  Sparsity: 64.4574%\n",
      "layer   3  Sparsity: 67.1913%\n",
      "total_backward_count 1086690 real_backward_count 91134   8.386%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.867722/  1.963990, val:  78.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7987%\n",
      "layer   2  Sparsity: 64.5489%\n",
      "layer   3  Sparsity: 67.0904%\n",
      "total_backward_count 1096480 real_backward_count 91614   8.355%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.866764/  1.957460, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.11 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8009%\n",
      "layer   2  Sparsity: 64.6921%\n",
      "layer   3  Sparsity: 66.9866%\n",
      "total_backward_count 1106270 real_backward_count 92123   8.327%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.868904/  1.966428, val:  77.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7416%\n",
      "layer   2  Sparsity: 64.4781%\n",
      "layer   3  Sparsity: 65.9072%\n",
      "total_backward_count 1116060 real_backward_count 92596   8.297%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.864601/  1.955860, val:  76.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.68 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7946%\n",
      "layer   2  Sparsity: 64.2235%\n",
      "layer   3  Sparsity: 66.6267%\n",
      "total_backward_count 1125850 real_backward_count 93102   8.269%\n",
      "fc layer 2 self.abs_max_out: 4847.0\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.856145/  1.949260, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.47 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7880%\n",
      "layer   2  Sparsity: 64.1503%\n",
      "layer   3  Sparsity: 67.2774%\n",
      "total_backward_count 1135640 real_backward_count 93603   8.242%\n",
      "lif layer 2 self.abs_max_v: 8328.5\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.855858/  1.957347, val:  80.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.49 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7789%\n",
      "layer   2  Sparsity: 64.3845%\n",
      "layer   3  Sparsity: 67.1985%\n",
      "total_backward_count 1145430 real_backward_count 94081   8.214%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.857380/  1.944737, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.84 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8006%\n",
      "layer   2  Sparsity: 64.4455%\n",
      "layer   3  Sparsity: 67.6845%\n",
      "total_backward_count 1155220 real_backward_count 94523   8.182%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.853262/  1.949522, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7673%\n",
      "layer   2  Sparsity: 64.4286%\n",
      "layer   3  Sparsity: 67.0986%\n",
      "total_backward_count 1165010 real_backward_count 94988   8.153%\n",
      "fc layer 2 self.abs_max_out: 4849.0\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.857347/  1.955572, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.54 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 91.7747%\n",
      "layer   2  Sparsity: 64.6688%\n",
      "layer   3  Sparsity: 67.1216%\n",
      "total_backward_count 1174800 real_backward_count 95468   8.126%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.859085/  1.960472, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7713%\n",
      "layer   2  Sparsity: 64.4199%\n",
      "layer   3  Sparsity: 66.6861%\n",
      "total_backward_count 1184590 real_backward_count 95936   8.099%\n",
      "fc layer 2 self.abs_max_out: 4852.0\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.868371/  1.969691, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.87 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7957%\n",
      "layer   2  Sparsity: 64.2058%\n",
      "layer   3  Sparsity: 66.9654%\n",
      "total_backward_count 1194380 real_backward_count 96358   8.068%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.868209/  1.963430, val:  81.25%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7758%\n",
      "layer   2  Sparsity: 64.3688%\n",
      "layer   3  Sparsity: 67.2133%\n",
      "total_backward_count 1204170 real_backward_count 96846   8.043%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.867738/  1.959464, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.98 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7779%\n",
      "layer   2  Sparsity: 64.5643%\n",
      "layer   3  Sparsity: 66.7879%\n",
      "total_backward_count 1213960 real_backward_count 97309   8.016%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.859761/  1.946218, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.46 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8047%\n",
      "layer   2  Sparsity: 64.3022%\n",
      "layer   3  Sparsity: 67.3606%\n",
      "total_backward_count 1223750 real_backward_count 97730   7.986%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.859521/  1.954326, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.37 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7578%\n",
      "layer   2  Sparsity: 64.2054%\n",
      "layer   3  Sparsity: 67.4649%\n",
      "total_backward_count 1233540 real_backward_count 98169   7.958%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.854997/  1.947503, val:  72.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8127%\n",
      "layer   2  Sparsity: 64.0311%\n",
      "layer   3  Sparsity: 67.6174%\n",
      "total_backward_count 1243330 real_backward_count 98616   7.932%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.855389/  1.958329, val:  82.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.68 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8070%\n",
      "layer   2  Sparsity: 64.2877%\n",
      "layer   3  Sparsity: 67.0750%\n",
      "total_backward_count 1253120 real_backward_count 99081   7.907%\n",
      "lif layer 1 self.abs_max_v: 14624.5\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.864406/  1.963003, val:  77.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7689%\n",
      "layer   2  Sparsity: 64.1490%\n",
      "layer   3  Sparsity: 67.4547%\n",
      "total_backward_count 1262910 real_backward_count 99547   7.882%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.864234/  1.951885, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7827%\n",
      "layer   2  Sparsity: 64.0365%\n",
      "layer   3  Sparsity: 67.7096%\n",
      "total_backward_count 1272700 real_backward_count 99963   7.854%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.860887/  1.948606, val:  71.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.57 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7685%\n",
      "layer   2  Sparsity: 64.1191%\n",
      "layer   3  Sparsity: 67.4148%\n",
      "total_backward_count 1282490 real_backward_count 100429   7.831%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.864097/  1.952543, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7790%\n",
      "layer   2  Sparsity: 64.0281%\n",
      "layer   3  Sparsity: 67.8318%\n",
      "total_backward_count 1292280 real_backward_count 100859   7.805%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.857097/  1.946247, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7948%\n",
      "layer   2  Sparsity: 64.2927%\n",
      "layer   3  Sparsity: 67.9562%\n",
      "total_backward_count 1302070 real_backward_count 101257   7.777%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.856150/  1.950630, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.01 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7919%\n",
      "layer   2  Sparsity: 64.3097%\n",
      "layer   3  Sparsity: 67.3121%\n",
      "total_backward_count 1311860 real_backward_count 101716   7.754%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.852060/  1.942433, val:  81.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7804%\n",
      "layer   2  Sparsity: 64.4273%\n",
      "layer   3  Sparsity: 66.6267%\n",
      "total_backward_count 1321650 real_backward_count 102184   7.732%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.846600/  1.939558, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.63 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7657%\n",
      "layer   2  Sparsity: 64.4087%\n",
      "layer   3  Sparsity: 66.5655%\n",
      "total_backward_count 1331440 real_backward_count 102625   7.708%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.845770/  1.934066, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.03 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7906%\n",
      "layer   2  Sparsity: 64.3477%\n",
      "layer   3  Sparsity: 67.4781%\n",
      "total_backward_count 1341230 real_backward_count 103066   7.684%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.851181/  1.938210, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8071%\n",
      "layer   2  Sparsity: 64.4175%\n",
      "layer   3  Sparsity: 67.3356%\n",
      "total_backward_count 1351020 real_backward_count 103477   7.659%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.845347/  1.945048, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.91 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7744%\n",
      "layer   2  Sparsity: 64.4196%\n",
      "layer   3  Sparsity: 67.4504%\n",
      "total_backward_count 1360810 real_backward_count 103909   7.636%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.846696/  1.924492, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7928%\n",
      "layer   2  Sparsity: 64.3075%\n",
      "layer   3  Sparsity: 66.8329%\n",
      "total_backward_count 1370600 real_backward_count 104380   7.616%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.850721/  1.949113, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.99 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7503%\n",
      "layer   2  Sparsity: 64.1342%\n",
      "layer   3  Sparsity: 66.5628%\n",
      "total_backward_count 1380390 real_backward_count 104816   7.593%\n",
      "fc layer 2 self.abs_max_out: 4878.0\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.851942/  1.940149, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7887%\n",
      "layer   2  Sparsity: 64.2099%\n",
      "layer   3  Sparsity: 66.6639%\n",
      "total_backward_count 1390180 real_backward_count 105245   7.571%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.843171/  1.938843, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7677%\n",
      "layer   2  Sparsity: 64.2768%\n",
      "layer   3  Sparsity: 67.5469%\n",
      "total_backward_count 1399970 real_backward_count 105689   7.549%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.845272/  1.944213, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.60 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7645%\n",
      "layer   2  Sparsity: 64.4449%\n",
      "layer   3  Sparsity: 67.8229%\n",
      "total_backward_count 1409760 real_backward_count 106105   7.526%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.846149/  1.942765, val:  79.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7982%\n",
      "layer   2  Sparsity: 64.3495%\n",
      "layer   3  Sparsity: 68.5358%\n",
      "total_backward_count 1419550 real_backward_count 106508   7.503%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.853794/  1.946912, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8049%\n",
      "layer   2  Sparsity: 64.2382%\n",
      "layer   3  Sparsity: 67.5966%\n",
      "total_backward_count 1429340 real_backward_count 106877   7.477%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.844700/  1.951757, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7976%\n",
      "layer   2  Sparsity: 64.1442%\n",
      "layer   3  Sparsity: 67.4711%\n",
      "total_backward_count 1439130 real_backward_count 107288   7.455%\n",
      "fc layer 2 self.abs_max_out: 4976.0\n",
      "lif layer 2 self.abs_max_v: 8355.0\n",
      "lif layer 2 self.abs_max_v: 8655.5\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.845838/  1.944063, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.04 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7794%\n",
      "layer   2  Sparsity: 64.2495%\n",
      "layer   3  Sparsity: 68.0630%\n",
      "total_backward_count 1448920 real_backward_count 107672   7.431%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.848682/  1.939765, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7925%\n",
      "layer   2  Sparsity: 64.4422%\n",
      "layer   3  Sparsity: 67.5070%\n",
      "total_backward_count 1458710 real_backward_count 108084   7.410%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.841545/  1.936915, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.68 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7818%\n",
      "layer   2  Sparsity: 64.1166%\n",
      "layer   3  Sparsity: 67.5800%\n",
      "total_backward_count 1468500 real_backward_count 108472   7.387%\n",
      "lif layer 2 self.abs_max_v: 8750.5\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.843430/  1.944322, val:  72.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.10 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8275%\n",
      "layer   2  Sparsity: 63.7667%\n",
      "layer   3  Sparsity: 67.6914%\n",
      "total_backward_count 1478290 real_backward_count 108832   7.362%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.837152/  1.934086, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7953%\n",
      "layer   2  Sparsity: 64.1986%\n",
      "layer   3  Sparsity: 68.2977%\n",
      "total_backward_count 1488080 real_backward_count 109217   7.339%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.828707/  1.917118, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.61 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7947%\n",
      "layer   2  Sparsity: 64.2857%\n",
      "layer   3  Sparsity: 68.6249%\n",
      "total_backward_count 1497870 real_backward_count 109587   7.316%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.837917/  1.928534, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7743%\n",
      "layer   2  Sparsity: 64.4306%\n",
      "layer   3  Sparsity: 68.0815%\n",
      "total_backward_count 1507660 real_backward_count 110011   7.297%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.840082/  1.935492, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7716%\n",
      "layer   2  Sparsity: 64.3925%\n",
      "layer   3  Sparsity: 68.0505%\n",
      "total_backward_count 1517450 real_backward_count 110399   7.275%\n",
      "fc layer 2 self.abs_max_out: 4984.0\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.838556/  1.931830, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8049%\n",
      "layer   2  Sparsity: 64.2642%\n",
      "layer   3  Sparsity: 68.3161%\n",
      "total_backward_count 1527240 real_backward_count 110788   7.254%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.834687/  1.930707, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.96 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7869%\n",
      "layer   2  Sparsity: 64.2915%\n",
      "layer   3  Sparsity: 67.9886%\n",
      "total_backward_count 1537030 real_backward_count 111194   7.234%\n",
      "fc layer 1 self.abs_max_out: 8417.0\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.835093/  1.928064, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7766%\n",
      "layer   2  Sparsity: 64.4843%\n",
      "layer   3  Sparsity: 67.9084%\n",
      "total_backward_count 1546820 real_backward_count 111560   7.212%\n",
      "fc layer 1 self.abs_max_out: 8527.0\n",
      "fc layer 2 self.abs_max_out: 5056.0\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.830520/  1.927636, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.20 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.8059%\n",
      "layer   2  Sparsity: 64.4901%\n",
      "layer   3  Sparsity: 68.0504%\n",
      "total_backward_count 1556610 real_backward_count 111964   7.193%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.830032/  1.930966, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.87 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8013%\n",
      "layer   2  Sparsity: 64.4121%\n",
      "layer   3  Sparsity: 67.7306%\n",
      "total_backward_count 1566400 real_backward_count 112302   7.169%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.827586/  1.920757, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.47 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8097%\n",
      "layer   2  Sparsity: 64.1582%\n",
      "layer   3  Sparsity: 67.2445%\n",
      "total_backward_count 1576190 real_backward_count 112677   7.149%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.819241/  1.922610, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7895%\n",
      "layer   2  Sparsity: 64.4454%\n",
      "layer   3  Sparsity: 67.4166%\n",
      "total_backward_count 1585980 real_backward_count 113071   7.129%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.820867/  1.920085, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7794%\n",
      "layer   2  Sparsity: 64.5641%\n",
      "layer   3  Sparsity: 67.6680%\n",
      "total_backward_count 1595770 real_backward_count 113450   7.109%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.821716/  1.922711, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7827%\n",
      "layer   2  Sparsity: 64.7196%\n",
      "layer   3  Sparsity: 68.4156%\n",
      "total_backward_count 1605560 real_backward_count 113804   7.088%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.831286/  1.923488, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7795%\n",
      "layer   2  Sparsity: 64.6468%\n",
      "layer   3  Sparsity: 68.6125%\n",
      "total_backward_count 1615350 real_backward_count 114158   7.067%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.827908/  1.923863, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.77 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7890%\n",
      "layer   2  Sparsity: 64.2972%\n",
      "layer   3  Sparsity: 68.0758%\n",
      "total_backward_count 1625140 real_backward_count 114538   7.048%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.827967/  1.921897, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.93 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7975%\n",
      "layer   2  Sparsity: 64.3276%\n",
      "layer   3  Sparsity: 67.8297%\n",
      "total_backward_count 1634930 real_backward_count 114885   7.027%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.829752/  1.926887, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.98 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7698%\n",
      "layer   2  Sparsity: 64.2951%\n",
      "layer   3  Sparsity: 67.3053%\n",
      "total_backward_count 1644720 real_backward_count 115262   7.008%\n",
      "fc layer 2 self.abs_max_out: 5098.0\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.834603/  1.929324, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.8173%\n",
      "layer   2  Sparsity: 64.1314%\n",
      "layer   3  Sparsity: 67.5792%\n",
      "total_backward_count 1654510 real_backward_count 115621   6.988%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.828424/  1.930622, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7904%\n",
      "layer   2  Sparsity: 64.1701%\n",
      "layer   3  Sparsity: 68.2770%\n",
      "total_backward_count 1664300 real_backward_count 115975   6.968%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.831182/  1.926946, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.23 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 91.7900%\n",
      "layer   2  Sparsity: 63.9196%\n",
      "layer   3  Sparsity: 68.5697%\n",
      "total_backward_count 1674090 real_backward_count 116326   6.949%\n",
      "fc layer 1 self.abs_max_out: 8554.0\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.829850/  1.936420, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.16 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7593%\n",
      "layer   2  Sparsity: 63.9676%\n",
      "layer   3  Sparsity: 68.4958%\n",
      "total_backward_count 1683880 real_backward_count 116649   6.927%\n",
      "fc layer 1 self.abs_max_out: 8565.0\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.826834/  1.923307, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.05 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7748%\n",
      "layer   2  Sparsity: 64.2704%\n",
      "layer   3  Sparsity: 68.0707%\n",
      "total_backward_count 1693670 real_backward_count 117033   6.910%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.823745/  1.923243, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.66 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7722%\n",
      "layer   2  Sparsity: 64.0920%\n",
      "layer   3  Sparsity: 68.4255%\n",
      "total_backward_count 1703460 real_backward_count 117397   6.892%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.832670/  1.925810, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7664%\n",
      "layer   2  Sparsity: 63.8549%\n",
      "layer   3  Sparsity: 67.7049%\n",
      "total_backward_count 1713250 real_backward_count 117768   6.874%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.830211/  1.926734, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7560%\n",
      "layer   2  Sparsity: 63.9840%\n",
      "layer   3  Sparsity: 68.8204%\n",
      "total_backward_count 1723040 real_backward_count 118108   6.855%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.821961/  1.924319, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.13 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7857%\n",
      "layer   2  Sparsity: 64.0961%\n",
      "layer   3  Sparsity: 69.2753%\n",
      "total_backward_count 1732830 real_backward_count 118424   6.834%\n",
      "fc layer 2 self.abs_max_out: 5260.0\n",
      "lif layer 1 self.abs_max_v: 14648.0\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.825227/  1.932807, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.31 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7837%\n",
      "layer   2  Sparsity: 63.9144%\n",
      "layer   3  Sparsity: 68.9842%\n",
      "total_backward_count 1742620 real_backward_count 118783   6.816%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.830520/  1.934424, val:  79.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.19 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7719%\n",
      "layer   2  Sparsity: 63.9067%\n",
      "layer   3  Sparsity: 68.1098%\n",
      "total_backward_count 1752410 real_backward_count 119135   6.798%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.823123/  1.926943, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.13 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7942%\n",
      "layer   2  Sparsity: 63.8991%\n",
      "layer   3  Sparsity: 67.6160%\n",
      "total_backward_count 1762200 real_backward_count 119555   6.784%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.820670/  1.920184, val:  78.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7643%\n",
      "layer   2  Sparsity: 63.7084%\n",
      "layer   3  Sparsity: 67.6594%\n",
      "total_backward_count 1771990 real_backward_count 119936   6.768%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.819943/  1.919050, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.20 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7951%\n",
      "layer   2  Sparsity: 63.7729%\n",
      "layer   3  Sparsity: 67.9363%\n",
      "total_backward_count 1781780 real_backward_count 120278   6.750%\n",
      "fc layer 1 self.abs_max_out: 8598.0\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.823452/  1.922894, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.72 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7569%\n",
      "layer   2  Sparsity: 63.9892%\n",
      "layer   3  Sparsity: 68.2555%\n",
      "total_backward_count 1791570 real_backward_count 120593   6.731%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.816339/  1.914156, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.16 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7780%\n",
      "layer   2  Sparsity: 63.9434%\n",
      "layer   3  Sparsity: 67.8666%\n",
      "total_backward_count 1801360 real_backward_count 120952   6.714%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.819176/  1.927738, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.50 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7892%\n",
      "layer   2  Sparsity: 63.9275%\n",
      "layer   3  Sparsity: 67.5920%\n",
      "total_backward_count 1811150 real_backward_count 121284   6.697%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.819139/  1.924287, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.07 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7604%\n",
      "layer   2  Sparsity: 64.0406%\n",
      "layer   3  Sparsity: 68.1874%\n",
      "total_backward_count 1820940 real_backward_count 121645   6.680%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.827174/  1.928248, val:  80.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7719%\n",
      "layer   2  Sparsity: 63.8970%\n",
      "layer   3  Sparsity: 68.2199%\n",
      "total_backward_count 1830730 real_backward_count 122002   6.664%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.823659/  1.934078, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7876%\n",
      "layer   2  Sparsity: 64.1572%\n",
      "layer   3  Sparsity: 68.0367%\n",
      "total_backward_count 1840520 real_backward_count 122342   6.647%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.824143/  1.925891, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.38 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7808%\n",
      "layer   2  Sparsity: 64.2145%\n",
      "layer   3  Sparsity: 68.4943%\n",
      "total_backward_count 1850310 real_backward_count 122650   6.629%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.822213/  1.922913, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7570%\n",
      "layer   2  Sparsity: 64.1902%\n",
      "layer   3  Sparsity: 69.2381%\n",
      "total_backward_count 1860100 real_backward_count 122981   6.612%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.822929/  1.919277, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.63 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7620%\n",
      "layer   2  Sparsity: 63.9611%\n",
      "layer   3  Sparsity: 69.0046%\n",
      "total_backward_count 1869890 real_backward_count 123276   6.593%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.815484/  1.913550, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.47 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7812%\n",
      "layer   2  Sparsity: 63.9845%\n",
      "layer   3  Sparsity: 68.2790%\n",
      "total_backward_count 1879680 real_backward_count 123562   6.574%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.814943/  1.915304, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7721%\n",
      "layer   2  Sparsity: 63.8754%\n",
      "layer   3  Sparsity: 68.2038%\n",
      "total_backward_count 1889470 real_backward_count 123916   6.558%\n",
      "lif layer 1 self.abs_max_v: 14652.5\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.812591/  1.925884, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.8045%\n",
      "layer   2  Sparsity: 63.7762%\n",
      "layer   3  Sparsity: 67.8536%\n",
      "total_backward_count 1899260 real_backward_count 124227   6.541%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.816510/  1.912976, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 91.7872%\n",
      "layer   2  Sparsity: 64.1457%\n",
      "layer   3  Sparsity: 68.0730%\n",
      "total_backward_count 1909050 real_backward_count 124558   6.525%\n",
      "fc layer 1 self.abs_max_out: 8600.0\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.805752/  1.903304, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.7496%\n",
      "layer   2  Sparsity: 64.0775%\n",
      "layer   3  Sparsity: 67.7947%\n",
      "total_backward_count 1918840 real_backward_count 124863   6.507%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.803341/  1.902485, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.94 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7988%\n",
      "layer   2  Sparsity: 64.1515%\n",
      "layer   3  Sparsity: 68.1877%\n",
      "total_backward_count 1928630 real_backward_count 125217   6.493%\n",
      "fc layer 2 self.abs_max_out: 5267.0\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.807311/  1.906449, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 91.8388%\n",
      "layer   2  Sparsity: 64.3646%\n",
      "layer   3  Sparsity: 68.1917%\n",
      "total_backward_count 1938420 real_backward_count 125560   6.477%\n",
      "fc layer 1 self.abs_max_out: 8632.0\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.807143/  1.908383, val:  77.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.00 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7802%\n",
      "layer   2  Sparsity: 64.3443%\n",
      "layer   3  Sparsity: 66.9876%\n",
      "total_backward_count 1948210 real_backward_count 125887   6.462%\n",
      "lif layer 1 self.abs_max_v: 14695.5\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.808685/  1.903441, val:  83.75%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.05 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 91.7946%\n",
      "layer   2  Sparsity: 64.4114%\n",
      "layer   3  Sparsity: 67.0541%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da3c07af8c48a98a6155c076a53037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.80869</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>1.90344</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-146</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cr7decnd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cr7decnd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_184602-cr7decnd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cd3extht with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_232713-cd3extht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cd3extht' target=\"_blank\">fearless-sweep-151</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cd3extht' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cd3extht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251117_232722_775', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 681.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "lif layer 2 self.abs_max_v: 1750.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 903.0\n",
      "fc layer 2 self.abs_max_out: 1919.0\n",
      "lif layer 2 self.abs_max_v: 2536.0\n",
      "fc layer 3 self.abs_max_out: 765.0\n",
      "lif layer 1 self.abs_max_v: 1181.0\n",
      "fc layer 2 self.abs_max_out: 2083.0\n",
      "lif layer 2 self.abs_max_v: 3045.0\n",
      "fc layer 1 self.abs_max_out: 989.0\n",
      "lif layer 1 self.abs_max_v: 1242.5\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1601.5\n",
      "lif layer 2 self.abs_max_v: 3055.5\n",
      "fc layer 1 self.abs_max_out: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1825.0\n",
      "fc layer 2 self.abs_max_out: 2133.0\n",
      "lif layer 2 self.abs_max_v: 3099.5\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "fc layer 2 self.abs_max_out: 2177.0\n",
      "lif layer 2 self.abs_max_v: 3637.5\n",
      "fc layer 1 self.abs_max_out: 1239.0\n",
      "fc layer 2 self.abs_max_out: 2499.0\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 1 self.abs_max_out: 1369.0\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "fc layer 1 self.abs_max_out: 1655.0\n",
      "lif layer 1 self.abs_max_v: 1978.5\n",
      "lif layer 1 self.abs_max_v: 2184.5\n",
      "fc layer 3 self.abs_max_out: 1019.0\n",
      "lif layer 1 self.abs_max_v: 2498.5\n",
      "fc layer 1 self.abs_max_out: 1807.0\n",
      "fc layer 2 self.abs_max_out: 2666.0\n",
      "fc layer 1 self.abs_max_out: 1925.0\n",
      "lif layer 1 self.abs_max_v: 2818.0\n",
      "fc layer 1 self.abs_max_out: 1932.0\n",
      "lif layer 1 self.abs_max_v: 3062.0\n",
      "fc layer 1 self.abs_max_out: 2065.0\n",
      "lif layer 1 self.abs_max_v: 3551.5\n",
      "lif layer 2 self.abs_max_v: 3654.5\n",
      "fc layer 1 self.abs_max_out: 2204.0\n",
      "fc layer 2 self.abs_max_out: 2722.0\n",
      "fc layer 2 self.abs_max_out: 3080.0\n",
      "lif layer 2 self.abs_max_v: 4008.5\n",
      "fc layer 1 self.abs_max_out: 2394.0\n",
      "lif layer 2 self.abs_max_v: 4015.0\n",
      "fc layer 1 self.abs_max_out: 2433.0\n",
      "fc layer 1 self.abs_max_out: 2434.0\n",
      "lif layer 2 self.abs_max_v: 4020.5\n",
      "lif layer 2 self.abs_max_v: 4216.0\n",
      "lif layer 2 self.abs_max_v: 4253.0\n",
      "lif layer 2 self.abs_max_v: 4462.5\n",
      "lif layer 1 self.abs_max_v: 3607.0\n",
      "lif layer 1 self.abs_max_v: 3818.0\n",
      "fc layer 1 self.abs_max_out: 2498.0\n",
      "lif layer 1 self.abs_max_v: 3938.5\n",
      "fc layer 1 self.abs_max_out: 2675.0\n",
      "fc layer 1 self.abs_max_out: 3289.0\n",
      "lif layer 2 self.abs_max_v: 4479.0\n",
      "lif layer 2 self.abs_max_v: 4664.0\n",
      "lif layer 1 self.abs_max_v: 4141.0\n",
      "lif layer 1 self.abs_max_v: 4771.5\n",
      "lif layer 1 self.abs_max_v: 4881.5\n",
      "lif layer 1 self.abs_max_v: 4965.0\n",
      "fc layer 2 self.abs_max_out: 3163.0\n",
      "lif layer 2 self.abs_max_v: 5010.5\n",
      "lif layer 2 self.abs_max_v: 5054.5\n",
      "fc layer 1 self.abs_max_out: 3426.0\n",
      "lif layer 1 self.abs_max_v: 5090.5\n",
      "lif layer 2 self.abs_max_v: 5323.0\n",
      "fc layer 2 self.abs_max_out: 3380.0\n",
      "lif layer 2 self.abs_max_v: 5555.0\n",
      "fc layer 1 self.abs_max_out: 3457.0\n",
      "fc layer 1 self.abs_max_out: 3658.0\n",
      "lif layer 1 self.abs_max_v: 5118.5\n",
      "lif layer 1 self.abs_max_v: 5281.5\n",
      "lif layer 1 self.abs_max_v: 5493.5\n",
      "fc layer 3 self.abs_max_out: 1042.0\n",
      "fc layer 2 self.abs_max_out: 3601.0\n",
      "lif layer 2 self.abs_max_v: 5621.5\n",
      "fc layer 3 self.abs_max_out: 1049.0\n",
      "lif layer 2 self.abs_max_v: 5725.5\n",
      "lif layer 2 self.abs_max_v: 5869.0\n",
      "fc layer 3 self.abs_max_out: 1117.0\n",
      "lif layer 1 self.abs_max_v: 5541.5\n",
      "lif layer 1 self.abs_max_v: 5613.5\n",
      "lif layer 1 self.abs_max_v: 5833.0\n",
      "fc layer 1 self.abs_max_out: 3762.0\n",
      "fc layer 1 self.abs_max_out: 3818.0\n",
      "lif layer 1 self.abs_max_v: 5952.5\n",
      "lif layer 1 self.abs_max_v: 6161.0\n",
      "lif layer 1 self.abs_max_v: 6312.0\n",
      "lif layer 1 self.abs_max_v: 6845.0\n",
      "lif layer 1 self.abs_max_v: 7171.5\n",
      "fc layer 1 self.abs_max_out: 3925.0\n",
      "lif layer 2 self.abs_max_v: 5893.5\n",
      "lif layer 2 self.abs_max_v: 6037.0\n",
      "lif layer 2 self.abs_max_v: 6271.5\n",
      "fc layer 1 self.abs_max_out: 4186.0\n",
      "fc layer 1 self.abs_max_out: 4271.0\n",
      "fc layer 1 self.abs_max_out: 4903.0\n",
      "lif layer 1 self.abs_max_v: 7881.0\n",
      "lif layer 1 self.abs_max_v: 8027.5\n",
      "lif layer 1 self.abs_max_v: 8500.0\n",
      "lif layer 1 self.abs_max_v: 8556.5\n",
      "fc layer 2 self.abs_max_out: 3680.0\n",
      "fc layer 3 self.abs_max_out: 1162.0\n",
      "fc layer 1 self.abs_max_out: 4920.0\n",
      "fc layer 1 self.abs_max_out: 5333.0\n",
      "fc layer 1 self.abs_max_out: 5344.0\n",
      "fc layer 1 self.abs_max_out: 5920.0\n",
      "lif layer 1 self.abs_max_v: 9748.0\n",
      "lif layer 1 self.abs_max_v: 9862.0\n",
      "lif layer 2 self.abs_max_v: 6286.0\n",
      "lif layer 2 self.abs_max_v: 6450.5\n",
      "lif layer 2 self.abs_max_v: 6580.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.934187/  2.034107, val:  35.83%, val_best:  35.83%, tr:  84.07%, tr_best:  84.07%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7313%\n",
      "layer   2  Sparsity: 64.0578%\n",
      "layer   3  Sparsity: 61.8183%\n",
      "total_backward_count 9790 real_backward_count 3196  32.646%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3742.0\n",
      "lif layer 1 self.abs_max_v: 10025.5\n",
      "lif layer 2 self.abs_max_v: 6597.5\n",
      "fc layer 3 self.abs_max_out: 1172.0\n",
      "fc layer 3 self.abs_max_out: 1180.0\n",
      "fc layer 2 self.abs_max_out: 3749.0\n",
      "lif layer 2 self.abs_max_v: 6811.0\n",
      "lif layer 2 self.abs_max_v: 7072.5\n",
      "fc layer 2 self.abs_max_out: 3877.0\n",
      "fc layer 1 self.abs_max_out: 6056.0\n",
      "fc layer 1 self.abs_max_out: 6396.0\n",
      "lif layer 1 self.abs_max_v: 10708.5\n",
      "lif layer 1 self.abs_max_v: 11109.5\n",
      "fc layer 2 self.abs_max_out: 4123.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.897401/  2.011864, val:  52.50%, val_best:  52.50%, tr:  94.38%, tr_best:  94.38%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7265%\n",
      "layer   2  Sparsity: 66.9005%\n",
      "layer   3  Sparsity: 62.4249%\n",
      "total_backward_count 19580 real_backward_count 5464  27.906%\n",
      "fc layer 2 self.abs_max_out: 4259.0\n",
      "fc layer 3 self.abs_max_out: 1223.0\n",
      "lif layer 2 self.abs_max_v: 7315.0\n",
      "fc layer 2 self.abs_max_out: 4340.0\n",
      "fc layer 2 self.abs_max_out: 4367.0\n",
      "fc layer 2 self.abs_max_out: 4419.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.886113/  2.022775, val:  52.50%, val_best:  52.50%, tr:  97.34%, tr_best:  97.34%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7154%\n",
      "layer   2  Sparsity: 67.8903%\n",
      "layer   3  Sparsity: 60.9856%\n",
      "total_backward_count 29370 real_backward_count 7349  25.022%\n",
      "fc layer 1 self.abs_max_out: 6551.0\n",
      "fc layer 2 self.abs_max_out: 4439.0\n",
      "fc layer 3 self.abs_max_out: 1242.0\n",
      "fc layer 1 self.abs_max_out: 6601.0\n",
      "fc layer 1 self.abs_max_out: 7207.0\n",
      "lif layer 1 self.abs_max_v: 11833.0\n",
      "lif layer 1 self.abs_max_v: 12009.5\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.886319/  2.002166, val:  50.42%, val_best:  52.50%, tr:  97.96%, tr_best:  97.96%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7299%\n",
      "layer   2  Sparsity: 69.6417%\n",
      "layer   3  Sparsity: 60.7689%\n",
      "total_backward_count 39160 real_backward_count 9122  23.294%\n",
      "lif layer 2 self.abs_max_v: 7322.5\n",
      "fc layer 2 self.abs_max_out: 4517.0\n",
      "fc layer 3 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 4523.0\n",
      "fc layer 2 self.abs_max_out: 4534.0\n",
      "fc layer 2 self.abs_max_out: 4553.0\n",
      "fc layer 2 self.abs_max_out: 4701.0\n",
      "fc layer 2 self.abs_max_out: 4711.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.882135/  2.005551, val:  47.92%, val_best:  52.50%, tr:  98.98%, tr_best:  98.98%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7267%\n",
      "layer   2  Sparsity: 68.3969%\n",
      "layer   3  Sparsity: 60.4550%\n",
      "total_backward_count 48950 real_backward_count 10717  21.894%\n",
      "fc layer 2 self.abs_max_out: 4753.0\n",
      "fc layer 2 self.abs_max_out: 4962.0\n",
      "lif layer 2 self.abs_max_v: 7381.0\n",
      "lif layer 2 self.abs_max_v: 7540.5\n",
      "lif layer 2 self.abs_max_v: 7778.5\n",
      "lif layer 2 self.abs_max_v: 8134.5\n",
      "fc layer 1 self.abs_max_out: 7507.0\n",
      "lif layer 1 self.abs_max_v: 12285.0\n",
      "lif layer 1 self.abs_max_v: 12526.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.877302/  2.006461, val:  62.08%, val_best:  62.08%, tr:  98.88%, tr_best:  98.98%, epoch time: 87.96 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7061%\n",
      "layer   2  Sparsity: 68.9401%\n",
      "layer   3  Sparsity: 60.3917%\n",
      "total_backward_count 58740 real_backward_count 12276  20.899%\n",
      "lif layer 2 self.abs_max_v: 8314.0\n",
      "fc layer 1 self.abs_max_out: 7621.0\n",
      "lif layer 1 self.abs_max_v: 12783.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.881530/  2.003357, val:  51.25%, val_best:  62.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 87.91 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7092%\n",
      "layer   2  Sparsity: 68.3687%\n",
      "layer   3  Sparsity: 59.5563%\n",
      "total_backward_count 68530 real_backward_count 13735  20.042%\n",
      "lif layer 2 self.abs_max_v: 8467.0\n",
      "lif layer 1 self.abs_max_v: 13132.5\n",
      "fc layer 1 self.abs_max_out: 7763.0\n",
      "lif layer 1 self.abs_max_v: 13774.5\n",
      "lif layer 1 self.abs_max_v: 14175.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.869489/  1.983335, val:  52.08%, val_best:  62.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7171%\n",
      "layer   2  Sparsity: 69.5004%\n",
      "layer   3  Sparsity: 59.6511%\n",
      "total_backward_count 78320 real_backward_count 15049  19.215%\n",
      "fc layer 1 self.abs_max_out: 8121.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.877964/  1.991755, val:  57.50%, val_best:  62.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7287%\n",
      "layer   2  Sparsity: 70.1128%\n",
      "layer   3  Sparsity: 60.2637%\n",
      "total_backward_count 88110 real_backward_count 16372  18.581%\n",
      "lif layer 2 self.abs_max_v: 8530.5\n",
      "lif layer 2 self.abs_max_v: 8831.5\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.873482/  2.005739, val:  52.08%, val_best:  62.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7217%\n",
      "layer   2  Sparsity: 69.2489%\n",
      "layer   3  Sparsity: 59.9205%\n",
      "total_backward_count 97900 real_backward_count 17624  18.002%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.868908/  1.990044, val:  53.75%, val_best:  62.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7437%\n",
      "layer   2  Sparsity: 69.1205%\n",
      "layer   3  Sparsity: 60.7887%\n",
      "total_backward_count 107690 real_backward_count 18861  17.514%\n",
      "fc layer 3 self.abs_max_out: 1263.0\n",
      "fc layer 2 self.abs_max_out: 4983.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.883963/  2.006172, val:  56.67%, val_best:  62.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7414%\n",
      "layer   2  Sparsity: 68.6468%\n",
      "layer   3  Sparsity: 60.6580%\n",
      "total_backward_count 117480 real_backward_count 20133  17.137%\n",
      "lif layer 2 self.abs_max_v: 8951.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.879205/  1.995732, val:  54.17%, val_best:  62.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7216%\n",
      "layer   2  Sparsity: 68.5839%\n",
      "layer   3  Sparsity: 60.6360%\n",
      "total_backward_count 127270 real_backward_count 21280  16.720%\n",
      "fc layer 2 self.abs_max_out: 5001.0\n",
      "lif layer 2 self.abs_max_v: 9153.5\n",
      "lif layer 2 self.abs_max_v: 9550.5\n",
      "fc layer 2 self.abs_max_out: 5097.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.890914/  2.005232, val:  54.58%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.18 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7280%\n",
      "layer   2  Sparsity: 67.7716%\n",
      "layer   3  Sparsity: 60.6992%\n",
      "total_backward_count 137060 real_backward_count 22364  16.317%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.885634/  1.990878, val:  53.33%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.09 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7242%\n",
      "layer   2  Sparsity: 66.9139%\n",
      "layer   3  Sparsity: 60.4853%\n",
      "total_backward_count 146850 real_backward_count 23426  15.952%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.889545/  2.018307, val:  55.83%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7216%\n",
      "layer   2  Sparsity: 67.5377%\n",
      "layer   3  Sparsity: 61.5400%\n",
      "total_backward_count 156640 real_backward_count 24573  15.688%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.891704/  2.000612, val:  60.83%, val_best:  62.08%, tr:  99.08%, tr_best: 100.00%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.6844%\n",
      "layer   2  Sparsity: 68.0333%\n",
      "layer   3  Sparsity: 62.9243%\n",
      "total_backward_count 166430 real_backward_count 25634  15.402%\n",
      "fc layer 2 self.abs_max_out: 5100.0\n",
      "fc layer 1 self.abs_max_out: 8179.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.899672/  2.003055, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7152%\n",
      "layer   2  Sparsity: 67.3123%\n",
      "layer   3  Sparsity: 63.2174%\n",
      "total_backward_count 176220 real_backward_count 26656  15.127%\n",
      "fc layer 2 self.abs_max_out: 5228.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.902583/  1.995759, val:  68.33%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.37 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7237%\n",
      "layer   2  Sparsity: 66.5019%\n",
      "layer   3  Sparsity: 63.4692%\n",
      "total_backward_count 186010 real_backward_count 27751  14.919%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.891855/  2.013804, val:  51.25%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7228%\n",
      "layer   2  Sparsity: 66.6389%\n",
      "layer   3  Sparsity: 63.5623%\n",
      "total_backward_count 195800 real_backward_count 28770  14.694%\n",
      "fc layer 1 self.abs_max_out: 8555.0\n",
      "lif layer 1 self.abs_max_v: 14352.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.899397/  2.001319, val:  60.00%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7058%\n",
      "layer   2  Sparsity: 66.7793%\n",
      "layer   3  Sparsity: 63.1569%\n",
      "total_backward_count 205590 real_backward_count 29743  14.467%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.893264/  2.009139, val:  59.58%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.24 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7526%\n",
      "layer   2  Sparsity: 66.3188%\n",
      "layer   3  Sparsity: 63.7678%\n",
      "total_backward_count 215380 real_backward_count 30778  14.290%\n",
      "fc layer 1 self.abs_max_out: 8579.0\n",
      "lif layer 1 self.abs_max_v: 14409.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.893666/  1.988117, val:  65.00%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7377%\n",
      "layer   2  Sparsity: 66.1425%\n",
      "layer   3  Sparsity: 63.2424%\n",
      "total_backward_count 225170 real_backward_count 31737  14.095%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.893968/  1.990683, val:  71.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7117%\n",
      "layer   2  Sparsity: 66.0618%\n",
      "layer   3  Sparsity: 63.6240%\n",
      "total_backward_count 234960 real_backward_count 32637  13.890%\n",
      "fc layer 2 self.abs_max_out: 5298.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.892543/  1.995553, val:  64.17%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7469%\n",
      "layer   2  Sparsity: 66.1365%\n",
      "layer   3  Sparsity: 64.4299%\n",
      "total_backward_count 244750 real_backward_count 33514  13.693%\n",
      "fc layer 1 self.abs_max_out: 8698.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.890165/  1.985211, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.77 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7338%\n",
      "layer   2  Sparsity: 66.3132%\n",
      "layer   3  Sparsity: 63.9350%\n",
      "total_backward_count 254540 real_backward_count 34493  13.551%\n",
      "fc layer 2 self.abs_max_out: 5370.0\n",
      "fc layer 1 self.abs_max_out: 8952.0\n",
      "lif layer 1 self.abs_max_v: 14927.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.891475/  1.997912, val:  62.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.68 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7240%\n",
      "layer   2  Sparsity: 66.4553%\n",
      "layer   3  Sparsity: 64.6446%\n",
      "total_backward_count 264330 real_backward_count 35370  13.381%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.890851/  1.981641, val:  64.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7176%\n",
      "layer   2  Sparsity: 66.3382%\n",
      "layer   3  Sparsity: 65.1629%\n",
      "total_backward_count 274120 real_backward_count 36272  13.232%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.886490/  1.997135, val:  64.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.34 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7131%\n",
      "layer   2  Sparsity: 66.2540%\n",
      "layer   3  Sparsity: 65.7038%\n",
      "total_backward_count 283910 real_backward_count 37083  13.062%\n",
      "fc layer 1 self.abs_max_out: 8995.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.886412/  1.975124, val:  72.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.79 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7295%\n",
      "layer   2  Sparsity: 66.2389%\n",
      "layer   3  Sparsity: 65.1800%\n",
      "total_backward_count 293700 real_backward_count 37885  12.899%\n",
      "fc layer 2 self.abs_max_out: 5564.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.891593/  1.989665, val:  71.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.88 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7257%\n",
      "layer   2  Sparsity: 65.6280%\n",
      "layer   3  Sparsity: 65.9020%\n",
      "total_backward_count 303490 real_backward_count 38689  12.748%\n",
      "fc layer 3 self.abs_max_out: 1276.0\n",
      "fc layer 1 self.abs_max_out: 9023.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.887239/  1.981759, val:  67.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7576%\n",
      "layer   2  Sparsity: 66.1491%\n",
      "layer   3  Sparsity: 66.2344%\n",
      "total_backward_count 313280 real_backward_count 39506  12.610%\n",
      "fc layer 1 self.abs_max_out: 9169.0\n",
      "lif layer 1 self.abs_max_v: 15054.5\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.882245/  1.996455, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7112%\n",
      "layer   2  Sparsity: 65.8671%\n",
      "layer   3  Sparsity: 65.8651%\n",
      "total_backward_count 323070 real_backward_count 40246  12.457%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.884028/  1.987299, val:  73.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7276%\n",
      "layer   2  Sparsity: 66.0267%\n",
      "layer   3  Sparsity: 66.2718%\n",
      "total_backward_count 332860 real_backward_count 41003  12.318%\n",
      "fc layer 1 self.abs_max_out: 9195.0\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.888837/  1.978151, val:  71.25%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7115%\n",
      "layer   2  Sparsity: 65.9928%\n",
      "layer   3  Sparsity: 67.2914%\n",
      "total_backward_count 342650 real_backward_count 41789  12.196%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.889732/  1.993798, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7379%\n",
      "layer   2  Sparsity: 66.0756%\n",
      "layer   3  Sparsity: 67.2588%\n",
      "total_backward_count 352440 real_backward_count 42524  12.066%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.888503/  1.987255, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7297%\n",
      "layer   2  Sparsity: 66.3482%\n",
      "layer   3  Sparsity: 67.4699%\n",
      "total_backward_count 362230 real_backward_count 43241  11.937%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.876868/  1.990798, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7269%\n",
      "layer   2  Sparsity: 66.1210%\n",
      "layer   3  Sparsity: 67.5143%\n",
      "total_backward_count 372020 real_backward_count 43917  11.805%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.878217/  1.974618, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.85 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7176%\n",
      "layer   2  Sparsity: 66.1486%\n",
      "layer   3  Sparsity: 67.0673%\n",
      "total_backward_count 381810 real_backward_count 44599  11.681%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.871064/  1.977640, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7232%\n",
      "layer   2  Sparsity: 66.1922%\n",
      "layer   3  Sparsity: 67.6083%\n",
      "total_backward_count 391600 real_backward_count 45285  11.564%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.884870/  1.992419, val:  60.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7462%\n",
      "layer   2  Sparsity: 65.9163%\n",
      "layer   3  Sparsity: 66.8546%\n",
      "total_backward_count 401390 real_backward_count 45968  11.452%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.872454/  1.968410, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7227%\n",
      "layer   2  Sparsity: 66.2065%\n",
      "layer   3  Sparsity: 67.5644%\n",
      "total_backward_count 411180 real_backward_count 46614  11.337%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.868083/  1.966643, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.05 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7300%\n",
      "layer   2  Sparsity: 66.2936%\n",
      "layer   3  Sparsity: 68.4011%\n",
      "total_backward_count 420970 real_backward_count 47227  11.219%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.877198/  1.973801, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7327%\n",
      "layer   2  Sparsity: 66.2191%\n",
      "layer   3  Sparsity: 68.6074%\n",
      "total_backward_count 430760 real_backward_count 47819  11.101%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.873369/  1.972847, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.55 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7281%\n",
      "layer   2  Sparsity: 66.1367%\n",
      "layer   3  Sparsity: 68.3304%\n",
      "total_backward_count 440550 real_backward_count 48413  10.989%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.871290/  1.968175, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.72 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7242%\n",
      "layer   2  Sparsity: 65.6756%\n",
      "layer   3  Sparsity: 68.8772%\n",
      "total_backward_count 450340 real_backward_count 49004  10.882%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.873948/  1.969389, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.08 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7165%\n",
      "layer   2  Sparsity: 65.9056%\n",
      "layer   3  Sparsity: 68.4449%\n",
      "total_backward_count 460130 real_backward_count 49631  10.786%\n",
      "lif layer 1 self.abs_max_v: 15074.0\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.885399/  1.988939, val:  76.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7223%\n",
      "layer   2  Sparsity: 65.9453%\n",
      "layer   3  Sparsity: 69.1646%\n",
      "total_backward_count 469920 real_backward_count 50214  10.686%\n",
      "lif layer 1 self.abs_max_v: 15293.0\n",
      "lif layer 1 self.abs_max_v: 15549.5\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.882220/  1.977893, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7442%\n",
      "layer   2  Sparsity: 65.8655%\n",
      "layer   3  Sparsity: 69.1023%\n",
      "total_backward_count 479710 real_backward_count 50759  10.581%\n",
      "lif layer 1 self.abs_max_v: 15627.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.878331/  1.973648, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7342%\n",
      "layer   2  Sparsity: 65.8942%\n",
      "layer   3  Sparsity: 69.2136%\n",
      "total_backward_count 489500 real_backward_count 51325  10.485%\n",
      "lif layer 2 self.abs_max_v: 9635.5\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.879784/  1.979045, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7346%\n",
      "layer   2  Sparsity: 65.8420%\n",
      "layer   3  Sparsity: 69.1260%\n",
      "total_backward_count 499290 real_backward_count 51902  10.395%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.891621/  1.977087, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.63 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7270%\n",
      "layer   2  Sparsity: 65.7836%\n",
      "layer   3  Sparsity: 69.2110%\n",
      "total_backward_count 509080 real_backward_count 52447  10.302%\n",
      "fc layer 3 self.abs_max_out: 1280.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.881852/  1.962181, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7339%\n",
      "layer   2  Sparsity: 65.8215%\n",
      "layer   3  Sparsity: 69.3845%\n",
      "total_backward_count 518870 real_backward_count 52997  10.214%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.876645/  1.964151, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7120%\n",
      "layer   2  Sparsity: 65.7623%\n",
      "layer   3  Sparsity: 70.1108%\n",
      "total_backward_count 528660 real_backward_count 53508  10.121%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.872317/  1.960828, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.90 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7159%\n",
      "layer   2  Sparsity: 65.8242%\n",
      "layer   3  Sparsity: 69.6729%\n",
      "total_backward_count 538450 real_backward_count 54015  10.032%\n",
      "fc layer 3 self.abs_max_out: 1298.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.865405/  1.964764, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7146%\n",
      "layer   2  Sparsity: 65.9428%\n",
      "layer   3  Sparsity: 69.3396%\n",
      "total_backward_count 548240 real_backward_count 54513   9.943%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.867812/  1.962195, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.33 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7336%\n",
      "layer   2  Sparsity: 66.0347%\n",
      "layer   3  Sparsity: 69.5362%\n",
      "total_backward_count 558030 real_backward_count 54958   9.849%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.861664/  1.955248, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.22 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7442%\n",
      "layer   2  Sparsity: 65.8055%\n",
      "layer   3  Sparsity: 69.5166%\n",
      "total_backward_count 567820 real_backward_count 55432   9.762%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.860172/  1.950345, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7308%\n",
      "layer   2  Sparsity: 65.7616%\n",
      "layer   3  Sparsity: 68.8751%\n",
      "total_backward_count 577610 real_backward_count 55917   9.681%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.862422/  1.964701, val:  67.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7394%\n",
      "layer   2  Sparsity: 65.7555%\n",
      "layer   3  Sparsity: 68.7629%\n",
      "total_backward_count 587400 real_backward_count 56365   9.596%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.864428/  1.954728, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7303%\n",
      "layer   2  Sparsity: 65.7854%\n",
      "layer   3  Sparsity: 68.5043%\n",
      "total_backward_count 597190 real_backward_count 56791   9.510%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.855438/  1.951310, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.96 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7333%\n",
      "layer   2  Sparsity: 65.6859%\n",
      "layer   3  Sparsity: 68.6000%\n",
      "total_backward_count 606980 real_backward_count 57219   9.427%\n",
      "lif layer 1 self.abs_max_v: 15684.5\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.851952/  1.953615, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.88 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 65.6080%\n",
      "layer   3  Sparsity: 69.3177%\n",
      "total_backward_count 616770 real_backward_count 57657   9.348%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.844826/  1.948315, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.14 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7183%\n",
      "layer   2  Sparsity: 65.6224%\n",
      "layer   3  Sparsity: 69.0804%\n",
      "total_backward_count 626560 real_backward_count 58018   9.260%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.845348/  1.946364, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7494%\n",
      "layer   2  Sparsity: 65.5085%\n",
      "layer   3  Sparsity: 69.3603%\n",
      "total_backward_count 636350 real_backward_count 58444   9.184%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.843521/  1.957031, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.48 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7158%\n",
      "layer   2  Sparsity: 65.4421%\n",
      "layer   3  Sparsity: 70.2475%\n",
      "total_backward_count 646140 real_backward_count 58864   9.110%\n",
      "lif layer 2 self.abs_max_v: 9664.5\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.842397/  1.946580, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.76 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7193%\n",
      "layer   2  Sparsity: 65.5607%\n",
      "layer   3  Sparsity: 70.1934%\n",
      "total_backward_count 655930 real_backward_count 59274   9.037%\n",
      "fc layer 3 self.abs_max_out: 1329.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.844506/  1.939793, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7059%\n",
      "layer   2  Sparsity: 65.3442%\n",
      "layer   3  Sparsity: 69.6942%\n",
      "total_backward_count 665720 real_backward_count 59697   8.967%\n",
      "lif layer 2 self.abs_max_v: 9712.0\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.835849/  1.924182, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7339%\n",
      "layer   2  Sparsity: 65.6358%\n",
      "layer   3  Sparsity: 69.2577%\n",
      "total_backward_count 675510 real_backward_count 60069   8.892%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.830423/  1.949132, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.68 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7101%\n",
      "layer   2  Sparsity: 65.7162%\n",
      "layer   3  Sparsity: 69.8444%\n",
      "total_backward_count 685300 real_backward_count 60444   8.820%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.846934/  1.945472, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.76 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7246%\n",
      "layer   2  Sparsity: 65.5917%\n",
      "layer   3  Sparsity: 70.1520%\n",
      "total_backward_count 695090 real_backward_count 60856   8.755%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.849886/  1.952210, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.33 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7223%\n",
      "layer   2  Sparsity: 65.5841%\n",
      "layer   3  Sparsity: 69.8694%\n",
      "total_backward_count 704880 real_backward_count 61252   8.690%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.842663/  1.936439, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.64 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 89.7291%\n",
      "layer   2  Sparsity: 65.6428%\n",
      "layer   3  Sparsity: 69.7368%\n",
      "total_backward_count 714670 real_backward_count 61645   8.626%\n",
      "fc layer 1 self.abs_max_out: 9380.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.842913/  1.938481, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.27 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 89.6970%\n",
      "layer   2  Sparsity: 65.3996%\n",
      "layer   3  Sparsity: 70.2808%\n",
      "total_backward_count 724460 real_backward_count 62027   8.562%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.836835/  1.934410, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7033%\n",
      "layer   2  Sparsity: 65.7486%\n",
      "layer   3  Sparsity: 70.0926%\n",
      "total_backward_count 734250 real_backward_count 62433   8.503%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.830730/  1.920027, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.43 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.6959%\n",
      "layer   2  Sparsity: 65.7553%\n",
      "layer   3  Sparsity: 69.6307%\n",
      "total_backward_count 744040 real_backward_count 62779   8.438%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.825835/  1.924661, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7350%\n",
      "layer   2  Sparsity: 65.5798%\n",
      "layer   3  Sparsity: 69.5919%\n",
      "total_backward_count 753830 real_backward_count 63117   8.373%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.828239/  1.941020, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7241%\n",
      "layer   2  Sparsity: 65.5109%\n",
      "layer   3  Sparsity: 69.7158%\n",
      "total_backward_count 763620 real_backward_count 63469   8.312%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.828033/  1.938437, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.81 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7398%\n",
      "layer   2  Sparsity: 65.0909%\n",
      "layer   3  Sparsity: 69.5707%\n",
      "total_backward_count 773410 real_backward_count 63830   8.253%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.830808/  1.932592, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.47 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7056%\n",
      "layer   2  Sparsity: 65.0463%\n",
      "layer   3  Sparsity: 70.0756%\n",
      "total_backward_count 783200 real_backward_count 64193   8.196%\n",
      "lif layer 1 self.abs_max_v: 15699.0\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.832157/  1.926746, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7232%\n",
      "layer   2  Sparsity: 64.6205%\n",
      "layer   3  Sparsity: 69.4552%\n",
      "total_backward_count 792990 real_backward_count 64533   8.138%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.824858/  1.935316, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7357%\n",
      "layer   2  Sparsity: 64.9160%\n",
      "layer   3  Sparsity: 69.5694%\n",
      "total_backward_count 802780 real_backward_count 64856   8.079%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.822028/  1.916308, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7208%\n",
      "layer   2  Sparsity: 65.0468%\n",
      "layer   3  Sparsity: 70.0146%\n",
      "total_backward_count 812570 real_backward_count 65146   8.017%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.819916/  1.931069, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.86 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7220%\n",
      "layer   2  Sparsity: 64.8204%\n",
      "layer   3  Sparsity: 70.1254%\n",
      "total_backward_count 822360 real_backward_count 65438   7.957%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.828923/  1.935552, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.63 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 89.7288%\n",
      "layer   2  Sparsity: 65.0321%\n",
      "layer   3  Sparsity: 69.8552%\n",
      "total_backward_count 832150 real_backward_count 65797   7.907%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.831825/  1.928393, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.58 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 89.7492%\n",
      "layer   2  Sparsity: 65.1621%\n",
      "layer   3  Sparsity: 69.8586%\n",
      "total_backward_count 841940 real_backward_count 66143   7.856%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.823997/  1.916597, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.6979%\n",
      "layer   2  Sparsity: 65.1870%\n",
      "layer   3  Sparsity: 69.7511%\n",
      "total_backward_count 851730 real_backward_count 66454   7.802%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.819359/  1.923575, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7384%\n",
      "layer   2  Sparsity: 65.3104%\n",
      "layer   3  Sparsity: 70.0970%\n",
      "total_backward_count 861520 real_backward_count 66797   7.753%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.818633/  1.914729, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7493%\n",
      "layer   2  Sparsity: 65.3386%\n",
      "layer   3  Sparsity: 70.0240%\n",
      "total_backward_count 871310 real_backward_count 67117   7.703%\n",
      "lif layer 1 self.abs_max_v: 15740.5\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.814696/  1.919794, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.06 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7246%\n",
      "layer   2  Sparsity: 65.6565%\n",
      "layer   3  Sparsity: 70.9196%\n",
      "total_backward_count 881100 real_backward_count 67442   7.654%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.818106/  1.922989, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.77 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7223%\n",
      "layer   2  Sparsity: 65.5433%\n",
      "layer   3  Sparsity: 71.1787%\n",
      "total_backward_count 890890 real_backward_count 67730   7.603%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.817135/  1.915803, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.45 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.6908%\n",
      "layer   2  Sparsity: 64.9023%\n",
      "layer   3  Sparsity: 71.0733%\n",
      "total_backward_count 900680 real_backward_count 68024   7.553%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.820823/  1.919707, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.69 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7163%\n",
      "layer   2  Sparsity: 64.8972%\n",
      "layer   3  Sparsity: 71.5067%\n",
      "total_backward_count 910470 real_backward_count 68309   7.503%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.821320/  1.926204, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7205%\n",
      "layer   2  Sparsity: 65.0502%\n",
      "layer   3  Sparsity: 70.7479%\n",
      "total_backward_count 920260 real_backward_count 68607   7.455%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.823174/  1.925439, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7193%\n",
      "layer   2  Sparsity: 65.1292%\n",
      "layer   3  Sparsity: 70.7719%\n",
      "total_backward_count 930050 real_backward_count 68890   7.407%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.823271/  1.935190, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.6903%\n",
      "layer   2  Sparsity: 65.2618%\n",
      "layer   3  Sparsity: 71.0328%\n",
      "total_backward_count 939840 real_backward_count 69149   7.358%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.822836/  1.939964, val:  80.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.03 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.6989%\n",
      "layer   2  Sparsity: 65.2287%\n",
      "layer   3  Sparsity: 71.4038%\n",
      "total_backward_count 949630 real_backward_count 69397   7.308%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.825430/  1.925458, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7117%\n",
      "layer   2  Sparsity: 65.2276%\n",
      "layer   3  Sparsity: 71.2077%\n",
      "total_backward_count 959420 real_backward_count 69668   7.261%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.822413/  1.926656, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.46 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 65.2776%\n",
      "layer   3  Sparsity: 71.0148%\n",
      "total_backward_count 969210 real_backward_count 69910   7.213%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.817906/  1.920364, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.84 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7158%\n",
      "layer   2  Sparsity: 65.2694%\n",
      "layer   3  Sparsity: 71.0886%\n",
      "total_backward_count 979000 real_backward_count 70157   7.166%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.815390/  1.927244, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.34 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7418%\n",
      "layer   2  Sparsity: 65.2452%\n",
      "layer   3  Sparsity: 70.4506%\n",
      "total_backward_count 988790 real_backward_count 70406   7.120%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.815480/  1.929254, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7214%\n",
      "layer   2  Sparsity: 65.0178%\n",
      "layer   3  Sparsity: 70.8432%\n",
      "total_backward_count 998580 real_backward_count 70648   7.075%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.811260/  1.917831, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.48 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7222%\n",
      "layer   2  Sparsity: 65.3224%\n",
      "layer   3  Sparsity: 70.8700%\n",
      "total_backward_count 1008370 real_backward_count 70860   7.027%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.807055/  1.918168, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7103%\n",
      "layer   2  Sparsity: 65.3676%\n",
      "layer   3  Sparsity: 71.2066%\n",
      "total_backward_count 1018160 real_backward_count 71036   6.977%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.806228/  1.916545, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7372%\n",
      "layer   2  Sparsity: 65.4708%\n",
      "layer   3  Sparsity: 71.0578%\n",
      "total_backward_count 1027950 real_backward_count 71270   6.933%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.805794/  1.911243, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.44 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7395%\n",
      "layer   2  Sparsity: 65.2422%\n",
      "layer   3  Sparsity: 70.6549%\n",
      "total_backward_count 1037740 real_backward_count 71497   6.890%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.805603/  1.915742, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7104%\n",
      "layer   2  Sparsity: 65.0916%\n",
      "layer   3  Sparsity: 70.7327%\n",
      "total_backward_count 1047530 real_backward_count 71759   6.850%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.810534/  1.915415, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.11 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7210%\n",
      "layer   2  Sparsity: 65.0292%\n",
      "layer   3  Sparsity: 71.3854%\n",
      "total_backward_count 1057320 real_backward_count 72009   6.811%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.803078/  1.905204, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 65.3159%\n",
      "layer   3  Sparsity: 71.1056%\n",
      "total_backward_count 1067110 real_backward_count 72224   6.768%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.804327/  1.911495, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.05 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7078%\n",
      "layer   2  Sparsity: 65.5174%\n",
      "layer   3  Sparsity: 70.7143%\n",
      "total_backward_count 1076900 real_backward_count 72447   6.727%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.800040/  1.912030, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.18 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7455%\n",
      "layer   2  Sparsity: 65.6011%\n",
      "layer   3  Sparsity: 70.8280%\n",
      "total_backward_count 1086690 real_backward_count 72632   6.684%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.799082/  1.906534, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.55 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7015%\n",
      "layer   2  Sparsity: 65.4527%\n",
      "layer   3  Sparsity: 71.3324%\n",
      "total_backward_count 1096480 real_backward_count 72856   6.645%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.793957/  1.901976, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.00 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7433%\n",
      "layer   2  Sparsity: 65.4438%\n",
      "layer   3  Sparsity: 71.9491%\n",
      "total_backward_count 1106270 real_backward_count 73051   6.603%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.786153/  1.894719, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7174%\n",
      "layer   2  Sparsity: 65.4099%\n",
      "layer   3  Sparsity: 71.6427%\n",
      "total_backward_count 1116060 real_backward_count 73245   6.563%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.789167/  1.895330, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7151%\n",
      "layer   2  Sparsity: 65.3488%\n",
      "layer   3  Sparsity: 71.7011%\n",
      "total_backward_count 1125850 real_backward_count 73441   6.523%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.789664/  1.894544, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7047%\n",
      "layer   2  Sparsity: 65.5570%\n",
      "layer   3  Sparsity: 71.4159%\n",
      "total_backward_count 1135640 real_backward_count 73645   6.485%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.791844/  1.892358, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7090%\n",
      "layer   2  Sparsity: 65.2625%\n",
      "layer   3  Sparsity: 71.3906%\n",
      "total_backward_count 1145430 real_backward_count 73851   6.447%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.786870/  1.893636, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.21 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7306%\n",
      "layer   2  Sparsity: 65.2120%\n",
      "layer   3  Sparsity: 71.5747%\n",
      "total_backward_count 1155220 real_backward_count 74040   6.409%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.783762/  1.894839, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.36 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7238%\n",
      "layer   2  Sparsity: 65.0339%\n",
      "layer   3  Sparsity: 71.6103%\n",
      "total_backward_count 1165010 real_backward_count 74263   6.374%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.787690/  1.892953, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.66 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7601%\n",
      "layer   2  Sparsity: 64.9293%\n",
      "layer   3  Sparsity: 71.1815%\n",
      "total_backward_count 1174800 real_backward_count 74473   6.339%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.788830/  1.899778, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7175%\n",
      "layer   2  Sparsity: 64.9294%\n",
      "layer   3  Sparsity: 71.3453%\n",
      "total_backward_count 1184590 real_backward_count 74669   6.303%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.788773/  1.894509, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7306%\n",
      "layer   2  Sparsity: 65.0216%\n",
      "layer   3  Sparsity: 71.3522%\n",
      "total_backward_count 1194380 real_backward_count 74869   6.268%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.793096/  1.905271, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.46 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7413%\n",
      "layer   2  Sparsity: 65.1229%\n",
      "layer   3  Sparsity: 71.3109%\n",
      "total_backward_count 1204170 real_backward_count 75073   6.234%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.788983/  1.896319, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7119%\n",
      "layer   2  Sparsity: 64.7450%\n",
      "layer   3  Sparsity: 70.6755%\n",
      "total_backward_count 1213960 real_backward_count 75270   6.200%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.782837/  1.897773, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.64 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7276%\n",
      "layer   2  Sparsity: 64.7667%\n",
      "layer   3  Sparsity: 70.9723%\n",
      "total_backward_count 1223750 real_backward_count 75449   6.165%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.782892/  1.887146, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.84 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7400%\n",
      "layer   2  Sparsity: 64.9285%\n",
      "layer   3  Sparsity: 71.3272%\n",
      "total_backward_count 1233540 real_backward_count 75633   6.131%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.775175/  1.880599, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.17 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7567%\n",
      "layer   2  Sparsity: 65.1677%\n",
      "layer   3  Sparsity: 71.1720%\n",
      "total_backward_count 1243330 real_backward_count 75821   6.098%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.774960/  1.889817, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7229%\n",
      "layer   2  Sparsity: 64.8377%\n",
      "layer   3  Sparsity: 71.1614%\n",
      "total_backward_count 1253120 real_backward_count 75996   6.065%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.777284/  1.890414, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.72 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7394%\n",
      "layer   2  Sparsity: 64.8693%\n",
      "layer   3  Sparsity: 71.3263%\n",
      "total_backward_count 1262910 real_backward_count 76199   6.034%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.772695/  1.878665, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7097%\n",
      "layer   2  Sparsity: 64.8487%\n",
      "layer   3  Sparsity: 71.1809%\n",
      "total_backward_count 1272700 real_backward_count 76394   6.003%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.771212/  1.887574, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.38 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7403%\n",
      "layer   2  Sparsity: 64.9194%\n",
      "layer   3  Sparsity: 70.8091%\n",
      "total_backward_count 1282490 real_backward_count 76581   5.971%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.768591/  1.878034, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7297%\n",
      "layer   2  Sparsity: 65.0597%\n",
      "layer   3  Sparsity: 71.3403%\n",
      "total_backward_count 1292280 real_backward_count 76728   5.937%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.771782/  1.886939, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.14 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7163%\n",
      "layer   2  Sparsity: 65.1110%\n",
      "layer   3  Sparsity: 71.4073%\n",
      "total_backward_count 1302070 real_backward_count 76919   5.907%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.773140/  1.877456, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7208%\n",
      "layer   2  Sparsity: 65.4176%\n",
      "layer   3  Sparsity: 71.5943%\n",
      "total_backward_count 1311860 real_backward_count 77112   5.878%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.764819/  1.877169, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.84 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7440%\n",
      "layer   2  Sparsity: 65.4249%\n",
      "layer   3  Sparsity: 71.6911%\n",
      "total_backward_count 1321650 real_backward_count 77278   5.847%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.766389/  1.876879, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.61 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7257%\n",
      "layer   2  Sparsity: 65.4422%\n",
      "layer   3  Sparsity: 71.5283%\n",
      "total_backward_count 1331440 real_backward_count 77445   5.817%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.767469/  1.880061, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7390%\n",
      "layer   2  Sparsity: 65.2016%\n",
      "layer   3  Sparsity: 71.6838%\n",
      "total_backward_count 1341230 real_backward_count 77593   5.785%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.769933/  1.872501, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.13 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7351%\n",
      "layer   2  Sparsity: 64.9658%\n",
      "layer   3  Sparsity: 71.6485%\n",
      "total_backward_count 1351020 real_backward_count 77765   5.756%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.768651/  1.882870, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.77 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7255%\n",
      "layer   2  Sparsity: 65.0343%\n",
      "layer   3  Sparsity: 71.6004%\n",
      "total_backward_count 1360810 real_backward_count 77952   5.728%\n",
      "fc layer 1 self.abs_max_out: 9435.0\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.768633/  1.880583, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.00 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7340%\n",
      "layer   2  Sparsity: 65.0486%\n",
      "layer   3  Sparsity: 71.4485%\n",
      "total_backward_count 1370600 real_backward_count 78134   5.701%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.762339/  1.880210, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.39 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7095%\n",
      "layer   2  Sparsity: 65.0445%\n",
      "layer   3  Sparsity: 71.5239%\n",
      "total_backward_count 1380390 real_backward_count 78311   5.673%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.764379/  1.877577, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7395%\n",
      "layer   2  Sparsity: 65.1816%\n",
      "layer   3  Sparsity: 71.8577%\n",
      "total_backward_count 1390180 real_backward_count 78486   5.646%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.767451/  1.879302, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7331%\n",
      "layer   2  Sparsity: 65.1678%\n",
      "layer   3  Sparsity: 71.9403%\n",
      "total_backward_count 1399970 real_backward_count 78644   5.618%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.770746/  1.873905, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.6915%\n",
      "layer   2  Sparsity: 65.2284%\n",
      "layer   3  Sparsity: 71.5716%\n",
      "total_backward_count 1409760 real_backward_count 78776   5.588%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.760649/  1.875030, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.00 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7333%\n",
      "layer   2  Sparsity: 65.1064%\n",
      "layer   3  Sparsity: 71.0609%\n",
      "total_backward_count 1419550 real_backward_count 78933   5.560%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.758371/  1.871416, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.39 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7252%\n",
      "layer   2  Sparsity: 65.3014%\n",
      "layer   3  Sparsity: 71.2871%\n",
      "total_backward_count 1429340 real_backward_count 79083   5.533%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.760090/  1.871645, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7232%\n",
      "layer   2  Sparsity: 65.4283%\n",
      "layer   3  Sparsity: 71.6051%\n",
      "total_backward_count 1439130 real_backward_count 79240   5.506%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.756452/  1.875421, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.07 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7258%\n",
      "layer   2  Sparsity: 65.2049%\n",
      "layer   3  Sparsity: 71.6456%\n",
      "total_backward_count 1448920 real_backward_count 79358   5.477%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.760265/  1.864805, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7198%\n",
      "layer   2  Sparsity: 64.8996%\n",
      "layer   3  Sparsity: 71.9204%\n",
      "total_backward_count 1458710 real_backward_count 79518   5.451%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.753627/  1.873648, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.63 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7284%\n",
      "layer   2  Sparsity: 65.0773%\n",
      "layer   3  Sparsity: 72.2096%\n",
      "total_backward_count 1468500 real_backward_count 79659   5.425%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.758164/  1.867176, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7241%\n",
      "layer   2  Sparsity: 65.0932%\n",
      "layer   3  Sparsity: 72.0137%\n",
      "total_backward_count 1478290 real_backward_count 79833   5.400%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.751775/  1.860752, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.64 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7310%\n",
      "layer   2  Sparsity: 65.3520%\n",
      "layer   3  Sparsity: 72.1526%\n",
      "total_backward_count 1488080 real_backward_count 79965   5.374%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.752023/  1.866390, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7165%\n",
      "layer   2  Sparsity: 65.3658%\n",
      "layer   3  Sparsity: 72.0978%\n",
      "total_backward_count 1497870 real_backward_count 80112   5.348%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.752071/  1.863250, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7270%\n",
      "layer   2  Sparsity: 65.3830%\n",
      "layer   3  Sparsity: 71.6821%\n",
      "total_backward_count 1507660 real_backward_count 80229   5.321%\n",
      "lif layer 1 self.abs_max_v: 15848.5\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.751416/  1.863397, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.46 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7451%\n",
      "layer   2  Sparsity: 65.4470%\n",
      "layer   3  Sparsity: 71.5796%\n",
      "total_backward_count 1517450 real_backward_count 80371   5.296%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.749887/  1.864552, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7010%\n",
      "layer   2  Sparsity: 65.2708%\n",
      "layer   3  Sparsity: 71.6547%\n",
      "total_backward_count 1527240 real_backward_count 80521   5.272%\n",
      "lif layer 1 self.abs_max_v: 16009.5\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.748631/  1.855592, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.86 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7347%\n",
      "layer   2  Sparsity: 65.2917%\n",
      "layer   3  Sparsity: 71.8212%\n",
      "total_backward_count 1537030 real_backward_count 80666   5.248%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.750900/  1.866382, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.6965%\n",
      "layer   2  Sparsity: 65.3277%\n",
      "layer   3  Sparsity: 72.0206%\n",
      "total_backward_count 1546820 real_backward_count 80797   5.223%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.750724/  1.854406, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.26 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7264%\n",
      "layer   2  Sparsity: 65.2402%\n",
      "layer   3  Sparsity: 71.9609%\n",
      "total_backward_count 1556610 real_backward_count 80930   5.199%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.749781/  1.869579, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.20 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7181%\n",
      "layer   2  Sparsity: 65.2315%\n",
      "layer   3  Sparsity: 71.8042%\n",
      "total_backward_count 1566400 real_backward_count 81060   5.175%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.750003/  1.870082, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7278%\n",
      "layer   2  Sparsity: 65.3860%\n",
      "layer   3  Sparsity: 71.6450%\n",
      "total_backward_count 1576190 real_backward_count 81191   5.151%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.749367/  1.861924, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7062%\n",
      "layer   2  Sparsity: 65.4010%\n",
      "layer   3  Sparsity: 71.1951%\n",
      "total_backward_count 1585980 real_backward_count 81299   5.126%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.751685/  1.864985, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7273%\n",
      "layer   2  Sparsity: 65.1582%\n",
      "layer   3  Sparsity: 71.3311%\n",
      "total_backward_count 1595770 real_backward_count 81420   5.102%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.748982/  1.872354, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.69 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7344%\n",
      "layer   2  Sparsity: 65.0154%\n",
      "layer   3  Sparsity: 71.4886%\n",
      "total_backward_count 1605560 real_backward_count 81555   5.080%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.748373/  1.861838, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.6926%\n",
      "layer   2  Sparsity: 64.9148%\n",
      "layer   3  Sparsity: 71.6902%\n",
      "total_backward_count 1615350 real_backward_count 81660   5.055%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.745802/  1.864552, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.27 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7063%\n",
      "layer   2  Sparsity: 64.9784%\n",
      "layer   3  Sparsity: 71.6735%\n",
      "total_backward_count 1625140 real_backward_count 81796   5.033%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.742854/  1.856138, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7067%\n",
      "layer   2  Sparsity: 64.9126%\n",
      "layer   3  Sparsity: 71.8438%\n",
      "total_backward_count 1634930 real_backward_count 81898   5.009%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.740140/  1.859520, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7226%\n",
      "layer   2  Sparsity: 65.0717%\n",
      "layer   3  Sparsity: 71.6842%\n",
      "total_backward_count 1644720 real_backward_count 82010   4.986%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.741580/  1.867353, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.90 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7607%\n",
      "layer   2  Sparsity: 64.9911%\n",
      "layer   3  Sparsity: 71.9298%\n",
      "total_backward_count 1654510 real_backward_count 82125   4.964%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.744373/  1.862269, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7453%\n",
      "layer   2  Sparsity: 65.1924%\n",
      "layer   3  Sparsity: 72.2961%\n",
      "total_backward_count 1664300 real_backward_count 82246   4.942%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.739206/  1.857879, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.18 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7369%\n",
      "layer   2  Sparsity: 65.2200%\n",
      "layer   3  Sparsity: 72.5401%\n",
      "total_backward_count 1674090 real_backward_count 82330   4.918%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.741789/  1.860957, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.60 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7556%\n",
      "layer   2  Sparsity: 65.0392%\n",
      "layer   3  Sparsity: 72.2465%\n",
      "total_backward_count 1683880 real_backward_count 82463   4.897%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.745545/  1.863137, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7488%\n",
      "layer   2  Sparsity: 64.8872%\n",
      "layer   3  Sparsity: 71.6967%\n",
      "total_backward_count 1693670 real_backward_count 82602   4.877%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.744088/  1.862116, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.34 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7356%\n",
      "layer   2  Sparsity: 64.9095%\n",
      "layer   3  Sparsity: 71.7439%\n",
      "total_backward_count 1703460 real_backward_count 82719   4.856%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.744231/  1.862346, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.95 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7290%\n",
      "layer   2  Sparsity: 65.1764%\n",
      "layer   3  Sparsity: 71.5032%\n",
      "total_backward_count 1713250 real_backward_count 82836   4.835%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.749798/  1.862161, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 89.6970%\n",
      "layer   2  Sparsity: 65.2150%\n",
      "layer   3  Sparsity: 71.7237%\n",
      "total_backward_count 1723040 real_backward_count 82983   4.816%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.745888/  1.858181, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7318%\n",
      "layer   2  Sparsity: 65.0995%\n",
      "layer   3  Sparsity: 71.8457%\n",
      "total_backward_count 1732830 real_backward_count 83114   4.796%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.742241/  1.865992, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.53 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 89.7182%\n",
      "layer   2  Sparsity: 65.0831%\n",
      "layer   3  Sparsity: 71.9604%\n",
      "total_backward_count 1742620 real_backward_count 83215   4.775%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.742409/  1.861226, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.16 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7386%\n",
      "layer   2  Sparsity: 65.0433%\n",
      "layer   3  Sparsity: 72.1160%\n",
      "total_backward_count 1752410 real_backward_count 83310   4.754%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.742383/  1.865614, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7336%\n",
      "layer   2  Sparsity: 65.1753%\n",
      "layer   3  Sparsity: 72.4621%\n",
      "total_backward_count 1762200 real_backward_count 83417   4.734%\n",
      "lif layer 2 self.abs_max_v: 9730.5\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.744260/  1.863782, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.70 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 89.7360%\n",
      "layer   2  Sparsity: 65.1025%\n",
      "layer   3  Sparsity: 72.3398%\n",
      "total_backward_count 1771990 real_backward_count 83518   4.713%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.743626/  1.865072, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7143%\n",
      "layer   2  Sparsity: 64.9134%\n",
      "layer   3  Sparsity: 72.0059%\n",
      "total_backward_count 1781780 real_backward_count 83619   4.693%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.741986/  1.859702, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7165%\n",
      "layer   2  Sparsity: 64.9308%\n",
      "layer   3  Sparsity: 72.0000%\n",
      "total_backward_count 1791570 real_backward_count 83745   4.674%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.740276/  1.865307, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.60 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7254%\n",
      "layer   2  Sparsity: 64.9628%\n",
      "layer   3  Sparsity: 71.9268%\n",
      "total_backward_count 1801360 real_backward_count 83848   4.655%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.737049/  1.867436, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.98 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7395%\n",
      "layer   2  Sparsity: 64.9052%\n",
      "layer   3  Sparsity: 71.7348%\n",
      "total_backward_count 1811150 real_backward_count 83960   4.636%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.741559/  1.861067, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.35 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7203%\n",
      "layer   2  Sparsity: 65.0111%\n",
      "layer   3  Sparsity: 71.3986%\n",
      "total_backward_count 1820940 real_backward_count 84073   4.617%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.742600/  1.860996, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7276%\n",
      "layer   2  Sparsity: 64.8351%\n",
      "layer   3  Sparsity: 71.7105%\n",
      "total_backward_count 1830730 real_backward_count 84197   4.599%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.738279/  1.860502, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.35 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 89.7313%\n",
      "layer   2  Sparsity: 64.9583%\n",
      "layer   3  Sparsity: 71.7506%\n",
      "total_backward_count 1840520 real_backward_count 84319   4.581%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.735626/  1.857099, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.00 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 89.7190%\n",
      "layer   2  Sparsity: 65.1218%\n",
      "layer   3  Sparsity: 71.7968%\n",
      "total_backward_count 1850310 real_backward_count 84423   4.563%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.739221/  1.857592, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7306%\n",
      "layer   2  Sparsity: 64.7593%\n",
      "layer   3  Sparsity: 71.5269%\n",
      "total_backward_count 1860100 real_backward_count 84536   4.545%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.738793/  1.851808, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.22 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 89.7340%\n",
      "layer   2  Sparsity: 64.9055%\n",
      "layer   3  Sparsity: 71.4589%\n",
      "total_backward_count 1869890 real_backward_count 84643   4.527%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.732108/  1.853319, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7129%\n",
      "layer   2  Sparsity: 65.0871%\n",
      "layer   3  Sparsity: 71.9074%\n",
      "total_backward_count 1879680 real_backward_count 84751   4.509%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.725839/  1.847533, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7418%\n",
      "layer   2  Sparsity: 64.8853%\n",
      "layer   3  Sparsity: 71.8451%\n",
      "total_backward_count 1889470 real_backward_count 84864   4.491%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.725723/  1.853284, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.28 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7200%\n",
      "layer   2  Sparsity: 64.8579%\n",
      "layer   3  Sparsity: 71.6156%\n",
      "total_backward_count 1899260 real_backward_count 84957   4.473%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.733093/  1.856323, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.66 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 89.7150%\n",
      "layer   2  Sparsity: 64.9623%\n",
      "layer   3  Sparsity: 72.0050%\n",
      "total_backward_count 1909050 real_backward_count 85049   4.455%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.729560/  1.853317, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.56 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 89.7081%\n",
      "layer   2  Sparsity: 64.7861%\n",
      "layer   3  Sparsity: 72.1857%\n",
      "total_backward_count 1918840 real_backward_count 85152   4.438%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.727719/  1.854244, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.90 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 89.7206%\n",
      "layer   2  Sparsity: 64.8906%\n",
      "layer   3  Sparsity: 72.2621%\n",
      "total_backward_count 1928630 real_backward_count 85258   4.421%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.728305/  1.849871, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.74 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7414%\n",
      "layer   2  Sparsity: 64.9203%\n",
      "layer   3  Sparsity: 72.4922%\n",
      "total_backward_count 1938420 real_backward_count 85343   4.403%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.726409/  1.849796, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7175%\n",
      "layer   2  Sparsity: 64.9950%\n",
      "layer   3  Sparsity: 72.3995%\n",
      "total_backward_count 1948210 real_backward_count 85427   4.385%\n",
      "lif layer 1 self.abs_max_v: 16027.5\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.726775/  1.852495, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.7275%\n",
      "layer   2  Sparsity: 64.9570%\n",
      "layer   3  Sparsity: 72.3418%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44863ea34d524a4793bfd4de4893da26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.72677</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>1.8525</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-151</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cd3extht' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cd3extht</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_232713-cd3extht/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: om0tgw21 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_041941-om0tgw21</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/om0tgw21' target=\"_blank\">wild-sweep-157</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/om0tgw21' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/om0tgw21</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_041950_633', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 503.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "lif layer 2 self.abs_max_v: 1446.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 1 self.abs_max_out: 660.0\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 2 self.abs_max_out: 1977.0\n",
      "lif layer 2 self.abs_max_v: 2371.5\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 2 self.abs_max_v: 2764.0\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 1206.5\n",
      "lif layer 2 self.abs_max_v: 2948.5\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 1498.5\n",
      "fc layer 2 self.abs_max_out: 2063.0\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "fc layer 1 self.abs_max_out: 1175.0\n",
      "lif layer 2 self.abs_max_v: 3324.5\n",
      "fc layer 1 self.abs_max_out: 1217.0\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "fc layer 3 self.abs_max_out: 832.0\n",
      "fc layer 2 self.abs_max_out: 2472.0\n",
      "lif layer 1 self.abs_max_v: 1603.0\n",
      "lif layer 1 self.abs_max_v: 1717.0\n",
      "fc layer 3 self.abs_max_out: 1146.0\n",
      "lif layer 1 self.abs_max_v: 1772.0\n",
      "fc layer 1 self.abs_max_out: 1336.0\n",
      "lif layer 1 self.abs_max_v: 2037.5\n",
      "fc layer 1 self.abs_max_out: 1348.0\n",
      "lif layer 1 self.abs_max_v: 2202.0\n",
      "fc layer 2 self.abs_max_out: 2478.0\n",
      "lif layer 2 self.abs_max_v: 3773.0\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "fc layer 1 self.abs_max_out: 1381.0\n",
      "fc layer 1 self.abs_max_out: 1677.0\n",
      "fc layer 3 self.abs_max_out: 1177.0\n",
      "lif layer 1 self.abs_max_v: 2459.5\n",
      "fc layer 2 self.abs_max_out: 2939.0\n",
      "lif layer 2 self.abs_max_v: 4580.0\n",
      "lif layer 1 self.abs_max_v: 2680.5\n",
      "fc layer 1 self.abs_max_out: 1764.0\n",
      "fc layer 1 self.abs_max_out: 1950.0\n",
      "fc layer 1 self.abs_max_out: 2007.0\n",
      "lif layer 1 self.abs_max_v: 2977.0\n",
      "fc layer 3 self.abs_max_out: 1235.0\n",
      "fc layer 1 self.abs_max_out: 2145.0\n",
      "fc layer 1 self.abs_max_out: 2361.0\n",
      "lif layer 1 self.abs_max_v: 2980.5\n",
      "fc layer 1 self.abs_max_out: 2510.0\n",
      "lif layer 1 self.abs_max_v: 3166.0\n",
      "lif layer 1 self.abs_max_v: 3991.0\n",
      "fc layer 1 self.abs_max_out: 2746.0\n",
      "fc layer 1 self.abs_max_out: 2780.0\n",
      "fc layer 2 self.abs_max_out: 3116.0\n",
      "lif layer 2 self.abs_max_v: 4610.0\n",
      "fc layer 3 self.abs_max_out: 1490.0\n",
      "fc layer 1 self.abs_max_out: 3207.0\n",
      "lif layer 1 self.abs_max_v: 4626.5\n",
      "fc layer 1 self.abs_max_out: 3252.0\n",
      "lif layer 1 self.abs_max_v: 5238.5\n",
      "lif layer 2 self.abs_max_v: 4920.5\n",
      "lif layer 2 self.abs_max_v: 5282.5\n",
      "fc layer 3 self.abs_max_out: 1532.0\n",
      "lif layer 2 self.abs_max_v: 5448.5\n",
      "fc layer 1 self.abs_max_out: 3293.0\n",
      "fc layer 2 self.abs_max_out: 3204.0\n",
      "lif layer 2 self.abs_max_v: 5484.5\n",
      "lif layer 2 self.abs_max_v: 5526.5\n",
      "lif layer 2 self.abs_max_v: 5590.5\n",
      "fc layer 2 self.abs_max_out: 3441.0\n",
      "lif layer 2 self.abs_max_v: 5638.0\n",
      "lif layer 2 self.abs_max_v: 5676.0\n",
      "fc layer 2 self.abs_max_out: 3556.0\n",
      "lif layer 2 self.abs_max_v: 6222.5\n",
      "fc layer 1 self.abs_max_out: 3878.0\n",
      "lif layer 1 self.abs_max_v: 5508.0\n",
      "fc layer 1 self.abs_max_out: 4237.0\n",
      "fc layer 3 self.abs_max_out: 1616.0\n",
      "fc layer 3 self.abs_max_out: 1673.0\n",
      "fc layer 3 self.abs_max_out: 1762.0\n",
      "fc layer 3 self.abs_max_out: 1837.0\n",
      "fc layer 3 self.abs_max_out: 1852.0\n",
      "fc layer 1 self.abs_max_out: 4300.0\n",
      "lif layer 1 self.abs_max_v: 5858.5\n",
      "lif layer 1 self.abs_max_v: 5996.5\n",
      "lif layer 1 self.abs_max_v: 6009.0\n",
      "fc layer 1 self.abs_max_out: 4535.0\n",
      "fc layer 2 self.abs_max_out: 3707.0\n",
      "fc layer 1 self.abs_max_out: 4858.0\n",
      "lif layer 1 self.abs_max_v: 6442.5\n",
      "lif layer 1 self.abs_max_v: 6446.5\n",
      "lif layer 2 self.abs_max_v: 6269.0\n",
      "lif layer 1 self.abs_max_v: 6674.5\n",
      "lif layer 1 self.abs_max_v: 7133.0\n",
      "lif layer 1 self.abs_max_v: 7201.5\n",
      "lif layer 1 self.abs_max_v: 7232.0\n",
      "lif layer 2 self.abs_max_v: 6417.5\n",
      "lif layer 2 self.abs_max_v: 6442.0\n",
      "fc layer 3 self.abs_max_out: 1883.0\n",
      "fc layer 3 self.abs_max_out: 1907.0\n",
      "fc layer 2 self.abs_max_out: 3725.0\n",
      "fc layer 3 self.abs_max_out: 1910.0\n",
      "lif layer 1 self.abs_max_v: 7507.5\n",
      "lif layer 1 self.abs_max_v: 7553.5\n",
      "lif layer 1 self.abs_max_v: 8037.5\n",
      "lif layer 2 self.abs_max_v: 6516.0\n",
      "fc layer 1 self.abs_max_out: 5187.0\n",
      "lif layer 1 self.abs_max_v: 8043.0\n",
      "lif layer 1 self.abs_max_v: 8638.5\n",
      "lif layer 2 self.abs_max_v: 6536.0\n",
      "lif layer 1 self.abs_max_v: 8891.5\n",
      "lif layer 1 self.abs_max_v: 9227.0\n",
      "fc layer 2 self.abs_max_out: 3973.0\n",
      "lif layer 2 self.abs_max_v: 6703.5\n",
      "fc layer 1 self.abs_max_out: 5211.0\n",
      "fc layer 1 self.abs_max_out: 5589.0\n",
      "lif layer 1 self.abs_max_v: 9260.0\n",
      "lif layer 1 self.abs_max_v: 9811.0\n",
      "fc layer 1 self.abs_max_out: 5785.0\n",
      "lif layer 1 self.abs_max_v: 10681.5\n",
      "lif layer 1 self.abs_max_v: 10846.0\n",
      "fc layer 1 self.abs_max_out: 6015.0\n",
      "fc layer 1 self.abs_max_out: 7093.0\n",
      "lif layer 1 self.abs_max_v: 12355.0\n",
      "fc layer 2 self.abs_max_out: 3985.0\n",
      "fc layer 2 self.abs_max_out: 4041.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.642298/  1.979178, val:  32.08%, val_best:  32.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 88.13 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1945%\n",
      "layer   2  Sparsity: 68.1105%\n",
      "layer   3  Sparsity: 58.6640%\n",
      "total_backward_count 9790 real_backward_count 1619  16.537%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 6840.5\n",
      "lif layer 2 self.abs_max_v: 7286.5\n",
      "fc layer 2 self.abs_max_out: 4143.0\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "fc layer 3 self.abs_max_out: 2029.0\n",
      "fc layer 3 self.abs_max_out: 2047.0\n",
      "fc layer 3 self.abs_max_out: 2052.0\n",
      "fc layer 3 self.abs_max_out: 2068.0\n",
      "fc layer 3 self.abs_max_out: 2220.0\n",
      "fc layer 2 self.abs_max_out: 4339.0\n",
      "fc layer 2 self.abs_max_out: 4426.0\n",
      "lif layer 2 self.abs_max_v: 7301.5\n",
      "fc layer 1 self.abs_max_out: 7122.0\n",
      "fc layer 2 self.abs_max_out: 5171.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.537917/  1.887057, val:  40.00%, val_best:  40.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 87.61 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2108%\n",
      "layer   2  Sparsity: 70.1330%\n",
      "layer   3  Sparsity: 58.1891%\n",
      "total_backward_count 19580 real_backward_count 2954  15.087%\n",
      "lif layer 2 self.abs_max_v: 7537.5\n",
      "lif layer 2 self.abs_max_v: 8119.0\n",
      "lif layer 2 self.abs_max_v: 8146.5\n",
      "lif layer 2 self.abs_max_v: 8405.0\n",
      "fc layer 3 self.abs_max_out: 2223.0\n",
      "fc layer 3 self.abs_max_out: 2286.0\n",
      "lif layer 2 self.abs_max_v: 8724.0\n",
      "fc layer 3 self.abs_max_out: 2335.0\n",
      "fc layer 3 self.abs_max_out: 2443.0\n",
      "fc layer 3 self.abs_max_out: 2485.0\n",
      "fc layer 3 self.abs_max_out: 2551.0\n",
      "lif layer 2 self.abs_max_v: 8750.5\n",
      "fc layer 1 self.abs_max_out: 7557.0\n",
      "lif layer 1 self.abs_max_v: 13604.5\n",
      "lif layer 2 self.abs_max_v: 8826.5\n",
      "lif layer 2 self.abs_max_v: 8986.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.487951/  1.838650, val:  40.00%, val_best:  40.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2184%\n",
      "layer   2  Sparsity: 71.3858%\n",
      "layer   3  Sparsity: 58.4698%\n",
      "total_backward_count 29370 real_backward_count 4281  14.576%\n",
      "lif layer 2 self.abs_max_v: 9026.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.460856/  1.828400, val:  38.33%, val_best:  40.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 86.87 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1967%\n",
      "layer   2  Sparsity: 70.9870%\n",
      "layer   3  Sparsity: 59.8462%\n",
      "total_backward_count 39160 real_backward_count 5610  14.326%\n",
      "lif layer 2 self.abs_max_v: 9125.5\n",
      "lif layer 2 self.abs_max_v: 9261.0\n",
      "fc layer 1 self.abs_max_out: 7693.0\n",
      "fc layer 1 self.abs_max_out: 8247.0\n",
      "lif layer 1 self.abs_max_v: 14507.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.454248/  1.827839, val:  48.75%, val_best:  48.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.91 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2005%\n",
      "layer   2  Sparsity: 71.2256%\n",
      "layer   3  Sparsity: 59.9966%\n",
      "total_backward_count 48950 real_backward_count 6837  13.967%\n",
      "fc layer 3 self.abs_max_out: 2562.0\n",
      "fc layer 2 self.abs_max_out: 5430.0\n",
      "lif layer 2 self.abs_max_v: 9741.5\n",
      "lif layer 1 self.abs_max_v: 14538.0\n",
      "lif layer 1 self.abs_max_v: 14970.5\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.437315/  1.822824, val:  46.25%, val_best:  48.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 87.90 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1989%\n",
      "layer   2  Sparsity: 71.1848%\n",
      "layer   3  Sparsity: 60.2406%\n",
      "total_backward_count 58740 real_backward_count 8086  13.766%\n",
      "lif layer 2 self.abs_max_v: 9805.5\n",
      "lif layer 2 self.abs_max_v: 9952.5\n",
      "lif layer 1 self.abs_max_v: 15277.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.465812/  1.792611, val:  50.42%, val_best:  50.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.34 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1888%\n",
      "layer   2  Sparsity: 71.0666%\n",
      "layer   3  Sparsity: 61.7302%\n",
      "total_backward_count 68530 real_backward_count 9316  13.594%\n",
      "fc layer 1 self.abs_max_out: 8778.0\n",
      "lif layer 1 self.abs_max_v: 15311.0\n",
      "fc layer 2 self.abs_max_out: 5528.0\n",
      "fc layer 2 self.abs_max_out: 5570.0\n",
      "fc layer 2 self.abs_max_out: 5674.0\n",
      "fc layer 2 self.abs_max_out: 5757.0\n",
      "fc layer 2 self.abs_max_out: 5933.0\n",
      "fc layer 2 self.abs_max_out: 5959.0\n",
      "fc layer 2 self.abs_max_out: 5991.0\n",
      "fc layer 2 self.abs_max_out: 6267.0\n",
      "fc layer 1 self.abs_max_out: 9633.0\n",
      "lif layer 1 self.abs_max_v: 16724.5\n",
      "fc layer 2 self.abs_max_out: 6548.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.442896/  1.755117, val:  50.83%, val_best:  50.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2093%\n",
      "layer   2  Sparsity: 70.5496%\n",
      "layer   3  Sparsity: 62.0847%\n",
      "total_backward_count 78320 real_backward_count 10451  13.344%\n",
      "fc layer 3 self.abs_max_out: 2566.0\n",
      "fc layer 3 self.abs_max_out: 2618.0\n",
      "fc layer 3 self.abs_max_out: 2639.0\n",
      "fc layer 2 self.abs_max_out: 6844.0\n",
      "lif layer 1 self.abs_max_v: 16815.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.445195/  1.748713, val:  46.25%, val_best:  50.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2180%\n",
      "layer   2  Sparsity: 70.4946%\n",
      "layer   3  Sparsity: 61.8548%\n",
      "total_backward_count 88110 real_backward_count 11643  13.214%\n",
      "fc layer 3 self.abs_max_out: 2667.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.425802/  1.791641, val:  44.58%, val_best:  50.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2091%\n",
      "layer   2  Sparsity: 70.8521%\n",
      "layer   3  Sparsity: 63.0018%\n",
      "total_backward_count 97900 real_backward_count 12782  13.056%\n",
      "lif layer 2 self.abs_max_v: 10074.5\n",
      "lif layer 2 self.abs_max_v: 10410.0\n",
      "fc layer 1 self.abs_max_out: 9755.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.434458/  1.787739, val:  42.92%, val_best:  50.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2084%\n",
      "layer   2  Sparsity: 69.5752%\n",
      "layer   3  Sparsity: 63.4717%\n",
      "total_backward_count 107690 real_backward_count 13920  12.926%\n",
      "lif layer 1 self.abs_max_v: 16891.5\n",
      "lif layer 1 self.abs_max_v: 17833.5\n",
      "lif layer 1 self.abs_max_v: 18000.5\n",
      "lif layer 1 self.abs_max_v: 18524.5\n",
      "fc layer 1 self.abs_max_out: 10537.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.439395/  1.715951, val:  62.08%, val_best:  62.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1825%\n",
      "layer   2  Sparsity: 69.5476%\n",
      "layer   3  Sparsity: 63.2903%\n",
      "total_backward_count 117480 real_backward_count 15059  12.818%\n",
      "fc layer 1 self.abs_max_out: 10549.0\n",
      "lif layer 1 self.abs_max_v: 19317.5\n",
      "lif layer 1 self.abs_max_v: 19725.0\n",
      "fc layer 1 self.abs_max_out: 10709.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.411072/  1.759464, val:  34.17%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.12 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2026%\n",
      "layer   2  Sparsity: 69.9519%\n",
      "layer   3  Sparsity: 64.1127%\n",
      "total_backward_count 127270 real_backward_count 16165  12.701%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.383639/  1.711567, val:  50.83%, val_best:  62.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 88.40 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2116%\n",
      "layer   2  Sparsity: 69.4170%\n",
      "layer   3  Sparsity: 62.5336%\n",
      "total_backward_count 137060 real_backward_count 17269  12.600%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.390648/  1.666629, val:  57.08%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.95 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1877%\n",
      "layer   2  Sparsity: 68.6516%\n",
      "layer   3  Sparsity: 64.3004%\n",
      "total_backward_count 146850 real_backward_count 18341  12.490%\n",
      "fc layer 1 self.abs_max_out: 11507.0\n",
      "lif layer 1 self.abs_max_v: 19944.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.393605/  1.729870, val:  45.83%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 88.74 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 93.1901%\n",
      "layer   2  Sparsity: 68.8888%\n",
      "layer   3  Sparsity: 65.6192%\n",
      "total_backward_count 156640 real_backward_count 19455  12.420%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.383588/  1.711979, val:  51.67%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2028%\n",
      "layer   2  Sparsity: 68.8209%\n",
      "layer   3  Sparsity: 64.6501%\n",
      "total_backward_count 166430 real_backward_count 20528  12.334%\n",
      "lif layer 2 self.abs_max_v: 10755.5\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.387830/  1.666535, val:  64.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1748%\n",
      "layer   2  Sparsity: 68.6495%\n",
      "layer   3  Sparsity: 64.8022%\n",
      "total_backward_count 176220 real_backward_count 21606  12.261%\n",
      "lif layer 2 self.abs_max_v: 10931.5\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.364113/  1.662799, val:  50.00%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1858%\n",
      "layer   2  Sparsity: 68.4307%\n",
      "layer   3  Sparsity: 64.8888%\n",
      "total_backward_count 186010 real_backward_count 22664  12.184%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.326948/  1.657691, val:  46.67%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.66 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1815%\n",
      "layer   2  Sparsity: 68.4377%\n",
      "layer   3  Sparsity: 64.8174%\n",
      "total_backward_count 195800 real_backward_count 23714  12.111%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.353449/  1.753538, val:  47.08%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.23 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1894%\n",
      "layer   2  Sparsity: 68.6130%\n",
      "layer   3  Sparsity: 65.8499%\n",
      "total_backward_count 205590 real_backward_count 24784  12.055%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.376746/  1.655857, val:  63.75%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.93 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2033%\n",
      "layer   2  Sparsity: 68.0939%\n",
      "layer   3  Sparsity: 65.4202%\n",
      "total_backward_count 215380 real_backward_count 25797  11.977%\n",
      "fc layer 1 self.abs_max_out: 11764.0\n",
      "lif layer 1 self.abs_max_v: 20871.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.373044/  1.660800, val:  58.75%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.31 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1866%\n",
      "layer   2  Sparsity: 68.5095%\n",
      "layer   3  Sparsity: 67.0457%\n",
      "total_backward_count 225170 real_backward_count 26822  11.912%\n",
      "fc layer 1 self.abs_max_out: 11779.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.371266/  1.642724, val:  60.83%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1678%\n",
      "layer   2  Sparsity: 68.4724%\n",
      "layer   3  Sparsity: 66.6703%\n",
      "total_backward_count 234960 real_backward_count 27865  11.859%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.337698/  1.650133, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.57 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2021%\n",
      "layer   2  Sparsity: 67.7533%\n",
      "layer   3  Sparsity: 65.9936%\n",
      "total_backward_count 244750 real_backward_count 28898  11.807%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.373817/  1.614896, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1954%\n",
      "layer   2  Sparsity: 68.6678%\n",
      "layer   3  Sparsity: 68.5632%\n",
      "total_backward_count 254540 real_backward_count 29953  11.768%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.386361/  1.627409, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.91 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1960%\n",
      "layer   2  Sparsity: 68.2886%\n",
      "layer   3  Sparsity: 68.3659%\n",
      "total_backward_count 264330 real_backward_count 30934  11.703%\n",
      "lif layer 2 self.abs_max_v: 11065.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.376148/  1.602294, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.76 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1694%\n",
      "layer   2  Sparsity: 67.9276%\n",
      "layer   3  Sparsity: 65.6810%\n",
      "total_backward_count 274120 real_backward_count 31931  11.649%\n",
      "lif layer 1 self.abs_max_v: 22037.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.367067/  1.646697, val:  65.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.55 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1832%\n",
      "layer   2  Sparsity: 68.2642%\n",
      "layer   3  Sparsity: 66.3190%\n",
      "total_backward_count 283910 real_backward_count 32848  11.570%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.374119/  1.660494, val:  69.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1937%\n",
      "layer   2  Sparsity: 68.0619%\n",
      "layer   3  Sparsity: 67.4222%\n",
      "total_backward_count 293700 real_backward_count 33806  11.510%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.382024/  1.660272, val:  62.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2134%\n",
      "layer   2  Sparsity: 67.7286%\n",
      "layer   3  Sparsity: 67.5459%\n",
      "total_backward_count 303490 real_backward_count 34787  11.462%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.335029/  1.626529, val:  46.25%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.08 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1770%\n",
      "layer   2  Sparsity: 67.9448%\n",
      "layer   3  Sparsity: 65.2229%\n",
      "total_backward_count 313280 real_backward_count 35749  11.411%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.321620/  1.648343, val:  69.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.22 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1916%\n",
      "layer   2  Sparsity: 68.5409%\n",
      "layer   3  Sparsity: 65.6303%\n",
      "total_backward_count 323070 real_backward_count 36684  11.355%\n",
      "fc layer 1 self.abs_max_out: 12352.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.334998/  1.566425, val:  65.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2143%\n",
      "layer   2  Sparsity: 68.3309%\n",
      "layer   3  Sparsity: 65.7270%\n",
      "total_backward_count 332860 real_backward_count 37614  11.300%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.324856/  1.633718, val:  54.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.11 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1956%\n",
      "layer   2  Sparsity: 68.4291%\n",
      "layer   3  Sparsity: 68.1367%\n",
      "total_backward_count 342650 real_backward_count 38513  11.240%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.336125/  1.608778, val:  60.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1871%\n",
      "layer   2  Sparsity: 68.1000%\n",
      "layer   3  Sparsity: 67.6926%\n",
      "total_backward_count 352440 real_backward_count 39388  11.176%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.332357/  1.603474, val:  72.92%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.32 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2161%\n",
      "layer   2  Sparsity: 68.0130%\n",
      "layer   3  Sparsity: 66.7292%\n",
      "total_backward_count 362230 real_backward_count 40270  11.117%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.310374/  1.623469, val:  62.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.05 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2116%\n",
      "layer   2  Sparsity: 68.1284%\n",
      "layer   3  Sparsity: 68.0177%\n",
      "total_backward_count 372020 real_backward_count 41172  11.067%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.321117/  1.599524, val:  53.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.81 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1720%\n",
      "layer   2  Sparsity: 67.8872%\n",
      "layer   3  Sparsity: 65.8694%\n",
      "total_backward_count 381810 real_backward_count 42101  11.027%\n",
      "lif layer 2 self.abs_max_v: 11505.5\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.267495/  1.574707, val:  62.50%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1988%\n",
      "layer   2  Sparsity: 68.2467%\n",
      "layer   3  Sparsity: 64.8017%\n",
      "total_backward_count 391600 real_backward_count 42982  10.976%\n",
      "lif layer 2 self.abs_max_v: 11522.5\n",
      "lif layer 2 self.abs_max_v: 11910.5\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.289771/  1.581142, val:  59.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1883%\n",
      "layer   2  Sparsity: 67.8842%\n",
      "layer   3  Sparsity: 65.8721%\n",
      "total_backward_count 401390 real_backward_count 43852  10.925%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.306527/  1.604864, val:  76.67%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1981%\n",
      "layer   2  Sparsity: 67.8402%\n",
      "layer   3  Sparsity: 67.2726%\n",
      "total_backward_count 411180 real_backward_count 44748  10.883%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.308157/  1.595756, val:  66.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.94 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2090%\n",
      "layer   2  Sparsity: 67.9832%\n",
      "layer   3  Sparsity: 68.6895%\n",
      "total_backward_count 420970 real_backward_count 45671  10.849%\n",
      "fc layer 1 self.abs_max_out: 12597.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.290408/  1.532959, val:  69.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.08 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2191%\n",
      "layer   2  Sparsity: 67.8075%\n",
      "layer   3  Sparsity: 66.7513%\n",
      "total_backward_count 430760 real_backward_count 46538  10.804%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.274209/  1.599464, val:  56.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2322%\n",
      "layer   2  Sparsity: 67.7718%\n",
      "layer   3  Sparsity: 66.0326%\n",
      "total_backward_count 440550 real_backward_count 47404  10.760%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.264032/  1.553712, val:  70.00%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.48 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1859%\n",
      "layer   2  Sparsity: 67.5175%\n",
      "layer   3  Sparsity: 65.1258%\n",
      "total_backward_count 450340 real_backward_count 48230  10.710%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.280723/  1.583256, val:  63.33%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.18 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1889%\n",
      "layer   2  Sparsity: 67.4669%\n",
      "layer   3  Sparsity: 64.8034%\n",
      "total_backward_count 460130 real_backward_count 49062  10.663%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.269269/  1.579363, val:  61.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.74 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2122%\n",
      "layer   2  Sparsity: 67.3178%\n",
      "layer   3  Sparsity: 66.6378%\n",
      "total_backward_count 469920 real_backward_count 49860  10.610%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.258189/  1.604752, val:  60.83%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.96 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1742%\n",
      "layer   2  Sparsity: 67.6651%\n",
      "layer   3  Sparsity: 66.0336%\n",
      "total_backward_count 479710 real_backward_count 50677  10.564%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.290922/  1.523694, val:  69.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.69 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1846%\n",
      "layer   2  Sparsity: 67.7628%\n",
      "layer   3  Sparsity: 65.2272%\n",
      "total_backward_count 489500 real_backward_count 51500  10.521%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.293639/  1.592579, val:  70.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2043%\n",
      "layer   2  Sparsity: 67.3737%\n",
      "layer   3  Sparsity: 67.8485%\n",
      "total_backward_count 499290 real_backward_count 52345  10.484%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.314896/  1.590483, val:  75.00%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1710%\n",
      "layer   2  Sparsity: 67.2694%\n",
      "layer   3  Sparsity: 68.5548%\n",
      "total_backward_count 509080 real_backward_count 53164  10.443%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.308284/  1.620674, val:  66.67%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.08 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1970%\n",
      "layer   2  Sparsity: 67.2690%\n",
      "layer   3  Sparsity: 67.0225%\n",
      "total_backward_count 518870 real_backward_count 53978  10.403%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.303467/  1.538264, val:  74.58%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1851%\n",
      "layer   2  Sparsity: 67.2146%\n",
      "layer   3  Sparsity: 67.3385%\n",
      "total_backward_count 528660 real_backward_count 54781  10.362%\n",
      "fc layer 1 self.abs_max_out: 12673.0\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.259793/  1.534410, val:  67.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.90 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1971%\n",
      "layer   2  Sparsity: 67.1323%\n",
      "layer   3  Sparsity: 65.5450%\n",
      "total_backward_count 538450 real_backward_count 55611  10.328%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.260228/  1.570662, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.46 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1999%\n",
      "layer   2  Sparsity: 67.2921%\n",
      "layer   3  Sparsity: 65.5756%\n",
      "total_backward_count 548240 real_backward_count 56476  10.301%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.285639/  1.588371, val:  58.33%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1786%\n",
      "layer   2  Sparsity: 67.5028%\n",
      "layer   3  Sparsity: 66.6697%\n",
      "total_backward_count 558030 real_backward_count 57331  10.274%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.279281/  1.534876, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.93 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2015%\n",
      "layer   2  Sparsity: 67.6905%\n",
      "layer   3  Sparsity: 66.4411%\n",
      "total_backward_count 567820 real_backward_count 58153  10.241%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.259182/  1.548408, val:  76.25%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1961%\n",
      "layer   2  Sparsity: 67.0995%\n",
      "layer   3  Sparsity: 67.0926%\n",
      "total_backward_count 577610 real_backward_count 58914  10.200%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.276605/  1.570116, val:  62.08%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2067%\n",
      "layer   2  Sparsity: 66.7612%\n",
      "layer   3  Sparsity: 67.4630%\n",
      "total_backward_count 587400 real_backward_count 59735  10.169%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.293426/  1.580396, val:  60.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.47 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2038%\n",
      "layer   2  Sparsity: 67.2290%\n",
      "layer   3  Sparsity: 68.4090%\n",
      "total_backward_count 597190 real_backward_count 60575  10.143%\n",
      "fc layer 1 self.abs_max_out: 12725.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.285086/  1.519443, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.93 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1834%\n",
      "layer   2  Sparsity: 67.6881%\n",
      "layer   3  Sparsity: 68.4803%\n",
      "total_backward_count 606980 real_backward_count 61385  10.113%\n",
      "lif layer 1 self.abs_max_v: 22842.5\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.282032/  1.590251, val:  63.33%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.38 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1926%\n",
      "layer   2  Sparsity: 67.1644%\n",
      "layer   3  Sparsity: 68.1035%\n",
      "total_backward_count 616770 real_backward_count 62178  10.081%\n",
      "fc layer 1 self.abs_max_out: 12989.0\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.270409/  1.541636, val:  68.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.09 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2148%\n",
      "layer   2  Sparsity: 67.3179%\n",
      "layer   3  Sparsity: 67.5478%\n",
      "total_backward_count 626560 real_backward_count 62919  10.042%\n",
      "fc layer 1 self.abs_max_out: 13493.0\n",
      "lif layer 1 self.abs_max_v: 22938.5\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.302552/  1.579788, val:  64.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.18 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2196%\n",
      "layer   2  Sparsity: 66.7978%\n",
      "layer   3  Sparsity: 67.1942%\n",
      "total_backward_count 636350 real_backward_count 63706  10.011%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.290348/  1.564982, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.19 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1908%\n",
      "layer   2  Sparsity: 66.6009%\n",
      "layer   3  Sparsity: 67.9769%\n",
      "total_backward_count 646140 real_backward_count 64542   9.989%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.293766/  1.547332, val:  74.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1884%\n",
      "layer   2  Sparsity: 66.6395%\n",
      "layer   3  Sparsity: 67.2500%\n",
      "total_backward_count 655930 real_backward_count 65325   9.959%\n",
      "fc layer 1 self.abs_max_out: 14130.0\n",
      "lif layer 1 self.abs_max_v: 24686.5\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.290953/  1.564244, val:  74.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1794%\n",
      "layer   2  Sparsity: 66.8644%\n",
      "layer   3  Sparsity: 68.9163%\n",
      "total_backward_count 665720 real_backward_count 66111   9.931%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.297925/  1.568669, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1838%\n",
      "layer   2  Sparsity: 67.3962%\n",
      "layer   3  Sparsity: 69.7416%\n",
      "total_backward_count 675510 real_backward_count 66862   9.898%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.300046/  1.555853, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1906%\n",
      "layer   2  Sparsity: 67.0793%\n",
      "layer   3  Sparsity: 67.1934%\n",
      "total_backward_count 685300 real_backward_count 67618   9.867%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.277416/  1.472437, val:  80.83%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.09 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1998%\n",
      "layer   2  Sparsity: 66.8953%\n",
      "layer   3  Sparsity: 66.5345%\n",
      "total_backward_count 695090 real_backward_count 68381   9.838%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.244790/  1.584841, val:  57.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.00 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1849%\n",
      "layer   2  Sparsity: 66.9138%\n",
      "layer   3  Sparsity: 66.9644%\n",
      "total_backward_count 704880 real_backward_count 69132   9.808%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.234098/  1.487109, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.64 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1935%\n",
      "layer   2  Sparsity: 66.7244%\n",
      "layer   3  Sparsity: 66.5300%\n",
      "total_backward_count 714670 real_backward_count 69849   9.774%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.237303/  1.582226, val:  59.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1971%\n",
      "layer   2  Sparsity: 66.7804%\n",
      "layer   3  Sparsity: 66.6132%\n",
      "total_backward_count 724460 real_backward_count 70602   9.745%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.261843/  1.523828, val:  70.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2056%\n",
      "layer   2  Sparsity: 67.0189%\n",
      "layer   3  Sparsity: 67.3023%\n",
      "total_backward_count 734250 real_backward_count 71420   9.727%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.269846/  1.509458, val:  75.42%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1813%\n",
      "layer   2  Sparsity: 67.0311%\n",
      "layer   3  Sparsity: 67.2920%\n",
      "total_backward_count 744040 real_backward_count 72170   9.700%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.249560/  1.450591, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.81 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2089%\n",
      "layer   2  Sparsity: 66.9632%\n",
      "layer   3  Sparsity: 66.0206%\n",
      "total_backward_count 753830 real_backward_count 72912   9.672%\n",
      "fc layer 3 self.abs_max_out: 2693.0\n",
      "fc layer 3 self.abs_max_out: 2825.0\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.252346/  1.567273, val:  59.58%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 85.28 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 93.2041%\n",
      "layer   2  Sparsity: 67.0033%\n",
      "layer   3  Sparsity: 67.8333%\n",
      "total_backward_count 763620 real_backward_count 73674   9.648%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.239920/  1.506511, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2084%\n",
      "layer   2  Sparsity: 66.6808%\n",
      "layer   3  Sparsity: 67.5680%\n",
      "total_backward_count 773410 real_backward_count 74444   9.625%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.256994/  1.564969, val:  73.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.68 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1921%\n",
      "layer   2  Sparsity: 66.7340%\n",
      "layer   3  Sparsity: 68.0817%\n",
      "total_backward_count 783200 real_backward_count 75177   9.599%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.277323/  1.523983, val:  75.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.60 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1756%\n",
      "layer   2  Sparsity: 66.6483%\n",
      "layer   3  Sparsity: 66.9561%\n",
      "total_backward_count 792990 real_backward_count 75966   9.580%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.241584/  1.553263, val:  66.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.15 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2045%\n",
      "layer   2  Sparsity: 67.3747%\n",
      "layer   3  Sparsity: 67.9381%\n",
      "total_backward_count 802780 real_backward_count 76694   9.554%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.269357/  1.529692, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1926%\n",
      "layer   2  Sparsity: 67.1732%\n",
      "layer   3  Sparsity: 68.9105%\n",
      "total_backward_count 812570 real_backward_count 77428   9.529%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.280179/  1.556864, val:  62.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1801%\n",
      "layer   2  Sparsity: 67.4940%\n",
      "layer   3  Sparsity: 68.2718%\n",
      "total_backward_count 822360 real_backward_count 78184   9.507%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.265163/  1.535318, val:  58.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.87 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2051%\n",
      "layer   2  Sparsity: 66.9020%\n",
      "layer   3  Sparsity: 68.0419%\n",
      "total_backward_count 832150 real_backward_count 78929   9.485%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.259207/  1.553079, val:  63.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.24 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2021%\n",
      "layer   2  Sparsity: 67.1858%\n",
      "layer   3  Sparsity: 68.0655%\n",
      "total_backward_count 841940 real_backward_count 79658   9.461%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.242337/  1.503347, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1828%\n",
      "layer   2  Sparsity: 66.8096%\n",
      "layer   3  Sparsity: 67.7002%\n",
      "total_backward_count 851730 real_backward_count 80390   9.438%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.244273/  1.497511, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1624%\n",
      "layer   2  Sparsity: 66.4172%\n",
      "layer   3  Sparsity: 68.7453%\n",
      "total_backward_count 861520 real_backward_count 81123   9.416%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.271172/  1.503040, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.44 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1780%\n",
      "layer   2  Sparsity: 66.5644%\n",
      "layer   3  Sparsity: 69.4380%\n",
      "total_backward_count 871310 real_backward_count 81903   9.400%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.280551/  1.524110, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2049%\n",
      "layer   2  Sparsity: 66.5886%\n",
      "layer   3  Sparsity: 66.6379%\n",
      "total_backward_count 881100 real_backward_count 82685   9.384%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.285635/  1.553328, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2007%\n",
      "layer   2  Sparsity: 66.4620%\n",
      "layer   3  Sparsity: 67.2266%\n",
      "total_backward_count 890890 real_backward_count 83446   9.367%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.245522/  1.495399, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1714%\n",
      "layer   2  Sparsity: 66.3388%\n",
      "layer   3  Sparsity: 66.0671%\n",
      "total_backward_count 900680 real_backward_count 84168   9.345%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.213727/  1.502486, val:  66.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.24 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2104%\n",
      "layer   2  Sparsity: 66.9031%\n",
      "layer   3  Sparsity: 67.1943%\n",
      "total_backward_count 910470 real_backward_count 84917   9.327%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.264744/  1.535098, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.06 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2073%\n",
      "layer   2  Sparsity: 66.5877%\n",
      "layer   3  Sparsity: 68.9998%\n",
      "total_backward_count 920260 real_backward_count 85639   9.306%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.261280/  1.500995, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2130%\n",
      "layer   2  Sparsity: 66.7859%\n",
      "layer   3  Sparsity: 68.0128%\n",
      "total_backward_count 930050 real_backward_count 86304   9.280%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.251444/  1.518296, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2111%\n",
      "layer   2  Sparsity: 66.8619%\n",
      "layer   3  Sparsity: 67.7406%\n",
      "total_backward_count 939840 real_backward_count 87028   9.260%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.255122/  1.579640, val:  69.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.91 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1720%\n",
      "layer   2  Sparsity: 66.7225%\n",
      "layer   3  Sparsity: 67.8582%\n",
      "total_backward_count 949630 real_backward_count 87738   9.239%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.272720/  1.485709, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2059%\n",
      "layer   2  Sparsity: 66.9446%\n",
      "layer   3  Sparsity: 67.8954%\n",
      "total_backward_count 959420 real_backward_count 88449   9.219%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.243513/  1.575457, val:  67.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2065%\n",
      "layer   2  Sparsity: 66.9260%\n",
      "layer   3  Sparsity: 66.5508%\n",
      "total_backward_count 969210 real_backward_count 89118   9.195%\n",
      "fc layer 2 self.abs_max_out: 6905.0\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.229370/  1.481743, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.34 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2052%\n",
      "layer   2  Sparsity: 66.9606%\n",
      "layer   3  Sparsity: 67.8705%\n",
      "total_backward_count 979000 real_backward_count 89845   9.177%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.244146/  1.536775, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.00 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1904%\n",
      "layer   2  Sparsity: 67.0199%\n",
      "layer   3  Sparsity: 68.2599%\n",
      "total_backward_count 988790 real_backward_count 90603   9.163%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.268436/  1.530435, val:  75.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1898%\n",
      "layer   2  Sparsity: 66.8925%\n",
      "layer   3  Sparsity: 68.5712%\n",
      "total_backward_count 998580 real_backward_count 91336   9.147%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.285496/  1.580814, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.77 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2286%\n",
      "layer   2  Sparsity: 66.7732%\n",
      "layer   3  Sparsity: 69.6754%\n",
      "total_backward_count 1008370 real_backward_count 92047   9.128%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.263001/  1.498584, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.94 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1867%\n",
      "layer   2  Sparsity: 66.8425%\n",
      "layer   3  Sparsity: 67.3628%\n",
      "total_backward_count 1018160 real_backward_count 92792   9.114%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.239318/  1.446283, val:  84.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2109%\n",
      "layer   2  Sparsity: 66.8146%\n",
      "layer   3  Sparsity: 68.5073%\n",
      "total_backward_count 1027950 real_backward_count 93513   9.097%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.210994/  1.484683, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1907%\n",
      "layer   2  Sparsity: 66.5446%\n",
      "layer   3  Sparsity: 67.9676%\n",
      "total_backward_count 1037740 real_backward_count 94222   9.080%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.210101/  1.537979, val:  66.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.85 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2020%\n",
      "layer   2  Sparsity: 66.6359%\n",
      "layer   3  Sparsity: 68.6088%\n",
      "total_backward_count 1047530 real_backward_count 94897   9.059%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.279020/  1.584901, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1915%\n",
      "layer   2  Sparsity: 66.5000%\n",
      "layer   3  Sparsity: 69.7531%\n",
      "total_backward_count 1057320 real_backward_count 95617   9.043%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.283333/  1.536561, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.81 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1939%\n",
      "layer   2  Sparsity: 66.8779%\n",
      "layer   3  Sparsity: 68.8438%\n",
      "total_backward_count 1067110 real_backward_count 96322   9.026%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.288541/  1.569379, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1731%\n",
      "layer   2  Sparsity: 67.0546%\n",
      "layer   3  Sparsity: 69.5434%\n",
      "total_backward_count 1076900 real_backward_count 97015   9.009%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.288708/  1.539893, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.73 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2067%\n",
      "layer   2  Sparsity: 66.9932%\n",
      "layer   3  Sparsity: 68.9610%\n",
      "total_backward_count 1086690 real_backward_count 97645   8.986%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.292031/  1.586100, val:  69.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2090%\n",
      "layer   2  Sparsity: 66.7727%\n",
      "layer   3  Sparsity: 69.5589%\n",
      "total_backward_count 1096480 real_backward_count 98359   8.970%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.285977/  1.527739, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2104%\n",
      "layer   2  Sparsity: 66.8735%\n",
      "layer   3  Sparsity: 69.6868%\n",
      "total_backward_count 1106270 real_backward_count 99054   8.954%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.273352/  1.566414, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1661%\n",
      "layer   2  Sparsity: 67.3820%\n",
      "layer   3  Sparsity: 71.1126%\n",
      "total_backward_count 1116060 real_backward_count 99756   8.938%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.275775/  1.502621, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2086%\n",
      "layer   2  Sparsity: 67.1292%\n",
      "layer   3  Sparsity: 69.3548%\n",
      "total_backward_count 1125850 real_backward_count 100414   8.919%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.250972/  1.536437, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.18 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2003%\n",
      "layer   2  Sparsity: 67.0932%\n",
      "layer   3  Sparsity: 69.2099%\n",
      "total_backward_count 1135640 real_backward_count 101079   8.901%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.268493/  1.493739, val:  75.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.31 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1956%\n",
      "layer   2  Sparsity: 66.8973%\n",
      "layer   3  Sparsity: 68.6215%\n",
      "total_backward_count 1145430 real_backward_count 101743   8.883%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.239443/  1.524308, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2035%\n",
      "layer   2  Sparsity: 67.1051%\n",
      "layer   3  Sparsity: 68.0044%\n",
      "total_backward_count 1155220 real_backward_count 102399   8.864%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.240139/  1.537244, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.44 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1778%\n",
      "layer   2  Sparsity: 66.8544%\n",
      "layer   3  Sparsity: 67.9280%\n",
      "total_backward_count 1165010 real_backward_count 103096   8.849%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.266777/  1.603217, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1886%\n",
      "layer   2  Sparsity: 66.6068%\n",
      "layer   3  Sparsity: 69.1478%\n",
      "total_backward_count 1174800 real_backward_count 103846   8.839%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.291904/  1.529530, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.28 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1810%\n",
      "layer   2  Sparsity: 67.0655%\n",
      "layer   3  Sparsity: 67.9397%\n",
      "total_backward_count 1184590 real_backward_count 104565   8.827%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.265169/  1.570378, val:  76.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.84 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2013%\n",
      "layer   2  Sparsity: 67.4845%\n",
      "layer   3  Sparsity: 68.3477%\n",
      "total_backward_count 1194380 real_backward_count 105233   8.811%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.283880/  1.502442, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.26 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1859%\n",
      "layer   2  Sparsity: 67.1125%\n",
      "layer   3  Sparsity: 67.9901%\n",
      "total_backward_count 1204170 real_backward_count 105916   8.796%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.252490/  1.502305, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.36 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1968%\n",
      "layer   2  Sparsity: 67.2476%\n",
      "layer   3  Sparsity: 69.9563%\n",
      "total_backward_count 1213960 real_backward_count 106590   8.780%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.263107/  1.509638, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.46 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2134%\n",
      "layer   2  Sparsity: 66.8347%\n",
      "layer   3  Sparsity: 69.4231%\n",
      "total_backward_count 1223750 real_backward_count 107298   8.768%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.269346/  1.544143, val:  77.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.04 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1722%\n",
      "layer   2  Sparsity: 67.2268%\n",
      "layer   3  Sparsity: 68.8708%\n",
      "total_backward_count 1233540 real_backward_count 107955   8.752%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.242815/  1.535035, val:  68.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2161%\n",
      "layer   2  Sparsity: 66.9686%\n",
      "layer   3  Sparsity: 69.1982%\n",
      "total_backward_count 1243330 real_backward_count 108623   8.736%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.272307/  1.581291, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.98 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2095%\n",
      "layer   2  Sparsity: 66.8448%\n",
      "layer   3  Sparsity: 69.0148%\n",
      "total_backward_count 1253120 real_backward_count 109324   8.724%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.269953/  1.535565, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.11 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1794%\n",
      "layer   2  Sparsity: 67.0733%\n",
      "layer   3  Sparsity: 70.0564%\n",
      "total_backward_count 1262910 real_backward_count 110027   8.712%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.262025/  1.581219, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.87 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1992%\n",
      "layer   2  Sparsity: 66.6472%\n",
      "layer   3  Sparsity: 70.6708%\n",
      "total_backward_count 1272700 real_backward_count 110753   8.702%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.320056/  1.576863, val:  67.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1788%\n",
      "layer   2  Sparsity: 66.3982%\n",
      "layer   3  Sparsity: 70.9912%\n",
      "total_backward_count 1282490 real_backward_count 111471   8.692%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.269407/  1.491515, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.43 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1906%\n",
      "layer   2  Sparsity: 66.8862%\n",
      "layer   3  Sparsity: 69.4236%\n",
      "total_backward_count 1292280 real_backward_count 112144   8.678%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.240896/  1.533797, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.94 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2030%\n",
      "layer   2  Sparsity: 67.0601%\n",
      "layer   3  Sparsity: 69.9152%\n",
      "total_backward_count 1302070 real_backward_count 112748   8.659%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.273004/  1.532173, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.03 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2022%\n",
      "layer   2  Sparsity: 66.8607%\n",
      "layer   3  Sparsity: 69.5255%\n",
      "total_backward_count 1311860 real_backward_count 113428   8.646%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.262736/  1.542974, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.22 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1834%\n",
      "layer   2  Sparsity: 66.8125%\n",
      "layer   3  Sparsity: 71.1061%\n",
      "total_backward_count 1321650 real_backward_count 114091   8.632%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.258512/  1.520371, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1823%\n",
      "layer   2  Sparsity: 66.4853%\n",
      "layer   3  Sparsity: 71.1601%\n",
      "total_backward_count 1331440 real_backward_count 114729   8.617%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.271003/  1.477368, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.30 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1969%\n",
      "layer   2  Sparsity: 66.7106%\n",
      "layer   3  Sparsity: 71.5934%\n",
      "total_backward_count 1341230 real_backward_count 115444   8.607%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.276338/  1.533182, val:  78.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.32 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2116%\n",
      "layer   2  Sparsity: 66.3479%\n",
      "layer   3  Sparsity: 70.7957%\n",
      "total_backward_count 1351020 real_backward_count 116101   8.594%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.301407/  1.563910, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1848%\n",
      "layer   2  Sparsity: 66.7185%\n",
      "layer   3  Sparsity: 71.5724%\n",
      "total_backward_count 1360810 real_backward_count 116824   8.585%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.295778/  1.564656, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1994%\n",
      "layer   2  Sparsity: 66.3358%\n",
      "layer   3  Sparsity: 71.7705%\n",
      "total_backward_count 1370600 real_backward_count 117517   8.574%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.277885/  1.492231, val:  75.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.77 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1739%\n",
      "layer   2  Sparsity: 66.6434%\n",
      "layer   3  Sparsity: 72.1026%\n",
      "total_backward_count 1380390 real_backward_count 118175   8.561%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.264638/  1.522467, val:  77.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2024%\n",
      "layer   2  Sparsity: 66.6400%\n",
      "layer   3  Sparsity: 71.2244%\n",
      "total_backward_count 1390180 real_backward_count 118873   8.551%\n",
      "fc layer 2 self.abs_max_out: 6933.0\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.255802/  1.517441, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1836%\n",
      "layer   2  Sparsity: 66.7158%\n",
      "layer   3  Sparsity: 71.2242%\n",
      "total_backward_count 1399970 real_backward_count 119553   8.540%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.295073/  1.517486, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.1758%\n",
      "layer   2  Sparsity: 66.6931%\n",
      "layer   3  Sparsity: 71.6702%\n",
      "total_backward_count 1409760 real_backward_count 120251   8.530%\n",
      "fc layer 2 self.abs_max_out: 7016.0\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.264835/  1.543429, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2048%\n",
      "layer   2  Sparsity: 66.6620%\n",
      "layer   3  Sparsity: 71.0360%\n",
      "total_backward_count 1419550 real_backward_count 120971   8.522%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.253083/  1.498017, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.64 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2084%\n",
      "layer   2  Sparsity: 67.1026%\n",
      "layer   3  Sparsity: 70.6814%\n",
      "total_backward_count 1429340 real_backward_count 121634   8.510%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.239128/  1.573462, val:  65.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.91 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2061%\n",
      "layer   2  Sparsity: 66.8271%\n",
      "layer   3  Sparsity: 70.6858%\n",
      "total_backward_count 1439130 real_backward_count 122318   8.499%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.250094/  1.525162, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.87 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1904%\n",
      "layer   2  Sparsity: 66.8001%\n",
      "layer   3  Sparsity: 71.0400%\n",
      "total_backward_count 1448920 real_backward_count 122977   8.487%\n",
      "fc layer 2 self.abs_max_out: 7197.0\n",
      "fc layer 1 self.abs_max_out: 14335.0\n",
      "lif layer 1 self.abs_max_v: 24993.0\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.263343/  1.523502, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.14 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2018%\n",
      "layer   2  Sparsity: 66.5019%\n",
      "layer   3  Sparsity: 70.3260%\n",
      "total_backward_count 1458710 real_backward_count 123644   8.476%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.291500/  1.551437, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1959%\n",
      "layer   2  Sparsity: 66.4498%\n",
      "layer   3  Sparsity: 72.0637%\n",
      "total_backward_count 1468500 real_backward_count 124325   8.466%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.298380/  1.551952, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2262%\n",
      "layer   2  Sparsity: 66.8320%\n",
      "layer   3  Sparsity: 70.6859%\n",
      "total_backward_count 1478290 real_backward_count 125018   8.457%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.295239/  1.556826, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.91 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2011%\n",
      "layer   2  Sparsity: 67.0174%\n",
      "layer   3  Sparsity: 70.8089%\n",
      "total_backward_count 1488080 real_backward_count 125728   8.449%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.297287/  1.558016, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.05 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2042%\n",
      "layer   2  Sparsity: 66.5940%\n",
      "layer   3  Sparsity: 71.4348%\n",
      "total_backward_count 1497870 real_backward_count 126390   8.438%\n",
      "fc layer 2 self.abs_max_out: 7291.0\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.274979/  1.496547, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1842%\n",
      "layer   2  Sparsity: 66.5538%\n",
      "layer   3  Sparsity: 69.1795%\n",
      "total_backward_count 1507660 real_backward_count 127103   8.430%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.251462/  1.527713, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1844%\n",
      "layer   2  Sparsity: 66.9686%\n",
      "layer   3  Sparsity: 70.3294%\n",
      "total_backward_count 1517450 real_backward_count 127740   8.418%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.293695/  1.535212, val:  73.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.09 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2035%\n",
      "layer   2  Sparsity: 66.9479%\n",
      "layer   3  Sparsity: 71.2925%\n",
      "total_backward_count 1527240 real_backward_count 128394   8.407%\n",
      "fc layer 2 self.abs_max_out: 7372.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.296032/  1.618602, val:  55.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.78 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1954%\n",
      "layer   2  Sparsity: 66.6361%\n",
      "layer   3  Sparsity: 69.9676%\n",
      "total_backward_count 1537030 real_backward_count 129078   8.398%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.284123/  1.523596, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1835%\n",
      "layer   2  Sparsity: 66.7155%\n",
      "layer   3  Sparsity: 70.1537%\n",
      "total_backward_count 1546820 real_backward_count 129707   8.385%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.274919/  1.515770, val:  66.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2142%\n",
      "layer   2  Sparsity: 66.9407%\n",
      "layer   3  Sparsity: 68.4898%\n",
      "total_backward_count 1556610 real_backward_count 130377   8.376%\n",
      "fc layer 1 self.abs_max_out: 14633.0\n",
      "lif layer 1 self.abs_max_v: 25526.0\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.251477/  1.487362, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2033%\n",
      "layer   2  Sparsity: 66.8586%\n",
      "layer   3  Sparsity: 69.1804%\n",
      "total_backward_count 1566400 real_backward_count 130955   8.360%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.227483/  1.479633, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2141%\n",
      "layer   2  Sparsity: 66.9470%\n",
      "layer   3  Sparsity: 70.1024%\n",
      "total_backward_count 1576190 real_backward_count 131625   8.351%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.244283/  1.517621, val:  76.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1944%\n",
      "layer   2  Sparsity: 66.9068%\n",
      "layer   3  Sparsity: 69.5799%\n",
      "total_backward_count 1585980 real_backward_count 132312   8.343%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.255058/  1.528049, val:  70.42%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 88.00 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1924%\n",
      "layer   2  Sparsity: 66.9665%\n",
      "layer   3  Sparsity: 70.5402%\n",
      "total_backward_count 1595770 real_backward_count 132995   8.334%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.249344/  1.589506, val:  57.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2004%\n",
      "layer   2  Sparsity: 66.7093%\n",
      "layer   3  Sparsity: 69.9376%\n",
      "total_backward_count 1605560 real_backward_count 133623   8.323%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.243597/  1.560473, val:  69.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1832%\n",
      "layer   2  Sparsity: 66.7461%\n",
      "layer   3  Sparsity: 70.0431%\n",
      "total_backward_count 1615350 real_backward_count 134326   8.316%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.261129/  1.503078, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2003%\n",
      "layer   2  Sparsity: 66.4711%\n",
      "layer   3  Sparsity: 70.0548%\n",
      "total_backward_count 1625140 real_backward_count 135011   8.308%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.241358/  1.513306, val:  73.33%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.96 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.2011%\n",
      "layer   2  Sparsity: 66.7918%\n",
      "layer   3  Sparsity: 68.5293%\n",
      "total_backward_count 1634930 real_backward_count 135632   8.296%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.237962/  1.508005, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.79 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1844%\n",
      "layer   2  Sparsity: 67.0050%\n",
      "layer   3  Sparsity: 69.7940%\n",
      "total_backward_count 1644720 real_backward_count 136331   8.289%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.241799/  1.552221, val:  76.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2177%\n",
      "layer   2  Sparsity: 66.9488%\n",
      "layer   3  Sparsity: 69.7234%\n",
      "total_backward_count 1654510 real_backward_count 136990   8.280%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.244861/  1.516394, val:  77.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1969%\n",
      "layer   2  Sparsity: 66.9018%\n",
      "layer   3  Sparsity: 70.8533%\n",
      "total_backward_count 1664300 real_backward_count 137604   8.268%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.253098/  1.541357, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1970%\n",
      "layer   2  Sparsity: 66.9573%\n",
      "layer   3  Sparsity: 70.9665%\n",
      "total_backward_count 1674090 real_backward_count 138275   8.260%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.259830/  1.617974, val:  65.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.15 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1722%\n",
      "layer   2  Sparsity: 66.6043%\n",
      "layer   3  Sparsity: 71.1720%\n",
      "total_backward_count 1683880 real_backward_count 138929   8.251%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.288039/  1.565765, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.45 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1885%\n",
      "layer   2  Sparsity: 66.9827%\n",
      "layer   3  Sparsity: 71.5270%\n",
      "total_backward_count 1693670 real_backward_count 139642   8.245%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.283333/  1.558852, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.36 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1846%\n",
      "layer   2  Sparsity: 66.9863%\n",
      "layer   3  Sparsity: 71.2133%\n",
      "total_backward_count 1703460 real_backward_count 140300   8.236%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.281926/  1.552226, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.76 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 93.1874%\n",
      "layer   2  Sparsity: 66.9840%\n",
      "layer   3  Sparsity: 70.1311%\n",
      "total_backward_count 1713250 real_backward_count 140940   8.226%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.271409/  1.515682, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.64 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1678%\n",
      "layer   2  Sparsity: 67.3596%\n",
      "layer   3  Sparsity: 71.4095%\n",
      "total_backward_count 1723040 real_backward_count 141613   8.219%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.280534/  1.541440, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.44 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1931%\n",
      "layer   2  Sparsity: 66.9708%\n",
      "layer   3  Sparsity: 71.5332%\n",
      "total_backward_count 1732830 real_backward_count 142277   8.211%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.262815/  1.565223, val:  72.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1929%\n",
      "layer   2  Sparsity: 66.9212%\n",
      "layer   3  Sparsity: 71.5094%\n",
      "total_backward_count 1742620 real_backward_count 142911   8.201%\n",
      "fc layer 2 self.abs_max_out: 7480.0\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.250782/  1.560235, val:  62.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1904%\n",
      "layer   2  Sparsity: 67.2094%\n",
      "layer   3  Sparsity: 70.8737%\n",
      "total_backward_count 1752410 real_backward_count 143569   8.193%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.260143/  1.540349, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.11 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 93.2057%\n",
      "layer   2  Sparsity: 67.1241%\n",
      "layer   3  Sparsity: 69.7974%\n",
      "total_backward_count 1762200 real_backward_count 144230   8.185%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.276372/  1.498331, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1809%\n",
      "layer   2  Sparsity: 66.5853%\n",
      "layer   3  Sparsity: 71.0582%\n",
      "total_backward_count 1771990 real_backward_count 144923   8.179%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.270500/  1.509243, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.99 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.2046%\n",
      "layer   2  Sparsity: 66.8975%\n",
      "layer   3  Sparsity: 71.7754%\n",
      "total_backward_count 1781780 real_backward_count 145595   8.171%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.257066/  1.502573, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1729%\n",
      "layer   2  Sparsity: 66.5715%\n",
      "layer   3  Sparsity: 70.0734%\n",
      "total_backward_count 1791570 real_backward_count 146247   8.163%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.219444/  1.461307, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.96 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 93.1933%\n",
      "layer   2  Sparsity: 66.8460%\n",
      "layer   3  Sparsity: 69.4801%\n",
      "total_backward_count 1801360 real_backward_count 146895   8.155%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.214428/  1.467059, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.77 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1968%\n",
      "layer   2  Sparsity: 67.4208%\n",
      "layer   3  Sparsity: 71.1737%\n",
      "total_backward_count 1811150 real_backward_count 147600   8.150%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.232370/  1.514275, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.80 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1738%\n",
      "layer   2  Sparsity: 67.0152%\n",
      "layer   3  Sparsity: 69.7110%\n",
      "total_backward_count 1820940 real_backward_count 148324   8.145%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.233079/  1.505492, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.32 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1821%\n",
      "layer   2  Sparsity: 66.7876%\n",
      "layer   3  Sparsity: 70.5160%\n",
      "total_backward_count 1830730 real_backward_count 149027   8.140%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.267368/  1.642965, val:  53.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1946%\n",
      "layer   2  Sparsity: 66.4237%\n",
      "layer   3  Sparsity: 71.9129%\n",
      "total_backward_count 1840520 real_backward_count 149742   8.136%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.295388/  1.543326, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.61 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1865%\n",
      "layer   2  Sparsity: 66.2984%\n",
      "layer   3  Sparsity: 72.9771%\n",
      "total_backward_count 1850310 real_backward_count 150432   8.130%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.283911/  1.488405, val:  70.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1730%\n",
      "layer   2  Sparsity: 67.0295%\n",
      "layer   3  Sparsity: 72.9535%\n",
      "total_backward_count 1860100 real_backward_count 151099   8.123%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.264350/  1.503213, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.06 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 93.1823%\n",
      "layer   2  Sparsity: 66.9335%\n",
      "layer   3  Sparsity: 71.8819%\n",
      "total_backward_count 1869890 real_backward_count 151741   8.115%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.271676/  1.561132, val:  57.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1931%\n",
      "layer   2  Sparsity: 66.9208%\n",
      "layer   3  Sparsity: 71.5845%\n",
      "total_backward_count 1879680 real_backward_count 152359   8.106%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.264891/  1.496750, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1848%\n",
      "layer   2  Sparsity: 66.8298%\n",
      "layer   3  Sparsity: 72.0777%\n",
      "total_backward_count 1889470 real_backward_count 153010   8.098%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.245763/  1.534133, val:  73.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.46 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2063%\n",
      "layer   2  Sparsity: 66.7259%\n",
      "layer   3  Sparsity: 71.4334%\n",
      "total_backward_count 1899260 real_backward_count 153580   8.086%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.273749/  1.476479, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.34 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1995%\n",
      "layer   2  Sparsity: 66.9115%\n",
      "layer   3  Sparsity: 72.2976%\n",
      "total_backward_count 1909050 real_backward_count 154192   8.077%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.250122/  1.580625, val:  71.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.1649%\n",
      "layer   2  Sparsity: 67.1908%\n",
      "layer   3  Sparsity: 70.5838%\n",
      "total_backward_count 1918840 real_backward_count 154837   8.069%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.239069/  1.524337, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2054%\n",
      "layer   2  Sparsity: 67.1854%\n",
      "layer   3  Sparsity: 70.8767%\n",
      "total_backward_count 1928630 real_backward_count 155479   8.062%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.253143/  1.521911, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.2345%\n",
      "layer   2  Sparsity: 66.8845%\n",
      "layer   3  Sparsity: 70.7658%\n",
      "total_backward_count 1938420 real_backward_count 156177   8.057%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.245217/  1.501124, val:  74.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.98 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.1898%\n",
      "layer   2  Sparsity: 66.9182%\n",
      "layer   3  Sparsity: 71.3405%\n",
      "total_backward_count 1948210 real_backward_count 156792   8.048%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.269185/  1.563531, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.1993%\n",
      "layer   2  Sparsity: 66.9164%\n",
      "layer   3  Sparsity: 73.0206%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c09cab1c8e24bf7bee7eb4d00e5ac4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.26919</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.71667</td></tr><tr><td>val_loss</td><td>1.56353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sweep-157</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/om0tgw21' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/om0tgw21</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_041941-om0tgw21/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d0fvqo34 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_091234-d0fvqo34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d0fvqo34' target=\"_blank\">chocolate-sweep-162</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d0fvqo34' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d0fvqo34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_091243_328', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 195.0\n",
      "lif layer 1 self.abs_max_v: 195.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 331.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 1 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 264.0\n",
      "fc layer 2 self.abs_max_out: 403.0\n",
      "lif layer 2 self.abs_max_v: 501.5\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "lif layer 1 self.abs_max_v: 339.0\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "lif layer 2 self.abs_max_v: 624.5\n",
      "fc layer 1 self.abs_max_out: 296.0\n",
      "lif layer 1 self.abs_max_v: 353.5\n",
      "lif layer 2 self.abs_max_v: 771.5\n",
      "fc layer 1 self.abs_max_out: 335.0\n",
      "lif layer 1 self.abs_max_v: 470.0\n",
      "fc layer 1 self.abs_max_out: 429.0\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 1 self.abs_max_out: 439.0\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "lif layer 2 self.abs_max_v: 788.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 1 self.abs_max_out: 442.0\n",
      "lif layer 2 self.abs_max_v: 816.5\n",
      "fc layer 1 self.abs_max_out: 445.0\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "fc layer 2 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 529.0\n",
      "lif layer 2 self.abs_max_v: 867.5\n",
      "fc layer 1 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 588.5\n",
      "lif layer 1 self.abs_max_v: 660.0\n",
      "lif layer 1 self.abs_max_v: 666.0\n",
      "fc layer 1 self.abs_max_out: 474.0\n",
      "lif layer 1 self.abs_max_v: 708.0\n",
      "fc layer 1 self.abs_max_out: 512.0\n",
      "lif layer 1 self.abs_max_v: 719.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 1 self.abs_max_out: 515.0\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "fc layer 1 self.abs_max_out: 625.0\n",
      "lif layer 1 self.abs_max_v: 794.5\n",
      "fc layer 1 self.abs_max_out: 635.0\n",
      "fc layer 1 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 998.0\n",
      "fc layer 1 self.abs_max_out: 1139.0\n",
      "lif layer 1 self.abs_max_v: 1329.5\n",
      "fc layer 2 self.abs_max_out: 581.0\n",
      "fc layer 2 self.abs_max_out: 583.0\n",
      "lif layer 2 self.abs_max_v: 892.5\n",
      "lif layer 1 self.abs_max_v: 1355.5\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "lif layer 2 self.abs_max_v: 1150.5\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "lif layer 1 self.abs_max_v: 1385.5\n",
      "lif layer 1 self.abs_max_v: 1413.0\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "lif layer 1 self.abs_max_v: 1444.0\n",
      "lif layer 2 self.abs_max_v: 1177.0\n",
      "fc layer 1 self.abs_max_out: 1352.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "lif layer 2 self.abs_max_v: 1177.5\n",
      "lif layer 1 self.abs_max_v: 1564.5\n",
      "fc layer 2 self.abs_max_out: 749.0\n",
      "lif layer 2 self.abs_max_v: 1218.5\n",
      "lif layer 2 self.abs_max_v: 1270.5\n",
      "fc layer 2 self.abs_max_out: 800.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "lif layer 1 self.abs_max_v: 1742.0\n",
      "lif layer 2 self.abs_max_v: 1374.5\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "lif layer 2 self.abs_max_v: 1377.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "lif layer 2 self.abs_max_v: 1434.5\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "lif layer 1 self.abs_max_v: 1848.5\n",
      "fc layer 1 self.abs_max_out: 1561.0\n",
      "lif layer 1 self.abs_max_v: 2485.5\n",
      "lif layer 2 self.abs_max_v: 1459.5\n",
      "lif layer 1 self.abs_max_v: 2524.0\n",
      "lif layer 2 self.abs_max_v: 1515.0\n",
      "fc layer 1 self.abs_max_out: 1733.0\n",
      "fc layer 2 self.abs_max_out: 899.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 2 self.abs_max_v: 1544.0\n",
      "fc layer 1 self.abs_max_out: 1801.0\n",
      "lif layer 2 self.abs_max_v: 1544.5\n",
      "fc layer 2 self.abs_max_out: 1024.0\n",
      "fc layer 2 self.abs_max_out: 1060.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "lif layer 2 self.abs_max_v: 1598.5\n",
      "lif layer 2 self.abs_max_v: 1810.5\n",
      "fc layer 2 self.abs_max_out: 1081.0\n",
      "lif layer 2 self.abs_max_v: 1986.5\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "lif layer 1 self.abs_max_v: 2670.5\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "lif layer 2 self.abs_max_v: 2036.5\n",
      "lif layer 2 self.abs_max_v: 2058.5\n",
      "lif layer 2 self.abs_max_v: 2077.0\n",
      "lif layer 2 self.abs_max_v: 2077.5\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "fc layer 2 self.abs_max_out: 1201.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "fc layer 1 self.abs_max_out: 2069.0\n",
      "lif layer 1 self.abs_max_v: 2715.5\n",
      "lif layer 1 self.abs_max_v: 2817.5\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 3 self.abs_max_out: 555.0\n",
      "lif layer 1 self.abs_max_v: 2963.5\n",
      "lif layer 1 self.abs_max_v: 3018.5\n",
      "lif layer 1 self.abs_max_v: 3108.5\n",
      "lif layer 1 self.abs_max_v: 3169.5\n",
      "lif layer 1 self.abs_max_v: 3212.0\n",
      "lif layer 1 self.abs_max_v: 3450.5\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "lif layer 1 self.abs_max_v: 3503.0\n",
      "lif layer 2 self.abs_max_v: 2091.5\n",
      "lif layer 2 self.abs_max_v: 2127.0\n",
      "lif layer 1 self.abs_max_v: 3727.5\n",
      "fc layer 2 self.abs_max_out: 1205.0\n",
      "lif layer 2 self.abs_max_v: 2186.5\n",
      "lif layer 2 self.abs_max_v: 2217.5\n",
      "lif layer 2 self.abs_max_v: 2236.0\n",
      "lif layer 2 self.abs_max_v: 2243.0\n",
      "fc layer 1 self.abs_max_out: 2109.0\n",
      "fc layer 1 self.abs_max_out: 2132.0\n",
      "fc layer 1 self.abs_max_out: 2162.0\n",
      "lif layer 1 self.abs_max_v: 3905.0\n",
      "fc layer 1 self.abs_max_out: 2257.0\n",
      "lif layer 1 self.abs_max_v: 4085.0\n",
      "fc layer 1 self.abs_max_out: 2513.0\n",
      "lif layer 1 self.abs_max_v: 4421.5\n",
      "lif layer 1 self.abs_max_v: 4469.0\n",
      "fc layer 2 self.abs_max_out: 1221.0\n",
      "lif layer 2 self.abs_max_v: 2272.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 2 self.abs_max_out: 1253.0\n",
      "fc layer 2 self.abs_max_out: 1286.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "lif layer 1 self.abs_max_v: 4489.5\n",
      "fc layer 2 self.abs_max_out: 1417.0\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "lif layer 1 self.abs_max_v: 4530.5\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 1 self.abs_max_out: 2548.0\n",
      "lif layer 2 self.abs_max_v: 2287.0\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "lif layer 1 self.abs_max_v: 4538.5\n",
      "lif layer 1 self.abs_max_v: 4546.5\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "lif layer 2 self.abs_max_v: 2302.0\n",
      "lif layer 2 self.abs_max_v: 2424.0\n",
      "fc layer 1 self.abs_max_out: 2634.0\n",
      "lif layer 1 self.abs_max_v: 4561.5\n",
      "lif layer 1 self.abs_max_v: 4757.0\n",
      "lif layer 1 self.abs_max_v: 4857.5\n",
      "lif layer 2 self.abs_max_v: 2447.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.447567/  1.859664, val:  30.42%, val_best:  30.42%, tr:  99.39%, tr_best:  99.39%, epoch time: 88.32 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9369%\n",
      "layer   2  Sparsity: 68.2879%\n",
      "layer   3  Sparsity: 64.1701%\n",
      "total_backward_count 9790 real_backward_count 1271  12.983%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2477.0\n",
      "lif layer 2 self.abs_max_v: 2540.5\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "lif layer 2 self.abs_max_v: 2557.5\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 3 self.abs_max_out: 746.0\n",
      "fc layer 3 self.abs_max_out: 789.0\n",
      "fc layer 3 self.abs_max_out: 822.0\n",
      "lif layer 2 self.abs_max_v: 2587.0\n",
      "lif layer 2 self.abs_max_v: 2598.5\n",
      "lif layer 2 self.abs_max_v: 2764.5\n",
      "lif layer 2 self.abs_max_v: 2803.0\n",
      "lif layer 2 self.abs_max_v: 2952.5\n",
      "fc layer 1 self.abs_max_out: 2760.0\n",
      "fc layer 1 self.abs_max_out: 3099.0\n",
      "lif layer 1 self.abs_max_v: 5105.0\n",
      "lif layer 1 self.abs_max_v: 5109.0\n",
      "lif layer 1 self.abs_max_v: 5113.0\n",
      "lif layer 1 self.abs_max_v: 5126.5\n",
      "lif layer 1 self.abs_max_v: 5256.0\n",
      "lif layer 1 self.abs_max_v: 5495.5\n",
      "lif layer 1 self.abs_max_v: 5813.0\n",
      "fc layer 1 self.abs_max_out: 3408.0\n",
      "lif layer 1 self.abs_max_v: 5910.0\n",
      "fc layer 1 self.abs_max_out: 3492.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.333211/  1.756361, val:  40.83%, val_best:  40.83%, tr:  99.39%, tr_best:  99.39%, epoch time: 88.37 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9249%\n",
      "layer   2  Sparsity: 72.1909%\n",
      "layer   3  Sparsity: 68.5097%\n",
      "total_backward_count 19580 real_backward_count 2442  12.472%\n",
      "fc layer 3 self.abs_max_out: 827.0\n",
      "fc layer 3 self.abs_max_out: 836.0\n",
      "fc layer 3 self.abs_max_out: 894.0\n",
      "fc layer 3 self.abs_max_out: 910.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "lif layer 2 self.abs_max_v: 2957.5\n",
      "lif layer 2 self.abs_max_v: 3039.0\n",
      "lif layer 2 self.abs_max_v: 3069.0\n",
      "lif layer 1 self.abs_max_v: 6036.5\n",
      "fc layer 2 self.abs_max_out: 1710.0\n",
      "lif layer 2 self.abs_max_v: 3112.0\n",
      "fc layer 2 self.abs_max_out: 1746.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "fc layer 2 self.abs_max_out: 1864.0\n",
      "lif layer 1 self.abs_max_v: 6150.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.290854/  1.694599, val:  45.83%, val_best:  45.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 88.09 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 73.9338%\n",
      "layer   3  Sparsity: 70.0853%\n",
      "total_backward_count 29370 real_backward_count 3606  12.278%\n",
      "fc layer 1 self.abs_max_out: 3610.0\n",
      "lif layer 1 self.abs_max_v: 6375.5\n",
      "lif layer 1 self.abs_max_v: 6605.5\n",
      "fc layer 1 self.abs_max_out: 3673.0\n",
      "lif layer 1 self.abs_max_v: 6722.0\n",
      "lif layer 2 self.abs_max_v: 3169.0\n",
      "lif layer 2 self.abs_max_v: 3226.5\n",
      "lif layer 2 self.abs_max_v: 3314.5\n",
      "fc layer 1 self.abs_max_out: 3677.0\n",
      "lif layer 2 self.abs_max_v: 3374.5\n",
      "fc layer 1 self.abs_max_out: 3825.0\n",
      "lif layer 1 self.abs_max_v: 6766.0\n",
      "lif layer 1 self.abs_max_v: 6776.0\n",
      "fc layer 2 self.abs_max_out: 1886.0\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "fc layer 1 self.abs_max_out: 3898.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.296341/  1.783955, val:  43.75%, val_best:  45.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 87.13 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 75.0200%\n",
      "layer   3  Sparsity: 70.8964%\n",
      "total_backward_count 39160 real_backward_count 4811  12.285%\n",
      "lif layer 2 self.abs_max_v: 3416.5\n",
      "fc layer 3 self.abs_max_out: 922.0\n",
      "fc layer 3 self.abs_max_out: 972.0\n",
      "fc layer 1 self.abs_max_out: 4012.0\n",
      "lif layer 1 self.abs_max_v: 7237.0\n",
      "fc layer 1 self.abs_max_out: 4131.0\n",
      "lif layer 1 self.abs_max_v: 7307.0\n",
      "lif layer 1 self.abs_max_v: 7349.5\n",
      "lif layer 2 self.abs_max_v: 3417.5\n",
      "lif layer 2 self.abs_max_v: 3467.0\n",
      "lif layer 1 self.abs_max_v: 7391.5\n",
      "lif layer 1 self.abs_max_v: 7457.0\n",
      "fc layer 2 self.abs_max_out: 1894.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.248706/  1.712018, val:  45.83%, val_best:  45.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 87.37 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 75.4596%\n",
      "layer   3  Sparsity: 71.4011%\n",
      "total_backward_count 48950 real_backward_count 6004  12.266%\n",
      "fc layer 1 self.abs_max_out: 4418.0\n",
      "lif layer 1 self.abs_max_v: 7579.0\n",
      "lif layer 1 self.abs_max_v: 7802.5\n",
      "fc layer 2 self.abs_max_out: 1912.0\n",
      "lif layer 2 self.abs_max_v: 3567.5\n",
      "lif layer 2 self.abs_max_v: 3617.0\n",
      "fc layer 3 self.abs_max_out: 984.0\n",
      "fc layer 2 self.abs_max_out: 1937.0\n",
      "fc layer 2 self.abs_max_out: 1992.0\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "fc layer 2 self.abs_max_out: 2009.0\n",
      "lif layer 1 self.abs_max_v: 7841.0\n",
      "fc layer 1 self.abs_max_out: 4642.0\n",
      "lif layer 1 self.abs_max_v: 8325.0\n",
      "lif layer 1 self.abs_max_v: 8454.5\n",
      "lif layer 1 self.abs_max_v: 8610.5\n",
      "fc layer 1 self.abs_max_out: 4685.0\n",
      "lif layer 1 self.abs_max_v: 8990.5\n",
      "lif layer 1 self.abs_max_v: 9005.5\n",
      "fc layer 2 self.abs_max_out: 2010.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.199167/  1.649657, val:  45.00%, val_best:  45.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 87.99 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9070%\n",
      "layer   2  Sparsity: 76.5689%\n",
      "layer   3  Sparsity: 70.6197%\n",
      "total_backward_count 58740 real_backward_count 7203  12.263%\n",
      "lif layer 2 self.abs_max_v: 3630.0\n",
      "fc layer 1 self.abs_max_out: 4817.0\n",
      "fc layer 3 self.abs_max_out: 991.0\n",
      "lif layer 2 self.abs_max_v: 3653.0\n",
      "lif layer 2 self.abs_max_v: 3736.5\n",
      "fc layer 2 self.abs_max_out: 2030.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.200263/  1.706406, val:  40.00%, val_best:  45.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 75.5148%\n",
      "layer   3  Sparsity: 70.2108%\n",
      "total_backward_count 68530 real_backward_count 8346  12.179%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "fc layer 2 self.abs_max_out: 2156.0\n",
      "fc layer 2 self.abs_max_out: 2268.0\n",
      "fc layer 3 self.abs_max_out: 1000.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.137080/  1.526577, val:  48.33%, val_best:  48.33%, tr:  99.69%, tr_best:  99.90%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9136%\n",
      "layer   2  Sparsity: 75.0727%\n",
      "layer   3  Sparsity: 70.3089%\n",
      "total_backward_count 78320 real_backward_count 9449  12.065%\n",
      "lif layer 2 self.abs_max_v: 3784.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.133860/  1.504844, val:  55.83%, val_best:  55.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 87.34 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 75.5583%\n",
      "layer   3  Sparsity: 71.1836%\n",
      "total_backward_count 88110 real_backward_count 10607  12.038%\n",
      "fc layer 1 self.abs_max_out: 4963.0\n",
      "lif layer 2 self.abs_max_v: 3965.0\n",
      "lif layer 2 self.abs_max_v: 4069.0\n",
      "lif layer 2 self.abs_max_v: 4141.5\n",
      "fc layer 1 self.abs_max_out: 4969.0\n",
      "fc layer 1 self.abs_max_out: 5009.0\n",
      "lif layer 1 self.abs_max_v: 9164.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.130341/  1.614471, val:  48.33%, val_best:  55.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.12 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 75.9784%\n",
      "layer   3  Sparsity: 71.3900%\n",
      "total_backward_count 97900 real_backward_count 11765  12.017%\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.116856/  1.529130, val:  52.92%, val_best:  55.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 76.7963%\n",
      "layer   3  Sparsity: 72.5112%\n",
      "total_backward_count 107690 real_backward_count 12910  11.988%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.126822/  1.448025, val:  61.67%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.66 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9446%\n",
      "layer   2  Sparsity: 77.0653%\n",
      "layer   3  Sparsity: 72.6364%\n",
      "total_backward_count 117480 real_backward_count 13994  11.912%\n",
      "fc layer 2 self.abs_max_out: 2269.0\n",
      "lif layer 2 self.abs_max_v: 4188.0\n",
      "lif layer 1 self.abs_max_v: 9204.5\n",
      "fc layer 1 self.abs_max_out: 5232.0\n",
      "lif layer 1 self.abs_max_v: 9549.0\n",
      "fc layer 1 self.abs_max_out: 5241.0\n",
      "lif layer 2 self.abs_max_v: 4237.0\n",
      "lif layer 2 self.abs_max_v: 4317.5\n",
      "fc layer 1 self.abs_max_out: 5278.0\n",
      "lif layer 1 self.abs_max_v: 9641.5\n",
      "lif layer 1 self.abs_max_v: 9670.0\n",
      "lif layer 1 self.abs_max_v: 9741.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.081483/  1.549545, val:  43.75%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 76.7322%\n",
      "layer   3  Sparsity: 72.0918%\n",
      "total_backward_count 127270 real_backward_count 15037  11.815%\n",
      "fc layer 1 self.abs_max_out: 5310.0\n",
      "fc layer 1 self.abs_max_out: 5347.0\n",
      "lif layer 1 self.abs_max_v: 9866.5\n",
      "lif layer 1 self.abs_max_v: 10018.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.089992/  1.604076, val:  40.42%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 86.55 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.9257%\n",
      "layer   2  Sparsity: 77.2416%\n",
      "layer   3  Sparsity: 70.8377%\n",
      "total_backward_count 137060 real_backward_count 16112  11.755%\n",
      "fc layer 1 self.abs_max_out: 5400.0\n",
      "fc layer 3 self.abs_max_out: 1016.0\n",
      "fc layer 1 self.abs_max_out: 5416.0\n",
      "lif layer 1 self.abs_max_v: 10274.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.047236/  1.461525, val:  57.08%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9279%\n",
      "layer   2  Sparsity: 77.3821%\n",
      "layer   3  Sparsity: 69.8175%\n",
      "total_backward_count 146850 real_backward_count 17142  11.673%\n",
      "fc layer 1 self.abs_max_out: 5606.0\n",
      "fc layer 3 self.abs_max_out: 1019.0\n",
      "fc layer 3 self.abs_max_out: 1036.0\n",
      "fc layer 1 self.abs_max_out: 5788.0\n",
      "lif layer 1 self.abs_max_v: 10348.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.014372/  1.465893, val:  53.75%, val_best:  61.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9142%\n",
      "layer   2  Sparsity: 77.1767%\n",
      "layer   3  Sparsity: 69.2628%\n",
      "total_backward_count 156640 real_backward_count 18222  11.633%\n",
      "lif layer 1 self.abs_max_v: 10488.0\n",
      "fc layer 2 self.abs_max_out: 2319.0\n",
      "fc layer 1 self.abs_max_out: 5858.0\n",
      "lif layer 1 self.abs_max_v: 10651.0\n",
      "fc layer 1 self.abs_max_out: 5969.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.970888/  1.455650, val:  52.08%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.24 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.8762%\n",
      "layer   2  Sparsity: 77.0656%\n",
      "layer   3  Sparsity: 69.2278%\n",
      "total_backward_count 166430 real_backward_count 19275  11.581%\n",
      "fc layer 3 self.abs_max_out: 1068.0\n",
      "lif layer 1 self.abs_max_v: 10843.5\n",
      "fc layer 1 self.abs_max_out: 5995.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.975153/  1.400174, val:  57.92%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.22 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9105%\n",
      "layer   2  Sparsity: 77.2223%\n",
      "layer   3  Sparsity: 69.6610%\n",
      "total_backward_count 176220 real_backward_count 20326  11.534%\n",
      "fc layer 2 self.abs_max_out: 2340.0\n",
      "fc layer 2 self.abs_max_out: 2350.0\n",
      "fc layer 2 self.abs_max_out: 2372.0\n",
      "fc layer 1 self.abs_max_out: 6557.0\n",
      "lif layer 1 self.abs_max_v: 11349.0\n",
      "fc layer 2 self.abs_max_out: 2415.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.979881/  1.476807, val:  48.33%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.06 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 76.1680%\n",
      "layer   3  Sparsity: 69.6812%\n",
      "total_backward_count 186010 real_backward_count 21395  11.502%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.955380/  1.529128, val:  45.83%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 75.8812%\n",
      "layer   3  Sparsity: 68.7863%\n",
      "total_backward_count 195800 real_backward_count 22409  11.445%\n",
      "lif layer 2 self.abs_max_v: 4408.0\n",
      "lif layer 2 self.abs_max_v: 4420.0\n",
      "lif layer 2 self.abs_max_v: 4474.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.954514/  1.493657, val:  50.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8994%\n",
      "layer   2  Sparsity: 75.8448%\n",
      "layer   3  Sparsity: 70.0513%\n",
      "total_backward_count 205590 real_backward_count 23397  11.380%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.963313/  1.427162, val:  62.50%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.90 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9562%\n",
      "layer   2  Sparsity: 76.0698%\n",
      "layer   3  Sparsity: 70.9673%\n",
      "total_backward_count 215380 real_backward_count 24480  11.366%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.965020/  1.357275, val:  56.25%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.76 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9311%\n",
      "layer   2  Sparsity: 75.7786%\n",
      "layer   3  Sparsity: 70.1827%\n",
      "total_backward_count 225170 real_backward_count 25530  11.338%\n",
      "fc layer 3 self.abs_max_out: 1071.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.933806/  1.347060, val:  59.17%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 76.0814%\n",
      "layer   3  Sparsity: 70.2009%\n",
      "total_backward_count 234960 real_backward_count 26484  11.272%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.928911/  1.309788, val:  63.33%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.75 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9511%\n",
      "layer   2  Sparsity: 76.2428%\n",
      "layer   3  Sparsity: 70.4566%\n",
      "total_backward_count 244750 real_backward_count 27469  11.223%\n",
      "fc layer 3 self.abs_max_out: 1091.0\n",
      "fc layer 3 self.abs_max_out: 1101.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.915027/  1.330869, val:  60.42%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.14 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 76.1491%\n",
      "layer   3  Sparsity: 70.1286%\n",
      "total_backward_count 254540 real_backward_count 28543  11.214%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.928512/  1.333235, val:  61.25%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 76.2992%\n",
      "layer   3  Sparsity: 70.2836%\n",
      "total_backward_count 264330 real_backward_count 29506  11.163%\n",
      "fc layer 3 self.abs_max_out: 1120.0\n",
      "fc layer 3 self.abs_max_out: 1138.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.881794/  1.312993, val:  61.25%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.10 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9239%\n",
      "layer   2  Sparsity: 76.7477%\n",
      "layer   3  Sparsity: 70.1232%\n",
      "total_backward_count 274120 real_backward_count 30507  11.129%\n",
      "fc layer 3 self.abs_max_out: 1142.0\n",
      "fc layer 3 self.abs_max_out: 1169.0\n",
      "fc layer 3 self.abs_max_out: 1199.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.883690/  1.441579, val:  58.75%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 88.57 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9076%\n",
      "layer   2  Sparsity: 76.8173%\n",
      "layer   3  Sparsity: 69.8847%\n",
      "total_backward_count 283910 real_backward_count 31455  11.079%\n",
      "fc layer 1 self.abs_max_out: 7047.0\n",
      "lif layer 1 self.abs_max_v: 12375.5\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.887358/  1.397850, val:  52.08%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 88.82 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 76.8210%\n",
      "layer   3  Sparsity: 70.1073%\n",
      "total_backward_count 293700 real_backward_count 32402  11.032%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.864347/  1.358848, val:  56.67%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.53 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 76.5016%\n",
      "layer   3  Sparsity: 69.6985%\n",
      "total_backward_count 303490 real_backward_count 33340  10.986%\n",
      "fc layer 1 self.abs_max_out: 7094.0\n",
      "lif layer 1 self.abs_max_v: 12454.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.857263/  1.394218, val:  52.50%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.26 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9590%\n",
      "layer   2  Sparsity: 76.5631%\n",
      "layer   3  Sparsity: 70.6084%\n",
      "total_backward_count 313280 real_backward_count 34330  10.958%\n",
      "fc layer 1 self.abs_max_out: 7220.0\n",
      "lif layer 1 self.abs_max_v: 12706.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.839112/  1.349223, val:  56.25%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8996%\n",
      "layer   2  Sparsity: 75.6015%\n",
      "layer   3  Sparsity: 70.1043%\n",
      "total_backward_count 323070 real_backward_count 35268  10.917%\n",
      "fc layer 3 self.abs_max_out: 1230.0\n",
      "fc layer 3 self.abs_max_out: 1297.0\n",
      "fc layer 3 self.abs_max_out: 1368.0\n",
      "fc layer 3 self.abs_max_out: 1393.0\n",
      "fc layer 1 self.abs_max_out: 7285.0\n",
      "lif layer 1 self.abs_max_v: 12853.5\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.818288/  1.320717, val:  57.50%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.58 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 75.4840%\n",
      "layer   3  Sparsity: 71.1565%\n",
      "total_backward_count 332860 real_backward_count 36200  10.875%\n",
      "fc layer 1 self.abs_max_out: 7378.0\n",
      "lif layer 1 self.abs_max_v: 13043.5\n",
      "lif layer 2 self.abs_max_v: 4495.5\n",
      "lif layer 2 self.abs_max_v: 4543.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.855331/  1.317031, val:  57.08%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 88.61 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9063%\n",
      "layer   2  Sparsity: 75.4194%\n",
      "layer   3  Sparsity: 71.0811%\n",
      "total_backward_count 342650 real_backward_count 37179  10.850%\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "fc layer 2 self.abs_max_out: 2485.0\n",
      "lif layer 2 self.abs_max_v: 4695.0\n",
      "fc layer 2 self.abs_max_out: 2673.0\n",
      "lif layer 2 self.abs_max_v: 4761.5\n",
      "lif layer 2 self.abs_max_v: 4959.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.836030/  1.419024, val:  52.08%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.64 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9461%\n",
      "layer   2  Sparsity: 75.3487%\n",
      "layer   3  Sparsity: 71.0160%\n",
      "total_backward_count 352440 real_backward_count 38148  10.824%\n",
      "fc layer 2 self.abs_max_out: 2705.0\n",
      "lif layer 2 self.abs_max_v: 4978.5\n",
      "lif layer 1 self.abs_max_v: 13202.0\n",
      "fc layer 1 self.abs_max_out: 7761.0\n",
      "lif layer 1 self.abs_max_v: 13795.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.814626/  1.250774, val:  62.08%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.46 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 75.1106%\n",
      "layer   3  Sparsity: 70.4676%\n",
      "total_backward_count 362230 real_backward_count 39084  10.790%\n",
      "fc layer 3 self.abs_max_out: 1401.0\n",
      "fc layer 1 self.abs_max_out: 7764.0\n",
      "fc layer 2 self.abs_max_out: 2726.0\n",
      "lif layer 2 self.abs_max_v: 5030.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.755420/  1.367037, val:  59.17%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.52 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 75.0860%\n",
      "layer   3  Sparsity: 69.6822%\n",
      "total_backward_count 372020 real_backward_count 39899  10.725%\n",
      "fc layer 1 self.abs_max_out: 7944.0\n",
      "lif layer 1 self.abs_max_v: 14133.5\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.754318/  1.308896, val:  55.83%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.76 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 75.1833%\n",
      "layer   3  Sparsity: 70.1691%\n",
      "total_backward_count 381810 real_backward_count 40806  10.688%\n",
      "lif layer 2 self.abs_max_v: 5038.0\n",
      "lif layer 2 self.abs_max_v: 5102.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.767941/  1.299594, val:  62.92%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 89.05 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 74.8651%\n",
      "layer   3  Sparsity: 68.8831%\n",
      "total_backward_count 391600 real_backward_count 41716  10.653%\n",
      "fc layer 1 self.abs_max_out: 8037.0\n",
      "lif layer 1 self.abs_max_v: 14320.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.756672/  1.281969, val:  59.58%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.19 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9477%\n",
      "layer   2  Sparsity: 75.2842%\n",
      "layer   3  Sparsity: 67.9237%\n",
      "total_backward_count 401390 real_backward_count 42629  10.620%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.733160/  1.278942, val:  63.33%, val_best:  63.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 88.24 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9231%\n",
      "layer   2  Sparsity: 75.3216%\n",
      "layer   3  Sparsity: 68.6213%\n",
      "total_backward_count 411180 real_backward_count 43531  10.587%\n",
      "fc layer 3 self.abs_max_out: 1407.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.702718/  1.251847, val:  59.58%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.10 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9254%\n",
      "layer   2  Sparsity: 74.8941%\n",
      "layer   3  Sparsity: 68.7973%\n",
      "total_backward_count 420970 real_backward_count 44388  10.544%\n",
      "fc layer 2 self.abs_max_out: 2867.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.704326/  1.186025, val:  67.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.23 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9274%\n",
      "layer   2  Sparsity: 74.8707%\n",
      "layer   3  Sparsity: 69.1582%\n",
      "total_backward_count 430760 real_backward_count 45244  10.503%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.719151/  1.235759, val:  60.42%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.43 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9248%\n",
      "layer   2  Sparsity: 74.9686%\n",
      "layer   3  Sparsity: 70.1446%\n",
      "total_backward_count 440550 real_backward_count 46080  10.460%\n",
      "fc layer 1 self.abs_max_out: 8083.0\n",
      "lif layer 1 self.abs_max_v: 14370.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.716668/  1.232113, val:  62.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.79 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 74.3054%\n",
      "layer   3  Sparsity: 69.7188%\n",
      "total_backward_count 450340 real_backward_count 46951  10.426%\n",
      "fc layer 1 self.abs_max_out: 8120.0\n",
      "lif layer 1 self.abs_max_v: 14442.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.709821/  1.296331, val:  54.58%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 75.2780%\n",
      "layer   3  Sparsity: 69.2037%\n",
      "total_backward_count 460130 real_backward_count 47833  10.396%\n",
      "fc layer 1 self.abs_max_out: 8196.0\n",
      "lif layer 1 self.abs_max_v: 14609.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.683429/  1.226056, val:  57.92%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.84 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 75.6529%\n",
      "layer   3  Sparsity: 68.5092%\n",
      "total_backward_count 469920 real_backward_count 48675  10.358%\n",
      "fc layer 3 self.abs_max_out: 1456.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.670920/  1.133316, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.97 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9425%\n",
      "layer   2  Sparsity: 75.5974%\n",
      "layer   3  Sparsity: 69.0094%\n",
      "total_backward_count 479710 real_backward_count 49485  10.316%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.649073/  1.147361, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.39 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9284%\n",
      "layer   2  Sparsity: 75.4680%\n",
      "layer   3  Sparsity: 68.5199%\n",
      "total_backward_count 489500 real_backward_count 50327  10.281%\n",
      "lif layer 1 self.abs_max_v: 14621.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.649565/  1.210820, val:  57.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.39 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 75.2341%\n",
      "layer   3  Sparsity: 68.5308%\n",
      "total_backward_count 499290 real_backward_count 51143  10.243%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.643464/  1.162920, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.70 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 75.3424%\n",
      "layer   3  Sparsity: 68.3880%\n",
      "total_backward_count 509080 real_backward_count 51929  10.201%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.641944/  1.149005, val:  67.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.51 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9299%\n",
      "layer   2  Sparsity: 75.5961%\n",
      "layer   3  Sparsity: 69.0068%\n",
      "total_backward_count 518870 real_backward_count 52781  10.172%\n",
      "fc layer 3 self.abs_max_out: 1459.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.621537/  1.089508, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.24 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9044%\n",
      "layer   2  Sparsity: 75.7505%\n",
      "layer   3  Sparsity: 69.3358%\n",
      "total_backward_count 528660 real_backward_count 53563  10.132%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.620043/  1.158445, val:  64.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.57 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9200%\n",
      "layer   2  Sparsity: 75.9596%\n",
      "layer   3  Sparsity: 69.3174%\n",
      "total_backward_count 538450 real_backward_count 54340  10.092%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.625290/  1.067542, val:  70.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.25 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9145%\n",
      "layer   2  Sparsity: 75.9462%\n",
      "layer   3  Sparsity: 69.5653%\n",
      "total_backward_count 548240 real_backward_count 55136  10.057%\n",
      "fc layer 3 self.abs_max_out: 1462.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.608111/  1.280420, val:  62.50%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 88.58 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 75.8020%\n",
      "layer   3  Sparsity: 69.4565%\n",
      "total_backward_count 558030 real_backward_count 55947  10.026%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.616418/  1.173189, val:  65.83%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 88.54 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9417%\n",
      "layer   2  Sparsity: 75.9555%\n",
      "layer   3  Sparsity: 69.7525%\n",
      "total_backward_count 567820 real_backward_count 56687   9.983%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.613432/  1.107870, val:  66.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.99 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 76.1459%\n",
      "layer   3  Sparsity: 70.7986%\n",
      "total_backward_count 577610 real_backward_count 57474   9.950%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.617237/  1.194138, val:  58.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.92 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 75.8113%\n",
      "layer   3  Sparsity: 69.8567%\n",
      "total_backward_count 587400 real_backward_count 58206   9.909%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.605078/  1.143557, val:  63.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.32 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9277%\n",
      "layer   2  Sparsity: 75.3190%\n",
      "layer   3  Sparsity: 70.0106%\n",
      "total_backward_count 597190 real_backward_count 58969   9.874%\n",
      "fc layer 1 self.abs_max_out: 8362.0\n",
      "lif layer 1 self.abs_max_v: 14960.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.593460/  1.086086, val:  73.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9349%\n",
      "layer   2  Sparsity: 75.2772%\n",
      "layer   3  Sparsity: 69.6062%\n",
      "total_backward_count 606980 real_backward_count 59679   9.832%\n",
      "lif layer 2 self.abs_max_v: 5177.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.571153/  1.152361, val:  60.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.59 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.8898%\n",
      "layer   2  Sparsity: 74.8827%\n",
      "layer   3  Sparsity: 69.3177%\n",
      "total_backward_count 616770 real_backward_count 60482   9.806%\n",
      "fc layer 3 self.abs_max_out: 1473.0\n",
      "fc layer 3 self.abs_max_out: 1490.0\n",
      "fc layer 3 self.abs_max_out: 1506.0\n",
      "fc layer 3 self.abs_max_out: 1519.0\n",
      "fc layer 3 self.abs_max_out: 1612.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.550347/  1.076793, val:  66.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.74 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 75.4510%\n",
      "layer   3  Sparsity: 68.9055%\n",
      "total_backward_count 626560 real_backward_count 61146   9.759%\n",
      "lif layer 2 self.abs_max_v: 5204.5\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.548697/  1.121901, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.28 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9546%\n",
      "layer   2  Sparsity: 75.4008%\n",
      "layer   3  Sparsity: 68.8495%\n",
      "total_backward_count 636350 real_backward_count 61846   9.719%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.539931/  1.017347, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.61 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9114%\n",
      "layer   2  Sparsity: 75.2944%\n",
      "layer   3  Sparsity: 69.2864%\n",
      "total_backward_count 646140 real_backward_count 62539   9.679%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.538997/  1.050195, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.58 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 75.4718%\n",
      "layer   3  Sparsity: 67.6024%\n",
      "total_backward_count 655930 real_backward_count 63209   9.637%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.546873/  1.153110, val:  61.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8998%\n",
      "layer   2  Sparsity: 75.4333%\n",
      "layer   3  Sparsity: 68.3295%\n",
      "total_backward_count 665720 real_backward_count 63904   9.599%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.547084/  1.057214, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9334%\n",
      "layer   2  Sparsity: 75.4584%\n",
      "layer   3  Sparsity: 68.7291%\n",
      "total_backward_count 675510 real_backward_count 64624   9.567%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.493785/  1.044282, val:  69.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.78 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9102%\n",
      "layer   2  Sparsity: 75.6402%\n",
      "layer   3  Sparsity: 68.5920%\n",
      "total_backward_count 685300 real_backward_count 65308   9.530%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.521522/  1.050683, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.24 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 75.3221%\n",
      "layer   3  Sparsity: 68.2996%\n",
      "total_backward_count 695090 real_backward_count 65996   9.495%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.533185/  1.064812, val:  66.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.42 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9232%\n",
      "layer   2  Sparsity: 75.0904%\n",
      "layer   3  Sparsity: 68.6316%\n",
      "total_backward_count 704880 real_backward_count 66683   9.460%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.538152/  0.991660, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.90 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9288%\n",
      "layer   2  Sparsity: 75.3087%\n",
      "layer   3  Sparsity: 68.7501%\n",
      "total_backward_count 714670 real_backward_count 67335   9.422%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.511760/  1.067664, val:  67.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.90 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8941%\n",
      "layer   2  Sparsity: 75.3433%\n",
      "layer   3  Sparsity: 68.8871%\n",
      "total_backward_count 724460 real_backward_count 67962   9.381%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.508028/  1.049468, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.06 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8986%\n",
      "layer   2  Sparsity: 75.4986%\n",
      "layer   3  Sparsity: 69.8655%\n",
      "total_backward_count 734250 real_backward_count 68637   9.348%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.492039/  1.000168, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.86 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8861%\n",
      "layer   2  Sparsity: 75.3463%\n",
      "layer   3  Sparsity: 69.4219%\n",
      "total_backward_count 744040 real_backward_count 69264   9.309%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.514374/  1.095777, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.45 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9370%\n",
      "layer   2  Sparsity: 75.5653%\n",
      "layer   3  Sparsity: 70.1970%\n",
      "total_backward_count 753830 real_backward_count 69920   9.275%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.503489/  1.158234, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.75 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 75.9688%\n",
      "layer   3  Sparsity: 69.6296%\n",
      "total_backward_count 763620 real_backward_count 70547   9.238%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.482125/  1.022287, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9424%\n",
      "layer   2  Sparsity: 76.0640%\n",
      "layer   3  Sparsity: 69.1821%\n",
      "total_backward_count 773410 real_backward_count 71182   9.204%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.466235/  1.037746, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.18 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9021%\n",
      "layer   2  Sparsity: 76.0096%\n",
      "layer   3  Sparsity: 69.2692%\n",
      "total_backward_count 783200 real_backward_count 71794   9.167%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.479468/  1.017970, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.25 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9190%\n",
      "layer   2  Sparsity: 75.6616%\n",
      "layer   3  Sparsity: 68.5720%\n",
      "total_backward_count 792990 real_backward_count 72356   9.124%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.485044/  1.191585, val:  54.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.40 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 75.5086%\n",
      "layer   3  Sparsity: 68.4761%\n",
      "total_backward_count 802780 real_backward_count 72982   9.091%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.470786/  1.069205, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.94 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9199%\n",
      "layer   2  Sparsity: 76.0211%\n",
      "layer   3  Sparsity: 68.2024%\n",
      "total_backward_count 812570 real_backward_count 73591   9.057%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.465918/  1.116181, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.44 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9220%\n",
      "layer   2  Sparsity: 76.4224%\n",
      "layer   3  Sparsity: 68.6564%\n",
      "total_backward_count 822360 real_backward_count 74203   9.023%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.479211/  1.063626, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 77.1676%\n",
      "layer   3  Sparsity: 68.9224%\n",
      "total_backward_count 832150 real_backward_count 74827   8.992%\n",
      "fc layer 1 self.abs_max_out: 8438.0\n",
      "lif layer 1 self.abs_max_v: 15113.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.481561/  1.031100, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9449%\n",
      "layer   2  Sparsity: 76.6621%\n",
      "layer   3  Sparsity: 69.0321%\n",
      "total_backward_count 841940 real_backward_count 75426   8.959%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.472447/  1.006374, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8963%\n",
      "layer   2  Sparsity: 77.0174%\n",
      "layer   3  Sparsity: 68.6711%\n",
      "total_backward_count 851730 real_backward_count 75986   8.921%\n",
      "fc layer 1 self.abs_max_out: 8462.0\n",
      "lif layer 1 self.abs_max_v: 15163.5\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.471888/  1.094188, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.40 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9393%\n",
      "layer   2  Sparsity: 77.1251%\n",
      "layer   3  Sparsity: 68.4336%\n",
      "total_backward_count 861520 real_backward_count 76562   8.887%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.456913/  1.001524, val:  75.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 88.03 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 77.3859%\n",
      "layer   3  Sparsity: 68.2649%\n",
      "total_backward_count 871310 real_backward_count 77166   8.856%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.470412/  1.014552, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.56 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9218%\n",
      "layer   2  Sparsity: 76.8696%\n",
      "layer   3  Sparsity: 68.0749%\n",
      "total_backward_count 881100 real_backward_count 77772   8.827%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.470197/  1.102407, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.40 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9225%\n",
      "layer   2  Sparsity: 76.9568%\n",
      "layer   3  Sparsity: 68.1562%\n",
      "total_backward_count 890890 real_backward_count 78357   8.795%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.456929/  1.039427, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.44 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8826%\n",
      "layer   2  Sparsity: 77.3830%\n",
      "layer   3  Sparsity: 68.6515%\n",
      "total_backward_count 900680 real_backward_count 78874   8.757%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.439927/  0.978660, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 77.2461%\n",
      "layer   3  Sparsity: 69.1381%\n",
      "total_backward_count 910470 real_backward_count 79378   8.718%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.431823/  0.982582, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.25 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 77.2330%\n",
      "layer   3  Sparsity: 68.5486%\n",
      "total_backward_count 920260 real_backward_count 79868   8.679%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.438214/  1.168382, val:  60.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.75 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 76.9797%\n",
      "layer   3  Sparsity: 68.5226%\n",
      "total_backward_count 930050 real_backward_count 80386   8.643%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.422887/  0.992704, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.08 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8824%\n",
      "layer   2  Sparsity: 76.8298%\n",
      "layer   3  Sparsity: 69.1497%\n",
      "total_backward_count 939840 real_backward_count 80924   8.610%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.421126/  1.085228, val:  66.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 89.04 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.8978%\n",
      "layer   2  Sparsity: 76.9440%\n",
      "layer   3  Sparsity: 68.5709%\n",
      "total_backward_count 949630 real_backward_count 81438   8.576%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.432105/  0.959002, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.15 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9068%\n",
      "layer   2  Sparsity: 77.3174%\n",
      "layer   3  Sparsity: 68.7144%\n",
      "total_backward_count 959420 real_backward_count 82015   8.548%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.433346/  1.032763, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.95 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8816%\n",
      "layer   2  Sparsity: 77.4505%\n",
      "layer   3  Sparsity: 69.3370%\n",
      "total_backward_count 969210 real_backward_count 82522   8.514%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.432182/  0.952777, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 77.5885%\n",
      "layer   3  Sparsity: 68.8438%\n",
      "total_backward_count 979000 real_backward_count 83065   8.485%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.424215/  1.025288, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9453%\n",
      "layer   2  Sparsity: 78.0489%\n",
      "layer   3  Sparsity: 68.8301%\n",
      "total_backward_count 988790 real_backward_count 83584   8.453%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.432237/  1.029126, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.89 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9125%\n",
      "layer   2  Sparsity: 78.0177%\n",
      "layer   3  Sparsity: 68.8148%\n",
      "total_backward_count 998580 real_backward_count 84103   8.422%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.421031/  0.996764, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.95 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 78.0180%\n",
      "layer   3  Sparsity: 68.5878%\n",
      "total_backward_count 1008370 real_backward_count 84562   8.386%\n",
      "fc layer 3 self.abs_max_out: 1663.0\n",
      "fc layer 3 self.abs_max_out: 1676.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.412597/  1.012828, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.07 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9054%\n",
      "layer   2  Sparsity: 77.8410%\n",
      "layer   3  Sparsity: 69.8863%\n",
      "total_backward_count 1018160 real_backward_count 85049   8.353%\n",
      "fc layer 3 self.abs_max_out: 1685.0\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.422886/  0.999225, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 89.04 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.9300%\n",
      "layer   2  Sparsity: 78.0395%\n",
      "layer   3  Sparsity: 70.0865%\n",
      "total_backward_count 1027950 real_backward_count 85505   8.318%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.410511/  0.969690, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 89.23 seconds, 1.49 minutes\n",
      "layer   1  Sparsity: 87.9371%\n",
      "layer   2  Sparsity: 77.5884%\n",
      "layer   3  Sparsity: 69.9133%\n",
      "total_backward_count 1037740 real_backward_count 85975   8.285%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.413213/  0.992681, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.47 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 77.3965%\n",
      "layer   3  Sparsity: 69.5587%\n",
      "total_backward_count 1047530 real_backward_count 86421   8.250%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.419705/  0.986743, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 77.1288%\n",
      "layer   3  Sparsity: 70.2317%\n",
      "total_backward_count 1057320 real_backward_count 86881   8.217%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.415753/  0.983728, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.61 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.8958%\n",
      "layer   2  Sparsity: 77.2011%\n",
      "layer   3  Sparsity: 70.0932%\n",
      "total_backward_count 1067110 real_backward_count 87328   8.184%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.401408/  0.991781, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.83 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9002%\n",
      "layer   2  Sparsity: 77.3374%\n",
      "layer   3  Sparsity: 70.1452%\n",
      "total_backward_count 1076900 real_backward_count 87831   8.156%\n",
      "lif layer 1 self.abs_max_v: 15907.5\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.396000/  0.994414, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9488%\n",
      "layer   2  Sparsity: 77.5036%\n",
      "layer   3  Sparsity: 70.5867%\n",
      "total_backward_count 1086690 real_backward_count 88284   8.124%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.407049/  1.036693, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8934%\n",
      "layer   2  Sparsity: 77.5093%\n",
      "layer   3  Sparsity: 70.9600%\n",
      "total_backward_count 1096480 real_backward_count 88762   8.095%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.402903/  0.997777, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.05 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9416%\n",
      "layer   2  Sparsity: 77.5795%\n",
      "layer   3  Sparsity: 71.2184%\n",
      "total_backward_count 1106270 real_backward_count 89215   8.064%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.397322/  1.073587, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9226%\n",
      "layer   2  Sparsity: 77.4716%\n",
      "layer   3  Sparsity: 71.3535%\n",
      "total_backward_count 1116060 real_backward_count 89671   8.035%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.394307/  1.038224, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.69 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 77.2505%\n",
      "layer   3  Sparsity: 71.1877%\n",
      "total_backward_count 1125850 real_backward_count 90113   8.004%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.394829/  0.985794, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.62 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8946%\n",
      "layer   2  Sparsity: 77.4624%\n",
      "layer   3  Sparsity: 71.0733%\n",
      "total_backward_count 1135640 real_backward_count 90573   7.976%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.396580/  1.092125, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.79 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9046%\n",
      "layer   2  Sparsity: 77.2671%\n",
      "layer   3  Sparsity: 70.7829%\n",
      "total_backward_count 1145430 real_backward_count 91024   7.947%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.381598/  0.995700, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.31 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 77.5529%\n",
      "layer   3  Sparsity: 70.7338%\n",
      "total_backward_count 1155220 real_backward_count 91441   7.915%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.403020/  1.001676, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.57 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9164%\n",
      "layer   2  Sparsity: 77.4200%\n",
      "layer   3  Sparsity: 70.6400%\n",
      "total_backward_count 1165010 real_backward_count 91889   7.887%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.396692/  0.954826, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9637%\n",
      "layer   2  Sparsity: 77.3184%\n",
      "layer   3  Sparsity: 70.3511%\n",
      "total_backward_count 1174800 real_backward_count 92305   7.857%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.380894/  0.981866, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.78 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 77.3580%\n",
      "layer   3  Sparsity: 69.6697%\n",
      "total_backward_count 1184590 real_backward_count 92685   7.824%\n",
      "fc layer 3 self.abs_max_out: 1705.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.381285/  0.964172, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.27 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9270%\n",
      "layer   2  Sparsity: 77.6301%\n",
      "layer   3  Sparsity: 69.7595%\n",
      "total_backward_count 1194380 real_backward_count 93089   7.794%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.390215/  1.025248, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.50 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9389%\n",
      "layer   2  Sparsity: 77.6336%\n",
      "layer   3  Sparsity: 69.6696%\n",
      "total_backward_count 1204170 real_backward_count 93510   7.766%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.388899/  0.949891, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9110%\n",
      "layer   2  Sparsity: 77.3414%\n",
      "layer   3  Sparsity: 69.7627%\n",
      "total_backward_count 1213960 real_backward_count 93929   7.737%\n",
      "lif layer 1 self.abs_max_v: 15935.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.391008/  0.938195, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.99 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 77.5105%\n",
      "layer   3  Sparsity: 69.3457%\n",
      "total_backward_count 1223750 real_backward_count 94355   7.710%\n",
      "fc layer 1 self.abs_max_out: 8661.0\n",
      "lif layer 1 self.abs_max_v: 16228.5\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.365157/  0.961684, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.32 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9451%\n",
      "layer   2  Sparsity: 77.8508%\n",
      "layer   3  Sparsity: 69.6178%\n",
      "total_backward_count 1233540 real_backward_count 94752   7.681%\n",
      "fc layer 1 self.abs_max_out: 8772.0\n",
      "lif layer 1 self.abs_max_v: 16365.5\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.355496/  0.990932, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.27 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9661%\n",
      "layer   2  Sparsity: 77.8378%\n",
      "layer   3  Sparsity: 69.8102%\n",
      "total_backward_count 1243330 real_backward_count 95118   7.650%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.354518/  0.969148, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.00 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9169%\n",
      "layer   2  Sparsity: 77.9287%\n",
      "layer   3  Sparsity: 69.7328%\n",
      "total_backward_count 1253120 real_backward_count 95510   7.622%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.355970/  0.945372, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9431%\n",
      "layer   2  Sparsity: 77.9230%\n",
      "layer   3  Sparsity: 69.8648%\n",
      "total_backward_count 1262910 real_backward_count 95888   7.593%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.340168/  0.917424, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9023%\n",
      "layer   2  Sparsity: 77.6713%\n",
      "layer   3  Sparsity: 68.9340%\n",
      "total_backward_count 1272700 real_backward_count 96275   7.565%\n",
      "fc layer 3 self.abs_max_out: 1721.0\n",
      "fc layer 3 self.abs_max_out: 1745.0\n",
      "lif layer 1 self.abs_max_v: 16366.0\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.349973/  0.957209, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.67 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9455%\n",
      "layer   2  Sparsity: 77.5071%\n",
      "layer   3  Sparsity: 69.1205%\n",
      "total_backward_count 1282490 real_backward_count 96699   7.540%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.356530/  0.955042, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 77.5801%\n",
      "layer   3  Sparsity: 69.1133%\n",
      "total_backward_count 1292280 real_backward_count 97121   7.515%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.359860/  1.055112, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.24 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9112%\n",
      "layer   2  Sparsity: 77.5931%\n",
      "layer   3  Sparsity: 68.9437%\n",
      "total_backward_count 1302070 real_backward_count 97474   7.486%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.360541/  1.010473, val:  74.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 77.3850%\n",
      "layer   3  Sparsity: 69.3973%\n",
      "total_backward_count 1311860 real_backward_count 97829   7.457%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.359914/  1.030214, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9441%\n",
      "layer   2  Sparsity: 77.2766%\n",
      "layer   3  Sparsity: 69.2629%\n",
      "total_backward_count 1321650 real_backward_count 98224   7.432%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.348166/  0.981670, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.29 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9201%\n",
      "layer   2  Sparsity: 77.7049%\n",
      "layer   3  Sparsity: 69.6974%\n",
      "total_backward_count 1331440 real_backward_count 98605   7.406%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.336853/  0.916097, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.03 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9308%\n",
      "layer   2  Sparsity: 77.6609%\n",
      "layer   3  Sparsity: 69.4502%\n",
      "total_backward_count 1341230 real_backward_count 98950   7.378%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.344483/  0.912649, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.05 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 77.4900%\n",
      "layer   3  Sparsity: 69.3700%\n",
      "total_backward_count 1351020 real_backward_count 99287   7.349%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.333528/  0.934075, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.33 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 77.6866%\n",
      "layer   3  Sparsity: 69.6090%\n",
      "total_backward_count 1360810 real_backward_count 99606   7.320%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.324013/  0.908003, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.74 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9294%\n",
      "layer   2  Sparsity: 77.5414%\n",
      "layer   3  Sparsity: 69.2845%\n",
      "total_backward_count 1370600 real_backward_count 99934   7.291%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.320525/  0.925124, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9078%\n",
      "layer   2  Sparsity: 77.5739%\n",
      "layer   3  Sparsity: 69.3790%\n",
      "total_backward_count 1380390 real_backward_count 100275   7.264%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.319502/  0.938356, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.64 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9458%\n",
      "layer   2  Sparsity: 77.6038%\n",
      "layer   3  Sparsity: 69.3167%\n",
      "total_backward_count 1390180 real_backward_count 100611   7.237%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.317193/  0.931289, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9376%\n",
      "layer   2  Sparsity: 77.4609%\n",
      "layer   3  Sparsity: 69.1461%\n",
      "total_backward_count 1399970 real_backward_count 100910   7.208%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.313464/  0.994592, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.44 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8808%\n",
      "layer   2  Sparsity: 77.5302%\n",
      "layer   3  Sparsity: 68.9828%\n",
      "total_backward_count 1409760 real_backward_count 101213   7.179%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.317344/  1.006207, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.45 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9289%\n",
      "layer   2  Sparsity: 77.4136%\n",
      "layer   3  Sparsity: 69.2774%\n",
      "total_backward_count 1419550 real_backward_count 101539   7.153%\n",
      "lif layer 1 self.abs_max_v: 16509.0\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.321625/  0.977237, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 77.1494%\n",
      "layer   3  Sparsity: 69.5418%\n",
      "total_backward_count 1429340 real_backward_count 101878   7.128%\n",
      "fc layer 1 self.abs_max_out: 8808.0\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.321762/  0.959411, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 77.1850%\n",
      "layer   3  Sparsity: 68.8750%\n",
      "total_backward_count 1439130 real_backward_count 102196   7.101%\n",
      "fc layer 3 self.abs_max_out: 1770.0\n",
      "fc layer 3 self.abs_max_out: 1787.0\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.308504/  1.031810, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9180%\n",
      "layer   2  Sparsity: 77.0911%\n",
      "layer   3  Sparsity: 69.5315%\n",
      "total_backward_count 1448920 real_backward_count 102516   7.075%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.310997/  0.954744, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.77 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9117%\n",
      "layer   2  Sparsity: 77.4026%\n",
      "layer   3  Sparsity: 69.7966%\n",
      "total_backward_count 1458710 real_backward_count 102828   7.049%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.291683/  0.966878, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 77.5457%\n",
      "layer   3  Sparsity: 70.1995%\n",
      "total_backward_count 1468500 real_backward_count 103068   7.019%\n",
      "fc layer 1 self.abs_max_out: 8858.0\n",
      "lif layer 1 self.abs_max_v: 16579.5\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.291731/  0.936127, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9235%\n",
      "layer   2  Sparsity: 77.6294%\n",
      "layer   3  Sparsity: 69.6015%\n",
      "total_backward_count 1478290 real_backward_count 103341   6.991%\n",
      "fc layer 1 self.abs_max_out: 8924.0\n",
      "lif layer 1 self.abs_max_v: 16649.5\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.296609/  0.920693, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.55 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 77.6637%\n",
      "layer   3  Sparsity: 69.7728%\n",
      "total_backward_count 1488080 real_backward_count 103625   6.964%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.303427/  0.912246, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 77.7208%\n",
      "layer   3  Sparsity: 70.1292%\n",
      "total_backward_count 1497870 real_backward_count 103939   6.939%\n",
      "lif layer 1 self.abs_max_v: 16832.5\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.308598/  0.953730, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.43 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9245%\n",
      "layer   2  Sparsity: 77.6226%\n",
      "layer   3  Sparsity: 70.3591%\n",
      "total_backward_count 1507660 real_backward_count 104227   6.913%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.293555/  0.978869, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9502%\n",
      "layer   2  Sparsity: 77.8128%\n",
      "layer   3  Sparsity: 70.3375%\n",
      "total_backward_count 1517450 real_backward_count 104509   6.887%\n",
      "fc layer 1 self.abs_max_out: 9070.0\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.302177/  0.918125, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.05 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.8933%\n",
      "layer   2  Sparsity: 77.9366%\n",
      "layer   3  Sparsity: 69.6963%\n",
      "total_backward_count 1527240 real_backward_count 104798   6.862%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.286294/  0.929309, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 78.2438%\n",
      "layer   3  Sparsity: 69.5935%\n",
      "total_backward_count 1537030 real_backward_count 105095   6.838%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.293565/  0.965286, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.14 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.8926%\n",
      "layer   2  Sparsity: 78.0386%\n",
      "layer   3  Sparsity: 69.6124%\n",
      "total_backward_count 1546820 real_backward_count 105393   6.814%\n",
      "lif layer 1 self.abs_max_v: 16993.5\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.293152/  0.938071, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.31 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 78.0737%\n",
      "layer   3  Sparsity: 70.2546%\n",
      "total_backward_count 1556610 real_backward_count 105642   6.787%\n",
      "fc layer 1 self.abs_max_out: 9172.0\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.291251/  0.962558, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.27 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9141%\n",
      "layer   2  Sparsity: 78.0395%\n",
      "layer   3  Sparsity: 70.4247%\n",
      "total_backward_count 1566400 real_backward_count 105928   6.763%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.293431/  0.882014, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 78.2027%\n",
      "layer   3  Sparsity: 69.8841%\n",
      "total_backward_count 1576190 real_backward_count 106219   6.739%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.305358/  0.922299, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.56 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8972%\n",
      "layer   2  Sparsity: 78.2964%\n",
      "layer   3  Sparsity: 70.0205%\n",
      "total_backward_count 1585980 real_backward_count 106530   6.717%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.290195/  0.928703, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.55 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.9236%\n",
      "layer   2  Sparsity: 78.5295%\n",
      "layer   3  Sparsity: 69.6498%\n",
      "total_backward_count 1595770 real_backward_count 106763   6.690%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.284698/  0.970902, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.84 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9259%\n",
      "layer   2  Sparsity: 78.4341%\n",
      "layer   3  Sparsity: 69.6322%\n",
      "total_backward_count 1605560 real_backward_count 107028   6.666%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.286182/  0.847075, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.8838%\n",
      "layer   2  Sparsity: 78.2247%\n",
      "layer   3  Sparsity: 70.4670%\n",
      "total_backward_count 1615350 real_backward_count 107281   6.641%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.267635/  0.908548, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.87 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.8980%\n",
      "layer   2  Sparsity: 78.2497%\n",
      "layer   3  Sparsity: 71.0099%\n",
      "total_backward_count 1625140 real_backward_count 107491   6.614%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.284533/  0.929521, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.02 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 78.0142%\n",
      "layer   3  Sparsity: 70.3126%\n",
      "total_backward_count 1634930 real_backward_count 107725   6.589%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.286203/  0.939672, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9183%\n",
      "layer   2  Sparsity: 77.9083%\n",
      "layer   3  Sparsity: 70.2003%\n",
      "total_backward_count 1644720 real_backward_count 107984   6.565%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.279611/  0.898919, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9592%\n",
      "layer   2  Sparsity: 78.0693%\n",
      "layer   3  Sparsity: 70.3462%\n",
      "total_backward_count 1654510 real_backward_count 108225   6.541%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.287443/  0.906142, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.51 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9400%\n",
      "layer   2  Sparsity: 77.9297%\n",
      "layer   3  Sparsity: 69.7895%\n",
      "total_backward_count 1664300 real_backward_count 108489   6.519%\n",
      "lif layer 1 self.abs_max_v: 16997.0\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.281762/  0.943781, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.38 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9351%\n",
      "layer   2  Sparsity: 77.9984%\n",
      "layer   3  Sparsity: 69.6434%\n",
      "total_backward_count 1674090 real_backward_count 108728   6.495%\n",
      "fc layer 1 self.abs_max_out: 9250.0\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.278601/  0.963323, val:  77.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.15 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 87.9583%\n",
      "layer   2  Sparsity: 78.0081%\n",
      "layer   3  Sparsity: 69.8193%\n",
      "total_backward_count 1683880 real_backward_count 108959   6.471%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.278623/  0.932849, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.88 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9464%\n",
      "layer   2  Sparsity: 78.0064%\n",
      "layer   3  Sparsity: 69.4160%\n",
      "total_backward_count 1693670 real_backward_count 109195   6.447%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.276478/  0.897701, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 77.6432%\n",
      "layer   3  Sparsity: 69.4367%\n",
      "total_backward_count 1703460 real_backward_count 109395   6.422%\n",
      "fc layer 3 self.abs_max_out: 1792.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.276012/  0.850152, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.89 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9328%\n",
      "layer   2  Sparsity: 77.7903%\n",
      "layer   3  Sparsity: 69.5486%\n",
      "total_backward_count 1713250 real_backward_count 109629   6.399%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.265693/  0.871825, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 88.87 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 87.8885%\n",
      "layer   2  Sparsity: 77.9109%\n",
      "layer   3  Sparsity: 69.7524%\n",
      "total_backward_count 1723040 real_backward_count 109840   6.375%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.276558/  0.887935, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.76 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9290%\n",
      "layer   2  Sparsity: 77.7780%\n",
      "layer   3  Sparsity: 69.3897%\n",
      "total_backward_count 1732830 real_backward_count 110059   6.351%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.259338/  0.912120, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9033%\n",
      "layer   2  Sparsity: 77.9226%\n",
      "layer   3  Sparsity: 69.8944%\n",
      "total_backward_count 1742620 real_backward_count 110261   6.327%\n",
      "fc layer 1 self.abs_max_out: 9256.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.258579/  0.937879, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.27 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 78.0637%\n",
      "layer   3  Sparsity: 69.6906%\n",
      "total_backward_count 1752410 real_backward_count 110461   6.303%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.262180/  0.937055, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.87 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9329%\n",
      "layer   2  Sparsity: 78.1459%\n",
      "layer   3  Sparsity: 69.3843%\n",
      "total_backward_count 1762200 real_backward_count 110692   6.281%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.264634/  0.983152, val:  74.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.22 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9406%\n",
      "layer   2  Sparsity: 78.1578%\n",
      "layer   3  Sparsity: 69.8238%\n",
      "total_backward_count 1771990 real_backward_count 110932   6.260%\n",
      "fc layer 3 self.abs_max_out: 1845.0\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.258837/  0.908574, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9135%\n",
      "layer   2  Sparsity: 77.9088%\n",
      "layer   3  Sparsity: 69.9537%\n",
      "total_backward_count 1781780 real_backward_count 111118   6.236%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.264266/  0.952026, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9184%\n",
      "layer   2  Sparsity: 77.9644%\n",
      "layer   3  Sparsity: 70.3995%\n",
      "total_backward_count 1791570 real_backward_count 111333   6.214%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.262908/  0.900973, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.34 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 77.9793%\n",
      "layer   3  Sparsity: 70.1862%\n",
      "total_backward_count 1801360 real_backward_count 111539   6.192%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.269232/  0.940965, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.57 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 78.2735%\n",
      "layer   3  Sparsity: 69.8931%\n",
      "total_backward_count 1811150 real_backward_count 111764   6.171%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.263755/  0.875792, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 78.1176%\n",
      "layer   3  Sparsity: 70.1829%\n",
      "total_backward_count 1820940 real_backward_count 111955   6.148%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.268264/  0.966428, val:  75.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.47 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9224%\n",
      "layer   2  Sparsity: 78.1648%\n",
      "layer   3  Sparsity: 70.1810%\n",
      "total_backward_count 1830730 real_backward_count 112203   6.129%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.274004/  0.959207, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.55 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 78.1217%\n",
      "layer   3  Sparsity: 70.1016%\n",
      "total_backward_count 1840520 real_backward_count 112447   6.110%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.255661/  0.875736, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9251%\n",
      "layer   2  Sparsity: 78.0475%\n",
      "layer   3  Sparsity: 69.8149%\n",
      "total_backward_count 1850310 real_backward_count 112653   6.088%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.250977/  0.855539, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.64 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 87.9298%\n",
      "layer   2  Sparsity: 78.0184%\n",
      "layer   3  Sparsity: 70.2744%\n",
      "total_backward_count 1860100 real_backward_count 112827   6.066%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.231877/  0.908461, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9260%\n",
      "layer   2  Sparsity: 78.2222%\n",
      "layer   3  Sparsity: 70.4839%\n",
      "total_backward_count 1869890 real_backward_count 113003   6.043%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.246228/  0.997905, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.13 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 78.5376%\n",
      "layer   3  Sparsity: 70.1935%\n",
      "total_backward_count 1879680 real_backward_count 113187   6.022%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.233571/  0.927038, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.48 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 78.5056%\n",
      "layer   3  Sparsity: 70.7711%\n",
      "total_backward_count 1889470 real_backward_count 113322   5.998%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.245389/  0.883841, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9115%\n",
      "layer   2  Sparsity: 78.3899%\n",
      "layer   3  Sparsity: 70.3628%\n",
      "total_backward_count 1899260 real_backward_count 113521   5.977%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.235597/  0.885827, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9172%\n",
      "layer   2  Sparsity: 78.2114%\n",
      "layer   3  Sparsity: 71.0899%\n",
      "total_backward_count 1909050 real_backward_count 113696   5.956%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.239170/  0.951516, val:  77.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.08 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9042%\n",
      "layer   2  Sparsity: 78.2838%\n",
      "layer   3  Sparsity: 71.0505%\n",
      "total_backward_count 1918840 real_backward_count 113848   5.933%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.250229/  0.909624, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.83 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9185%\n",
      "layer   2  Sparsity: 78.4097%\n",
      "layer   3  Sparsity: 70.2846%\n",
      "total_backward_count 1928630 real_backward_count 114049   5.913%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.235946/  0.921322, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 87.9420%\n",
      "layer   2  Sparsity: 78.2986%\n",
      "layer   3  Sparsity: 70.2687%\n",
      "total_backward_count 1938420 real_backward_count 114201   5.891%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.245012/  0.882096, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.43 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9119%\n",
      "layer   2  Sparsity: 78.3146%\n",
      "layer   3  Sparsity: 70.6646%\n",
      "total_backward_count 1948210 real_backward_count 114372   5.871%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.253353/  0.964362, val:  76.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.50 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 87.9202%\n",
      "layer   2  Sparsity: 78.5540%\n",
      "layer   3  Sparsity: 70.9699%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de380a40e02418b9b6036eff9055e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñà‚ñá‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.25335</td></tr><tr><td>val_acc_best</td><td>0.85</td></tr><tr><td>val_acc_now</td><td>0.7625</td></tr><tr><td>val_loss</td><td>0.96436</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sweep-162</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d0fvqo34' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d0fvqo34</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_091234-d0fvqo34/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7rtkdkef with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_140541-7rtkdkef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rtkdkef' target=\"_blank\">firm-sweep-167</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rtkdkef' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rtkdkef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_140550_099', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 648.0\n",
      "lif layer 1 self.abs_max_v: 648.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 777.0\n",
      "lif layer 1 self.abs_max_v: 878.0\n",
      "lif layer 1 self.abs_max_v: 879.5\n",
      "lif layer 1 self.abs_max_v: 960.5\n",
      "fc layer 1 self.abs_max_out: 698.0\n",
      "lif layer 1 self.abs_max_v: 1086.0\n",
      "fc layer 2 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 128.0\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "fc layer 2 self.abs_max_out: 250.0\n",
      "lif layer 2 self.abs_max_v: 250.0\n",
      "lif layer 1 self.abs_max_v: 1127.5\n",
      "fc layer 2 self.abs_max_out: 252.0\n",
      "lif layer 2 self.abs_max_v: 293.0\n",
      "lif layer 1 self.abs_max_v: 1151.0\n",
      "fc layer 2 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 390.5\n",
      "fc layer 1 self.abs_max_out: 1101.0\n",
      "lif layer 1 self.abs_max_v: 1183.0\n",
      "fc layer 2 self.abs_max_out: 256.0\n",
      "lif layer 1 self.abs_max_v: 1363.0\n",
      "fc layer 2 self.abs_max_out: 335.0\n",
      "lif layer 1 self.abs_max_v: 1374.0\n",
      "lif layer 1 self.abs_max_v: 1458.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 537.0\n",
      "fc layer 2 self.abs_max_out: 485.0\n",
      "lif layer 2 self.abs_max_v: 596.5\n",
      "fc layer 2 self.abs_max_out: 679.0\n",
      "lif layer 2 self.abs_max_v: 826.0\n",
      "fc layer 1 self.abs_max_out: 1184.0\n",
      "lif layer 1 self.abs_max_v: 1542.5\n",
      "lif layer 1 self.abs_max_v: 1569.0\n",
      "lif layer 1 self.abs_max_v: 1639.0\n",
      "fc layer 1 self.abs_max_out: 1367.0\n",
      "lif layer 1 self.abs_max_v: 1685.0\n",
      "fc layer 1 self.abs_max_out: 1900.0\n",
      "lif layer 1 self.abs_max_v: 2178.0\n",
      "fc layer 2 self.abs_max_out: 791.0\n",
      "lif layer 2 self.abs_max_v: 860.0\n",
      "lif layer 2 self.abs_max_v: 920.5\n",
      "fc layer 2 self.abs_max_out: 948.0\n",
      "lif layer 2 self.abs_max_v: 1158.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 1 self.abs_max_v: 2346.5\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "lif layer 2 self.abs_max_v: 1282.0\n",
      "lif layer 2 self.abs_max_v: 1306.5\n",
      "fc layer 3 self.abs_max_out: 153.0\n",
      "lif layer 2 self.abs_max_v: 1389.5\n",
      "fc layer 1 self.abs_max_out: 2042.0\n",
      "lif layer 2 self.abs_max_v: 1673.5\n",
      "fc layer 2 self.abs_max_out: 1345.0\n",
      "lif layer 2 self.abs_max_v: 1730.0\n",
      "fc layer 1 self.abs_max_out: 2233.0\n",
      "fc layer 1 self.abs_max_out: 2319.0\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "lif layer 1 self.abs_max_v: 2504.5\n",
      "fc layer 1 self.abs_max_out: 2365.0\n",
      "lif layer 2 self.abs_max_v: 1804.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "fc layer 1 self.abs_max_out: 2369.0\n",
      "fc layer 3 self.abs_max_out: 220.0\n",
      "fc layer 1 self.abs_max_out: 2879.0\n",
      "lif layer 1 self.abs_max_v: 2879.0\n",
      "fc layer 2 self.abs_max_out: 1576.0\n",
      "lif layer 2 self.abs_max_v: 1992.0\n",
      "fc layer 1 self.abs_max_out: 3463.0\n",
      "lif layer 1 self.abs_max_v: 3463.0\n",
      "lif layer 2 self.abs_max_v: 2170.0\n",
      "fc layer 3 self.abs_max_out: 222.0\n",
      "fc layer 2 self.abs_max_out: 1673.0\n",
      "lif layer 2 self.abs_max_v: 2385.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 2 self.abs_max_out: 1910.0\n",
      "fc layer 1 self.abs_max_out: 3915.0\n",
      "lif layer 1 self.abs_max_v: 3915.0\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "lif layer 2 self.abs_max_v: 3180.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 2 self.abs_max_out: 2194.0\n",
      "fc layer 2 self.abs_max_out: 2248.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "lif layer 2 self.abs_max_v: 3560.5\n",
      "lif layer 2 self.abs_max_v: 3784.5\n",
      "fc layer 2 self.abs_max_out: 2406.0\n",
      "fc layer 3 self.abs_max_out: 467.0\n",
      "fc layer 2 self.abs_max_out: 2422.0\n",
      "fc layer 2 self.abs_max_out: 2455.0\n",
      "fc layer 2 self.abs_max_out: 2582.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "fc layer 2 self.abs_max_out: 2900.0\n",
      "fc layer 3 self.abs_max_out: 555.0\n",
      "fc layer 2 self.abs_max_out: 3168.0\n",
      "fc layer 1 self.abs_max_out: 3983.0\n",
      "lif layer 1 self.abs_max_v: 3983.0\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 4301.0\n",
      "lif layer 1 self.abs_max_v: 4301.0\n",
      "fc layer 2 self.abs_max_out: 3321.0\n",
      "lif layer 2 self.abs_max_v: 3820.5\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "fc layer 3 self.abs_max_out: 595.0\n",
      "fc layer 1 self.abs_max_out: 4361.0\n",
      "lif layer 1 self.abs_max_v: 4361.0\n",
      "fc layer 1 self.abs_max_out: 5229.0\n",
      "lif layer 1 self.abs_max_v: 5229.0\n",
      "fc layer 1 self.abs_max_out: 5231.0\n",
      "lif layer 1 self.abs_max_v: 5231.0\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "fc layer 3 self.abs_max_out: 641.0\n",
      "fc layer 1 self.abs_max_out: 5410.0\n",
      "lif layer 1 self.abs_max_v: 5410.0\n",
      "fc layer 1 self.abs_max_out: 5514.0\n",
      "lif layer 1 self.abs_max_v: 5514.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 2 self.abs_max_out: 3322.0\n",
      "fc layer 2 self.abs_max_out: 3342.0\n",
      "fc layer 2 self.abs_max_out: 3411.0\n",
      "lif layer 2 self.abs_max_v: 3964.0\n",
      "fc layer 1 self.abs_max_out: 5647.0\n",
      "lif layer 1 self.abs_max_v: 5647.0\n",
      "fc layer 1 self.abs_max_out: 6048.0\n",
      "lif layer 1 self.abs_max_v: 6048.0\n",
      "fc layer 2 self.abs_max_out: 3494.0\n",
      "lif layer 2 self.abs_max_v: 4104.5\n",
      "lif layer 2 self.abs_max_v: 4116.5\n",
      "fc layer 2 self.abs_max_out: 3508.0\n",
      "fc layer 2 self.abs_max_out: 3532.0\n",
      "fc layer 2 self.abs_max_out: 3626.0\n",
      "fc layer 2 self.abs_max_out: 3926.0\n",
      "fc layer 1 self.abs_max_out: 6319.0\n",
      "lif layer 1 self.abs_max_v: 6319.0\n",
      "fc layer 2 self.abs_max_out: 4097.0\n",
      "lif layer 2 self.abs_max_v: 4275.5\n",
      "fc layer 1 self.abs_max_out: 6415.0\n",
      "lif layer 1 self.abs_max_v: 6415.0\n",
      "lif layer 2 self.abs_max_v: 4418.5\n",
      "lif layer 2 self.abs_max_v: 4855.5\n",
      "lif layer 2 self.abs_max_v: 5173.0\n",
      "lif layer 2 self.abs_max_v: 5774.5\n",
      "fc layer 1 self.abs_max_out: 6595.0\n",
      "lif layer 1 self.abs_max_v: 6595.0\n",
      "fc layer 1 self.abs_max_out: 6815.0\n",
      "lif layer 1 self.abs_max_v: 6815.0\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "fc layer 1 self.abs_max_out: 6962.0\n",
      "lif layer 1 self.abs_max_v: 6962.0\n",
      "fc layer 1 self.abs_max_out: 6995.0\n",
      "lif layer 1 self.abs_max_v: 6995.0\n",
      "fc layer 1 self.abs_max_out: 7185.0\n",
      "lif layer 1 self.abs_max_v: 7185.0\n",
      "fc layer 1 self.abs_max_out: 7452.0\n",
      "lif layer 1 self.abs_max_v: 7452.0\n",
      "fc layer 2 self.abs_max_out: 4229.0\n",
      "fc layer 2 self.abs_max_out: 4522.0\n",
      "fc layer 1 self.abs_max_out: 7465.0\n",
      "lif layer 1 self.abs_max_v: 7465.0\n",
      "fc layer 2 self.abs_max_out: 4733.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.149279/  2.171586, val:  49.17%, val_best:  49.17%, tr:  66.19%, tr_best:  66.19%, epoch time: 88.11 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.9496%\n",
      "layer   2  Sparsity: 81.4880%\n",
      "layer   3  Sparsity: 88.8152%\n",
      "total_backward_count 9790 real_backward_count 5201  53.126%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 760.0\n",
      "fc layer 1 self.abs_max_out: 7845.0\n",
      "lif layer 1 self.abs_max_v: 7845.0\n",
      "fc layer 3 self.abs_max_out: 791.0\n",
      "fc layer 3 self.abs_max_out: 797.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "fc layer 1 self.abs_max_out: 8069.0\n",
      "lif layer 1 self.abs_max_v: 8069.0\n",
      "fc layer 1 self.abs_max_out: 8110.0\n",
      "lif layer 1 self.abs_max_v: 8110.0\n",
      "fc layer 1 self.abs_max_out: 8520.0\n",
      "lif layer 1 self.abs_max_v: 8520.0\n",
      "fc layer 1 self.abs_max_out: 8633.0\n",
      "lif layer 1 self.abs_max_v: 8633.0\n",
      "fc layer 1 self.abs_max_out: 9037.0\n",
      "lif layer 1 self.abs_max_v: 9037.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.115126/  2.169866, val:  56.67%, val_best:  56.67%, tr:  86.41%, tr_best:  86.41%, epoch time: 86.66 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9629%\n",
      "layer   2  Sparsity: 79.1904%\n",
      "layer   3  Sparsity: 86.0615%\n",
      "total_backward_count 19580 real_backward_count 8738  44.627%\n",
      "fc layer 1 self.abs_max_out: 9263.0\n",
      "lif layer 1 self.abs_max_v: 9263.0\n",
      "lif layer 2 self.abs_max_v: 5818.0\n",
      "fc layer 2 self.abs_max_out: 4793.0\n",
      "fc layer 1 self.abs_max_out: 9464.0\n",
      "lif layer 1 self.abs_max_v: 9464.0\n",
      "fc layer 3 self.abs_max_out: 1000.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.122753/  2.172344, val:  50.00%, val_best:  56.67%, tr:  91.93%, tr_best:  91.93%, epoch time: 87.09 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9747%\n",
      "layer   2  Sparsity: 78.5769%\n",
      "layer   3  Sparsity: 84.3195%\n",
      "total_backward_count 29370 real_backward_count 11735  39.956%\n",
      "fc layer 1 self.abs_max_out: 9692.0\n",
      "lif layer 1 self.abs_max_v: 9692.0\n",
      "fc layer 1 self.abs_max_out: 9720.0\n",
      "lif layer 1 self.abs_max_v: 9720.0\n",
      "fc layer 1 self.abs_max_out: 10075.0\n",
      "lif layer 1 self.abs_max_v: 10075.0\n",
      "fc layer 1 self.abs_max_out: 10954.0\n",
      "lif layer 1 self.abs_max_v: 10954.0\n",
      "fc layer 2 self.abs_max_out: 4821.0\n",
      "fc layer 2 self.abs_max_out: 4897.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.122884/  2.179890, val:  50.83%, val_best:  56.67%, tr:  94.48%, tr_best:  94.48%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9626%\n",
      "layer   2  Sparsity: 78.1212%\n",
      "layer   3  Sparsity: 83.0664%\n",
      "total_backward_count 39160 real_backward_count 14450  36.900%\n",
      "fc layer 1 self.abs_max_out: 11169.0\n",
      "lif layer 1 self.abs_max_v: 11169.0\n",
      "fc layer 2 self.abs_max_out: 4990.0\n",
      "fc layer 2 self.abs_max_out: 5816.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.130531/  2.172640, val:  41.25%, val_best:  56.67%, tr:  96.53%, tr_best:  96.53%, epoch time: 87.03 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9446%\n",
      "layer   2  Sparsity: 77.3489%\n",
      "layer   3  Sparsity: 81.7321%\n",
      "total_backward_count 48950 real_backward_count 16914  34.554%\n",
      "fc layer 1 self.abs_max_out: 11329.0\n",
      "lif layer 1 self.abs_max_v: 11329.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.131522/  2.188003, val:  60.42%, val_best:  60.42%, tr:  95.91%, tr_best:  96.53%, epoch time: 87.19 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9560%\n",
      "layer   2  Sparsity: 77.2938%\n",
      "layer   3  Sparsity: 81.6932%\n",
      "total_backward_count 58740 real_backward_count 19321  32.892%\n",
      "fc layer 1 self.abs_max_out: 11472.0\n",
      "lif layer 1 self.abs_max_v: 11472.0\n",
      "fc layer 1 self.abs_max_out: 11591.0\n",
      "lif layer 1 self.abs_max_v: 11591.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.136264/  2.185290, val:  50.83%, val_best:  60.42%, tr:  97.14%, tr_best:  97.14%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9642%\n",
      "layer   2  Sparsity: 77.3689%\n",
      "layer   3  Sparsity: 81.5012%\n",
      "total_backward_count 68530 real_backward_count 21639  31.576%\n",
      "fc layer 1 self.abs_max_out: 11820.0\n",
      "lif layer 1 self.abs_max_v: 11820.0\n",
      "fc layer 1 self.abs_max_out: 12653.0\n",
      "lif layer 1 self.abs_max_v: 12653.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.143081/  2.183498, val:  48.33%, val_best:  60.42%, tr:  97.55%, tr_best:  97.55%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9593%\n",
      "layer   2  Sparsity: 77.1560%\n",
      "layer   3  Sparsity: 81.5024%\n",
      "total_backward_count 78320 real_backward_count 23823  30.418%\n",
      "fc layer 1 self.abs_max_out: 12805.0\n",
      "lif layer 1 self.abs_max_v: 12805.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.147334/  2.185065, val:  51.67%, val_best:  60.42%, tr:  98.26%, tr_best:  98.26%, epoch time: 87.30 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9598%\n",
      "layer   2  Sparsity: 77.1785%\n",
      "layer   3  Sparsity: 81.6227%\n",
      "total_backward_count 88110 real_backward_count 25988  29.495%\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.148108/  2.193897, val:  53.75%, val_best:  60.42%, tr:  97.96%, tr_best:  98.26%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9524%\n",
      "layer   2  Sparsity: 76.8498%\n",
      "layer   3  Sparsity: 81.8016%\n",
      "total_backward_count 97900 real_backward_count 28052  28.654%\n",
      "fc layer 1 self.abs_max_out: 13063.0\n",
      "lif layer 1 self.abs_max_v: 13063.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.149457/  2.193559, val:  52.92%, val_best:  60.42%, tr:  98.26%, tr_best:  98.26%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9432%\n",
      "layer   2  Sparsity: 76.3443%\n",
      "layer   3  Sparsity: 81.7864%\n",
      "total_backward_count 107690 real_backward_count 30159  28.005%\n",
      "fc layer 1 self.abs_max_out: 13442.0\n",
      "lif layer 1 self.abs_max_v: 13442.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.155713/  2.187392, val:  61.67%, val_best:  61.67%, tr:  98.57%, tr_best:  98.57%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9355%\n",
      "layer   2  Sparsity: 76.5057%\n",
      "layer   3  Sparsity: 81.9396%\n",
      "total_backward_count 117480 real_backward_count 32230  27.434%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.149161/  2.187804, val:  48.75%, val_best:  61.67%, tr:  99.08%, tr_best:  99.08%, epoch time: 87.49 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9494%\n",
      "layer   2  Sparsity: 76.5125%\n",
      "layer   3  Sparsity: 82.1710%\n",
      "total_backward_count 127270 real_backward_count 34247  26.909%\n",
      "fc layer 1 self.abs_max_out: 13588.0\n",
      "lif layer 1 self.abs_max_v: 13588.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.148428/  2.189955, val:  54.58%, val_best:  61.67%, tr:  98.77%, tr_best:  99.08%, epoch time: 87.63 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9240%\n",
      "layer   2  Sparsity: 76.2676%\n",
      "layer   3  Sparsity: 81.9131%\n",
      "total_backward_count 137060 real_backward_count 36293  26.480%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.152453/  2.192373, val:  47.50%, val_best:  61.67%, tr:  98.77%, tr_best:  99.08%, epoch time: 87.53 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9326%\n",
      "layer   2  Sparsity: 76.2982%\n",
      "layer   3  Sparsity: 82.1847%\n",
      "total_backward_count 146850 real_backward_count 38266  26.058%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.151424/  2.194100, val:  45.83%, val_best:  61.67%, tr:  98.88%, tr_best:  99.08%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9350%\n",
      "layer   2  Sparsity: 76.0405%\n",
      "layer   3  Sparsity: 82.0345%\n",
      "total_backward_count 156640 real_backward_count 40259  25.702%\n",
      "lif layer 2 self.abs_max_v: 5879.5\n",
      "fc layer 1 self.abs_max_out: 13642.0\n",
      "lif layer 1 self.abs_max_v: 13642.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.152078/  2.196002, val:  61.25%, val_best:  61.67%, tr:  98.47%, tr_best:  99.08%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9474%\n",
      "layer   2  Sparsity: 76.0436%\n",
      "layer   3  Sparsity: 82.0385%\n",
      "total_backward_count 166430 real_backward_count 42110  25.302%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.157314/  2.198361, val:  62.92%, val_best:  62.92%, tr:  98.98%, tr_best:  99.08%, epoch time: 87.48 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9628%\n",
      "layer   2  Sparsity: 75.8261%\n",
      "layer   3  Sparsity: 82.2814%\n",
      "total_backward_count 176220 real_backward_count 44084  25.016%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.156917/  2.189783, val:  61.25%, val_best:  62.92%, tr:  98.98%, tr_best:  99.08%, epoch time: 87.58 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9541%\n",
      "layer   2  Sparsity: 75.7790%\n",
      "layer   3  Sparsity: 82.3099%\n",
      "total_backward_count 186010 real_backward_count 46048  24.756%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.159410/  2.203104, val:  41.67%, val_best:  62.92%, tr:  99.18%, tr_best:  99.18%, epoch time: 87.38 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9559%\n",
      "layer   2  Sparsity: 75.8027%\n",
      "layer   3  Sparsity: 82.4535%\n",
      "total_backward_count 195800 real_backward_count 47855  24.441%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.162230/  2.202400, val:  48.75%, val_best:  62.92%, tr:  98.98%, tr_best:  99.18%, epoch time: 87.66 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9711%\n",
      "layer   2  Sparsity: 75.7733%\n",
      "layer   3  Sparsity: 82.5344%\n",
      "total_backward_count 205590 real_backward_count 49677  24.163%\n",
      "fc layer 1 self.abs_max_out: 14021.0\n",
      "lif layer 1 self.abs_max_v: 14021.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.160656/  2.193447, val:  58.33%, val_best:  62.92%, tr:  99.18%, tr_best:  99.18%, epoch time: 87.20 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9574%\n",
      "layer   2  Sparsity: 75.5603%\n",
      "layer   3  Sparsity: 81.7934%\n",
      "total_backward_count 215380 real_backward_count 51569  23.943%\n",
      "fc layer 1 self.abs_max_out: 14045.0\n",
      "lif layer 1 self.abs_max_v: 14045.0\n",
      "lif layer 2 self.abs_max_v: 5917.5\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.157664/  2.190838, val:  62.08%, val_best:  62.92%, tr:  99.39%, tr_best:  99.39%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9497%\n",
      "layer   2  Sparsity: 75.3537%\n",
      "layer   3  Sparsity: 81.7740%\n",
      "total_backward_count 225170 real_backward_count 53462  23.743%\n",
      "lif layer 2 self.abs_max_v: 6120.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.159323/  2.198381, val:  56.67%, val_best:  62.92%, tr:  99.39%, tr_best:  99.39%, epoch time: 87.65 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9606%\n",
      "layer   2  Sparsity: 75.4786%\n",
      "layer   3  Sparsity: 82.1694%\n",
      "total_backward_count 234960 real_backward_count 55280  23.527%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.161467/  2.199354, val:  57.92%, val_best:  62.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 87.44 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9617%\n",
      "layer   2  Sparsity: 75.4173%\n",
      "layer   3  Sparsity: 81.9142%\n",
      "total_backward_count 244750 real_backward_count 57062  23.314%\n",
      "fc layer 1 self.abs_max_out: 14054.0\n",
      "lif layer 1 self.abs_max_v: 14054.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.158668/  2.189640, val:  58.75%, val_best:  62.92%, tr:  99.18%, tr_best:  99.59%, epoch time: 86.67 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9443%\n",
      "layer   2  Sparsity: 75.2208%\n",
      "layer   3  Sparsity: 81.8066%\n",
      "total_backward_count 254540 real_backward_count 59000  23.179%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.159433/  2.191044, val:  67.08%, val_best:  67.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 86.79 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9560%\n",
      "layer   2  Sparsity: 75.4552%\n",
      "layer   3  Sparsity: 82.0104%\n",
      "total_backward_count 264330 real_backward_count 60775  22.992%\n",
      "fc layer 1 self.abs_max_out: 14324.0\n",
      "lif layer 1 self.abs_max_v: 14324.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.160105/  2.199063, val:  65.00%, val_best:  67.08%, tr:  99.28%, tr_best:  99.59%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9468%\n",
      "layer   2  Sparsity: 75.3703%\n",
      "layer   3  Sparsity: 81.8954%\n",
      "total_backward_count 274120 real_backward_count 62565  22.824%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.156650/  2.198537, val:  50.83%, val_best:  67.08%, tr:  98.98%, tr_best:  99.59%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9484%\n",
      "layer   2  Sparsity: 75.0409%\n",
      "layer   3  Sparsity: 82.1238%\n",
      "total_backward_count 283910 real_backward_count 64311  22.652%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.155814/  2.191350, val:  60.83%, val_best:  67.08%, tr:  99.49%, tr_best:  99.59%, epoch time: 87.45 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9343%\n",
      "layer   2  Sparsity: 74.7934%\n",
      "layer   3  Sparsity: 81.6159%\n",
      "total_backward_count 293700 real_backward_count 66052  22.490%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.155404/  2.192535, val:  53.75%, val_best:  67.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 87.63 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9417%\n",
      "layer   2  Sparsity: 74.8322%\n",
      "layer   3  Sparsity: 81.8680%\n",
      "total_backward_count 303490 real_backward_count 67841  22.354%\n",
      "fc layer 2 self.abs_max_out: 5993.0\n",
      "lif layer 2 self.abs_max_v: 6204.5\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.155721/  2.189495, val:  52.92%, val_best:  67.08%, tr:  99.28%, tr_best:  99.59%, epoch time: 86.96 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9411%\n",
      "layer   2  Sparsity: 74.9804%\n",
      "layer   3  Sparsity: 81.9121%\n",
      "total_backward_count 313280 real_backward_count 69560  22.204%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.157095/  2.198387, val:  70.42%, val_best:  70.42%, tr:  99.08%, tr_best:  99.59%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9456%\n",
      "layer   2  Sparsity: 74.9950%\n",
      "layer   3  Sparsity: 81.8797%\n",
      "total_backward_count 323070 real_backward_count 71246  22.053%\n",
      "lif layer 2 self.abs_max_v: 6253.0\n",
      "lif layer 2 self.abs_max_v: 6400.0\n",
      "lif layer 2 self.abs_max_v: 6557.5\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.157140/  2.189831, val:  60.83%, val_best:  70.42%, tr:  99.18%, tr_best:  99.59%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9522%\n",
      "layer   2  Sparsity: 74.8789%\n",
      "layer   3  Sparsity: 81.7961%\n",
      "total_backward_count 332860 real_backward_count 73031  21.940%\n",
      "lif layer 2 self.abs_max_v: 6575.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.155145/  2.189386, val:  54.58%, val_best:  70.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 87.00 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9327%\n",
      "layer   2  Sparsity: 74.6397%\n",
      "layer   3  Sparsity: 81.6856%\n",
      "total_backward_count 342650 real_backward_count 74692  21.798%\n",
      "fc layer 1 self.abs_max_out: 14325.0\n",
      "lif layer 1 self.abs_max_v: 14325.0\n",
      "lif layer 2 self.abs_max_v: 6729.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.157712/  2.195183, val:  60.83%, val_best:  70.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 86.64 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9469%\n",
      "layer   2  Sparsity: 74.8273%\n",
      "layer   3  Sparsity: 81.9188%\n",
      "total_backward_count 352440 real_backward_count 76319  21.654%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.155208/  2.190259, val:  68.33%, val_best:  70.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9493%\n",
      "layer   2  Sparsity: 74.8465%\n",
      "layer   3  Sparsity: 81.7856%\n",
      "total_backward_count 362230 real_backward_count 78001  21.534%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.155275/  2.195492, val:  55.42%, val_best:  70.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9517%\n",
      "layer   2  Sparsity: 74.6664%\n",
      "layer   3  Sparsity: 82.4217%\n",
      "total_backward_count 372020 real_backward_count 79619  21.402%\n",
      "fc layer 1 self.abs_max_out: 14331.0\n",
      "lif layer 1 self.abs_max_v: 14331.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.154436/  2.192335, val:  70.83%, val_best:  70.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 86.35 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9748%\n",
      "layer   2  Sparsity: 74.7853%\n",
      "layer   3  Sparsity: 82.2981%\n",
      "total_backward_count 381810 real_backward_count 81327  21.300%\n",
      "fc layer 1 self.abs_max_out: 14458.0\n",
      "lif layer 1 self.abs_max_v: 14458.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.156488/  2.193426, val:  65.83%, val_best:  70.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 86.76 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9558%\n",
      "layer   2  Sparsity: 74.7163%\n",
      "layer   3  Sparsity: 82.4673%\n",
      "total_backward_count 391600 real_backward_count 82936  21.179%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.158207/  2.189982, val:  60.00%, val_best:  70.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 87.38 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9570%\n",
      "layer   2  Sparsity: 74.6731%\n",
      "layer   3  Sparsity: 81.9414%\n",
      "total_backward_count 401390 real_backward_count 84520  21.057%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.157792/  2.192450, val:  64.17%, val_best:  70.83%, tr:  99.28%, tr_best:  99.90%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9419%\n",
      "layer   2  Sparsity: 74.8231%\n",
      "layer   3  Sparsity: 82.5280%\n",
      "total_backward_count 411180 real_backward_count 86111  20.942%\n",
      "fc layer 2 self.abs_max_out: 6070.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.152145/  2.187362, val:  66.67%, val_best:  70.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9474%\n",
      "layer   2  Sparsity: 74.6021%\n",
      "layer   3  Sparsity: 82.4677%\n",
      "total_backward_count 420970 real_backward_count 87724  20.839%\n",
      "fc layer 1 self.abs_max_out: 14576.0\n",
      "lif layer 1 self.abs_max_v: 14576.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.152287/  2.188756, val:  69.58%, val_best:  70.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9650%\n",
      "layer   2  Sparsity: 74.3947%\n",
      "layer   3  Sparsity: 82.2626%\n",
      "total_backward_count 430760 real_backward_count 89310  20.733%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.153364/  2.188378, val:  61.25%, val_best:  70.83%, tr:  98.98%, tr_best:  99.90%, epoch time: 87.37 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9650%\n",
      "layer   2  Sparsity: 74.5429%\n",
      "layer   3  Sparsity: 82.3834%\n",
      "total_backward_count 440550 real_backward_count 90900  20.633%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.156646/  2.189185, val:  70.42%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.74 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9576%\n",
      "layer   2  Sparsity: 74.4986%\n",
      "layer   3  Sparsity: 82.3397%\n",
      "total_backward_count 450340 real_backward_count 92547  20.550%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.155796/  2.183257, val:  65.83%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 87.71 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9596%\n",
      "layer   2  Sparsity: 74.5629%\n",
      "layer   3  Sparsity: 82.3923%\n",
      "total_backward_count 460130 real_backward_count 94121  20.455%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.147132/  2.187352, val:  63.33%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9497%\n",
      "layer   2  Sparsity: 74.4229%\n",
      "layer   3  Sparsity: 81.8655%\n",
      "total_backward_count 469920 real_backward_count 95639  20.352%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.154211/  2.192229, val:  66.67%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9601%\n",
      "layer   2  Sparsity: 74.4513%\n",
      "layer   3  Sparsity: 82.0575%\n",
      "total_backward_count 479710 real_backward_count 97204  20.263%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.154609/  2.189084, val:  60.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.39 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9428%\n",
      "layer   2  Sparsity: 74.4856%\n",
      "layer   3  Sparsity: 82.1640%\n",
      "total_backward_count 489500 real_backward_count 98831  20.190%\n",
      "fc layer 1 self.abs_max_out: 14580.0\n",
      "lif layer 1 self.abs_max_v: 14580.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.158187/  2.197696, val:  69.58%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.51 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9555%\n",
      "layer   2  Sparsity: 74.4516%\n",
      "layer   3  Sparsity: 82.1450%\n",
      "total_backward_count 499290 real_backward_count 100422  20.113%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.153907/  2.192320, val:  70.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9345%\n",
      "layer   2  Sparsity: 74.4991%\n",
      "layer   3  Sparsity: 81.9392%\n",
      "total_backward_count 509080 real_backward_count 102001  20.036%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.150746/  2.193408, val:  67.92%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.74 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9549%\n",
      "layer   2  Sparsity: 74.5497%\n",
      "layer   3  Sparsity: 81.6773%\n",
      "total_backward_count 518870 real_backward_count 103564  19.960%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.151181/  2.181386, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.94 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.9509%\n",
      "layer   2  Sparsity: 74.5306%\n",
      "layer   3  Sparsity: 81.8447%\n",
      "total_backward_count 528660 real_backward_count 105128  19.886%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.152819/  2.192015, val:  72.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.82 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9620%\n",
      "layer   2  Sparsity: 74.5197%\n",
      "layer   3  Sparsity: 82.4158%\n",
      "total_backward_count 538450 real_backward_count 106643  19.806%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.156317/  2.190335, val:  65.83%, val_best:  72.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 86.96 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9548%\n",
      "layer   2  Sparsity: 74.4251%\n",
      "layer   3  Sparsity: 82.5559%\n",
      "total_backward_count 548240 real_backward_count 108161  19.729%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.158629/  2.191790, val:  62.08%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9529%\n",
      "layer   2  Sparsity: 74.4502%\n",
      "layer   3  Sparsity: 82.9222%\n",
      "total_backward_count 558030 real_backward_count 109671  19.653%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.159926/  2.187000, val:  70.42%, val_best:  72.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.85 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9420%\n",
      "layer   2  Sparsity: 74.3101%\n",
      "layer   3  Sparsity: 82.4594%\n",
      "total_backward_count 567820 real_backward_count 111128  19.571%\n",
      "lif layer 2 self.abs_max_v: 6952.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.156080/  2.185414, val:  76.67%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9454%\n",
      "layer   2  Sparsity: 74.4083%\n",
      "layer   3  Sparsity: 82.4399%\n",
      "total_backward_count 577610 real_backward_count 112561  19.487%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.154789/  2.197944, val:  49.58%, val_best:  76.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 87.25 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9353%\n",
      "layer   2  Sparsity: 74.3949%\n",
      "layer   3  Sparsity: 82.5269%\n",
      "total_backward_count 587400 real_backward_count 114026  19.412%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.163214/  2.195073, val:  62.92%, val_best:  76.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.95 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9620%\n",
      "layer   2  Sparsity: 74.5926%\n",
      "layer   3  Sparsity: 82.7100%\n",
      "total_backward_count 597190 real_backward_count 115523  19.344%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.159872/  2.189484, val:  77.50%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 87.42 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9415%\n",
      "layer   2  Sparsity: 74.5520%\n",
      "layer   3  Sparsity: 82.7944%\n",
      "total_backward_count 606980 real_backward_count 117024  19.280%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.156631/  2.194860, val:  75.42%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 87.59 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9374%\n",
      "layer   2  Sparsity: 74.2559%\n",
      "layer   3  Sparsity: 82.6780%\n",
      "total_backward_count 616770 real_backward_count 118460  19.207%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.152507/  2.185792, val:  71.25%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.54 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9477%\n",
      "layer   2  Sparsity: 74.2155%\n",
      "layer   3  Sparsity: 82.6153%\n",
      "total_backward_count 626560 real_backward_count 119951  19.144%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.150282/  2.188501, val:  77.08%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 88.02 seconds, 1.47 minutes\n",
      "layer   1  Sparsity: 93.9418%\n",
      "layer   2  Sparsity: 73.9716%\n",
      "layer   3  Sparsity: 82.6508%\n",
      "total_backward_count 636350 real_backward_count 121402  19.078%\n",
      "lif layer 2 self.abs_max_v: 7037.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.152797/  2.191904, val:  70.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9432%\n",
      "layer   2  Sparsity: 74.0375%\n",
      "layer   3  Sparsity: 82.3298%\n",
      "total_backward_count 646140 real_backward_count 122851  19.013%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.156014/  2.185960, val:  74.17%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.23 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9494%\n",
      "layer   2  Sparsity: 74.1689%\n",
      "layer   3  Sparsity: 82.5237%\n",
      "total_backward_count 655930 real_backward_count 124317  18.953%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.151174/  2.186908, val:  67.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.41 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9579%\n",
      "layer   2  Sparsity: 74.1318%\n",
      "layer   3  Sparsity: 82.5850%\n",
      "total_backward_count 665720 real_backward_count 125724  18.885%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.155403/  2.191967, val:  73.75%, val_best:  77.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 86.48 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9365%\n",
      "layer   2  Sparsity: 74.0558%\n",
      "layer   3  Sparsity: 83.0557%\n",
      "total_backward_count 675510 real_backward_count 127143  18.822%\n",
      "lif layer 2 self.abs_max_v: 7042.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.156547/  2.196146, val:  70.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9635%\n",
      "layer   2  Sparsity: 73.8904%\n",
      "layer   3  Sparsity: 82.7099%\n",
      "total_backward_count 685300 real_backward_count 128560  18.760%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.155592/  2.187097, val:  78.75%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.43 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9531%\n",
      "layer   2  Sparsity: 74.0426%\n",
      "layer   3  Sparsity: 82.7513%\n",
      "total_backward_count 695090 real_backward_count 129997  18.702%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.154865/  2.197085, val:  63.75%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.89 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9460%\n",
      "layer   2  Sparsity: 74.0534%\n",
      "layer   3  Sparsity: 82.8666%\n",
      "total_backward_count 704880 real_backward_count 131392  18.640%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.153878/  2.189848, val:  73.75%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.67 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9520%\n",
      "layer   2  Sparsity: 73.8573%\n",
      "layer   3  Sparsity: 82.7370%\n",
      "total_backward_count 714670 real_backward_count 132759  18.576%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.153170/  2.179885, val:  66.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.57 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9583%\n",
      "layer   2  Sparsity: 73.8852%\n",
      "layer   3  Sparsity: 82.5662%\n",
      "total_backward_count 724460 real_backward_count 134132  18.515%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.149109/  2.188684, val:  74.17%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.52 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9340%\n",
      "layer   2  Sparsity: 74.1757%\n",
      "layer   3  Sparsity: 82.6393%\n",
      "total_backward_count 734250 real_backward_count 135603  18.468%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.149367/  2.182569, val:  78.75%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.57 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9499%\n",
      "layer   2  Sparsity: 74.0866%\n",
      "layer   3  Sparsity: 82.6027%\n",
      "total_backward_count 744040 real_backward_count 137022  18.416%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.152488/  2.188143, val:  70.83%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9474%\n",
      "layer   2  Sparsity: 73.9933%\n",
      "layer   3  Sparsity: 82.6947%\n",
      "total_backward_count 753830 real_backward_count 138368  18.355%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.150138/  2.185004, val:  72.50%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.82 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9641%\n",
      "layer   2  Sparsity: 73.9720%\n",
      "layer   3  Sparsity: 82.5316%\n",
      "total_backward_count 763620 real_backward_count 139712  18.296%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.145525/  2.181374, val:  68.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.86 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9441%\n",
      "layer   2  Sparsity: 73.9256%\n",
      "layer   3  Sparsity: 82.0980%\n",
      "total_backward_count 773410 real_backward_count 141090  18.243%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.149165/  2.183918, val:  73.33%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9636%\n",
      "layer   2  Sparsity: 74.0537%\n",
      "layer   3  Sparsity: 82.6109%\n",
      "total_backward_count 783200 real_backward_count 142437  18.187%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.148622/  2.183906, val:  74.17%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 86.90 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9590%\n",
      "layer   2  Sparsity: 74.0199%\n",
      "layer   3  Sparsity: 82.8742%\n",
      "total_backward_count 792990 real_backward_count 143781  18.132%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.150963/  2.196103, val:  59.58%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.39 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9597%\n",
      "layer   2  Sparsity: 73.7756%\n",
      "layer   3  Sparsity: 82.5816%\n",
      "total_backward_count 802780 real_backward_count 145160  18.082%\n",
      "lif layer 2 self.abs_max_v: 7052.5\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.153884/  2.179307, val:  69.17%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 86.58 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9452%\n",
      "layer   2  Sparsity: 73.7273%\n",
      "layer   3  Sparsity: 82.1844%\n",
      "total_backward_count 812570 real_backward_count 146546  18.035%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.149488/  2.190844, val:  71.67%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.63 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9594%\n",
      "layer   2  Sparsity: 73.8079%\n",
      "layer   3  Sparsity: 82.5817%\n",
      "total_backward_count 822360 real_backward_count 147916  17.987%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.152843/  2.183725, val:  71.67%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.35 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9438%\n",
      "layer   2  Sparsity: 73.6770%\n",
      "layer   3  Sparsity: 82.8748%\n",
      "total_backward_count 832150 real_backward_count 149258  17.936%\n",
      "lif layer 1 self.abs_max_v: 14712.0\n",
      "lif layer 1 self.abs_max_v: 14726.0\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.149673/  2.194672, val:  69.17%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 84.63 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 93.9711%\n",
      "layer   2  Sparsity: 73.7820%\n",
      "layer   3  Sparsity: 82.7128%\n",
      "total_backward_count 841940 real_backward_count 150662  17.895%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.149084/  2.185230, val:  65.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.47 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9605%\n",
      "layer   2  Sparsity: 73.7126%\n",
      "layer   3  Sparsity: 82.1605%\n",
      "total_backward_count 851730 real_backward_count 151983  17.844%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.145369/  2.183607, val:  76.67%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9537%\n",
      "layer   2  Sparsity: 73.7965%\n",
      "layer   3  Sparsity: 81.9311%\n",
      "total_backward_count 861520 real_backward_count 153304  17.795%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.145476/  2.179886, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 86.85 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9533%\n",
      "layer   2  Sparsity: 73.6395%\n",
      "layer   3  Sparsity: 81.9969%\n",
      "total_backward_count 871310 real_backward_count 154666  17.751%\n",
      "lif layer 1 self.abs_max_v: 14884.0\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.145803/  2.179001, val:  81.25%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.78 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9496%\n",
      "layer   2  Sparsity: 73.5066%\n",
      "layer   3  Sparsity: 82.1625%\n",
      "total_backward_count 881100 real_backward_count 156071  17.713%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.145413/  2.184194, val:  77.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9700%\n",
      "layer   2  Sparsity: 73.7114%\n",
      "layer   3  Sparsity: 82.6533%\n",
      "total_backward_count 890890 real_backward_count 157420  17.670%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.144739/  2.172777, val:  77.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9505%\n",
      "layer   2  Sparsity: 73.7247%\n",
      "layer   3  Sparsity: 82.5267%\n",
      "total_backward_count 900680 real_backward_count 158736  17.624%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.147032/  2.181434, val:  76.25%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.40 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9530%\n",
      "layer   2  Sparsity: 73.5873%\n",
      "layer   3  Sparsity: 82.3397%\n",
      "total_backward_count 910470 real_backward_count 160061  17.580%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.151451/  2.192593, val:  71.67%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9467%\n",
      "layer   2  Sparsity: 73.5199%\n",
      "layer   3  Sparsity: 82.5141%\n",
      "total_backward_count 920260 real_backward_count 161420  17.541%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.151689/  2.186330, val:  73.75%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9438%\n",
      "layer   2  Sparsity: 73.6060%\n",
      "layer   3  Sparsity: 82.9436%\n",
      "total_backward_count 930050 real_backward_count 162707  17.494%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.148675/  2.180474, val:  80.83%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.67 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9469%\n",
      "layer   2  Sparsity: 73.8469%\n",
      "layer   3  Sparsity: 82.9331%\n",
      "total_backward_count 939840 real_backward_count 164022  17.452%\n",
      "lif layer 1 self.abs_max_v: 15203.5\n",
      "lif layer 1 self.abs_max_v: 15238.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.148618/  2.184588, val:  77.50%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 86.12 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9608%\n",
      "layer   2  Sparsity: 73.8317%\n",
      "layer   3  Sparsity: 82.6575%\n",
      "total_backward_count 949630 real_backward_count 165347  17.412%\n",
      "lif layer 2 self.abs_max_v: 7109.0\n",
      "lif layer 1 self.abs_max_v: 15291.5\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.147322/  2.182942, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.15 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9339%\n",
      "layer   2  Sparsity: 73.5511%\n",
      "layer   3  Sparsity: 82.8411%\n",
      "total_backward_count 959420 real_backward_count 166662  17.371%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.146643/  2.186206, val:  70.00%, val_best:  81.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 86.88 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9383%\n",
      "layer   2  Sparsity: 73.6378%\n",
      "layer   3  Sparsity: 82.7627%\n",
      "total_backward_count 969210 real_backward_count 167961  17.330%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.143526/  2.178174, val:  77.92%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.00 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9356%\n",
      "layer   2  Sparsity: 73.7406%\n",
      "layer   3  Sparsity: 82.2114%\n",
      "total_backward_count 979000 real_backward_count 169267  17.290%\n",
      "fc layer 1 self.abs_max_out: 14587.0\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.139482/  2.176156, val:  78.75%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 87.38 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9482%\n",
      "layer   2  Sparsity: 73.5982%\n",
      "layer   3  Sparsity: 82.0669%\n",
      "total_backward_count 988790 real_backward_count 170555  17.249%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.141586/  2.177825, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9624%\n",
      "layer   2  Sparsity: 73.5545%\n",
      "layer   3  Sparsity: 82.6397%\n",
      "total_backward_count 998580 real_backward_count 171869  17.211%\n",
      "fc layer 1 self.abs_max_out: 14626.0\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.142297/  2.181613, val:  77.92%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.05 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9542%\n",
      "layer   2  Sparsity: 73.5810%\n",
      "layer   3  Sparsity: 82.2244%\n",
      "total_backward_count 1008370 real_backward_count 173190  17.175%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.140197/  2.174984, val:  69.58%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.99 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9353%\n",
      "layer   2  Sparsity: 73.4595%\n",
      "layer   3  Sparsity: 82.1986%\n",
      "total_backward_count 1018160 real_backward_count 174449  17.134%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.140542/  2.176814, val:  80.83%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 87.06 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9437%\n",
      "layer   2  Sparsity: 73.4548%\n",
      "layer   3  Sparsity: 82.6173%\n",
      "total_backward_count 1027950 real_backward_count 175751  17.097%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.141067/  2.183064, val:  73.33%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.88 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9456%\n",
      "layer   2  Sparsity: 73.4269%\n",
      "layer   3  Sparsity: 82.4905%\n",
      "total_backward_count 1037740 real_backward_count 177076  17.064%\n",
      "lif layer 1 self.abs_max_v: 15311.5\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.140817/  2.182070, val:  60.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.57 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9754%\n",
      "layer   2  Sparsity: 73.4643%\n",
      "layer   3  Sparsity: 82.8217%\n",
      "total_backward_count 1047530 real_backward_count 178365  17.027%\n",
      "fc layer 1 self.abs_max_out: 14630.0\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.145477/  2.180727, val:  72.08%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 87.45 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9370%\n",
      "layer   2  Sparsity: 73.4324%\n",
      "layer   3  Sparsity: 82.6060%\n",
      "total_backward_count 1057320 real_backward_count 179615  16.988%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.147334/  2.180215, val:  72.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 87.30 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9567%\n",
      "layer   2  Sparsity: 73.3779%\n",
      "layer   3  Sparsity: 82.9261%\n",
      "total_backward_count 1067110 real_backward_count 180907  16.953%\n",
      "fc layer 1 self.abs_max_out: 14718.0\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.141532/  2.173883, val:  77.92%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.16 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9587%\n",
      "layer   2  Sparsity: 73.4322%\n",
      "layer   3  Sparsity: 82.4432%\n",
      "total_backward_count 1076900 real_backward_count 182094  16.909%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.142913/  2.176155, val:  77.92%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 87.21 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9474%\n",
      "layer   2  Sparsity: 73.3454%\n",
      "layer   3  Sparsity: 82.8675%\n",
      "total_backward_count 1086690 real_backward_count 183353  16.873%\n",
      "lif layer 1 self.abs_max_v: 15374.0\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.142495/  2.183705, val:  69.58%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.11 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9476%\n",
      "layer   2  Sparsity: 73.2287%\n",
      "layer   3  Sparsity: 83.0195%\n",
      "total_backward_count 1096480 real_backward_count 184626  16.838%\n",
      "lif layer 1 self.abs_max_v: 15635.5\n",
      "lif layer 1 self.abs_max_v: 15806.0\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.147834/  2.183137, val:  82.92%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.07 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9343%\n",
      "layer   2  Sparsity: 73.2938%\n",
      "layer   3  Sparsity: 82.7725%\n",
      "total_backward_count 1106270 real_backward_count 185932  16.807%\n",
      "fc layer 1 self.abs_max_out: 14737.0\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.143705/  2.180182, val:  56.25%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.17 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9456%\n",
      "layer   2  Sparsity: 73.4591%\n",
      "layer   3  Sparsity: 82.3851%\n",
      "total_backward_count 1116060 real_backward_count 187158  16.770%\n",
      "lif layer 2 self.abs_max_v: 7122.5\n",
      "lif layer 1 self.abs_max_v: 15996.0\n",
      "lif layer 1 self.abs_max_v: 16081.0\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.141600/  2.173496, val:  64.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.96 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9443%\n",
      "layer   2  Sparsity: 73.3766%\n",
      "layer   3  Sparsity: 82.2897%\n",
      "total_backward_count 1125850 real_backward_count 188364  16.731%\n",
      "lif layer 2 self.abs_max_v: 7338.5\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.137980/  2.178335, val:  84.17%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 86.50 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 93.9495%\n",
      "layer   2  Sparsity: 73.4783%\n",
      "layer   3  Sparsity: 82.4658%\n",
      "total_backward_count 1135640 real_backward_count 189689  16.703%\n",
      "fc layer 1 self.abs_max_out: 14745.0\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.140334/  2.169894, val:  76.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 87.33 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 93.9477%\n",
      "layer   2  Sparsity: 73.3953%\n",
      "layer   3  Sparsity: 82.5087%\n",
      "total_backward_count 1145430 real_backward_count 190905  16.667%\n",
      "fc layer 1 self.abs_max_out: 14773.0\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.140832/  2.176330, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.99 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9499%\n",
      "layer   2  Sparsity: 73.3130%\n",
      "layer   3  Sparsity: 82.6402%\n",
      "total_backward_count 1155220 real_backward_count 192174  16.635%\n",
      "lif layer 2 self.abs_max_v: 7358.5\n",
      "lif layer 2 self.abs_max_v: 7390.5\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.143294/  2.175899, val:  70.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 86.97 seconds, 1.45 minutes\n",
      "layer   1  Sparsity: 93.9622%\n",
      "layer   2  Sparsity: 73.3508%\n",
      "layer   3  Sparsity: 82.8683%\n",
      "total_backward_count 1165010 real_backward_count 193458  16.606%\n",
      "fc layer 1 self.abs_max_out: 14803.0\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.140433/  2.174526, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.40 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 93.9506%\n",
      "layer   2  Sparsity: 73.2588%\n",
      "layer   3  Sparsity: 82.7983%\n",
      "total_backward_count 1174800 real_backward_count 194730  16.576%\n",
      "fc layer 1 self.abs_max_out: 14812.0\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.143204/  2.180528, val:  74.17%, val_best:  84.58%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9650%\n",
      "layer   2  Sparsity: 73.2414%\n",
      "layer   3  Sparsity: 82.7790%\n",
      "total_backward_count 1184590 real_backward_count 195968  16.543%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.139509/  2.171049, val:  76.25%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.9562%\n",
      "layer   2  Sparsity: 73.3630%\n",
      "layer   3  Sparsity: 82.4864%\n",
      "total_backward_count 1194380 real_backward_count 197204  16.511%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.136785/  2.173632, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.9637%\n",
      "layer   2  Sparsity: 73.1774%\n",
      "layer   3  Sparsity: 82.1822%\n",
      "total_backward_count 1204170 real_backward_count 198430  16.479%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.143500/  2.177202, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.9684%\n",
      "layer   2  Sparsity: 73.1577%\n",
      "layer   3  Sparsity: 82.4307%\n",
      "total_backward_count 1213960 real_backward_count 199697  16.450%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.140035/  2.170967, val:  85.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.9752%\n",
      "layer   2  Sparsity: 73.2160%\n",
      "layer   3  Sparsity: 82.2284%\n",
      "total_backward_count 1223750 real_backward_count 200917  16.418%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.135129/  2.171770, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9644%\n",
      "layer   2  Sparsity: 73.3980%\n",
      "layer   3  Sparsity: 82.2267%\n",
      "total_backward_count 1233540 real_backward_count 202130  16.386%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.138135/  2.179877, val:  65.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9580%\n",
      "layer   2  Sparsity: 73.4029%\n",
      "layer   3  Sparsity: 82.4482%\n",
      "total_backward_count 1243330 real_backward_count 203321  16.353%\n",
      "fc layer 1 self.abs_max_out: 14829.0\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.139286/  2.180808, val:  70.83%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9343%\n",
      "layer   2  Sparsity: 73.2223%\n",
      "layer   3  Sparsity: 82.6897%\n",
      "total_backward_count 1253120 real_backward_count 204539  16.322%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.145264/  2.178239, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9597%\n",
      "layer   2  Sparsity: 73.2918%\n",
      "layer   3  Sparsity: 82.8603%\n",
      "total_backward_count 1262910 real_backward_count 205784  16.294%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.145135/  2.177105, val:  82.50%, val_best:  85.42%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9562%\n",
      "layer   2  Sparsity: 73.2373%\n",
      "layer   3  Sparsity: 82.7529%\n",
      "total_backward_count 1272700 real_backward_count 207060  16.269%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.141348/  2.174477, val:  67.08%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9626%\n",
      "layer   2  Sparsity: 73.2833%\n",
      "layer   3  Sparsity: 82.3785%\n",
      "total_backward_count 1282490 real_backward_count 208299  16.242%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.136666/  2.170378, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9450%\n",
      "layer   2  Sparsity: 73.4030%\n",
      "layer   3  Sparsity: 82.6941%\n",
      "total_backward_count 1292280 real_backward_count 209510  16.212%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.134534/  2.177075, val:  71.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9615%\n",
      "layer   2  Sparsity: 73.2594%\n",
      "layer   3  Sparsity: 82.5331%\n",
      "total_backward_count 1302070 real_backward_count 210693  16.181%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.136664/  2.174698, val:  75.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9600%\n",
      "layer   2  Sparsity: 73.1586%\n",
      "layer   3  Sparsity: 82.5429%\n",
      "total_backward_count 1311860 real_backward_count 211955  16.157%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.137839/  2.175653, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9483%\n",
      "layer   2  Sparsity: 73.2512%\n",
      "layer   3  Sparsity: 82.5433%\n",
      "total_backward_count 1321650 real_backward_count 213193  16.131%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.141315/  2.175526, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9477%\n",
      "layer   2  Sparsity: 73.1791%\n",
      "layer   3  Sparsity: 82.5410%\n",
      "total_backward_count 1331440 real_backward_count 214441  16.106%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.136147/  2.168206, val:  80.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9302%\n",
      "layer   2  Sparsity: 73.2479%\n",
      "layer   3  Sparsity: 82.6421%\n",
      "total_backward_count 1341230 real_backward_count 215610  16.076%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.134178/  2.171923, val:  78.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.70 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 93.9350%\n",
      "layer   2  Sparsity: 73.5063%\n",
      "layer   3  Sparsity: 82.8283%\n",
      "total_backward_count 1351020 real_backward_count 216794  16.047%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.138356/  2.170373, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9599%\n",
      "layer   2  Sparsity: 73.3493%\n",
      "layer   3  Sparsity: 83.1161%\n",
      "total_backward_count 1360810 real_backward_count 217972  16.018%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.131741/  2.172707, val:  72.92%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9376%\n",
      "layer   2  Sparsity: 73.1790%\n",
      "layer   3  Sparsity: 82.8153%\n",
      "total_backward_count 1370600 real_backward_count 219164  15.990%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.132867/  2.170636, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9509%\n",
      "layer   2  Sparsity: 73.1187%\n",
      "layer   3  Sparsity: 82.5514%\n",
      "total_backward_count 1380390 real_backward_count 220373  15.965%\n",
      "lif layer 1 self.abs_max_v: 16154.0\n",
      "lif layer 1 self.abs_max_v: 16347.0\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.136750/  2.171807, val:  73.75%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9554%\n",
      "layer   2  Sparsity: 73.1802%\n",
      "layer   3  Sparsity: 82.6061%\n",
      "total_backward_count 1390180 real_backward_count 221600  15.940%\n",
      "lif layer 1 self.abs_max_v: 16351.5\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.133067/  2.166610, val:  75.83%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9569%\n",
      "layer   2  Sparsity: 73.2281%\n",
      "layer   3  Sparsity: 82.3699%\n",
      "total_backward_count 1399970 real_backward_count 222857  15.919%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.127771/  2.162132, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.9602%\n",
      "layer   2  Sparsity: 73.2920%\n",
      "layer   3  Sparsity: 82.0870%\n",
      "total_backward_count 1409760 real_backward_count 224021  15.891%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.123607/  2.160144, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9306%\n",
      "layer   2  Sparsity: 73.2289%\n",
      "layer   3  Sparsity: 81.9040%\n",
      "total_backward_count 1419550 real_backward_count 225256  15.868%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.123177/  2.163707, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9583%\n",
      "layer   2  Sparsity: 73.3954%\n",
      "layer   3  Sparsity: 82.1744%\n",
      "total_backward_count 1429340 real_backward_count 226379  15.838%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.132360/  2.172299, val:  75.83%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.9343%\n",
      "layer   2  Sparsity: 73.2959%\n",
      "layer   3  Sparsity: 81.7923%\n",
      "total_backward_count 1439130 real_backward_count 227551  15.812%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.131286/  2.171254, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9354%\n",
      "layer   2  Sparsity: 73.1858%\n",
      "layer   3  Sparsity: 82.1370%\n",
      "total_backward_count 1448920 real_backward_count 228722  15.786%\n",
      "fc layer 1 self.abs_max_out: 14846.0\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.131379/  2.169468, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9666%\n",
      "layer   2  Sparsity: 73.2126%\n",
      "layer   3  Sparsity: 82.3748%\n",
      "total_backward_count 1458710 real_backward_count 229884  15.759%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.128837/  2.170206, val:  75.42%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9535%\n",
      "layer   2  Sparsity: 73.1178%\n",
      "layer   3  Sparsity: 82.1542%\n",
      "total_backward_count 1468500 real_backward_count 231044  15.733%\n",
      "lif layer 2 self.abs_max_v: 7456.0\n",
      "lif layer 1 self.abs_max_v: 16546.5\n",
      "lif layer 1 self.abs_max_v: 16760.5\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.131314/  2.170834, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9481%\n",
      "layer   2  Sparsity: 73.1565%\n",
      "layer   3  Sparsity: 82.6111%\n",
      "total_backward_count 1478290 real_backward_count 232215  15.708%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.133364/  2.165534, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9520%\n",
      "layer   2  Sparsity: 73.1844%\n",
      "layer   3  Sparsity: 82.4382%\n",
      "total_backward_count 1488080 real_backward_count 233320  15.679%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.129569/  2.165872, val:  74.17%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9450%\n",
      "layer   2  Sparsity: 73.2324%\n",
      "layer   3  Sparsity: 82.0832%\n",
      "total_backward_count 1497870 real_backward_count 234494  15.655%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.128121/  2.160455, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9366%\n",
      "layer   2  Sparsity: 73.0537%\n",
      "layer   3  Sparsity: 82.2084%\n",
      "total_backward_count 1507660 real_backward_count 235658  15.631%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.127531/  2.166343, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9440%\n",
      "layer   2  Sparsity: 73.0998%\n",
      "layer   3  Sparsity: 82.4786%\n",
      "total_backward_count 1517450 real_backward_count 236808  15.606%\n",
      "lif layer 1 self.abs_max_v: 16866.0\n",
      "lif layer 1 self.abs_max_v: 17068.0\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.129032/  2.162358, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9605%\n",
      "layer   2  Sparsity: 73.3357%\n",
      "layer   3  Sparsity: 82.6977%\n",
      "total_backward_count 1527240 real_backward_count 237960  15.581%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.128786/  2.167077, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9283%\n",
      "layer   2  Sparsity: 73.0460%\n",
      "layer   3  Sparsity: 82.5918%\n",
      "total_backward_count 1537030 real_backward_count 239145  15.559%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.128046/  2.165600, val:  84.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9821%\n",
      "layer   2  Sparsity: 73.1090%\n",
      "layer   3  Sparsity: 82.3486%\n",
      "total_backward_count 1546820 real_backward_count 240295  15.535%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.122858/  2.168443, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9510%\n",
      "layer   2  Sparsity: 73.1548%\n",
      "layer   3  Sparsity: 82.3779%\n",
      "total_backward_count 1556610 real_backward_count 241473  15.513%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.125611/  2.161983, val:  72.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9610%\n",
      "layer   2  Sparsity: 73.0914%\n",
      "layer   3  Sparsity: 82.2546%\n",
      "total_backward_count 1566400 real_backward_count 242639  15.490%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.128059/  2.160999, val:  78.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9549%\n",
      "layer   2  Sparsity: 73.1507%\n",
      "layer   3  Sparsity: 82.4027%\n",
      "total_backward_count 1576190 real_backward_count 243796  15.467%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.124794/  2.163805, val:  69.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9500%\n",
      "layer   2  Sparsity: 73.1860%\n",
      "layer   3  Sparsity: 82.7250%\n",
      "total_backward_count 1585980 real_backward_count 244884  15.441%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.130557/  2.168602, val:  70.00%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9342%\n",
      "layer   2  Sparsity: 73.1801%\n",
      "layer   3  Sparsity: 82.3184%\n",
      "total_backward_count 1595770 real_backward_count 245989  15.415%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.128648/  2.172570, val:  76.67%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9430%\n",
      "layer   2  Sparsity: 73.0183%\n",
      "layer   3  Sparsity: 82.0140%\n",
      "total_backward_count 1605560 real_backward_count 247082  15.389%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.129250/  2.168489, val:  72.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9764%\n",
      "layer   2  Sparsity: 73.1958%\n",
      "layer   3  Sparsity: 82.2500%\n",
      "total_backward_count 1615350 real_backward_count 248220  15.366%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.126967/  2.165832, val:  80.42%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9548%\n",
      "layer   2  Sparsity: 73.2042%\n",
      "layer   3  Sparsity: 82.1785%\n",
      "total_backward_count 1625140 real_backward_count 249360  15.344%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.128917/  2.166334, val:  75.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9468%\n",
      "layer   2  Sparsity: 73.0138%\n",
      "layer   3  Sparsity: 82.1158%\n",
      "total_backward_count 1634930 real_backward_count 250457  15.319%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.127994/  2.163575, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9456%\n",
      "layer   2  Sparsity: 72.9702%\n",
      "layer   3  Sparsity: 81.9412%\n",
      "total_backward_count 1644720 real_backward_count 251635  15.300%\n",
      "lif layer 2 self.abs_max_v: 7467.0\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.126271/  2.161494, val:  78.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9509%\n",
      "layer   2  Sparsity: 73.0967%\n",
      "layer   3  Sparsity: 82.1318%\n",
      "total_backward_count 1654510 real_backward_count 252801  15.280%\n",
      "lif layer 2 self.abs_max_v: 7594.0\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.125313/  2.165743, val:  83.75%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9656%\n",
      "layer   2  Sparsity: 73.3820%\n",
      "layer   3  Sparsity: 82.1798%\n",
      "total_backward_count 1664300 real_backward_count 253916  15.257%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.129033/  2.167410, val:  68.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9363%\n",
      "layer   2  Sparsity: 73.1718%\n",
      "layer   3  Sparsity: 82.1802%\n",
      "total_backward_count 1674090 real_backward_count 255044  15.235%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.125646/  2.170665, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9342%\n",
      "layer   2  Sparsity: 73.0777%\n",
      "layer   3  Sparsity: 82.1016%\n",
      "total_backward_count 1683880 real_backward_count 256209  15.215%\n",
      "fc layer 1 self.abs_max_out: 14857.0\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.129361/  2.162785, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9554%\n",
      "layer   2  Sparsity: 73.1155%\n",
      "layer   3  Sparsity: 82.2911%\n",
      "total_backward_count 1693670 real_backward_count 257311  15.193%\n",
      "fc layer 1 self.abs_max_out: 14863.0\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  2.124623/  2.160730, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.9590%\n",
      "layer   2  Sparsity: 73.0568%\n",
      "layer   3  Sparsity: 82.3087%\n",
      "total_backward_count 1703460 real_backward_count 258417  15.170%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.123954/  2.163057, val:  78.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9386%\n",
      "layer   2  Sparsity: 73.0980%\n",
      "layer   3  Sparsity: 82.0081%\n",
      "total_backward_count 1713250 real_backward_count 259527  15.148%\n",
      "fc layer 1 self.abs_max_out: 14871.0\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.121912/  2.154599, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9582%\n",
      "layer   2  Sparsity: 73.2396%\n",
      "layer   3  Sparsity: 82.2923%\n",
      "total_backward_count 1723040 real_backward_count 260637  15.127%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.123505/  2.156171, val:  83.75%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9581%\n",
      "layer   2  Sparsity: 73.1826%\n",
      "layer   3  Sparsity: 82.4574%\n",
      "total_backward_count 1732830 real_backward_count 261696  15.102%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.118309/  2.156515, val:  76.67%, val_best:  88.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9445%\n",
      "layer   2  Sparsity: 73.1088%\n",
      "layer   3  Sparsity: 82.5584%\n",
      "total_backward_count 1742620 real_backward_count 262782  15.080%\n",
      "lif layer 1 self.abs_max_v: 17227.0\n",
      "lif layer 1 self.abs_max_v: 17429.5\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.119658/  2.160306, val:  78.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.9444%\n",
      "layer   2  Sparsity: 73.0838%\n",
      "layer   3  Sparsity: 82.1550%\n",
      "total_backward_count 1752410 real_backward_count 263853  15.057%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  2.122355/  2.161344, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.9312%\n",
      "layer   2  Sparsity: 73.0909%\n",
      "layer   3  Sparsity: 82.1520%\n",
      "total_backward_count 1762200 real_backward_count 264987  15.037%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  2.124080/  2.161521, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9504%\n",
      "layer   2  Sparsity: 73.1532%\n",
      "layer   3  Sparsity: 82.0140%\n",
      "total_backward_count 1771990 real_backward_count 266118  15.018%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.120300/  2.159406, val:  73.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9315%\n",
      "layer   2  Sparsity: 72.9432%\n",
      "layer   3  Sparsity: 82.0078%\n",
      "total_backward_count 1781780 real_backward_count 267257  14.999%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.123928/  2.167227, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9494%\n",
      "layer   2  Sparsity: 72.9711%\n",
      "layer   3  Sparsity: 82.0691%\n",
      "total_backward_count 1791570 real_backward_count 268323  14.977%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.122841/  2.167845, val:  65.00%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9335%\n",
      "layer   2  Sparsity: 72.9243%\n",
      "layer   3  Sparsity: 82.3985%\n",
      "total_backward_count 1801360 real_backward_count 269422  14.957%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.125820/  2.163435, val:  76.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9566%\n",
      "layer   2  Sparsity: 73.1287%\n",
      "layer   3  Sparsity: 82.6922%\n",
      "total_backward_count 1811150 real_backward_count 270502  14.935%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.122149/  2.160706, val:  76.67%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9599%\n",
      "layer   2  Sparsity: 73.1425%\n",
      "layer   3  Sparsity: 82.2308%\n",
      "total_backward_count 1820940 real_backward_count 271583  14.914%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.123432/  2.159819, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9495%\n",
      "layer   2  Sparsity: 72.9916%\n",
      "layer   3  Sparsity: 82.4459%\n",
      "total_backward_count 1830730 real_backward_count 272681  14.895%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.125183/  2.167444, val:  66.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9577%\n",
      "layer   2  Sparsity: 73.0495%\n",
      "layer   3  Sparsity: 81.9791%\n",
      "total_backward_count 1840520 real_backward_count 273761  14.874%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.119857/  2.159923, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9616%\n",
      "layer   2  Sparsity: 73.1095%\n",
      "layer   3  Sparsity: 82.0140%\n",
      "total_backward_count 1850310 real_backward_count 274847  14.854%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  2.124297/  2.156336, val:  81.25%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.9424%\n",
      "layer   2  Sparsity: 72.9328%\n",
      "layer   3  Sparsity: 82.2544%\n",
      "total_backward_count 1860100 real_backward_count 275916  14.833%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.124464/  2.165085, val:  77.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.9530%\n",
      "layer   2  Sparsity: 73.0459%\n",
      "layer   3  Sparsity: 82.5674%\n",
      "total_backward_count 1869890 real_backward_count 276948  14.811%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.126821/  2.163758, val:  70.83%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9591%\n",
      "layer   2  Sparsity: 73.1185%\n",
      "layer   3  Sparsity: 81.8396%\n",
      "total_backward_count 1879680 real_backward_count 278067  14.793%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.122228/  2.157661, val:  77.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9671%\n",
      "layer   2  Sparsity: 73.1778%\n",
      "layer   3  Sparsity: 81.5452%\n",
      "total_backward_count 1889470 real_backward_count 279175  14.775%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.116481/  2.160257, val:  70.00%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.9538%\n",
      "layer   2  Sparsity: 73.0330%\n",
      "layer   3  Sparsity: 81.6663%\n",
      "total_backward_count 1899260 real_backward_count 280234  14.755%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.118824/  2.158743, val:  73.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9458%\n",
      "layer   2  Sparsity: 73.0407%\n",
      "layer   3  Sparsity: 81.7746%\n",
      "total_backward_count 1909050 real_backward_count 281302  14.735%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.119507/  2.155348, val:  68.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.9577%\n",
      "layer   2  Sparsity: 73.0824%\n",
      "layer   3  Sparsity: 81.7236%\n",
      "total_backward_count 1918840 real_backward_count 282330  14.714%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.117705/  2.153738, val:  83.33%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9590%\n",
      "layer   2  Sparsity: 73.0831%\n",
      "layer   3  Sparsity: 81.8942%\n",
      "total_backward_count 1928630 real_backward_count 283412  14.695%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.114542/  2.147954, val:  83.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9414%\n",
      "layer   2  Sparsity: 73.1362%\n",
      "layer   3  Sparsity: 81.8450%\n",
      "total_backward_count 1938420 real_backward_count 284533  14.679%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.108518/  2.155204, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9463%\n",
      "layer   2  Sparsity: 73.1212%\n",
      "layer   3  Sparsity: 81.7070%\n",
      "total_backward_count 1948210 real_backward_count 285569  14.658%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.110225/  2.146717, val:  76.25%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.9407%\n",
      "layer   2  Sparsity: 72.9040%\n",
      "layer   3  Sparsity: 81.1368%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924e22d942c74f4ca98eb06a881e8071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÇ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>2.11023</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.7625</td></tr><tr><td>val_loss</td><td>2.14672</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-sweep-167</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rtkdkef' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7rtkdkef</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_140541-7rtkdkef/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9tmk8rqh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_184558-9tmk8rqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9tmk8rqh' target=\"_blank\">vague-sweep-172</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9tmk8rqh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9tmk8rqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_184608_435', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 467.0\n",
      "lif layer 1 self.abs_max_v: 467.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 839.0\n",
      "lif layer 2 self.abs_max_v: 839.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "fc layer 1 self.abs_max_out: 588.0\n",
      "lif layer 1 self.abs_max_v: 747.5\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "lif layer 2 self.abs_max_v: 1518.5\n",
      "fc layer 1 self.abs_max_out: 846.0\n",
      "lif layer 1 self.abs_max_v: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "lif layer 2 self.abs_max_v: 1760.5\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 1790.0\n",
      "lif layer 2 self.abs_max_v: 1861.5\n",
      "lif layer 2 self.abs_max_v: 1882.0\n",
      "lif layer 1 self.abs_max_v: 1278.5\n",
      "lif layer 2 self.abs_max_v: 2133.0\n",
      "lif layer 2 self.abs_max_v: 2253.5\n",
      "fc layer 1 self.abs_max_out: 987.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "lif layer 1 self.abs_max_v: 1330.0\n",
      "fc layer 1 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 1620.0\n",
      "lif layer 2 self.abs_max_v: 2265.5\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "lif layer 2 self.abs_max_v: 2660.5\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "lif layer 1 self.abs_max_v: 1677.0\n",
      "fc layer 1 self.abs_max_out: 1464.0\n",
      "lif layer 1 self.abs_max_v: 1703.5\n",
      "fc layer 3 self.abs_max_out: 1061.0\n",
      "lif layer 1 self.abs_max_v: 1798.5\n",
      "lif layer 2 self.abs_max_v: 2829.0\n",
      "fc layer 1 self.abs_max_out: 1791.0\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "lif layer 2 self.abs_max_v: 2943.5\n",
      "lif layer 1 self.abs_max_v: 1906.5\n",
      "lif layer 1 self.abs_max_v: 2000.5\n",
      "lif layer 1 self.abs_max_v: 2008.5\n",
      "lif layer 2 self.abs_max_v: 3010.0\n",
      "lif layer 2 self.abs_max_v: 3058.0\n",
      "lif layer 2 self.abs_max_v: 3251.5\n",
      "lif layer 2 self.abs_max_v: 3339.0\n",
      "lif layer 1 self.abs_max_v: 2153.5\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "fc layer 3 self.abs_max_out: 1311.0\n",
      "fc layer 2 self.abs_max_out: 2441.0\n",
      "lif layer 1 self.abs_max_v: 2173.5\n",
      "lif layer 2 self.abs_max_v: 3470.0\n",
      "fc layer 1 self.abs_max_out: 1863.0\n",
      "lif layer 1 self.abs_max_v: 2177.5\n",
      "fc layer 1 self.abs_max_out: 1892.0\n",
      "fc layer 1 self.abs_max_out: 2055.0\n",
      "fc layer 1 self.abs_max_out: 2203.0\n",
      "lif layer 1 self.abs_max_v: 2499.0\n",
      "fc layer 2 self.abs_max_out: 2481.0\n",
      "lif layer 2 self.abs_max_v: 3571.0\n",
      "lif layer 2 self.abs_max_v: 3603.0\n",
      "lif layer 2 self.abs_max_v: 3838.5\n",
      "fc layer 1 self.abs_max_out: 2721.0\n",
      "lif layer 1 self.abs_max_v: 2721.0\n",
      "lif layer 1 self.abs_max_v: 2932.5\n",
      "lif layer 2 self.abs_max_v: 3987.0\n",
      "lif layer 2 self.abs_max_v: 4124.5\n",
      "fc layer 2 self.abs_max_out: 2557.0\n",
      "fc layer 2 self.abs_max_out: 2790.0\n",
      "lif layer 1 self.abs_max_v: 3004.5\n",
      "lif layer 2 self.abs_max_v: 4282.0\n",
      "lif layer 1 self.abs_max_v: 3030.0\n",
      "fc layer 2 self.abs_max_out: 2874.0\n",
      "fc layer 1 self.abs_max_out: 2809.0\n",
      "lif layer 1 self.abs_max_v: 3196.0\n",
      "fc layer 1 self.abs_max_out: 3001.0\n",
      "lif layer 1 self.abs_max_v: 3234.0\n",
      "lif layer 1 self.abs_max_v: 3343.0\n",
      "lif layer 1 self.abs_max_v: 3401.5\n",
      "fc layer 1 self.abs_max_out: 3155.0\n",
      "lif layer 2 self.abs_max_v: 4332.5\n",
      "fc layer 3 self.abs_max_out: 1358.0\n",
      "fc layer 2 self.abs_max_out: 2883.0\n",
      "lif layer 2 self.abs_max_v: 4453.0\n",
      "lif layer 2 self.abs_max_v: 4821.5\n",
      "fc layer 1 self.abs_max_out: 3248.0\n",
      "lif layer 1 self.abs_max_v: 3518.0\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "lif layer 1 self.abs_max_v: 3602.5\n",
      "lif layer 1 self.abs_max_v: 3834.5\n",
      "lif layer 1 self.abs_max_v: 4017.5\n",
      "fc layer 2 self.abs_max_out: 2919.0\n",
      "fc layer 2 self.abs_max_out: 2979.0\n",
      "fc layer 2 self.abs_max_out: 3064.0\n",
      "fc layer 2 self.abs_max_out: 3263.0\n",
      "fc layer 2 self.abs_max_out: 3293.0\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "fc layer 1 self.abs_max_out: 3644.0\n",
      "fc layer 1 self.abs_max_out: 3986.0\n",
      "lif layer 1 self.abs_max_v: 4340.0\n",
      "lif layer 1 self.abs_max_v: 4557.0\n",
      "fc layer 2 self.abs_max_out: 3543.0\n",
      "fc layer 1 self.abs_max_out: 4045.0\n",
      "lif layer 2 self.abs_max_v: 5110.5\n",
      "lif layer 2 self.abs_max_v: 5197.0\n",
      "fc layer 1 self.abs_max_out: 4306.0\n",
      "lif layer 1 self.abs_max_v: 4592.5\n",
      "lif layer 1 self.abs_max_v: 4748.5\n",
      "lif layer 2 self.abs_max_v: 5511.0\n",
      "lif layer 2 self.abs_max_v: 5554.5\n",
      "fc layer 2 self.abs_max_out: 3742.0\n",
      "fc layer 1 self.abs_max_out: 4579.0\n",
      "fc layer 1 self.abs_max_out: 4604.0\n",
      "lif layer 1 self.abs_max_v: 4832.5\n",
      "lif layer 1 self.abs_max_v: 5449.0\n",
      "lif layer 1 self.abs_max_v: 5688.5\n",
      "lif layer 1 self.abs_max_v: 5824.5\n",
      "fc layer 1 self.abs_max_out: 4624.0\n",
      "fc layer 1 self.abs_max_out: 4866.0\n",
      "fc layer 1 self.abs_max_out: 5592.0\n",
      "lif layer 1 self.abs_max_v: 5852.5\n",
      "lif layer 1 self.abs_max_v: 6100.0\n",
      "lif layer 1 self.abs_max_v: 6286.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.892830/  2.036716, val:  46.67%, val_best:  46.67%, tr:  93.05%, tr_best:  93.05%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.2045%\n",
      "layer   3  Sparsity: 70.3073%\n",
      "total_backward_count 9790 real_backward_count 2798  28.580%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3775.0\n",
      "fc layer 1 self.abs_max_out: 6039.0\n",
      "lif layer 1 self.abs_max_v: 6303.0\n",
      "lif layer 1 self.abs_max_v: 6484.0\n",
      "lif layer 2 self.abs_max_v: 5752.0\n",
      "lif layer 1 self.abs_max_v: 6745.5\n",
      "lif layer 1 self.abs_max_v: 6782.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.873983/  2.022081, val:  44.58%, val_best:  46.67%, tr:  98.77%, tr_best:  98.77%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.3238%\n",
      "layer   3  Sparsity: 70.7771%\n",
      "total_backward_count 19580 real_backward_count 4671  23.856%\n",
      "lif layer 2 self.abs_max_v: 5890.0\n",
      "lif layer 1 self.abs_max_v: 6834.0\n",
      "lif layer 1 self.abs_max_v: 6944.0\n",
      "lif layer 1 self.abs_max_v: 7067.5\n",
      "lif layer 1 self.abs_max_v: 7093.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.877658/  2.000844, val:  47.50%, val_best:  47.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 74.0162%\n",
      "layer   3  Sparsity: 70.8767%\n",
      "total_backward_count 29370 real_backward_count 6300  21.450%\n",
      "lif layer 1 self.abs_max_v: 7971.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.863014/  2.005131, val:  50.42%, val_best:  50.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 82.70 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 74.1522%\n",
      "layer   3  Sparsity: 70.4295%\n",
      "total_backward_count 39160 real_backward_count 7832  20.000%\n",
      "lif layer 2 self.abs_max_v: 5891.5\n",
      "lif layer 1 self.abs_max_v: 8570.5\n",
      "lif layer 1 self.abs_max_v: 8648.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.853878/  2.006987, val:  45.42%, val_best:  50.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 85.01 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.8590%\n",
      "layer   3  Sparsity: 69.8976%\n",
      "total_backward_count 48950 real_backward_count 9264  18.925%\n",
      "fc layer 2 self.abs_max_out: 3785.0\n",
      "fc layer 2 self.abs_max_out: 3869.0\n",
      "lif layer 1 self.abs_max_v: 9020.0\n",
      "lif layer 1 self.abs_max_v: 9277.5\n",
      "fc layer 1 self.abs_max_out: 6340.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.835103/  1.993388, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.5998%\n",
      "layer   3  Sparsity: 69.6951%\n",
      "total_backward_count 58740 real_backward_count 10638  18.110%\n",
      "fc layer 2 self.abs_max_out: 3899.0\n",
      "lif layer 1 self.abs_max_v: 9720.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.822710/  1.965655, val:  60.00%, val_best:  60.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.4912%\n",
      "layer   3  Sparsity: 69.5340%\n",
      "total_backward_count 68530 real_backward_count 11947  17.433%\n",
      "fc layer 2 self.abs_max_out: 3982.0\n",
      "fc layer 3 self.abs_max_out: 1366.0\n",
      "fc layer 3 self.abs_max_out: 1411.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.802401/  1.954970, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.4406%\n",
      "layer   3  Sparsity: 69.1945%\n",
      "total_backward_count 78320 real_backward_count 13125  16.758%\n",
      "fc layer 2 self.abs_max_out: 3994.0\n",
      "lif layer 1 self.abs_max_v: 10087.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.810116/  1.932268, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.0399%\n",
      "layer   3  Sparsity: 70.5714%\n",
      "total_backward_count 88110 real_backward_count 14327  16.260%\n",
      "fc layer 2 self.abs_max_out: 4044.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.786052/  1.941678, val:  53.75%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.7295%\n",
      "layer   3  Sparsity: 71.1015%\n",
      "total_backward_count 97900 real_backward_count 15478  15.810%\n",
      "lif layer 1 self.abs_max_v: 10144.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.778879/  1.934394, val:  58.75%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.8768%\n",
      "layer   3  Sparsity: 71.3719%\n",
      "total_backward_count 107690 real_backward_count 16527  15.347%\n",
      "lif layer 1 self.abs_max_v: 10176.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.776489/  1.922501, val:  70.00%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.8578%\n",
      "layer   3  Sparsity: 71.5289%\n",
      "total_backward_count 117480 real_backward_count 17632  15.009%\n",
      "lif layer 2 self.abs_max_v: 6027.5\n",
      "lif layer 1 self.abs_max_v: 10602.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.763660/  1.899662, val:  69.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.8141%\n",
      "layer   3  Sparsity: 71.2935%\n",
      "total_backward_count 127270 real_backward_count 18628  14.637%\n",
      "fc layer 3 self.abs_max_out: 1478.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.758326/  1.907712, val:  58.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 73.1454%\n",
      "layer   3  Sparsity: 72.0687%\n",
      "total_backward_count 137060 real_backward_count 19563  14.273%\n",
      "lif layer 1 self.abs_max_v: 10648.5\n",
      "fc layer 1 self.abs_max_out: 6555.0\n",
      "lif layer 1 self.abs_max_v: 11243.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.762316/  1.881095, val:  69.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.9579%\n",
      "layer   3  Sparsity: 72.2993%\n",
      "total_backward_count 146850 real_backward_count 20403  13.894%\n",
      "lif layer 2 self.abs_max_v: 6033.0\n",
      "fc layer 1 self.abs_max_out: 6605.0\n",
      "lif layer 1 self.abs_max_v: 11540.0\n",
      "fc layer 2 self.abs_max_out: 4156.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.735870/  1.887772, val:  65.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.8233%\n",
      "layer   3  Sparsity: 71.8561%\n",
      "total_backward_count 156640 real_backward_count 21254  13.569%\n",
      "fc layer 1 self.abs_max_out: 6628.0\n",
      "lif layer 1 self.abs_max_v: 11615.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.724039/  1.876335, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.8055%\n",
      "layer   3  Sparsity: 72.5953%\n",
      "total_backward_count 166430 real_backward_count 22040  13.243%\n",
      "lif layer 1 self.abs_max_v: 11855.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.725871/  1.886297, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.5591%\n",
      "layer   3  Sparsity: 72.7037%\n",
      "total_backward_count 176220 real_backward_count 22775  12.924%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.726494/  1.865663, val:  74.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.6720%\n",
      "layer   3  Sparsity: 73.3048%\n",
      "total_backward_count 186010 real_backward_count 23555  12.663%\n",
      "fc layer 1 self.abs_max_out: 6872.0\n",
      "fc layer 1 self.abs_max_out: 6882.0\n",
      "lif layer 1 self.abs_max_v: 12411.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.723881/  1.853112, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.6016%\n",
      "layer   3  Sparsity: 73.7927%\n",
      "total_backward_count 195800 real_backward_count 24265  12.393%\n",
      "fc layer 2 self.abs_max_out: 4199.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.706234/  1.863333, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.5147%\n",
      "layer   3  Sparsity: 73.3361%\n",
      "total_backward_count 205590 real_backward_count 24930  12.126%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.698920/  1.844378, val:  75.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.3871%\n",
      "layer   3  Sparsity: 73.3946%\n",
      "total_backward_count 215380 real_backward_count 25642  11.905%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.698466/  1.838570, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2374%\n",
      "layer   3  Sparsity: 73.6468%\n",
      "total_backward_count 225170 real_backward_count 26299  11.680%\n",
      "fc layer 1 self.abs_max_out: 7126.0\n",
      "lif layer 1 self.abs_max_v: 12911.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.702767/  1.842919, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1766%\n",
      "layer   3  Sparsity: 74.7602%\n",
      "total_backward_count 234960 real_backward_count 26876  11.439%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.694157/  1.827775, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.5390%\n",
      "layer   3  Sparsity: 74.7412%\n",
      "total_backward_count 244750 real_backward_count 27476  11.226%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.677999/  1.816640, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.6694%\n",
      "layer   3  Sparsity: 74.5991%\n",
      "total_backward_count 254540 real_backward_count 28045  11.018%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.674784/  1.819887, val:  77.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.5211%\n",
      "layer   3  Sparsity: 74.6965%\n",
      "total_backward_count 264330 real_backward_count 28605  10.822%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.676018/  1.823947, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2155%\n",
      "layer   3  Sparsity: 75.1645%\n",
      "total_backward_count 274120 real_backward_count 29138  10.630%\n",
      "fc layer 2 self.abs_max_out: 4209.0\n",
      "lif layer 2 self.abs_max_v: 6192.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.664218/  1.821059, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.3073%\n",
      "layer   3  Sparsity: 75.0502%\n",
      "total_backward_count 283910 real_backward_count 29635  10.438%\n",
      "lif layer 2 self.abs_max_v: 6308.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.664522/  1.810211, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2098%\n",
      "layer   3  Sparsity: 75.3957%\n",
      "total_backward_count 293700 real_backward_count 30114  10.253%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.662798/  1.809169, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2363%\n",
      "layer   3  Sparsity: 74.9569%\n",
      "total_backward_count 303490 real_backward_count 30593  10.080%\n",
      "fc layer 2 self.abs_max_out: 4326.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.659901/  1.793554, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2527%\n",
      "layer   3  Sparsity: 74.8844%\n",
      "total_backward_count 313280 real_backward_count 31051   9.912%\n",
      "fc layer 2 self.abs_max_out: 4328.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.652713/  1.804899, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1091%\n",
      "layer   3  Sparsity: 74.7406%\n",
      "total_backward_count 323070 real_backward_count 31473   9.742%\n",
      "lif layer 2 self.abs_max_v: 6437.5\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.654027/  1.808656, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1361%\n",
      "layer   3  Sparsity: 74.8874%\n",
      "total_backward_count 332860 real_backward_count 31920   9.590%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.645641/  1.788792, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1073%\n",
      "layer   3  Sparsity: 75.2766%\n",
      "total_backward_count 342650 real_backward_count 32305   9.428%\n",
      "fc layer 2 self.abs_max_out: 4384.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.639707/  1.795719, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1401%\n",
      "layer   3  Sparsity: 75.8600%\n",
      "total_backward_count 352440 real_backward_count 32718   9.283%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.638157/  1.800498, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.5347%\n",
      "layer   3  Sparsity: 76.3910%\n",
      "total_backward_count 362230 real_backward_count 33108   9.140%\n",
      "lif layer 2 self.abs_max_v: 6524.0\n",
      "fc layer 3 self.abs_max_out: 1483.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.628388/  1.787748, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.30 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.3578%\n",
      "layer   3  Sparsity: 76.1186%\n",
      "total_backward_count 372020 real_backward_count 33469   8.997%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.626917/  1.802263, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2036%\n",
      "layer   3  Sparsity: 75.8898%\n",
      "total_backward_count 381810 real_backward_count 33805   8.854%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.621276/  1.782075, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1483%\n",
      "layer   3  Sparsity: 75.8158%\n",
      "total_backward_count 391600 real_backward_count 34115   8.712%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.615311/  1.770406, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.23 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.3089%\n",
      "layer   3  Sparsity: 75.3590%\n",
      "total_backward_count 401390 real_backward_count 34458   8.585%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.609638/  1.770850, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.83 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.1863%\n",
      "layer   3  Sparsity: 75.8909%\n",
      "total_backward_count 411180 real_backward_count 34737   8.448%\n",
      "fc layer 3 self.abs_max_out: 1527.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.610147/  1.763491, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.73 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.0790%\n",
      "layer   3  Sparsity: 75.7048%\n",
      "total_backward_count 420970 real_backward_count 35050   8.326%\n",
      "lif layer 2 self.abs_max_v: 6630.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.612328/  1.765763, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 85.26 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.3868%\n",
      "layer   3  Sparsity: 75.8998%\n",
      "total_backward_count 430760 real_backward_count 35338   8.204%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.608281/  1.762450, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 83.23 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.3039%\n",
      "layer   3  Sparsity: 75.7792%\n",
      "total_backward_count 440550 real_backward_count 35657   8.094%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.607714/  1.765168, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.2059%\n",
      "layer   3  Sparsity: 75.8295%\n",
      "total_backward_count 450340 real_backward_count 35937   7.980%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.602317/  1.775138, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.0962%\n",
      "layer   3  Sparsity: 76.1505%\n",
      "total_backward_count 460130 real_backward_count 36187   7.865%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.601323/  1.755734, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 72.0023%\n",
      "layer   3  Sparsity: 76.3590%\n",
      "total_backward_count 469920 real_backward_count 36427   7.752%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.595045/  1.779147, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 71.9186%\n",
      "layer   3  Sparsity: 76.3527%\n",
      "total_backward_count 479710 real_backward_count 36701   7.651%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.599528/  1.741469, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 71.9463%\n",
      "layer   3  Sparsity: 77.0989%\n",
      "total_backward_count 489500 real_backward_count 36938   7.546%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.600360/  1.767650, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 71.9096%\n",
      "layer   3  Sparsity: 76.2160%\n",
      "total_backward_count 499290 real_backward_count 37203   7.451%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.599485/  1.761941, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 71.9616%\n",
      "layer   3  Sparsity: 75.5772%\n",
      "total_backward_count 509080 real_backward_count 37431   7.353%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.588893/  1.751958, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.30 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 71.9380%\n",
      "layer   3  Sparsity: 76.1536%\n",
      "total_backward_count 518870 real_backward_count 37645   7.255%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.594886/  1.756614, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.77 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 93.8837%\n",
      "layer   2  Sparsity: 71.9552%\n",
      "layer   3  Sparsity: 76.3355%\n",
      "total_backward_count 528660 real_backward_count 37871   7.164%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [10,15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \n",
    "        \"random_select_ratio\": {\"values\": [1,2,3,4,5]},\n",
    "        \"leaky_temporal_filter\": {\"values\": [0.0, 0.25, 0.5, 0.75, 1.0]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "        random_select_ratio  =  wandb.config.random_select_ratio,\n",
    "        leaky_temporal_filter  =  wandb.config.leaky_temporal_filter,\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'hcapkd0n'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
