{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: q450j7i1\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b2en66w9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 9.279406753686184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_220744-b2en66w9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b2en66w9' target=\"_blank\">cool-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b2en66w9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b2en66w9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 27.19%\n",
      "Test loss: 1.895, Val Accuracy: 33.93%\n",
      "Epoch 2\n",
      "Train Accuracy: 31.99%\n",
      "Test loss: 1.862, Val Accuracy: 35.46%\n",
      "Epoch 3\n",
      "Train Accuracy: 32.31%\n",
      "Test loss: 1.866, Val Accuracy: 34.92%\n",
      "Epoch 4\n",
      "Train Accuracy: 32.73%\n",
      "Test loss: 1.848, Val Accuracy: 35.84%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14875f73c16b4df58ef5ac71c41e6af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='21.686 MB of 21.686 MB uploaded (21.184 MB deduped)\\r'), FloatProgress(value=1.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 97.5%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>iter_accuracy</td><td>▁▄▅▅▆▆▇▆▆▆▇▆▆▇▆▇▆▇▇▇▆▇▇▇█▆▆█▆▇▆▆▇▆▇▆██▇▇</td></tr><tr><td>tr_accuracy</td><td>▁▇▇█</td></tr><tr><td>val_accuracy</td><td>▁▇▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>iter_accuracy</td><td>34.52381</td></tr><tr><td>tr_accuracy</td><td>32.732</td></tr><tr><td>val_accuracy</td><td>35.84</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b2en66w9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/b2en66w9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 36 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_220744-b2en66w9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bgi8b0ac with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 1.297760363405997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_221001-bgi8b0ac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bgi8b0ac' target=\"_blank\">smart-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bgi8b0ac' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bgi8b0ac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (reservoir): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)\n",
      "  )\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 19.71%\n",
      "Test loss: 2.066, Val Accuracy: 25.35%\n",
      "Epoch 2\n",
      "Train Accuracy: 24.48%\n",
      "Test loss: 2.026, Val Accuracy: 27.93%\n",
      "Epoch 3\n",
      "Train Accuracy: 26.07%\n",
      "Test loss: 2.010, Val Accuracy: 29.08%\n",
      "Epoch 4\n",
      "Train Accuracy: 26.68%\n",
      "Test loss: 1.998, Val Accuracy: 29.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ea354c32a44537ab63a404d36c1d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.430 MB of 0.430 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>iter_accuracy</td><td>▁▃▂▄▄▄▄▄▆▅▇▆▆▆▇▆▆▇▅▇▆▆▆▆▆▆▆▆███▇▇▇▆▆█▇▇▆</td></tr><tr><td>tr_accuracy</td><td>▁▆▇█</td></tr><tr><td>val_accuracy</td><td>▁▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>iter_accuracy</td><td>25.29762</td></tr><tr><td>tr_accuracy</td><td>26.676</td></tr><tr><td>val_accuracy</td><td>29.65</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bgi8b0ac' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/bgi8b0ac</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_221001-bgi8b0ac/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s91ltvav with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 8.026153579512822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_221125-s91ltvav</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/s91ltvav' target=\"_blank\">usual-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/s91ltvav' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/s91ltvav</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 27.48%\n",
      "Test loss: 1.902, Val Accuracy: 33.78%\n",
      "Epoch 2\n",
      "Train Accuracy: 31.72%\n",
      "Test loss: 1.882, Val Accuracy: 34.76%\n",
      "Epoch 3\n",
      "Train Accuracy: 32.71%\n",
      "Test loss: 1.863, Val Accuracy: 35.25%\n",
      "Epoch 4\n",
      "Train Accuracy: 33.01%\n",
      "Test loss: 1.862, Val Accuracy: 35.16%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2d0e2af6f94cb6a008d42d6479165e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='21.825 MB of 21.825 MB uploaded (21.119 MB deduped)\\r'), FloatProgress(value=1.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 96.5%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>iter_accuracy</td><td>▁▂▅▅▅▄▅▇▅▆▆▄▇▅▆▆▆▆▆▆▅▆▄▇▅▇▇▆█▆▅█▅▆▆▆▆▇▆▅</td></tr><tr><td>tr_accuracy</td><td>▁▆██</td></tr><tr><td>val_accuracy</td><td>▁▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>iter_accuracy</td><td>30.05952</td></tr><tr><td>tr_accuracy</td><td>33.006</td></tr><tr><td>val_accuracy</td><td>35.16</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/s91ltvav' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/s91ltvav</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 36 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_221125-s91ltvav/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 98lbzdn9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 8.956314081032135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_221228-98lbzdn9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/98lbzdn9' target=\"_blank\">magic-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/q450j7i1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/98lbzdn9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/98lbzdn9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (reservoir): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)\n",
      "  )\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '4', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'CIFAR10' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} ba_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"values\": [0.001]},\n",
    "        \"batch_size\": {\"values\": [512]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"values\": [0.7]},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [4]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [32]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
