{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: clvrkh5o\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/clvrkh5o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ta0b2t7n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.3465086887168845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.025559495967044304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 2.685535262414482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_191105-ta0b2t7n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/ta0b2t7n' target=\"_blank\">misty-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/clvrkh5o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/clvrkh5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/clvrkh5o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/clvrkh5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/ta0b2t7n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/ta0b2t7n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 24.86%\n",
      "Test loss: 6.453, Val Accuracy: 26.73%\n",
      "Epoch 2\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '4', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'CIFAR10' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} ba_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [32]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
