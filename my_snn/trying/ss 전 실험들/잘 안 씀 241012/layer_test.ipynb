{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom FC test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom FC test\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class MyLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = F.linear(input, weight, bias)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # print('grad_output 크기', grad_output.shape, '\\n')\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output @ weight\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t() @ input\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        # print('input 크기', input.shape)\n",
    "        # print('weight 크기', weight.shape)\n",
    "        # print('bias 크기', bias.shape)\n",
    "\n",
    "        # print ('grad_input 크기', grad_input.shape)\n",
    "        # print ('grad_weight 크기', grad_weight.shape)\n",
    "        # print ('grad_bias 크기', grad_bias.shape)\n",
    "\n",
    "        # print(grad_weight.sum(axis=0))\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "# Usage\n",
    "BATCH = 2\n",
    "IN_FEATURE = 3\n",
    "OUT_FEATURE = 5   \n",
    "\n",
    "# data\n",
    "input = torch.randn(BATCH, IN_FEATURE, requires_grad=True)\n",
    "input2 = input.clone().detach().requires_grad_(True)\n",
    "weight = torch.randn(OUT_FEATURE, IN_FEATURE, requires_grad=True)\n",
    "weight2 = weight.clone().detach().requires_grad_(True)\n",
    "bias = torch.randn(OUT_FEATURE, requires_grad=True)\n",
    "bias2 = bias.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(\"\\n\\nnn.linear\")\n",
    "print(\"nn.linear\")\n",
    "print(\"nn.linear\")\n",
    "linear = nn.Linear(IN_FEATURE, OUT_FEATURE)\n",
    "linear.weight = nn.Parameter(weight2) \n",
    "linear.bias = nn.Parameter(bias2)\n",
    "\n",
    "# Forward pass\n",
    "output_linear = linear(input2)\n",
    "\n",
    "# Backward pass\n",
    "output_linear.sum().backward()\n",
    "print('input2.grad.shape',input2.grad.shape)\n",
    "# print(input2.grad)\n",
    "print('linear.weight.grad.shape',linear.weight.grad.shape)\n",
    "# print(linear.weight.grad)\n",
    "print('linear.bias.shape',linear.bias.shape)\n",
    "# print(linear.bias.grad)\n",
    "\n",
    "my_linear = MyLinear.apply\n",
    "\n",
    "\n",
    "print(\"\\n\\nMylinear\")\n",
    "print(\"Mylinear\")\n",
    "print(\"Mylinear\")\n",
    "output = my_linear(input, weight, bias)\n",
    "output.sum().backward()\n",
    "print('input.grad.shape',input.grad.shape)\n",
    "# print(input.grad)\n",
    "print('weight.grad.shape',weight.grad.shape)\n",
    "# print(weight.grad)\n",
    "print('bias.shape',bias.shape)\n",
    "# print(bias.grad)\n",
    "\n",
    "\n",
    "print(\"\\nsame?\")\n",
    "\n",
    "print('input_grad',torch.allclose(input.grad, input2.grad))\n",
    "print('weight_grad', torch.allclose(weight.grad, linear.weight.grad)) \n",
    "print('linear_grad', torch.allclose(bias.grad, linear.bias.grad)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom Conv test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom Conv test\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class MyConv2d(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None, stride=1, padding=1, dilation=1, groups=1):\n",
    "        # Save input and weight for backward pass\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        ctx.dilation = dilation\n",
    "        ctx.groups = groups\n",
    "\n",
    "        # Perform forward pass\n",
    "        output = F.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve tensors from the forward pass\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "        dilation = ctx.dilation\n",
    "        groups = ctx.groups\n",
    "\n",
    "        # Compute gradients w.r.t input and weight\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = F.conv_transpose2d(grad_output, weight, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(input, weight.shape, grad_output,\n",
    "                                                      stride=stride, padding=padding,\n",
    "                                                      dilation=dilation, groups=groups)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            # grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            # grad_bias = grad_output.sum(0).sum(-1).sum(-1)\n",
    "            grad_bias = grad_output.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None\n",
    "    \n",
    "    \n",
    "# Usage\n",
    "BATCH = 2\n",
    "CHANNEL = 2\n",
    "KERNEL = 3\n",
    "INPUT_H_W = 5\n",
    "WEIGHT_R_C = 3\n",
    "PADDING = 1\n",
    "\n",
    "# data\n",
    "input = torch.randn(BATCH, CHANNEL, INPUT_H_W, INPUT_H_W, requires_grad=True)\n",
    "input2 = input.clone().detach().requires_grad_(True)\n",
    "weight = torch.randn(KERNEL, CHANNEL, WEIGHT_R_C, WEIGHT_R_C, requires_grad=True)\n",
    "weight2 = weight.clone().detach().requires_grad_(True)\n",
    "bias = torch.randn(KERNEL, requires_grad=True)\n",
    "bias2 = bias.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "print(\"\\n\\nnn.Conv2d\")\n",
    "print(\"nn.Conv2d\")\n",
    "print(\"nn.Conv2d\")\n",
    "conv = nn.Conv2d(CHANNEL, KERNEL, WEIGHT_R_C, 1, PADDING, 1, 1)\n",
    "conv.weight = nn.Parameter(weight2)\n",
    "conv.bias = nn.Parameter(bias2)\n",
    "\n",
    "# Forward pass\n",
    "output_conv = conv(input2)\n",
    "\n",
    "# Backward pass\n",
    "output_conv.sum().backward()\n",
    "\n",
    "print('input2.grad.shape', input2.grad.shape)\n",
    "# print('input2.grad', input2.grad)\n",
    "print('conv.weight.grad.shape', conv.weight.grad.shape)\n",
    "# print('conv.weight.grad',conv.weight.grad)\n",
    "print('conv.bias.grad.shape', conv.bias.grad.shape)\n",
    "# print('conv.bias.grad',conv.bias.grad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nMyConv2d\")\n",
    "print(\"MyConv2d\")\n",
    "print(\"MyConv2d\")\n",
    "my_conv = MyConv2d.apply\n",
    "\n",
    "output = my_conv(input, weight, bias,1,PADDING,1,1)\n",
    "output.sum().backward()\n",
    "print('input.grad.shape', input.grad.shape)\n",
    "# print('input.grad', input.grad)\n",
    "print('weight.grad.shape', weight.grad.shape)\n",
    "# print('weight.grad', weight.grad)\n",
    "print('bias.grad.shape',bias.grad.shape)\n",
    "# print('bias.grad',bias.grad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nsame?\\n\")\n",
    "print('input_grad',torch.allclose(input.grad, input2.grad))\n",
    "print('weight_grad',torch.allclose(weight.grad, conv.weight.grad))\n",
    "print('linear_grad', torch.allclose(bias.grad, conv.bias.grad))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
