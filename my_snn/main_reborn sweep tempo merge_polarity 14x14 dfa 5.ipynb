{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30979/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"0\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 16, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.75,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = True,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yryz40rf\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fasnh6ig with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_155726-fasnh6ig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig' target=\"_blank\">restful-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 3, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 45e076f350e4b27d8ff87367a19d69df\n",
      "cache path doesn't exist\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c61eaeb3b64103b26574a209daeda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iter_acc</td><td>▅▄▄▃▃▃▅▂▂▅▂▃▂▂▂▃▁▃▂▃▄▆▂▃▇▅▄▁▅▂▆▄▄▆▄▆▃▆█▄</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_epoch_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>iter_acc</td><td>0.1875</td></tr><tr><td>tr_acc</td><td>0</td></tr><tr><td>tr_epoch_loss</td><td>0</td></tr><tr><td>val_acc_best</td><td>0</td></tr><tr><td>val_acc_now</td><td>0</td></tr><tr><td>val_loss</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">restful-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_155726-fasnh6ig/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run fasnh6ig errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_30979/263192303.py\", line 94, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_30979/2774805580.py\", line 276, in my_snn_system\n",
      "    for i, data in iterator:\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1183, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1229, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/_utils.py\", line 434, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 138, in __getitem__\n",
      "    data, targets = load_from_disk_cache(file_path)\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 212, in load_from_disk_cache\n",
      "    with h5py.File(file_path, \"r\") as f:\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '/data2/DVSGesture/cache/fast_dataloading_45e076f350e4b27d8ff87367a19d69df/train/40_0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 308, in __getitem__\n",
      "    return self.datasets[dataset_idx][sample_idx]\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 151, in __getitem__\n",
      "    data, targets = load_from_disk_cache(file_path)\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 214, in load_from_disk_cache\n",
      "    for index in f[name].keys():\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/_collections_abc.py\", line 720, in __iter__\n",
      "    yield from self._mapping\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/group.py\", line 502, in __iter__\n",
      "    for x in self.id.__iter__():\n",
      "  File \"h5py/h5g.pyx\", line 128, in h5py.h5g.GroupIter.__next__\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5l.pyx\", line 316, in h5py.h5l.LinkProxy.iterate\n",
      "RuntimeError: Link iteration failed (bad local heap signature)\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fasnh6ig errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_30979/263192303.py\", line 94, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_30979/2774805580.py\", line 276, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     for i, data in iterator:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = self._next_data()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1183, in _next_data\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._process_data(data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1229, in _process_data\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data.reraise()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/_utils.py\", line 434, in reraise\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise exception\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Caught RuntimeError in DataLoader worker process 1.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Original Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 138, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data, targets = load_from_disk_cache(file_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 212, in load_from_disk_cache\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     with h5py.File(file_path, \"r\") as f:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     fid = h5f.open(name, flags, fapl=fapl)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '/data2/DVSGesture/cache/fast_dataloading_45e076f350e4b27d8ff87367a19d69df/train/40_0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = fetcher.fetch(index)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 308, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.datasets[dataset_idx][sample_idx]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 151, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data, targets = load_from_disk_cache(file_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 214, in load_from_disk_cache\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     for index in f[name].keys():\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/_collections_abc.py\", line 720, in __iter__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     yield from self._mapping\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/group.py\", line 502, in __iter__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     for x in self.id.__iter__():\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/h5g.pyx\", line 128, in h5py.h5g.GroupIter.__next__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/h5l.pyx\", line 316, in h5py.h5l.LinkProxy.iterate\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Link iteration failed (bad local heap signature)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9o18owo8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_155945-9o18owo8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8' target=\"_blank\">vocal-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  3.375626/  4.924582, val:  43.33%, val_best:  43.33%, tr:  32.18%, tr_best:  32.18%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  4.411354/  5.846550, val:  35.42%, val_best:  43.33%, tr:  45.56%, tr_best:  45.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  4.374959/  6.216932, val:  35.00%, val_best:  43.33%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  3.781863/  6.351960, val:  41.67%, val_best:  43.33%, tr:  55.16%, tr_best:  55.16%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  4.105217/  5.605066, val:  56.25%, val_best:  56.25%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.718383/  3.675399, val:  51.67%, val_best:  56.25%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  3.386384/  5.642118, val:  49.17%, val_best:  56.25%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  3.616551/  5.396501, val:  50.00%, val_best:  56.25%, tr:  58.63%, tr_best:  62.00%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.751287/  4.964119, val:  53.33%, val_best:  56.25%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  4.728420/  4.619531, val:  53.33%, val_best:  56.25%, tr:  60.98%, tr_best:  62.82%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  3.861833/  4.854526, val:  58.75%, val_best:  58.75%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.841662/  4.876754, val:  47.92%, val_best:  58.75%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.943535/  3.461709, val:  61.25%, val_best:  61.25%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.981824/  5.390105, val:  50.83%, val_best:  61.25%, tr:  67.21%, tr_best:  70.28%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.982751/  4.572068, val:  61.67%, val_best:  61.67%, tr:  70.07%, tr_best:  70.28%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.994673/  4.406756, val:  58.33%, val_best:  61.67%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.836171/  4.360404, val:  62.50%, val_best:  62.50%, tr:  71.60%, tr_best:  73.44%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.816617/  5.424423, val:  61.25%, val_best:  62.50%, tr:  73.24%, tr_best:  73.44%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  2.485013/  3.814268, val:  65.83%, val_best:  65.83%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.669511/  3.262528, val:  68.75%, val_best:  68.75%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.870311/  3.994393, val:  67.08%, val_best:  68.75%, tr:  80.18%, tr_best:  81.72%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.892323/  6.426356, val:  72.08%, val_best:  72.08%, tr:  75.38%, tr_best:  81.72%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.916661/  3.352026, val:  71.67%, val_best:  72.08%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.636025/  5.987766, val:  62.08%, val_best:  72.08%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.418722/  4.139493, val:  65.42%, val_best:  72.08%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.466357/  4.220771, val:  67.92%, val_best:  72.08%, tr:  86.01%, tr_best:  88.25%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.810418/  5.617115, val:  63.33%, val_best:  72.08%, tr:  85.90%, tr_best:  88.25%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.458785/  4.161286, val:  76.25%, val_best:  76.25%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.393750/  4.261668, val:  73.75%, val_best:  76.25%, tr:  89.17%, tr_best:  89.48%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.143280/  4.867313, val:  71.25%, val_best:  76.25%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.199079/  4.413030, val:  71.67%, val_best:  76.25%, tr:  92.75%, tr_best:  93.56%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.882417/  4.749913, val:  71.67%, val_best:  76.25%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.081329/  4.926338, val:  67.92%, val_best:  76.25%, tr:  93.46%, tr_best:  95.30%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.197487/  4.877429, val:  72.50%, val_best:  76.25%, tr:  93.56%, tr_best:  95.30%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.984108/  5.131997, val:  72.08%, val_best:  76.25%, tr:  94.89%, tr_best:  95.30%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.750658/  4.895487, val:  70.83%, val_best:  76.25%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.733275/  4.703146, val:  74.17%, val_best:  76.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.834344/  5.506299, val:  70.83%, val_best:  76.25%, tr:  95.91%, tr_best:  98.06%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  1.077495/  5.172935, val:  72.92%, val_best:  76.25%, tr:  94.38%, tr_best:  98.06%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.830219/  4.922905, val:  75.42%, val_best:  76.25%, tr:  97.04%, tr_best:  98.06%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.706840/  4.866054, val:  75.00%, val_best:  76.25%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.592747/  5.185280, val:  74.17%, val_best:  76.25%, tr:  97.96%, tr_best:  98.06%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.607890/  5.022070, val:  78.33%, val_best:  78.33%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.498638/  5.774553, val:  74.17%, val_best:  78.33%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.551077/  6.247904, val:  73.75%, val_best:  78.33%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.480349/  5.110059, val:  75.00%, val_best:  78.33%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.403839/  5.547753, val:  76.25%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.417751/  5.763806, val:  75.00%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.348740/  5.703766, val:  74.17%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.406445/  5.658116, val:  75.42%, val_best:  78.33%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.346797/  5.996122, val:  74.17%, val_best:  78.33%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.338436/  5.991768, val:  77.08%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.352796/  5.773657, val:  76.67%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.251303/  5.833306, val:  75.42%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.305196/  5.744478, val:  79.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.269031/  5.948262, val:  77.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.260727/  6.389153, val:  76.67%, val_best:  79.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.287932/  6.065574, val:  79.17%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.206378/  6.340549, val:  74.58%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.199226/  6.156328, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.165120/  6.334200, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.218347/  6.301515, val:  78.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.229086/  6.548867, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.206777/  6.424737, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.165844/  6.553136, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.148494/  6.218416, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.134092/  6.576536, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.122098/  6.545312, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.149716/  6.500177, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.132718/  6.908567, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.122940/  6.756269, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.143760/  6.667559, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.111834/  6.922075, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.099359/  7.153737, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.095698/  6.775165, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.102958/  6.886825, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.147348/  7.126762, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.098712/  7.233177, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.105645/  7.316442, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.072043/  6.959475, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.067294/  7.111836, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.078573/  7.312032, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.068949/  7.075481, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.067747/  7.240650, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.072425/  7.365749, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.115504/  7.512792, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.068340/  7.061684, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.049083/  7.394644, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.096339/  7.196530, val:  77.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.070422/  7.142729, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.046580/  7.190536, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.057905/  7.529678, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.054658/  7.720180, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.053474/  7.473547, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.067817/  7.334467, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.040718/  7.485956, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.052300/  7.549210, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.056550/  7.520621, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033973/  7.603555, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.041235/  7.617035, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7060ec45f746f7999f66d99d391613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▃▃▄▆▄▄▇▇▇██▇▇███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▄▃▄▅▅▅▆▇▆▅▇▆▇▇▇█▇▇▇▇██████▇█████████▇█</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▅▅▅▆▅▇▇▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆▇▇▆█▅▅▅▃▅▃▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▃▄▅▅▆▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▄▃▄▅▅▅▆▇▆▅▇▆▇▇▇█▇▇▇▇██████▇█████████▇█</td></tr><tr><td>val_loss</td><td>▄▆▅▄▃▁▃▄▁▆▂▅▄▄▄▅▄▄▆▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04123</td></tr><tr><td>val_acc_best</td><td>0.8</td></tr><tr><td>val_acc_now</td><td>0.79167</td></tr><tr><td>val_loss</td><td>7.61703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_155945-9o18owo8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: if5mf2g5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_160618-if5mf2g5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5' target=\"_blank\">icy-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.337012/  3.260089, val:  42.50%, val_best:  42.50%, tr:  33.40%, tr_best:  33.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.454684/  2.734299, val:  46.67%, val_best:  46.67%, tr:  52.20%, tr_best:  52.20%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.322649/  3.660435, val:  55.42%, val_best:  55.42%, tr:  56.59%, tr_best:  56.59%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.446365/  3.104996, val:  51.67%, val_best:  55.42%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.984612/  1.943396, val:  61.67%, val_best:  61.67%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.940178/  2.476325, val:  54.58%, val_best:  61.67%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.635616/  1.767809, val:  61.25%, val_best:  61.67%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.487669/  2.575252, val:  52.92%, val_best:  61.67%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.334403/  1.688805, val:  65.83%, val_best:  65.83%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.583318/  1.808418, val:  66.67%, val_best:  66.67%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.377620/  1.468558, val:  66.67%, val_best:  66.67%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.968081/  2.111320, val:  60.42%, val_best:  66.67%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.332990/  1.439684, val:  77.50%, val_best:  77.50%, tr:  76.00%, tr_best:  79.47%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.999688/  1.824517, val:  71.25%, val_best:  77.50%, tr:  79.06%, tr_best:  79.47%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.101177/  2.228175, val:  68.75%, val_best:  77.50%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.594902/  1.615644, val:  76.67%, val_best:  77.50%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.496880/  1.489792, val:  80.00%, val_best:  80.00%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.616244/  1.901250, val:  70.00%, val_best:  80.00%, tr:  90.30%, tr_best:  93.36%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.556900/  1.899036, val:  73.75%, val_best:  80.00%, tr:  91.73%, tr_best:  93.36%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.550071/  1.503246, val:  79.17%, val_best:  80.00%, tr:  91.42%, tr_best:  93.36%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.350235/  1.745610, val:  75.42%, val_best:  80.00%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.415559/  1.839736, val:  72.08%, val_best:  80.00%, tr:  94.18%, tr_best:  96.02%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.343377/  1.825100, val:  75.83%, val_best:  80.00%, tr:  95.30%, tr_best:  96.02%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.267262/  1.783595, val:  81.67%, val_best:  81.67%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.206805/  1.637124, val:  81.67%, val_best:  81.67%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.153955/  1.575179, val:  83.33%, val_best:  83.33%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.133744/  1.703222, val:  82.92%, val_best:  83.33%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.126711/  1.629458, val:  84.17%, val_best:  84.17%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.100742/  1.628612, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.100535/  1.964477, val:  77.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.130499/  1.728528, val:  80.83%, val_best:  84.17%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.072704/  1.690548, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.067556/  1.654439, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.056996/  1.793394, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.057550/  1.753700, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.045737/  1.757158, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.030058/  1.758637, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.026752/  1.788976, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.026644/  1.790571, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.029296/  1.881343, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.023284/  1.834787, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.023142/  1.801984, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.018058/  1.849258, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.016033/  1.905129, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.022627/  1.855869, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.016819/  1.839126, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.017481/  1.910885, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.011696/  1.898611, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.023355/  1.886415, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.019010/  1.941561, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.012663/  1.927688, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.010791/  1.895817, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.007536/  1.908752, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.007220/  1.931646, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.006954/  1.951990, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.005509/  1.970693, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.006775/  2.008574, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.005568/  1.974713, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.005249/  1.990543, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.005621/  1.995314, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.006728/  2.008166, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.006238/  2.005769, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.005487/  2.051635, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.003979/  2.053753, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.003930/  2.054438, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.004248/  2.063787, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.008216/  2.071535, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.004468/  2.056513, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.004708/  2.063874, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.003543/  2.067139, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.003737/  2.088584, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.003297/  2.087028, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.003118/  2.077927, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.003155/  2.079436, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002560/  2.089953, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002990/  2.099266, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002696/  2.113652, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002209/  2.111343, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002074/  2.115354, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.002450/  2.130357, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.002405/  2.126160, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002405/  2.131227, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.002318/  2.125073, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.002282/  2.114488, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001954/  2.105329, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.002109/  2.113721, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002065/  2.123964, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001896/  2.120854, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.002316/  2.134632, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002162/  2.128128, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002581/  2.116694, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.002279/  2.143983, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.002293/  2.172338, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.002123/  2.138163, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.002580/  2.178327, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.002055/  2.133386, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001821/  2.163731, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001780/  2.134904, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.002366/  2.143330, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.002570/  2.150552, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bb1c56545645f2b590355ae1ef4215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▅▂▅▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▅▆▅▅▇▆▇▇▇▇▇█▇█▇█▇███████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▅▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▅▆▅▅▇▆▇▇▇▇▇█▇█▇█▇███████████████████</td></tr><tr><td>val_loss</td><td>▇█▃▅▂▁▃▂▁▂▂▂▃▂▂▂▂▂▂▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00257</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.15055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_160618-if5mf2g5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8vxt7zx6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_161147-8vxt7zx6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6' target=\"_blank\">dandy-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.177759/  1.767869, val:  41.67%, val_best:  41.67%, tr:  17.16%, tr_best:  17.16%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.579576/  1.792288, val:  50.42%, val_best:  50.42%, tr:  48.11%, tr_best:  48.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.616680/  1.629443, val:  57.08%, val_best:  57.08%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.419275/  1.941939, val:  51.67%, val_best:  57.08%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.380651/  1.543922, val:  63.75%, val_best:  63.75%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.277786/  1.511709, val:  56.25%, val_best:  63.75%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.321165/  1.669516, val:  60.83%, val_best:  63.75%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.358928/  2.006596, val:  51.67%, val_best:  63.75%, tr:  63.43%, tr_best:  67.31%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.201749/  1.467234, val:  60.00%, val_best:  63.75%, tr:  68.54%, tr_best:  68.54%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.502143/  1.740815, val:  55.42%, val_best:  63.75%, tr:  64.04%, tr_best:  68.54%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.311748/  1.231247, val:  70.00%, val_best:  70.00%, tr:  67.62%, tr_best:  68.54%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.137177/  1.785117, val:  59.17%, val_best:  70.00%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.156562/  1.328666, val:  71.25%, val_best:  71.25%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.009147/  1.670813, val:  60.83%, val_best:  71.25%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.045778/  2.023691, val:  61.67%, val_best:  71.25%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.858041/  1.563960, val:  62.08%, val_best:  71.25%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.887830/  1.468858, val:  74.17%, val_best:  74.17%, tr:  79.98%, tr_best:  82.64%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.900720/  1.557298, val:  64.58%, val_best:  74.17%, tr:  81.92%, tr_best:  82.64%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.758488/  1.677016, val:  64.58%, val_best:  74.17%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.802542/  1.511950, val:  67.92%, val_best:  74.17%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.856475/  1.820350, val:  62.92%, val_best:  74.17%, tr:  84.07%, tr_best:  85.60%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.806115/  1.659008, val:  70.83%, val_best:  74.17%, tr:  84.78%, tr_best:  85.60%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.597384/  1.400848, val:  77.92%, val_best:  77.92%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.625445/  1.780209, val:  65.83%, val_best:  77.92%, tr:  91.52%, tr_best:  92.75%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.567910/  1.592333, val:  75.83%, val_best:  77.92%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.515739/  1.388809, val:  78.33%, val_best:  78.33%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.557114/  1.886964, val:  70.42%, val_best:  78.33%, tr:  92.03%, tr_best:  94.08%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.515287/  1.598993, val:  72.92%, val_best:  78.33%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.612111/  1.623978, val:  78.33%, val_best:  78.33%, tr:  90.81%, tr_best:  94.48%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.455126/  1.576767, val:  77.50%, val_best:  78.33%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.468030/  1.678486, val:  79.17%, val_best:  79.17%, tr:  95.61%, tr_best:  97.85%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.411578/  1.687217, val:  78.75%, val_best:  79.17%, tr:  97.45%, tr_best:  97.85%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.539425/  1.702679, val:  77.50%, val_best:  79.17%, tr:  93.05%, tr_best:  97.85%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.424015/  1.868854, val:  79.17%, val_best:  79.17%, tr:  96.22%, tr_best:  97.85%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.439049/  1.844776, val:  78.33%, val_best:  79.17%, tr:  96.22%, tr_best:  97.85%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.356362/  1.616032, val:  81.25%, val_best:  81.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.312396/  1.551294, val:  84.58%, val_best:  84.58%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.349992/  1.680122, val:  83.33%, val_best:  84.58%, tr:  98.16%, tr_best:  98.88%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.357983/  1.846990, val:  72.50%, val_best:  84.58%, tr:  97.85%, tr_best:  98.88%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.372230/  1.684441, val:  82.50%, val_best:  84.58%, tr:  97.14%, tr_best:  98.88%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.276763/  1.823798, val:  75.83%, val_best:  84.58%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.302270/  1.725174, val:  82.08%, val_best:  84.58%, tr:  97.96%, tr_best:  98.98%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.279079/  1.732792, val:  83.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.265188/  1.850989, val:  79.58%, val_best:  84.58%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.256333/  1.769657, val:  79.58%, val_best:  84.58%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.230452/  1.736552, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.237203/  1.961126, val:  82.50%, val_best:  86.67%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.217414/  1.800170, val:  85.00%, val_best:  86.67%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.244355/  1.794164, val:  84.58%, val_best:  86.67%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.206079/  1.818823, val:  82.92%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.180891/  1.840598, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.183160/  1.823378, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.167777/  1.940293, val:  84.17%, val_best:  86.67%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.149384/  1.932339, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.148469/  1.902918, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.147529/  1.933115, val:  85.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.152409/  2.050434, val:  82.92%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.160633/  1.945492, val:  84.17%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.139429/  1.955546, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.139944/  1.968697, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.128921/  2.047884, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.121671/  2.086908, val:  84.58%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.112908/  2.082826, val:  85.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.113260/  2.057825, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.108503/  2.086049, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.101700/  2.132696, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.091546/  2.112172, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.097310/  2.106270, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.100106/  2.166396, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.095699/  2.202477, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.087246/  2.109648, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.091016/  2.206437, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.084482/  2.195636, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.082248/  2.269924, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.080090/  2.212375, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.077953/  2.214989, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.069510/  2.308018, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.068327/  2.220813, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.068934/  2.358262, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.068913/  2.246019, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.067295/  2.307035, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.071219/  2.294563, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.065294/  2.318426, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.070130/  2.367636, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.062798/  2.369966, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.064366/  2.427171, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.068922/  2.404682, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.074674/  2.361145, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.066103/  2.431033, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.057996/  2.417319, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.055227/  2.464743, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.056826/  2.380807, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.048108/  2.492890, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.060290/  2.486069, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.053566/  2.508778, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.046905/  2.515688, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.048450/  2.548788, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.048925/  2.560366, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.050309/  2.593634, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.048594/  2.518627, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c14f133b04c9b9a61c6ebd4c61a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▂▅▇▇▇█▇█▇█▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▂▃▅▄▄▅▅▆▅▆▆▇▇▇▇▇▇▇▇█▇█▇█▇▇█████████▇▇</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▅▆▆▆▇▇▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▆▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▂▃▅▄▄▅▅▆▅▆▆▇▇▇▇▇▇▇▇█▇█▇█▇▇█████████▇▇</td></tr><tr><td>val_loss</td><td>▃▃▂▅▃▁▅▂▂▃▂▄▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04859</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>2.51863</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_161147-8vxt7zx6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aud4paar with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_161715-aud4paar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar' target=\"_blank\">treasured-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.364254/  3.444064, val:  38.75%, val_best:  38.75%, tr:  29.93%, tr_best:  29.93%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.800476/  2.800792, val:  52.50%, val_best:  52.50%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.511064/  4.942358, val:  44.58%, val_best:  52.50%, tr:  51.99%, tr_best:  51.99%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.756983/  4.411570, val:  50.00%, val_best:  52.50%, tr:  58.53%, tr_best:  58.53%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.786238/  2.951656, val:  55.42%, val_best:  55.42%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.716480/  3.593223, val:  50.42%, val_best:  55.42%, tr:  63.53%, tr_best:  63.53%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.915297/  3.561498, val:  59.58%, val_best:  59.58%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.970849/  3.506444, val:  54.17%, val_best:  59.58%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.279908/  3.326262, val:  57.08%, val_best:  59.58%, tr:  66.39%, tr_best:  68.23%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  3.127134/  3.536241, val:  52.92%, val_best:  59.58%, tr:  63.43%, tr_best:  68.23%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.187211/  3.341280, val:  59.58%, val_best:  59.58%, tr:  67.52%, tr_best:  68.23%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.246208/  3.157217, val:  62.08%, val_best:  62.08%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.994955/  2.990803, val:  61.67%, val_best:  62.08%, tr:  71.20%, tr_best:  71.20%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.896069/  3.906312, val:  55.42%, val_best:  62.08%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.860709/  4.161561, val:  56.25%, val_best:  62.08%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.075395/  4.681872, val:  55.42%, val_best:  62.08%, tr:  73.75%, tr_best:  75.79%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.138122/  3.459884, val:  65.83%, val_best:  65.83%, tr:  75.18%, tr_best:  75.79%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.879036/  4.637856, val:  59.58%, val_best:  65.83%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.842557/  3.405706, val:  68.75%, val_best:  68.75%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.455800/  3.158424, val:  68.75%, val_best:  68.75%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.539481/  4.381956, val:  62.08%, val_best:  68.75%, tr:  83.25%, tr_best:  83.55%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.166104/  5.062113, val:  58.75%, val_best:  68.75%, tr:  80.18%, tr_best:  83.55%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.473376/  3.780391, val:  76.25%, val_best:  76.25%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.434365/  4.222356, val:  70.00%, val_best:  76.25%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.229558/  3.878329, val:  77.50%, val_best:  77.50%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.132174/  3.745704, val:  70.83%, val_best:  77.50%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.279749/  4.601786, val:  65.42%, val_best:  77.50%, tr:  89.58%, tr_best:  92.03%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.137110/  3.743865, val:  76.67%, val_best:  77.50%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.266669/  4.538523, val:  67.08%, val_best:  77.50%, tr:  90.40%, tr_best:  94.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.233508/  4.785876, val:  64.17%, val_best:  77.50%, tr:  93.56%, tr_best:  94.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.109261/  4.348667, val:  75.42%, val_best:  77.50%, tr:  94.08%, tr_best:  94.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.975196/  4.362831, val:  76.67%, val_best:  77.50%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.410273/  5.588483, val:  65.83%, val_best:  77.50%, tr:  91.11%, tr_best:  96.53%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.006734/  5.592145, val:  69.58%, val_best:  77.50%, tr:  95.61%, tr_best:  96.53%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.982735/  5.199594, val:  72.08%, val_best:  77.50%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.935123/  4.861407, val:  77.08%, val_best:  77.50%, tr:  96.94%, tr_best:  97.04%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.846158/  4.992792, val:  75.42%, val_best:  77.50%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.892742/  5.650120, val:  69.17%, val_best:  77.50%, tr:  97.45%, tr_best:  98.16%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.916555/  5.139957, val:  77.92%, val_best:  77.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.877786/  5.477777, val:  79.17%, val_best:  79.17%, tr:  96.42%, tr_best:  98.16%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.710892/  5.012998, val:  79.58%, val_best:  79.58%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.796885/  5.626713, val:  74.17%, val_best:  79.58%, tr:  98.26%, tr_best:  98.88%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.629547/  5.680094, val:  75.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.694272/  6.191647, val:  72.92%, val_best:  79.58%, tr:  98.67%, tr_best:  99.18%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.779547/  6.453382, val:  74.17%, val_best:  79.58%, tr:  98.77%, tr_best:  99.18%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.639009/  5.863923, val:  79.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.692899/  6.059294, val:  75.00%, val_best:  79.58%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.577556/  5.979143, val:  78.33%, val_best:  79.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.650701/  6.454761, val:  79.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.539503/  6.471018, val:  81.25%, val_best:  81.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.538288/  6.774906, val:  77.50%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.544933/  6.708885, val:  75.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.552473/  6.472380, val:  79.17%, val_best:  81.25%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.429608/  6.634486, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.466758/  6.777270, val:  79.58%, val_best:  81.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.460159/  6.988739, val:  81.67%, val_best:  81.67%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.464994/  7.434267, val:  76.25%, val_best:  81.67%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.460632/  7.163128, val:  79.17%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.394761/  7.267477, val:  79.58%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.425328/  7.464379, val:  80.00%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.445552/  7.310522, val:  82.08%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.429530/  7.667799, val:  78.75%, val_best:  82.08%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.377259/  7.766232, val:  77.92%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.376768/  7.786118, val:  80.42%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.319576/  7.940769, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.360402/  8.017546, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.339194/  7.855277, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.316680/  8.015875, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.309215/  8.475647, val:  80.42%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.277169/  8.479498, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.308949/  8.507063, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.326781/  8.409334, val:  79.58%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.239241/  8.663848, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.282894/  8.855862, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.265662/  8.563479, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.262529/  8.686619, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.240331/  8.959558, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.237789/  8.932541, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.260343/  9.351934, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.272440/  9.179317, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.226881/  9.122897, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.189232/  9.046495, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.216305/  9.343026, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.250716/  9.472028, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.204579/  9.350897, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.215309/  9.439935, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.193411/  9.321900, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.216113/  9.761829, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.241072/  9.393623, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.190044/  9.726390, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.177576/  9.724614, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.151476/  9.776415, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.125746/  9.959390, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.174645/  9.839767, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.175153/  9.960859, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.160499/ 10.189226, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.147680/ 10.430724, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.203150/ 10.523477, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.162383/ 10.346108, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.161944/ 10.082000, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c1b4cba60c47a785fd57b217244feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▅▄▄▆▅▇▇▇█▇▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▃▅▄▄▆▄▇▅▅▅▇▆█▇▇▇█████▇███▇█▇▇██▇▇▇▇█</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▄▅▆▆▆▆▇▇▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▇▅▇▅▅▅▄▅▃▃▃▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▃▅▄▄▆▄▇▅▅▅▇▆█▇▇▇█████▇███▇█▇▇██▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▁▃▁▂▂▁▂▃▁▃▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.16194</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>10.082</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_161715-aud4paar/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4wwmotsf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991c079270744e31a1ebaadb608a1f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113489212261306, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_162405-4wwmotsf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf' target=\"_blank\">jolly-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.255974/  1.794649, val:  28.33%, val_best:  28.33%, tr:  12.26%, tr_best:  12.26%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.447584/  1.374007, val:  56.25%, val_best:  56.25%, tr:  49.44%, tr_best:  49.44%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.271685/  1.370422, val:  59.17%, val_best:  59.17%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.076363/  1.430519, val:  55.83%, val_best:  59.17%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.042131/  1.322086, val:  63.33%, val_best:  63.33%, tr:  65.27%, tr_best:  66.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.979358/  1.174465, val:  65.42%, val_best:  65.42%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.980131/  1.141178, val:  66.67%, val_best:  66.67%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.931621/  1.143126, val:  72.50%, val_best:  72.50%, tr:  71.71%, tr_best:  71.81%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.846661/  1.219483, val:  64.17%, val_best:  72.50%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.806770/  1.260874, val:  68.33%, val_best:  72.50%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.848225/  1.211548, val:  69.17%, val_best:  72.50%, tr:  79.78%, tr_best:  81.41%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.714247/  1.151863, val:  79.17%, val_best:  79.17%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.622073/  1.080003, val:  82.92%, val_best:  82.92%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.592821/  1.142021, val:  82.50%, val_best:  82.92%, tr:  91.22%, tr_best:  91.62%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.556177/  1.358047, val:  70.42%, val_best:  82.92%, tr:  91.42%, tr_best:  91.62%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.481127/  1.228372, val:  80.83%, val_best:  82.92%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.439013/  1.212624, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.399943/  1.239657, val:  81.25%, val_best:  82.92%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.357254/  1.310441, val:  80.42%, val_best:  82.92%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.347358/  1.198072, val:  85.00%, val_best:  85.00%, tr:  97.45%, tr_best:  97.65%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.297294/  1.380537, val:  78.33%, val_best:  85.00%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.289627/  1.360016, val:  85.00%, val_best:  85.00%, tr:  98.16%, tr_best:  98.67%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.278937/  1.404645, val:  80.42%, val_best:  85.00%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.275446/  1.392614, val:  81.67%, val_best:  85.00%, tr:  98.37%, tr_best:  98.98%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.243805/  1.398935, val:  83.75%, val_best:  85.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.202857/  1.456874, val:  83.33%, val_best:  85.00%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.207546/  1.486244, val:  85.00%, val_best:  85.00%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.181242/  1.544554, val:  84.17%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.165685/  1.493030, val:  84.58%, val_best:  85.00%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.149535/  1.695074, val:  79.17%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.155572/  1.627908, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.151485/  1.657331, val:  82.08%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.167811/  1.685501, val:  82.50%, val_best:  85.00%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.131060/  1.727040, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.119735/  1.805142, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.115524/  1.790252, val:  82.50%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.093009/  1.748719, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.091824/  1.843053, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.096797/  1.832674, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.081316/  1.843225, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.070998/  1.845201, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.061086/  1.941512, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.064064/  1.906011, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.059933/  1.942955, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.056217/  1.973621, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.052101/  2.007511, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.050164/  1.968748, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.051952/  2.030425, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.050920/  1.981389, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.040537/  2.046527, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.039788/  2.052551, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.045400/  2.067800, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.041909/  2.092551, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.033276/  2.107158, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.028859/  2.100258, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.027081/  2.105584, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.031503/  2.174173, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.027545/  2.164366, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.030097/  2.191032, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.026088/  2.225728, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.023934/  2.151983, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.025296/  2.234951, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.022565/  2.192113, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.021190/  2.228780, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.021179/  2.268256, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.021038/  2.315913, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.019384/  2.284978, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.022650/  2.281950, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.019097/  2.281934, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.018251/  2.309144, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.016003/  2.334571, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.016030/  2.301430, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.015940/  2.332711, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.016914/  2.358677, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014311/  2.377785, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.015170/  2.319848, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.015982/  2.481783, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.016414/  2.346095, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.017350/  2.342394, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.014484/  2.330831, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.011201/  2.391643, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.013324/  2.363090, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.013394/  2.402677, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.013639/  2.418149, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.013833/  2.468368, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.011364/  2.450069, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.015356/  2.446522, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.013673/  2.444415, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.016079/  2.485945, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.017404/  2.481196, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.010788/  2.495262, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.012371/  2.520899, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.011556/  2.551409, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.010907/  2.558093, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.010315/  2.549564, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.009746/  2.506722, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009529/  2.572533, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008709/  2.552748, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.006697/  2.503835, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.007798/  2.572679, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29c7881ffb74760bc293dc8eea05660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▃▂█▇▆▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▆▆█▆▇████▇▇▇██▇██▇██████████████▇████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▆▆███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▆▆█▆▇████▇▇▇██▇██▇██████████████▇████</td></tr><tr><td>val_loss</td><td>▄▂▂▁▂▁▂▂▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0078</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>2.57268</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_162405-4wwmotsf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aun0wfgg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_162934-aun0wfgg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg' target=\"_blank\">soft-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.040516/  1.642006, val:  55.00%, val_best:  55.00%, tr:  27.17%, tr_best:  27.17%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.550143/  1.873266, val:  49.17%, val_best:  55.00%, tr:  51.58%, tr_best:  51.58%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.622550/  1.669430, val:  57.50%, val_best:  57.50%, tr:  54.14%, tr_best:  54.14%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.293943/  1.977846, val:  55.83%, val_best:  57.50%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.354115/  1.542797, val:  62.92%, val_best:  62.92%, tr:  61.59%, tr_best:  63.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.276774/  1.588674, val:  54.58%, val_best:  62.92%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.230379/  1.757703, val:  60.00%, val_best:  62.92%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.398552/  2.213275, val:  55.42%, val_best:  62.92%, tr:  64.96%, tr_best:  66.70%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.121745/  1.554287, val:  60.00%, val_best:  62.92%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.502731/  1.743191, val:  62.50%, val_best:  62.92%, tr:  65.37%, tr_best:  70.58%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.187423/  1.219841, val:  75.83%, val_best:  75.83%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.112035/  1.663177, val:  63.33%, val_best:  75.83%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.125620/  1.364929, val:  71.67%, val_best:  75.83%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.852706/  1.586628, val:  61.25%, val_best:  75.83%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.950517/  1.812055, val:  64.58%, val_best:  75.83%, tr:  80.08%, tr_best:  80.69%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695665/  1.407705, val:  69.58%, val_best:  75.83%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.690188/  1.454104, val:  76.67%, val_best:  76.67%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.684235/  1.535680, val:  70.42%, val_best:  76.67%, tr:  87.44%, tr_best:  88.66%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.604962/  1.412773, val:  79.17%, val_best:  79.17%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.545171/  1.377163, val:  79.17%, val_best:  79.17%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.520745/  1.588545, val:  68.33%, val_best:  79.17%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.581843/  1.550013, val:  75.83%, val_best:  79.17%, tr:  91.83%, tr_best:  92.24%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.437616/  1.418960, val:  79.58%, val_best:  79.58%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.405296/  1.676802, val:  75.83%, val_best:  79.58%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.425702/  1.573171, val:  79.58%, val_best:  79.58%, tr:  95.40%, tr_best:  96.42%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.366101/  1.519008, val:  80.00%, val_best:  80.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.346354/  1.718673, val:  76.67%, val_best:  80.00%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.321981/  1.589448, val:  79.58%, val_best:  80.00%, tr:  97.45%, tr_best:  97.65%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.283775/  1.517297, val:  83.33%, val_best:  83.33%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.272821/  1.765906, val:  76.25%, val_best:  83.33%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.249871/  1.619042, val:  81.67%, val_best:  83.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.225793/  1.665401, val:  82.08%, val_best:  83.33%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.327429/  1.642328, val:  82.08%, val_best:  83.33%, tr:  96.94%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.220426/  1.842655, val:  80.42%, val_best:  83.33%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.230818/  1.796905, val:  80.00%, val_best:  83.33%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.191670/  1.653128, val:  83.75%, val_best:  83.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.170803/  1.675033, val:  82.92%, val_best:  83.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.184137/  1.724534, val:  85.83%, val_best:  85.83%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.172902/  1.694957, val:  84.58%, val_best:  85.83%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.190999/  1.638858, val:  87.50%, val_best:  87.50%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.141605/  1.661387, val:  83.75%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.138470/  1.746879, val:  85.83%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.128380/  1.799114, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.125076/  1.830211, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.119388/  1.879802, val:  84.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.103815/  1.851572, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.104387/  1.942153, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.093042/  1.893952, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.099276/  1.910627, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.084603/  1.907686, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.074872/  1.919549, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.065433/  1.925734, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.070708/  2.010888, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.062743/  2.009382, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.066973/  1.990938, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.055979/  2.006928, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.058404/  2.155457, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.067781/  2.013330, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.054138/  2.094468, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.052688/  2.094179, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.048649/  2.116537, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.048545/  2.150365, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.049908/  2.172894, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.051428/  2.120027, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.045165/  2.123672, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.044897/  2.159294, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.046397/  2.190489, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.044869/  2.196114, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.040653/  2.181042, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.037838/  2.225475, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.036113/  2.183403, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.038867/  2.244186, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.033992/  2.258068, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.031342/  2.275935, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.031066/  2.225717, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.032031/  2.272989, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.024831/  2.340320, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.025319/  2.363508, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.024136/  2.429628, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.023436/  2.344496, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.019394/  2.358616, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.025281/  2.396485, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.025266/  2.426712, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.026510/  2.368071, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.027278/  2.390736, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.022376/  2.450972, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.022716/  2.343374, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.024559/  2.373593, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.024390/  2.409000, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.018715/  2.370669, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.015984/  2.432915, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.018121/  2.419651, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.020993/  2.468354, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.018900/  2.473497, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.017190/  2.503860, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.016020/  2.535366, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.016636/  2.521703, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.016006/  2.500131, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.016884/  2.466167, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.017244/  2.505944, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7859b734d5465f981775af64ea9a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▂▅▅▁▆▇▇██▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▁▃▅▃▄▆▅▆▆▆▇▇███▇██▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▆▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▃▅▅▆▆▆▆▆▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▁▃▅▃▄▆▅▆▆▆▇▇███▇██▇██████████████████</td></tr><tr><td>val_loss</td><td>▃▃▂▆▃▁▄▂▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01724</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>2.50594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-25</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_162934-aun0wfgg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ip0s7a3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb30ca3ecade4c80b5a5f26dbbbc9790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113427677709195, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_163506-9ip0s7a3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3' target=\"_blank\">genial-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.305396/  2.111218, val:  19.17%, val_best:  19.17%, tr:   9.40%, tr_best:   9.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.699203/  1.455139, val:  50.42%, val_best:  50.42%, tr:  37.39%, tr_best:  37.39%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.251002/  1.293518, val:  59.17%, val_best:  59.17%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.101699/  1.366040, val:  59.58%, val_best:  59.58%, tr:  63.64%, tr_best:  63.64%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.051564/  1.172195, val:  66.67%, val_best:  66.67%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.006531/  1.187820, val:  64.17%, val_best:  66.67%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.962090/  1.214793, val:  61.67%, val_best:  66.67%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.965383/  1.218809, val:  61.25%, val_best:  66.67%, tr:  67.82%, tr_best:  70.38%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.935547/  1.150730, val:  67.92%, val_best:  67.92%, tr:  68.95%, tr_best:  70.38%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.876291/  1.309818, val:  60.00%, val_best:  67.92%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.904200/  1.288254, val:  62.92%, val_best:  67.92%, tr:  73.34%, tr_best:  74.77%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.869970/  1.163118, val:  72.08%, val_best:  72.08%, tr:  73.34%, tr_best:  74.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.824630/  1.160297, val:  71.25%, val_best:  72.08%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.814169/  1.129000, val:  70.00%, val_best:  72.08%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.781957/  1.447923, val:  65.42%, val_best:  72.08%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.754031/  1.145749, val:  75.42%, val_best:  75.42%, tr:  79.06%, tr_best:  81.00%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.736852/  1.140446, val:  71.67%, val_best:  75.42%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.701477/  1.185807, val:  72.50%, val_best:  75.42%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.677294/  1.300672, val:  69.58%, val_best:  75.42%, tr:  86.11%, tr_best:  86.41%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.676341/  1.257019, val:  71.67%, val_best:  75.42%, tr:  85.60%, tr_best:  86.41%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.679741/  1.230954, val:  70.83%, val_best:  75.42%, tr:  85.80%, tr_best:  86.41%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.608238/  1.433563, val:  70.42%, val_best:  75.42%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.584824/  1.293576, val:  67.08%, val_best:  75.42%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.588511/  1.150824, val:  77.92%, val_best:  77.92%, tr:  91.22%, tr_best:  91.62%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.534270/  1.227142, val:  78.33%, val_best:  78.33%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.541524/  1.222217, val:  75.42%, val_best:  78.33%, tr:  91.83%, tr_best:  95.40%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.594831/  1.194868, val:  75.83%, val_best:  78.33%, tr:  88.56%, tr_best:  95.40%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.509628/  1.337001, val:  72.92%, val_best:  78.33%, tr:  94.18%, tr_best:  95.40%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.495934/  1.227807, val:  80.42%, val_best:  80.42%, tr:  94.48%, tr_best:  95.40%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.445957/  1.345022, val:  71.25%, val_best:  80.42%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.448369/  1.298983, val:  73.33%, val_best:  80.42%, tr:  96.32%, tr_best:  96.63%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.432984/  1.282349, val:  76.25%, val_best:  80.42%, tr:  96.42%, tr_best:  96.63%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.470449/  1.292142, val:  76.67%, val_best:  80.42%, tr:  93.36%, tr_best:  96.63%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.431250/  1.331490, val:  73.33%, val_best:  80.42%, tr:  95.81%, tr_best:  96.63%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.412668/  1.336563, val:  77.08%, val_best:  80.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.372017/  1.344720, val:  80.00%, val_best:  80.42%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.358682/  1.326062, val:  82.08%, val_best:  82.08%, tr:  97.85%, tr_best:  98.26%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.350713/  1.440310, val:  73.33%, val_best:  82.08%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.385123/  1.317410, val:  77.92%, val_best:  82.08%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.356483/  1.350035, val:  81.25%, val_best:  82.08%, tr:  97.34%, tr_best:  98.26%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.321186/  1.312757, val:  82.08%, val_best:  82.08%, tr:  97.85%, tr_best:  98.26%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.329135/  1.402122, val:  80.00%, val_best:  82.08%, tr:  97.55%, tr_best:  98.26%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.297287/  1.347606, val:  81.25%, val_best:  82.08%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.294904/  1.447161, val:  75.83%, val_best:  82.08%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.300371/  1.445148, val:  76.67%, val_best:  82.08%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.270501/  1.414950, val:  80.42%, val_best:  82.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.272118/  1.438061, val:  79.17%, val_best:  82.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.254753/  1.438618, val:  78.75%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.277224/  1.466854, val:  80.00%, val_best:  82.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.246045/  1.439476, val:  81.25%, val_best:  82.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.229539/  1.471748, val:  78.33%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.230495/  1.481717, val:  80.00%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.231312/  1.531207, val:  81.25%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.218309/  1.521851, val:  78.75%, val_best:  82.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.200975/  1.501706, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.193795/  1.565947, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.204722/  1.563462, val:  82.08%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.197064/  1.508280, val:  83.33%, val_best:  84.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.171372/  1.554160, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.179808/  1.651070, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.172693/  1.615369, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.185133/  1.612537, val:  78.33%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.163319/  1.643281, val:  78.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.171514/  1.658182, val:  80.83%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.171313/  1.609245, val:  82.50%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.147116/  1.653720, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.134929/  1.627900, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.138620/  1.691944, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.154565/  1.726142, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.143177/  1.707724, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.137044/  1.717020, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.128650/  1.751790, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.113639/  1.817807, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.139280/  1.783728, val:  82.08%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.128704/  1.762757, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.105600/  1.793963, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.118291/  1.793824, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.121397/  1.829144, val:  83.75%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.125361/  1.843383, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.131758/  1.839104, val:  81.67%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.116138/  1.854860, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.105949/  1.833159, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.108596/  1.895337, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.114023/  1.933228, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.096126/  1.852567, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.104709/  1.888085, val:  82.08%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.095559/  1.863306, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.091648/  1.930743, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.084029/  1.879617, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.076067/  1.950858, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.089658/  1.932786, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.079884/  1.949038, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.075741/  1.988927, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.078477/  1.979764, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.077056/  2.009168, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.074213/  1.997348, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.072934/  2.054961, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.072094/  2.015152, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.067559/  2.068703, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.071169/  1.976241, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fef5617285148a7b838ea2afc7a3c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▆▅▆▆▅▆▇▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▅▇▆▇▇▆▇▇▇▇█▇██▇▇████▇▇███▇█████▇████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▇▇▇▇█▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▅▇▆▇▇▆▇▇▇▇█▇██▇▇████▇▇███▇█████▇████</td></tr><tr><td>val_loss</td><td>█▂▁▁▂▁▃▁▂▃▁▁▂▂▂▃▂▂▃▃▃▄▄▄▅▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07117</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>1.97624</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-sweep-29</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_163506-9ip0s7a3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fpsj5zyt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a468a733a7c4f75b17a958bdfc7177e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113546576557888, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164034-fpsj5zyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt' target=\"_blank\">azure-sweep-34</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.135064/  2.348142, val:  45.00%, val_best:  45.00%, tr:  26.56%, tr_best:  26.56%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.917111/  2.421806, val:  52.92%, val_best:  52.92%, tr:  50.87%, tr_best:  50.87%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.260753/  2.014679, val:  52.50%, val_best:  52.92%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.700029/  2.913277, val:  57.08%, val_best:  57.08%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.070586/  2.466565, val:  60.83%, val_best:  60.83%, tr:  58.43%, tr_best:  60.98%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.974281/  2.510710, val:  56.25%, val_best:  60.83%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.456344/  2.469453, val:  58.75%, val_best:  60.83%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.739878/  2.664713, val:  54.17%, val_best:  60.83%, tr:  64.76%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.713847/  2.751289, val:  55.00%, val_best:  60.83%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.096435/  2.797124, val:  59.17%, val_best:  60.83%, tr:  67.62%, tr_best:  68.64%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.593845/  1.728211, val:  68.33%, val_best:  68.33%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.624188/  2.273625, val:  65.00%, val_best:  68.33%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.366256/  1.997888, val:  65.83%, val_best:  68.33%, tr:  77.43%, tr_best:  77.43%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.306336/  2.404990, val:  62.50%, val_best:  68.33%, tr:  78.45%, tr_best:  78.45%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.562997/  2.758612, val:  65.42%, val_best:  68.33%, tr:  78.35%, tr_best:  78.45%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.974711/  2.258536, val:  70.83%, val_best:  70.83%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.903604/  1.960086, val:  80.00%, val_best:  80.00%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.898043/  2.249439, val:  72.50%, val_best:  80.00%, tr:  88.25%, tr_best:  89.17%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.863013/  2.361640, val:  75.00%, val_best:  80.00%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.835367/  2.126679, val:  80.42%, val_best:  80.42%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.702799/  2.647707, val:  67.50%, val_best:  80.42%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.849705/  2.180631, val:  79.17%, val_best:  80.42%, tr:  91.32%, tr_best:  94.28%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.627266/  2.232721, val:  78.75%, val_best:  80.42%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.675346/  2.746020, val:  74.17%, val_best:  80.42%, tr:  94.89%, tr_best:  95.20%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.572461/  2.423426, val:  80.83%, val_best:  80.83%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.555514/  2.337517, val:  79.58%, val_best:  80.83%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.474505/  2.414984, val:  79.17%, val_best:  80.83%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.469093/  2.691437, val:  76.25%, val_best:  80.83%, tr:  97.55%, tr_best:  97.75%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.456499/  2.521243, val:  82.92%, val_best:  82.92%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.387633/  2.855560, val:  79.58%, val_best:  82.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.372882/  2.779290, val:  82.50%, val_best:  82.92%, tr:  98.37%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.364165/  2.667890, val:  82.50%, val_best:  82.92%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.505896/  2.728811, val:  80.42%, val_best:  82.92%, tr:  97.14%, tr_best:  99.18%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.387118/  2.998614, val:  83.33%, val_best:  83.33%, tr:  97.96%, tr_best:  99.18%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.369427/  3.229547, val:  77.92%, val_best:  83.33%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.303415/  2.691154, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.249374/  2.695435, val:  85.83%, val_best:  85.83%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.250952/  2.816796, val:  85.00%, val_best:  85.83%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.294056/  2.873309, val:  82.92%, val_best:  85.83%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.309798/  2.796141, val:  86.25%, val_best:  86.25%, tr:  98.88%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.219641/  2.762543, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.211970/  2.841761, val:  84.17%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.181153/  2.971166, val:  79.58%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.204013/  2.931836, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.181774/  3.080180, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.164444/  3.323238, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.160665/  3.321762, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.164442/  3.211581, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.156313/  3.246644, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.167254/  3.290756, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.135145/  3.414236, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.133105/  3.296542, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.135538/  3.443089, val:  82.92%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.132860/  3.484478, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.125570/  3.407904, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.110793/  3.401881, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.102220/  3.673701, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.113155/  3.458656, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.094956/  3.459064, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.093048/  3.698795, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.096622/  3.679455, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.107340/  3.662870, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.091717/  3.709290, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.198414/  3.899753, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.114333/  3.827221, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.101458/  3.784963, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.094442/  4.059870, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.095331/  4.018095, val:  86.67%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.085894/  3.966964, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.057995/  4.053779, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.056270/  3.920878, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.065530/  4.122053, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.076394/  4.150880, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.069386/  4.169332, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.066232/  4.078281, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.052409/  4.288164, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.053953/  4.289522, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053649/  4.327401, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.044496/  4.346035, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.048964/  4.318016, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.054606/  4.343048, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.040182/  4.264418, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.045545/  4.403891, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.044003/  4.324257, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044437/  4.273188, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.054824/  4.345763, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.047066/  4.325257, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.040543/  4.423947, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.040323/  4.357699, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.041144/  4.431307, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.039271/  4.545905, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.041248/  4.382458, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.034382/  4.529025, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.038688/  4.499335, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.034813/  4.517359, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.023365/  4.559685, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.030883/  4.574547, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.032475/  4.558115, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.040467/  4.578133, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030222/  4.585063, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5aa78484664049b66a065ce5e7e1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▅▅▅▇▇▇█▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▃▄▄▆▇▇▇▇▇▇███▇▇▇█▇██▇████▇███████▇██</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▇▅▆▄▄▄▃▂▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▄▅▅▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▃▄▄▆▇▇▇▇▇▇███▇▇▇█▇██▇████▇███████▇██</td></tr><tr><td>val_loss</td><td>▂▁▂▃▃▁▃▂▁▁▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▆▇▇▇▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03022</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>4.58506</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-sweep-34</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_164034-fpsj5zyt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pyrwsu69 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164650-pyrwsu69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69' target=\"_blank\">trim-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.062186/  2.395987, val:  44.58%, val_best:  44.58%, tr:  30.54%, tr_best:  30.54%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.932272/  2.177964, val:  51.25%, val_best:  51.25%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.009413/  1.797581, val:  56.67%, val_best:  56.67%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.426914/  2.523172, val:  56.25%, val_best:  56.67%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.787200/  1.792446, val:  62.92%, val_best:  62.92%, tr:  61.18%, tr_best:  66.09%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.719016/  2.198045, val:  58.33%, val_best:  62.92%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.163717/  2.508123, val:  60.83%, val_best:  62.92%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.472037/  2.152541, val:  59.17%, val_best:  62.92%, tr:  68.95%, tr_best:  71.60%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.214181/  1.654069, val:  61.67%, val_best:  62.92%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.613171/  2.570397, val:  53.75%, val_best:  62.92%, tr:  70.48%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.207835/  2.052088, val:  58.75%, val_best:  62.92%, tr:  73.14%, tr_best:  73.14%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.542757/  2.610566, val:  63.33%, val_best:  63.33%, tr:  72.63%, tr_best:  73.14%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.462697/  1.669866, val:  72.92%, val_best:  72.92%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.068347/  2.035656, val:  65.42%, val_best:  72.92%, tr:  78.35%, tr_best:  78.35%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.285327/  2.627961, val:  63.75%, val_best:  72.92%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.877170/  2.274029, val:  67.08%, val_best:  72.92%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.895368/  2.154450, val:  72.50%, val_best:  72.92%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.731003/  2.051186, val:  70.00%, val_best:  72.92%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.700490/  2.331778, val:  68.33%, val_best:  72.92%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.629375/  2.138595, val:  75.83%, val_best:  75.83%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.578407/  2.284514, val:  74.17%, val_best:  75.83%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.586689/  2.689306, val:  70.42%, val_best:  75.83%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.493521/  2.378192, val:  75.83%, val_best:  75.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.494291/  2.669397, val:  75.42%, val_best:  75.83%, tr:  96.12%, tr_best:  96.53%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.395209/  2.602473, val:  77.92%, val_best:  77.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.422087/  2.663994, val:  77.08%, val_best:  77.92%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.387727/  2.741148, val:  77.50%, val_best:  77.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.382440/  2.938202, val:  77.92%, val_best:  77.92%, tr:  98.47%, tr_best:  98.77%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.289783/  2.659052, val:  81.25%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.311728/  2.983512, val:  79.17%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.287006/  3.127795, val:  80.83%, val_best:  81.25%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.237401/  3.171818, val:  76.25%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.271529/  3.322640, val:  75.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.222970/  3.324409, val:  78.33%, val_best:  81.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.243223/  3.555316, val:  75.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222751/  3.366916, val:  80.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171153/  3.391801, val:  81.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.163252/  3.529157, val:  78.75%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.150880/  3.376877, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.171795/  3.613594, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.135498/  3.662543, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.112801/  3.644865, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.109654/  3.816520, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.128493/  3.744687, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.129770/  3.745157, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.092817/  3.856912, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.110130/  3.776510, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.098390/  3.879533, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.077045/  3.927565, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.082830/  4.332655, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.082940/  4.092589, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.081213/  4.066563, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.081532/  4.166774, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.071735/  4.143727, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.077904/  4.281452, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.102459/  4.300975, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.087348/  4.528956, val:  75.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.062137/  4.454435, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.061949/  4.447115, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.075246/  4.401570, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.060665/  4.622108, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.048169/  4.708529, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.040685/  4.518310, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.038630/  4.411102, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.033288/  4.501129, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.038497/  4.617074, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.039794/  4.594819, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.046207/  4.557834, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.072226/  4.797017, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.043040/  4.666761, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.039252/  4.662484, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.030064/  4.797895, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.045557/  4.730133, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.033137/  4.767644, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.036926/  4.849226, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.038322/  4.949713, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058650/  4.869298, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.043149/  5.034408, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.043755/  5.122185, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.051200/  5.103598, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.033358/  5.295014, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.047769/  4.883702, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.036011/  5.132040, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.029694/  4.937526, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044499/  5.091215, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043552/  5.126673, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042657/  5.159236, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.031548/  5.157077, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.027336/  5.177079, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.030779/  5.099520, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.025766/  5.258143, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.023230/  5.093557, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031848/  5.331208, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.018090/  5.196564, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.023957/  5.249485, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.038331/  5.359312, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.037034/  5.286173, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028969/  5.336520, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.022347/  5.271390, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023963/  5.262772, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475845f01c744abd811bf7dddfc63916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▇▅▅▂▆▇▅█▇███▇██▇███████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▄▃▆▅▆▇▆▇▇▇▇█▇█▇██▇▇██▇███▇▇████▇██▇▇█</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▆▆▅▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▆▆▆▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▄▃▆▅▆▇▆▇▇▇▇█▇█▇██▇▇██▇███▇▇████▇██▇▇█</td></tr><tr><td>val_loss</td><td>▂▁▁▂▃▁▃▂▂▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▆▇▇▇▇██▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02396</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>5.26277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-37</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_164650-pyrwsu69/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 597cvjd1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_165242-597cvjd1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1' target=\"_blank\">graceful-sweep-39</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.318805/  2.210244, val:  20.00%, val_best:  20.00%, tr:   9.19%, tr_best:   9.19%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.772503/  1.428594, val:  57.92%, val_best:  57.92%, tr:  38.00%, tr_best:  38.00%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.310495/  1.356579, val:  55.83%, val_best:  57.92%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.101270/  1.336439, val:  58.33%, val_best:  58.33%, tr:  64.15%, tr_best:  64.15%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.075656/  1.288629, val:  59.58%, val_best:  59.58%, tr:  62.82%, tr_best:  64.15%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.996204/  1.210784, val:  62.50%, val_best:  62.50%, tr:  68.44%, tr_best:  68.44%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.966449/  1.234998, val:  60.83%, val_best:  62.50%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.931269/  1.220983, val:  61.67%, val_best:  62.50%, tr:  68.64%, tr_best:  69.15%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.919397/  1.122737, val:  64.17%, val_best:  64.17%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.848325/  1.235111, val:  61.25%, val_best:  64.17%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.886516/  1.307322, val:  62.92%, val_best:  64.17%, tr:  75.38%, tr_best:  75.49%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.843915/  1.214096, val:  63.33%, val_best:  64.17%, tr:  74.06%, tr_best:  75.49%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.791331/  1.180076, val:  63.75%, val_best:  64.17%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.789663/  1.240403, val:  64.17%, val_best:  64.17%, tr:  78.65%, tr_best:  78.65%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.715044/  1.433705, val:  64.58%, val_best:  64.58%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.686299/  1.252988, val:  65.00%, val_best:  65.00%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.647108/  1.229423, val:  67.08%, val_best:  67.08%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.620437/  1.263187, val:  69.58%, val_best:  69.58%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.586220/  1.360671, val:  66.67%, val_best:  69.58%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.573204/  1.296395, val:  69.58%, val_best:  69.58%, tr:  90.09%, tr_best:  90.81%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.556452/  1.316498, val:  66.67%, val_best:  69.58%, tr:  90.60%, tr_best:  90.81%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.494077/  1.457673, val:  67.08%, val_best:  69.58%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.474826/  1.377802, val:  69.17%, val_best:  69.58%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.488248/  1.335735, val:  74.17%, val_best:  74.17%, tr:  94.48%, tr_best:  95.30%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.418628/  1.415621, val:  72.92%, val_best:  74.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.416415/  1.379255, val:  71.67%, val_best:  74.17%, tr:  94.59%, tr_best:  96.83%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.414250/  1.352293, val:  77.08%, val_best:  77.08%, tr:  95.71%, tr_best:  96.83%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.367924/  1.516313, val:  75.42%, val_best:  77.08%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.332235/  1.443123, val:  74.58%, val_best:  77.08%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.298641/  1.544621, val:  71.25%, val_best:  77.08%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.283481/  1.523626, val:  72.50%, val_best:  77.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.280684/  1.530439, val:  78.33%, val_best:  78.33%, tr:  98.67%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.316187/  1.599973, val:  71.67%, val_best:  78.33%, tr:  97.34%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.290718/  1.631603, val:  76.25%, val_best:  78.33%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.253567/  1.609717, val:  74.58%, val_best:  78.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.217485/  1.646970, val:  75.83%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192557/  1.624180, val:  77.92%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.194858/  1.681047, val:  77.92%, val_best:  78.33%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.200941/  1.646473, val:  79.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.167189/  1.770751, val:  75.00%, val_best:  79.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.175144/  1.745195, val:  75.00%, val_best:  79.58%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.171604/  1.719100, val:  77.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.140545/  1.791715, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.128460/  1.771028, val:  77.08%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.128419/  1.858317, val:  74.17%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.118835/  1.854908, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.109903/  1.883399, val:  79.58%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.101471/  1.868499, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.111395/  1.897279, val:  80.42%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.099955/  1.980149, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.086770/  1.994833, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.081568/  1.982269, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.086296/  2.083234, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.077490/  2.026960, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.075247/  2.055609, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.061841/  2.055895, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.063706/  2.112296, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.065352/  2.105452, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.061281/  2.113639, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.063396/  2.136526, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.052958/  2.152933, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.051542/  2.149691, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.048617/  2.226520, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.054391/  2.284744, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.055692/  2.180856, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.045647/  2.280824, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.039179/  2.257237, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.038166/  2.271627, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.041401/  2.296675, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.037734/  2.263813, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.035291/  2.274694, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.037053/  2.324774, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.036267/  2.355351, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.039400/  2.410007, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.037208/  2.414623, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.031648/  2.415062, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.026893/  2.367258, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.033897/  2.523752, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.039499/  2.466230, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.034374/  2.450610, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.027541/  2.469490, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.022908/  2.496147, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.023113/  2.517245, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.022990/  2.524898, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.019521/  2.477964, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.022837/  2.563119, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.023714/  2.532770, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.021604/  2.571520, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.016297/  2.571679, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.019144/  2.572850, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.018133/  2.559130, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.019105/  2.632702, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.018280/  2.638778, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.019552/  2.652483, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.016407/  2.629497, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.015731/  2.668269, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.018343/  2.677304, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.017681/  2.753611, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.016281/  2.676286, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.020668/  2.670245, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da0190541804a58bea04833c52ed905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▄▇▆▇▅▇█▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇█▇▇▇█████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇█▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>▆▂▁▁▁▁▂▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02067</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>2.67025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-39</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_165242-597cvjd1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: klrdac15 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_165812-klrdac15</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15' target=\"_blank\">dark-sweep-41</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309219, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.030512/  1.678581, val:  49.17%, val_best:  49.17%, tr:  21.25%, tr_best:  21.25%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.437040/  1.430297, val:  57.08%, val_best:  57.08%, tr:  52.40%, tr_best:  52.40%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.166490/  1.438994, val:  55.00%, val_best:  57.08%, tr:  60.88%, tr_best:  60.88%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.090239/  1.297438, val:  64.17%, val_best:  64.17%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.037390/  1.301868, val:  61.67%, val_best:  64.17%, tr:  69.05%, tr_best:  69.05%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.000910/  1.338546, val:  61.25%, val_best:  64.17%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.994178/  1.323386, val:  62.50%, val_best:  64.17%, tr:  68.64%, tr_best:  70.99%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.981850/  1.270464, val:  65.42%, val_best:  65.42%, tr:  69.36%, tr_best:  70.99%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.913207/  1.388049, val:  60.42%, val_best:  65.42%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.937273/  1.378559, val:  65.00%, val_best:  65.42%, tr:  73.03%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.896307/  1.284199, val:  71.67%, val_best:  71.67%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.847823/  1.231609, val:  73.33%, val_best:  73.33%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.808100/  1.275456, val:  72.50%, val_best:  73.33%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.769586/  1.543463, val:  69.17%, val_best:  73.33%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.743970/  1.289895, val:  73.33%, val_best:  73.33%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.710916/  1.317821, val:  73.75%, val_best:  73.75%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.677988/  1.398424, val:  74.58%, val_best:  74.58%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.625889/  1.493693, val:  70.00%, val_best:  74.58%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.624810/  1.371965, val:  74.17%, val_best:  74.58%, tr:  92.13%, tr_best:  92.44%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.601085/  1.485054, val:  71.25%, val_best:  74.58%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.571917/  1.462354, val:  72.92%, val_best:  74.58%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.529248/  1.528203, val:  72.50%, val_best:  74.58%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.542228/  1.469005, val:  80.00%, val_best:  80.00%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.484097/  1.510545, val:  79.58%, val_best:  80.00%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.474766/  1.549213, val:  75.83%, val_best:  80.00%, tr:  96.83%, tr_best:  97.04%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.507410/  1.542966, val:  79.17%, val_best:  80.00%, tr:  95.91%, tr_best:  97.04%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.430397/  1.610787, val:  78.33%, val_best:  80.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.433632/  1.619803, val:  82.50%, val_best:  82.50%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.419218/  1.795434, val:  74.17%, val_best:  82.50%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.411251/  1.627755, val:  80.42%, val_best:  82.50%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.396910/  1.745959, val:  79.58%, val_best:  82.50%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.391482/  1.734411, val:  80.42%, val_best:  82.50%, tr:  97.96%, tr_best:  98.37%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.359470/  1.800742, val:  80.83%, val_best:  82.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.350944/  1.896027, val:  80.00%, val_best:  82.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.342049/  1.930975, val:  82.50%, val_best:  82.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.326643/  1.834182, val:  82.50%, val_best:  82.50%, tr:  98.67%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.304016/  2.002824, val:  78.75%, val_best:  82.50%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.327067/  1.894813, val:  82.08%, val_best:  82.50%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.293819/  1.974611, val:  84.58%, val_best:  84.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.287332/  1.974986, val:  84.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.269948/  2.110264, val:  80.83%, val_best:  84.58%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.268224/  2.078310, val:  81.67%, val_best:  84.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.255084/  2.167964, val:  82.50%, val_best:  84.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.235727/  2.119823, val:  82.50%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.244317/  2.234583, val:  82.50%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.245396/  2.272496, val:  83.33%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.230579/  2.316329, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.216525/  2.273062, val:  81.67%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.197593/  2.334180, val:  81.25%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.187620/  2.399100, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.184547/  2.391267, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.195622/  2.521014, val:  83.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.183916/  2.480004, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.165481/  2.504408, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.175693/  2.531630, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.168706/  2.568130, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.173943/  2.586096, val:  84.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.155942/  2.579063, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.144336/  2.632550, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.164397/  2.645462, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.154119/  2.709978, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.162563/  2.770606, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.142049/  2.744117, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.124962/  2.868333, val:  82.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.122618/  2.847694, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.117883/  2.825746, val:  83.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.137307/  2.860445, val:  84.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.146424/  2.922956, val:  83.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.127995/  3.005267, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.110804/  2.917452, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.113099/  3.010923, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.099290/  3.032896, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.101636/  3.030765, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.102947/  3.077376, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.106148/  3.136700, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.107884/  3.154456, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.112288/  3.248230, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.106655/  3.235567, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.098614/  3.210622, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.097743/  3.300218, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.087968/  3.221530, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.082949/  3.320432, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.089126/  3.357121, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.079311/  3.327686, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.107448/  3.402605, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.103553/  3.466336, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.095166/  3.384451, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.098599/  3.498098, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.069661/  3.476065, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.077462/  3.421753, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.077030/  3.526197, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.073842/  3.523541, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.078491/  3.696885, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.067479/  3.670784, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.064959/  3.664366, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.059086/  3.669058, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.066375/  3.681804, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.055239/  3.731021, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.070220/  3.678856, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30480c73e2e8464d869431bc413ca1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▄▅▆▄██████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇██▇████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▆▆▇▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇██▇████████████████████████</td></tr><tr><td>val_loss</td><td>▄▂▁▁▁▁▂▁▁▂▂▂▃▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07022</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>3.67886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-41</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_165812-klrdac15/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hqsrvmfl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19a667a243849949c1007ed10d01774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112981998465127, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_170453-hqsrvmfl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl' target=\"_blank\">ethereal-sweep-43</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.253323/  1.765929, val:  38.33%, val_best:  38.33%, tr:  13.38%, tr_best:  13.38%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.511758/  1.988629, val:  50.83%, val_best:  50.83%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.611668/  1.840533, val:  56.67%, val_best:  56.67%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.452601/  2.138407, val:  50.00%, val_best:  56.67%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.425102/  1.544960, val:  62.50%, val_best:  62.50%, tr:  61.39%, tr_best:  61.39%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.326070/  1.660033, val:  56.67%, val_best:  62.50%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.351721/  1.740631, val:  60.00%, val_best:  62.50%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.472630/  1.659437, val:  63.75%, val_best:  63.75%, tr:  65.68%, tr_best:  67.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.166623/  1.507021, val:  62.50%, val_best:  63.75%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.481194/  1.640425, val:  65.42%, val_best:  65.42%, tr:  69.66%, tr_best:  71.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.355739/  1.421762, val:  69.58%, val_best:  69.58%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.191274/  1.731272, val:  66.25%, val_best:  69.58%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.157648/  1.571059, val:  74.17%, val_best:  74.17%, tr:  76.00%, tr_best:  76.61%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.047196/  1.905127, val:  63.33%, val_best:  74.17%, tr:  79.88%, tr_best:  79.88%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.138409/  2.007575, val:  70.83%, val_best:  74.17%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.807623/  1.930568, val:  70.42%, val_best:  74.17%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.891349/  1.785194, val:  77.50%, val_best:  77.50%, tr:  87.44%, tr_best:  88.97%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.843770/  1.822308, val:  79.58%, val_best:  79.58%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.731335/  1.875055, val:  79.58%, val_best:  79.58%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.726594/  1.773581, val:  79.58%, val_best:  79.58%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.691131/  1.919801, val:  79.17%, val_best:  79.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.677214/  2.106114, val:  75.00%, val_best:  79.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.617743/  1.978966, val:  79.58%, val_best:  79.58%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.619838/  2.085046, val:  82.08%, val_best:  82.08%, tr:  94.99%, tr_best:  95.81%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.575177/  2.118035, val:  80.00%, val_best:  82.08%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.518957/  1.969795, val:  84.58%, val_best:  84.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.534052/  2.078116, val:  83.33%, val_best:  84.58%, tr:  97.14%, tr_best:  97.34%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.483941/  2.331681, val:  75.83%, val_best:  84.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.509145/  2.250173, val:  83.75%, val_best:  84.58%, tr:  97.65%, tr_best:  97.85%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.452508/  2.539420, val:  79.58%, val_best:  84.58%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.445803/  2.195746, val:  86.25%, val_best:  86.25%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.406524/  2.516564, val:  82.08%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.577702/  2.973837, val:  72.50%, val_best:  86.25%, tr:  96.02%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.456043/  2.819457, val:  80.42%, val_best:  86.25%, tr:  98.57%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.410303/  2.874006, val:  82.92%, val_best:  86.25%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.365640/  2.764460, val:  85.00%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.339329/  2.668807, val:  87.08%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.348269/  2.747555, val:  85.00%, val_best:  87.08%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.385121/  2.791430, val:  85.42%, val_best:  87.08%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.330159/  2.913253, val:  85.42%, val_best:  87.08%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.297723/  2.984876, val:  84.17%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.290734/  3.085657, val:  83.33%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.250836/  3.063665, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.266245/  3.204490, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.269421/  3.308914, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.273942/  3.302929, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.225014/  3.334401, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.225493/  3.343555, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.237100/  3.448946, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.224939/  3.488646, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.205718/  3.537910, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.206952/  3.688695, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.207965/  3.698255, val:  85.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.201338/  3.905701, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.192889/  3.784663, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.199326/  3.880504, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.184847/  4.016809, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.161488/  4.014282, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.162391/  4.088997, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.154122/  4.290989, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.161288/  4.280174, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.165245/  4.335113, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.167979/  4.380980, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.156888/  4.445717, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.149900/  4.438478, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.131143/  4.483879, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.124440/  4.527740, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.142066/  4.612941, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.143669/  4.712356, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.121949/  4.791534, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.119967/  4.713686, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.131761/  4.720039, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.117413/  4.793860, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.148624/  4.932520, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.122942/  4.752527, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.113134/  4.808766, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.094022/  4.925261, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.095570/  4.946665, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.095363/  5.082295, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.095163/  5.232117, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.091544/  5.069768, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.093498/  5.187819, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.087709/  5.214963, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.092781/  5.356602, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.082836/  5.175879, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.087973/  5.425270, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.102739/  5.286963, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.095318/  5.472827, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.109029/  5.515829, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.085764/  5.330654, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.082066/  5.529750, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.083677/  5.574675, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.076181/  5.598844, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.070736/  5.620394, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.069504/  5.599156, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.075637/  5.783115, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.072909/  5.814156, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.077149/  5.719594, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.074706/  5.652952, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.079862/  5.601643, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d8f823e5084242b2bf13e330595b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▂▅▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▅▆▆▇▇▆▇▇▇▆██████████▇█▇█████▇█████▇█</td></tr><tr><td>tr_acc</td><td>▁▅▅▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▆▄▄▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▅▆▆▇▇▆▇▇▇▆██████████▇█▇█████▇█████▇█</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07986</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>5.60164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-43</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_170453-hqsrvmfl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dtgw2w8d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171110-dtgw2w8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d' target=\"_blank\">deep-sweep-45</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.241928/  1.732256, val:  41.25%, val_best:  41.25%, tr:  13.69%, tr_best:  13.69%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.426778/  1.694993, val:  56.25%, val_best:  56.25%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.510367/  1.499763, val:  57.92%, val_best:  57.92%, tr:  59.04%, tr_best:  59.04%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.213689/  1.719357, val:  54.58%, val_best:  57.92%, tr:  65.27%, tr_best:  65.27%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.199136/  1.322595, val:  62.08%, val_best:  62.08%, tr:  63.84%, tr_best:  65.27%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.061224/  1.362986, val:  61.25%, val_best:  62.08%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.056197/  1.326440, val:  64.17%, val_best:  64.17%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.085115/  1.224115, val:  68.33%, val_best:  68.33%, tr:  69.46%, tr_best:  70.79%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.055146/  1.446075, val:  64.58%, val_best:  68.33%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.031130/  1.395791, val:  62.50%, val_best:  68.33%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.030412/  1.304922, val:  71.25%, val_best:  71.25%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.890050/  1.339263, val:  71.25%, val_best:  71.25%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.760786/  1.211170, val:  79.17%, val_best:  79.17%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.749820/  1.280827, val:  81.67%, val_best:  81.67%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.752633/  1.666925, val:  74.17%, val_best:  81.67%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.606293/  1.376468, val:  82.50%, val_best:  82.50%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.579093/  1.364524, val:  80.00%, val_best:  82.50%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.508265/  1.427017, val:  82.50%, val_best:  82.50%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.515138/  1.447799, val:  84.17%, val_best:  84.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.496850/  1.419511, val:  84.17%, val_best:  84.17%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.434394/  1.515021, val:  84.17%, val_best:  84.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.417364/  1.576975, val:  82.08%, val_best:  84.17%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.392547/  1.619514, val:  80.83%, val_best:  84.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.398906/  1.603530, val:  85.00%, val_best:  85.00%, tr:  97.55%, tr_best:  98.47%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.382188/  1.708744, val:  85.83%, val_best:  85.83%, tr:  98.06%, tr_best:  98.47%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.357598/  1.761010, val:  83.33%, val_best:  85.83%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.337110/  1.734960, val:  86.25%, val_best:  86.25%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.319749/  1.926165, val:  79.17%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.298305/  1.824659, val:  84.58%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.305065/  1.992830, val:  82.08%, val_best:  86.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.307400/  1.943748, val:  82.50%, val_best:  86.25%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.278015/  1.979821, val:  85.42%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.275924/  2.010406, val:  85.00%, val_best:  86.25%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.258831/  2.087749, val:  84.17%, val_best:  86.25%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.273977/  2.239629, val:  80.83%, val_best:  86.25%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.245046/  2.129827, val:  86.25%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.235650/  2.117629, val:  85.42%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.205171/  2.178760, val:  83.75%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.223763/  2.166179, val:  87.08%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.180114/  2.327710, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.189393/  2.397435, val:  86.25%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.170833/  2.413211, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.186980/  2.450851, val:  85.00%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.163194/  2.463600, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.181356/  2.484248, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.157595/  2.594248, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.158860/  2.575316, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.156610/  2.690062, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.172761/  2.710411, val:  87.50%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.139817/  2.767637, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.125100/  2.759830, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.121371/  2.815148, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.125158/  2.838930, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.113746/  2.974573, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.101725/  2.924242, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.118513/  2.905219, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.109436/  3.066618, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.095460/  3.026918, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.099435/  2.982941, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.108875/  3.168121, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.094910/  3.143734, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.111829/  3.222238, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.105233/  3.269783, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.092370/  3.178758, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.087288/  3.223400, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.078591/  3.341731, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.073290/  3.328445, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.085053/  3.309978, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.081140/  3.424587, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.072963/  3.495421, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.079825/  3.471228, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.078747/  3.533216, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.070523/  3.483029, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.080121/  3.593130, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.072901/  3.680131, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.078790/  3.601433, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.076220/  3.708626, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.078510/  3.826304, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.090337/  3.753058, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.079186/  3.793157, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.055510/  3.917493, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.060350/  3.889609, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.066357/  3.855721, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.072167/  4.013806, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.062263/  3.871153, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.059768/  3.965374, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.061472/  3.954705, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.059634/  4.095017, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.050664/  4.020766, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.061326/  4.007430, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.060682/  3.945527, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.053575/  4.008485, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.047886/  4.105787, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.048141/  4.023148, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.051081/  4.036030, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.038347/  4.129725, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.043175/  4.096999, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.043645/  4.007375, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.044682/  4.085727, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.039490/  4.099744, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6287f62b622845148f8a4f00288621ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▃▁▇▇▆▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▄▇▆▇█▇██▇██▇████████████████████▇███</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▄▇▆▇█▇██▇██▇████████████████████▇███</td></tr><tr><td>val_loss</td><td>▂▂▁▁▁▁▂▂▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03949</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>4.09974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-45</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171110-dtgw2w8d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llzgtquc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171701-llzgtquc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc' target=\"_blank\">rare-sweep-47</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.267550/  1.789936, val:  40.00%, val_best:  40.00%, tr:  11.54%, tr_best:  11.54%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.453109/  1.428581, val:  57.08%, val_best:  57.08%, tr:  51.69%, tr_best:  51.69%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.368202/  1.351913, val:  60.42%, val_best:  60.42%, tr:  59.55%, tr_best:  59.55%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.169459/  1.472526, val:  56.25%, val_best:  60.42%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.114242/  1.334611, val:  62.92%, val_best:  62.92%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.009384/  1.248120, val:  62.92%, val_best:  62.92%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.001380/  1.234303, val:  64.58%, val_best:  64.58%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.001089/  1.124841, val:  68.33%, val_best:  68.33%, tr:  69.77%, tr_best:  72.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.023363/  1.246347, val:  66.67%, val_best:  68.33%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.985452/  1.252468, val:  63.75%, val_best:  68.33%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.970935/  1.171034, val:  71.67%, val_best:  71.67%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.809606/  1.171789, val:  74.58%, val_best:  74.58%, tr:  82.33%, tr_best:  82.33%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.713953/  1.093451, val:  82.50%, val_best:  82.50%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.687643/  1.109947, val:  81.25%, val_best:  82.50%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.695422/  1.342220, val:  73.75%, val_best:  82.50%, tr:  86.62%, tr_best:  87.13%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.557454/  1.170050, val:  76.25%, val_best:  82.50%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.566574/  1.104410, val:  82.50%, val_best:  82.50%, tr:  90.60%, tr_best:  91.62%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.497351/  1.095610, val:  84.58%, val_best:  84.58%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.476041/  1.179812, val:  82.50%, val_best:  84.58%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.450163/  1.125065, val:  82.50%, val_best:  84.58%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.425781/  1.206522, val:  82.08%, val_best:  84.58%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.372507/  1.219955, val:  85.42%, val_best:  85.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.360393/  1.257549, val:  81.25%, val_best:  85.42%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.372565/  1.229142, val:  85.42%, val_best:  85.42%, tr:  96.32%, tr_best:  97.65%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.325692/  1.218983, val:  84.17%, val_best:  85.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.337816/  1.249061, val:  84.17%, val_best:  85.42%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.322258/  1.225932, val:  85.42%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.302042/  1.436864, val:  77.50%, val_best:  85.42%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.278606/  1.223891, val:  86.25%, val_best:  86.25%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.248801/  1.505903, val:  76.25%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.253824/  1.306579, val:  85.00%, val_best:  86.25%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.237451/  1.308197, val:  85.42%, val_best:  86.25%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.278118/  1.434853, val:  81.67%, val_best:  86.25%, tr:  98.47%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.249512/  1.491594, val:  82.92%, val_best:  86.25%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.253653/  1.531891, val:  80.00%, val_best:  86.25%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.212439/  1.401450, val:  84.58%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.191506/  1.360086, val:  85.83%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.190469/  1.433068, val:  85.00%, val_best:  86.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.182410/  1.447386, val:  85.83%, val_best:  86.25%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.178098/  1.436105, val:  85.00%, val_best:  86.25%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.152969/  1.450444, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.155400/  1.475345, val:  87.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.141288/  1.575122, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.134649/  1.583155, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.130998/  1.575995, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.129443/  1.650661, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.119951/  1.645833, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.128443/  1.642946, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.130009/  1.616219, val:  86.67%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.125203/  1.671433, val:  86.67%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.105559/  1.594225, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.098040/  1.701161, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.100097/  1.699679, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.082719/  1.739190, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.079881/  1.690143, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.082690/  1.746564, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.082018/  1.835865, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.074967/  1.799432, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.072538/  1.844192, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.068444/  1.864274, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.068445/  1.875567, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.069637/  1.877753, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.065256/  1.965138, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.065015/  1.902912, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.068019/  1.849861, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.058949/  1.909259, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.055784/  1.934732, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.057801/  1.943013, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.055997/  1.992320, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.048848/  2.057525, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048404/  2.020181, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.052290/  2.046600, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.050794/  2.031936, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.052275/  2.049447, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.050121/  2.059045, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.041302/  2.095466, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.034624/  2.086566, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.042368/  2.110610, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.042367/  2.128665, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.036410/  2.109977, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.041459/  2.154525, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.039424/  2.170097, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.036685/  2.170605, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.039139/  2.199093, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.033926/  2.137697, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.033870/  2.220136, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.037168/  2.157977, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.034921/  2.167686, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.032135/  2.240903, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.038357/  2.210830, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.033585/  2.241731, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.033065/  2.237284, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.035400/  2.328768, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.032421/  2.250282, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.030193/  2.308053, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027045/  2.311895, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.034059/  2.295158, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028591/  2.324856, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026209/  2.293934, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030616/  2.287906, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5bb833e72a41958033245f355353da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▂▁▆█▅▇█████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▄▇▆▇▇█▇█▆▇▇█████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▄▇▆▇▇█▇█▆▇▇█████████████████████████</td></tr><tr><td>val_loss</td><td>▅▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03062</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>2.28791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-sweep-47</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171701-llzgtquc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llwcavbj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_172219-llwcavbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj' target=\"_blank\">true-sweep-49</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.101465/  3.324221, val:  40.00%, val_best:  40.00%, tr:  34.42%, tr_best:  34.42%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.458319/  2.152571, val:  50.83%, val_best:  50.83%, tr:  48.42%, tr_best:  48.42%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.574161/  1.896996, val:  51.25%, val_best:  51.25%, tr:  50.97%, tr_best:  50.97%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.102342/  3.321609, val:  51.67%, val_best:  51.67%, tr:  59.24%, tr_best:  59.24%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.209733/  2.727570, val:  54.17%, val_best:  54.17%, tr:  57.71%, tr_best:  59.24%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.101476/  2.293980, val:  61.25%, val_best:  61.25%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.509898/  2.312093, val:  63.33%, val_best:  63.33%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.604437/  2.643199, val:  50.00%, val_best:  63.33%, tr:  64.66%, tr_best:  65.17%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.770214/  2.604050, val:  54.17%, val_best:  63.33%, tr:  64.76%, tr_best:  65.17%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.678787/  2.791501, val:  59.17%, val_best:  63.33%, tr:  62.21%, tr_best:  65.17%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.855636/  1.672830, val:  67.50%, val_best:  67.50%, tr:  68.34%, tr_best:  68.34%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.635617/  3.229590, val:  47.50%, val_best:  67.50%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.713413/  1.890387, val:  63.33%, val_best:  67.50%, tr:  72.83%, tr_best:  73.54%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.339501/  2.658611, val:  56.67%, val_best:  67.50%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.706740/  2.333525, val:  62.92%, val_best:  67.50%, tr:  73.34%, tr_best:  73.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.144755/  1.836629, val:  71.67%, val_best:  71.67%, tr:  81.10%, tr_best:  81.10%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.068119/  1.646398, val:  77.50%, val_best:  77.50%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.081779/  2.243674, val:  67.92%, val_best:  77.50%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.831110/  2.115815, val:  71.67%, val_best:  77.50%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.864666/  1.890798, val:  69.17%, val_best:  77.50%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.849365/  2.572092, val:  65.42%, val_best:  77.50%, tr:  86.21%, tr_best:  87.44%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.093296/  2.282149, val:  69.58%, val_best:  77.50%, tr:  84.17%, tr_best:  87.44%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.587583/  2.038528, val:  77.50%, val_best:  77.50%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.853653/  2.578312, val:  69.58%, val_best:  77.50%, tr:  91.42%, tr_best:  92.95%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.573387/  1.890000, val:  76.67%, val_best:  77.50%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.430167/  1.785288, val:  80.00%, val_best:  80.00%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.393878/  2.101542, val:  72.92%, val_best:  80.00%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.413775/  2.281378, val:  67.92%, val_best:  80.00%, tr:  95.81%, tr_best:  96.22%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.539942/  1.859173, val:  85.42%, val_best:  85.42%, tr:  93.36%, tr_best:  96.22%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.327119/  1.942501, val:  79.58%, val_best:  85.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.359799/  2.495146, val:  79.58%, val_best:  85.42%, tr:  96.83%, tr_best:  97.75%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.310374/  2.156969, val:  77.08%, val_best:  85.42%, tr:  97.65%, tr_best:  97.75%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.366888/  1.950995, val:  77.08%, val_best:  85.42%, tr:  96.42%, tr_best:  97.75%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.382363/  2.537815, val:  80.83%, val_best:  85.42%, tr:  96.53%, tr_best:  97.75%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.359548/  2.135107, val:  79.17%, val_best:  85.42%, tr:  97.34%, tr_best:  97.75%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.249746/  2.006282, val:  81.25%, val_best:  85.42%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.179295/  1.851519, val:  85.00%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.207066/  2.110089, val:  80.00%, val_best:  85.42%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.175540/  2.018707, val:  80.83%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.192950/  1.871792, val:  85.42%, val_best:  85.42%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.160561/  2.060950, val:  79.17%, val_best:  85.42%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.129935/  2.039451, val:  84.17%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.110293/  2.027510, val:  84.17%, val_best:  85.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.100277/  2.039883, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.085017/  1.962710, val:  85.00%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.079050/  1.984434, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.089790/  2.241035, val:  82.08%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.098312/  2.002106, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.095746/  2.120288, val:  83.33%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.070806/  2.018092, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.057389/  2.114974, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.048694/  2.076350, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.056105/  2.079623, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.053925/  2.047844, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.045665/  2.093442, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.044584/  2.059523, val:  86.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.048268/  2.273936, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.064674/  2.094952, val:  87.08%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.050105/  2.244826, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.039918/  2.256194, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.037566/  2.148859, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.045643/  2.148104, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.051063/  2.262684, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.032731/  2.266573, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.027800/  2.317322, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.028965/  2.313595, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.026965/  2.276298, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.032717/  2.337733, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.032430/  2.321970, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.022627/  2.302335, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.017729/  2.370704, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.019072/  2.426221, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.018664/  2.383372, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.017321/  2.322412, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014650/  2.362238, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.012349/  2.364682, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.015869/  2.441809, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.014854/  2.432870, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.023957/  2.515612, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.033188/  2.397029, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.019132/  2.433089, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.015510/  2.403132, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.013415/  2.464687, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.017772/  2.487386, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.016816/  2.447635, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.016685/  2.472792, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.013439/  2.446386, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.011043/  2.484624, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.009350/  2.458170, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.011191/  2.484500, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.009700/  2.508697, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.007340/  2.479566, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.007156/  2.553486, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.007934/  2.462073, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.011370/  2.512015, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.010633/  2.479834, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009567/  2.483004, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.007450/  2.459566, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.006292/  2.454797, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.006837/  2.487900, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759aea3b24a4dc8a032a5cfab4f6fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▆▆▅▅▇▇▇▇▇█▇█▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▂▄▄▄▅▅▅▆▆▇▇▇▇█████▇███████████████▇██</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▅▅▆▇▆▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▇▅█▅▅▄▃▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▄▅▅▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▂▄▄▄▅▅▅▆▆▇▇▇▇█████▇███████████████▇██</td></tr><tr><td>val_loss</td><td>█▁▅▅▅▁▃▃▁▃▁▂▁▁▂▂▁▂▁▂▂▂▂▂▃▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00684</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.4879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-49</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_172219-llwcavbj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zx4hzndk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_172749-zx4hzndk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk' target=\"_blank\">lemon-sweep-51</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.277348/  1.836137, val:  38.33%, val_best:  38.33%, tr:  11.64%, tr_best:  11.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.530444/  1.553826, val:  56.67%, val_best:  56.67%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.463367/  1.667596, val:  58.75%, val_best:  58.75%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.294098/  1.665490, val:  55.83%, val_best:  58.75%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.244285/  1.353583, val:  60.00%, val_best:  60.00%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.151195/  1.490206, val:  57.50%, val_best:  60.00%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.153702/  1.427458, val:  62.08%, val_best:  62.08%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.209397/  1.390607, val:  64.58%, val_best:  64.58%, tr:  66.70%, tr_best:  68.64%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.088095/  1.211686, val:  70.83%, val_best:  70.83%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.171045/  1.371074, val:  67.50%, val_best:  70.83%, tr:  71.09%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.110695/  1.217043, val:  63.75%, val_best:  70.83%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.935977/  1.408988, val:  65.00%, val_best:  70.83%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.914243/  1.222930, val:  79.17%, val_best:  79.17%, tr:  77.02%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.871244/  1.425935, val:  65.42%, val_best:  79.17%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.922090/  1.598731, val:  69.58%, val_best:  79.17%, tr:  80.18%, tr_best:  82.94%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695977/  1.327556, val:  75.00%, val_best:  79.17%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.701702/  1.301763, val:  81.25%, val_best:  81.25%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.680548/  1.317927, val:  79.58%, val_best:  81.25%, tr:  88.66%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.585737/  1.324111, val:  78.33%, val_best:  81.25%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.554468/  1.228999, val:  82.08%, val_best:  82.08%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.535847/  1.342901, val:  80.42%, val_best:  82.08%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.513860/  1.349173, val:  79.58%, val_best:  82.08%, tr:  94.08%, tr_best:  94.48%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.471891/  1.408997, val:  79.58%, val_best:  82.08%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.497822/  1.387035, val:  82.50%, val_best:  82.50%, tr:  94.59%, tr_best:  95.91%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.468783/  1.394951, val:  80.83%, val_best:  82.50%, tr:  95.61%, tr_best:  95.91%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.403341/  1.387302, val:  81.25%, val_best:  82.50%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.413393/  1.404403, val:  83.33%, val_best:  83.33%, tr:  96.12%, tr_best:  97.34%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.361680/  1.597591, val:  75.42%, val_best:  83.33%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.380964/  1.429426, val:  87.50%, val_best:  87.50%, tr:  97.55%, tr_best:  98.16%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.336191/  1.652869, val:  78.33%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.344023/  1.515484, val:  84.17%, val_best:  87.50%, tr:  98.06%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.307300/  1.519455, val:  86.25%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.393205/  1.703383, val:  77.50%, val_best:  87.50%, tr:  96.02%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.306611/  1.724924, val:  78.33%, val_best:  87.50%, tr:  97.75%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.312883/  1.701099, val:  80.83%, val_best:  87.50%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.270493/  1.616362, val:  85.42%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.258408/  1.578617, val:  88.33%, val_best:  88.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.243536/  1.666261, val:  83.33%, val_best:  88.33%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.257409/  1.555349, val:  88.33%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.241105/  1.588049, val:  87.08%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.214323/  1.602924, val:  87.50%, val_best:  88.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.220465/  1.699192, val:  87.92%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.212917/  1.650550, val:  86.25%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.189541/  1.713008, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.209767/  1.759459, val:  85.42%, val_best:  88.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.166522/  1.698560, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.174053/  1.721082, val:  86.25%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.167443/  1.746748, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.186920/  1.805695, val:  87.08%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.156417/  1.805167, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.142309/  1.833036, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.139528/  1.836177, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.162317/  1.891918, val:  86.67%, val_best:  89.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.128954/  1.912712, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.116131/  1.866073, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.115214/  1.888060, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.117821/  1.913230, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.135084/  1.911174, val:  88.75%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.104586/  1.948376, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.104840/  2.015811, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.092165/  1.970539, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.107049/  2.049332, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.106323/  2.027441, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.089233/  2.024719, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089171/  2.074878, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.079638/  2.135362, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.079328/  2.145368, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.094060/  2.111733, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.079194/  2.134221, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.070059/  2.148082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.072394/  2.139723, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.075009/  2.162890, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.078230/  2.215109, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.071419/  2.255758, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.067724/  2.226867, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.068169/  2.239712, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058137/  2.247678, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.054274/  2.261737, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.056432/  2.325218, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.056006/  2.288713, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.055132/  2.354102, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.057748/  2.332878, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.058765/  2.442618, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.059759/  2.437325, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.055195/  2.384619, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.046222/  2.474452, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.052044/  2.423485, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.052900/  2.462993, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.050090/  2.479652, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.046342/  2.468138, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.043438/  2.495166, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.040929/  2.491071, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.037502/  2.527401, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.035432/  2.525148, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.034233/  2.599615, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.036343/  2.571501, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.040404/  2.559836, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.039971/  2.591554, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.034482/  2.584780, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.038479/  2.592654, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605fc7ffd84f4b3e8ad6ce2fab08f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▂▁▆▇▇██▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>tr_acc</td><td>▁▅▅▅▆▆▆▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>val_loss</td><td>▄▃▂▂▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03848</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.59265</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-51</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_172749-zx4hzndk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ukzcg63z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173319-ukzcg63z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z' target=\"_blank\">cosmic-sweep-53</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.319476/  2.168869, val:  15.00%, val_best:  15.00%, tr:   9.40%, tr_best:   9.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.546559/  1.394541, val:  57.50%, val_best:  57.50%, tr:  45.56%, tr_best:  45.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.361095/  1.347619, val:  57.92%, val_best:  57.92%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.168066/  1.364703, val:  56.25%, val_best:  57.92%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.111191/  1.360562, val:  64.58%, val_best:  64.58%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.020129/  1.230537, val:  64.17%, val_best:  64.58%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.009647/  1.213413, val:  65.00%, val_best:  65.00%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.003902/  1.176388, val:  72.08%, val_best:  72.08%, tr:  69.36%, tr_best:  72.11%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.008450/  1.274724, val:  64.58%, val_best:  72.08%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.943574/  1.347100, val:  61.67%, val_best:  72.08%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.939643/  1.174111, val:  72.92%, val_best:  72.92%, tr:  74.46%, tr_best:  76.40%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.807263/  1.173712, val:  76.67%, val_best:  76.67%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.713175/  1.111352, val:  82.08%, val_best:  82.08%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.686933/  1.135550, val:  80.00%, val_best:  82.08%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.701552/  1.443170, val:  71.67%, val_best:  82.08%, tr:  86.93%, tr_best:  87.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.576296/  1.147456, val:  81.67%, val_best:  82.08%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.554653/  1.123438, val:  84.17%, val_best:  84.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.500317/  1.145159, val:  84.58%, val_best:  84.58%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.473771/  1.235066, val:  82.50%, val_best:  84.58%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.453774/  1.129544, val:  83.75%, val_best:  84.58%, tr:  95.20%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.417704/  1.190487, val:  85.83%, val_best:  85.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.391449/  1.228569, val:  82.92%, val_best:  85.83%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.363348/  1.228766, val:  83.75%, val_best:  85.83%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.364034/  1.211714, val:  85.42%, val_best:  85.83%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.336103/  1.263916, val:  83.33%, val_best:  85.83%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.324088/  1.262189, val:  87.50%, val_best:  87.50%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.315723/  1.267444, val:  87.92%, val_best:  87.92%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.290609/  1.388012, val:  84.17%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.282847/  1.273947, val:  87.08%, val_best:  87.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.253795/  1.496196, val:  77.92%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.266297/  1.330569, val:  87.92%, val_best:  87.92%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.235620/  1.388238, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.263707/  1.422997, val:  84.58%, val_best:  87.92%, tr:  98.37%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.245322/  1.483279, val:  85.00%, val_best:  87.92%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.231372/  1.586224, val:  78.75%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.214156/  1.484141, val:  86.67%, val_best:  87.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184852/  1.435867, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.173412/  1.483765, val:  85.42%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.180633/  1.489602, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.176510/  1.547179, val:  88.33%, val_best:  88.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.156040/  1.472144, val:  89.58%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.157954/  1.526698, val:  85.00%, val_best:  89.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.154075/  1.538508, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.132665/  1.573861, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.129036/  1.680215, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.125832/  1.658053, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.125388/  1.662654, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.129947/  1.647644, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.119372/  1.675312, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.116225/  1.678526, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102924/  1.735842, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.096691/  1.729942, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.097066/  1.781765, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.082823/  1.764519, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.087560/  1.788373, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.080971/  1.820014, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.075115/  1.886615, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.085490/  1.877929, val:  89.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.071388/  1.890186, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.068798/  1.936669, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076526/  1.908287, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.079891/  1.899697, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.071702/  1.965082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.068042/  1.891309, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.062879/  1.935400, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.062603/  1.935610, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.058572/  2.021835, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.062719/  2.068077, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.056839/  2.032468, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.059113/  2.075200, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.050592/  2.077132, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.053072/  2.075650, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.045561/  2.104312, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.048279/  2.073460, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.050015/  2.103623, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.041874/  2.110681, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.042547/  2.152984, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.038319/  2.201250, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.040931/  2.212138, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.043858/  2.180299, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.045234/  2.221424, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.038475/  2.217281, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.037684/  2.278193, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.043997/  2.310470, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038367/  2.251507, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.041988/  2.268560, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.038767/  2.270885, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.035475/  2.313851, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.039516/  2.279841, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.035011/  2.255051, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.027546/  2.281515, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.029989/  2.306377, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.024913/  2.329683, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.027511/  2.324182, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.026752/  2.322279, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022845/  2.368027, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.023173/  2.374410, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.024822/  2.412791, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.023658/  2.432441, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.029568/  2.377930, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5213ca7a93b1419eb87b16aab49bcb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▂▇▇▅████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▅▇▆█▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▅▇▆█▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02957</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.37793</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-sweep-53</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_173319-ukzcg63z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y0kr0fa9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173838-y0kr0fa9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9' target=\"_blank\">breezy-sweep-55</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.277348/  1.836137, val:  38.33%, val_best:  38.33%, tr:  11.64%, tr_best:  11.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.530444/  1.553826, val:  56.67%, val_best:  56.67%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.463367/  1.667596, val:  58.75%, val_best:  58.75%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.294098/  1.665490, val:  55.83%, val_best:  58.75%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.244285/  1.353583, val:  60.00%, val_best:  60.00%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.151195/  1.490206, val:  57.50%, val_best:  60.00%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.153702/  1.427458, val:  62.08%, val_best:  62.08%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.209397/  1.390607, val:  64.58%, val_best:  64.58%, tr:  66.70%, tr_best:  68.64%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.088095/  1.211686, val:  70.83%, val_best:  70.83%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.171045/  1.371074, val:  67.50%, val_best:  70.83%, tr:  71.09%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.110695/  1.217043, val:  63.75%, val_best:  70.83%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.935977/  1.408988, val:  65.00%, val_best:  70.83%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.914243/  1.222930, val:  79.17%, val_best:  79.17%, tr:  77.02%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.871244/  1.425935, val:  65.42%, val_best:  79.17%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.922090/  1.598731, val:  69.58%, val_best:  79.17%, tr:  80.18%, tr_best:  82.94%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695977/  1.327556, val:  75.00%, val_best:  79.17%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.701702/  1.301763, val:  81.25%, val_best:  81.25%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.680548/  1.317927, val:  79.58%, val_best:  81.25%, tr:  88.66%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.585737/  1.324111, val:  78.33%, val_best:  81.25%, tr:  92.13%, tr_best:  92.13%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.125,0.25,0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.01,0.1]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1,2,3,4,5,6,7]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True, False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [True,False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [True]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay, #wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = '6pj3lh8j'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
